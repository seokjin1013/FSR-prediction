{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task6\n",
    "\n",
    "Index_X = FSR_for_coord\n",
    "\n",
    "Index_y = x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-08-07_15-54-13/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-08-07_15-54-13\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "0.6762"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_coord',\n",
    "        'index_y': ['x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-07 15:54:13,592] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 15:54:16,571\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-08-07 15:54:19,054\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-07 16:32:01</td></tr>\n",
       "<tr><td>Running for: </td><td>00:37:42.13        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.0/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.6816992035046199 | Iter 32.000: -0.6827039755769686 | Iter 16.000: -0.6829547986475025 | Iter 8.000: -0.6833289486773015 | Iter 4.000: -0.6841073452595434 | Iter 2.000: -0.6858221042874402 | Iter 1.000: -0.6939546607596015<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X      </th><th>index_y             </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_ac1e6a66</td><td>TERMINATED</td><td>172.26.215.93:1270 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7ac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        9.92124e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      161.269   </td><td style=\"text-align: right;\">0.696995</td><td style=\"text-align: right;\">0.382   </td><td style=\"text-align: right;\">0.0904071</td></tr>\n",
       "<tr><td>FSR_Trainable_c926e410</td><td>TERMINATED</td><td>172.26.215.93:1343 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__dd40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0304479  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">       34.0262  </td><td style=\"text-align: right;\">0.993285</td><td style=\"text-align: right;\">0.752498</td><td style=\"text-align: right;\">0.168617 </td></tr>\n",
       "<tr><td>FSR_Trainable_4d593f19</td><td>TERMINATED</td><td>172.26.215.93:1514 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.7301e-05 </td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       11.7039  </td><td style=\"text-align: right;\">0.724866</td><td style=\"text-align: right;\">0.359287</td><td style=\"text-align: right;\">0.0904162</td></tr>\n",
       "<tr><td>FSR_Trainable_ccd8e300</td><td>TERMINATED</td><td>172.26.215.93:1679 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1a80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000816782</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       15.5484  </td><td style=\"text-align: right;\">0.746066</td><td style=\"text-align: right;\">0.422133</td><td style=\"text-align: right;\">0.103669 </td></tr>\n",
       "<tr><td>FSR_Trainable_1d8d705d</td><td>TERMINATED</td><td>172.26.215.93:2038 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b5c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.57856e-05</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">      479.719   </td><td style=\"text-align: right;\">0.68573 </td><td style=\"text-align: right;\">0.32003 </td><td style=\"text-align: right;\">0.092289 </td></tr>\n",
       "<tr><td>FSR_Trainable_a1ace408</td><td>TERMINATED</td><td>172.26.215.93:2228 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0512706  </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       51.9265  </td><td style=\"text-align: right;\">0.720483</td><td style=\"text-align: right;\">0.418572</td><td style=\"text-align: right;\">0.10345  </td></tr>\n",
       "<tr><td>FSR_Trainable_8eb9515c</td><td>TERMINATED</td><td>172.26.215.93:2449 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a8c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000332632</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.79156 </td><td style=\"text-align: right;\">0.748827</td><td style=\"text-align: right;\">0.460227</td><td style=\"text-align: right;\">0.117795 </td></tr>\n",
       "<tr><td>FSR_Trainable_4b64ac9b</td><td>TERMINATED</td><td>172.26.215.93:2716 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7fc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.50031e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">      120.08    </td><td style=\"text-align: right;\">0.706221</td><td style=\"text-align: right;\">0.359838</td><td style=\"text-align: right;\">0.0934638</td></tr>\n",
       "<tr><td>FSR_Trainable_69dc294f</td><td>TERMINATED</td><td>172.26.215.93:2964 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00362757 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        6.73141 </td><td style=\"text-align: right;\">0.908441</td><td style=\"text-align: right;\">0.627795</td><td style=\"text-align: right;\">0.120668 </td></tr>\n",
       "<tr><td>FSR_Trainable_7c041d7c</td><td>TERMINATED</td><td>172.26.215.93:3194 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__15c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.011583   </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        5.78085 </td><td style=\"text-align: right;\">0.790998</td><td style=\"text-align: right;\">0.461788</td><td style=\"text-align: right;\">0.107269 </td></tr>\n",
       "<tr><td>FSR_Trainable_640db335</td><td>TERMINATED</td><td>172.26.215.93:3416 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d8c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0202294  </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      543.245   </td><td style=\"text-align: right;\">0.692678</td><td style=\"text-align: right;\">0.382626</td><td style=\"text-align: right;\">0.100957 </td></tr>\n",
       "<tr><td>FSR_Trainable_2e373880</td><td>TERMINATED</td><td>172.26.215.93:3646 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1b00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00108439 </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.47794 </td><td style=\"text-align: right;\">0.770788</td><td style=\"text-align: right;\">0.449656</td><td style=\"text-align: right;\">0.108879 </td></tr>\n",
       "<tr><td>FSR_Trainable_7971d6f7</td><td>TERMINATED</td><td>172.26.215.93:3839 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b3c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00611387 </td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.71625 </td><td style=\"text-align: right;\">0.732806</td><td style=\"text-align: right;\">0.3979  </td><td style=\"text-align: right;\">0.0985882</td></tr>\n",
       "<tr><td>FSR_Trainable_9dc25b0b</td><td>TERMINATED</td><td>172.26.215.93:4086 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0357061  </td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">       10.4021  </td><td style=\"text-align: right;\">1.23892 </td><td style=\"text-align: right;\">0.890692</td><td style=\"text-align: right;\">0.248278 </td></tr>\n",
       "<tr><td>FSR_Trainable_85bae870</td><td>TERMINATED</td><td>172.26.215.93:4277 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8fc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.36843e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      114.812   </td><td style=\"text-align: right;\">0.68192 </td><td style=\"text-align: right;\">0.320629</td><td style=\"text-align: right;\">0.094771 </td></tr>\n",
       "<tr><td>FSR_Trainable_35b72e99</td><td>TERMINATED</td><td>172.26.215.93:4506 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ac00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.76754e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      112.946   </td><td style=\"text-align: right;\">0.683652</td><td style=\"text-align: right;\">0.321643</td><td style=\"text-align: right;\">0.0961289</td></tr>\n",
       "<tr><td>FSR_Trainable_040aabb1</td><td>TERMINATED</td><td>172.26.215.93:4830 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ba80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.4829e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        5.26206 </td><td style=\"text-align: right;\">0.69464 </td><td style=\"text-align: right;\">0.353857</td><td style=\"text-align: right;\">0.0958047</td></tr>\n",
       "<tr><td>FSR_Trainable_6955b5c7</td><td>TERMINATED</td><td>172.26.215.93:5025 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ac00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.94936e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      146.448   </td><td style=\"text-align: right;\">0.687698</td><td style=\"text-align: right;\">0.371893</td><td style=\"text-align: right;\">0.100687 </td></tr>\n",
       "<tr><td>FSR_Trainable_5f88c1e0</td><td>TERMINATED</td><td>172.26.215.93:5234 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6980</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.66055e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.58748 </td><td style=\"text-align: right;\">0.698099</td><td style=\"text-align: right;\">0.366219</td><td style=\"text-align: right;\">0.0987782</td></tr>\n",
       "<tr><td>FSR_Trainable_1bb266e5</td><td>TERMINATED</td><td>172.26.215.93:5482 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.04528e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.99426 </td><td style=\"text-align: right;\">0.698779</td><td style=\"text-align: right;\">0.345517</td><td style=\"text-align: right;\">0.0949297</td></tr>\n",
       "<tr><td>FSR_Trainable_0c822194</td><td>TERMINATED</td><td>172.26.215.93:5729 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.9998e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        8.03885 </td><td style=\"text-align: right;\">0.692575</td><td style=\"text-align: right;\">0.337937</td><td style=\"text-align: right;\">0.0956393</td></tr>\n",
       "<tr><td>FSR_Trainable_cfdd4deb</td><td>TERMINATED</td><td>172.26.215.93:5960 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e840</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.62538e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        7.22135 </td><td style=\"text-align: right;\">0.693037</td><td style=\"text-align: right;\">0.349186</td><td style=\"text-align: right;\">0.0956282</td></tr>\n",
       "<tr><td>FSR_Trainable_ebbdefae</td><td>TERMINATED</td><td>172.26.215.93:6185 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fc00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.00781e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      109.983   </td><td style=\"text-align: right;\">0.679958</td><td style=\"text-align: right;\">0.334063</td><td style=\"text-align: right;\">0.0991985</td></tr>\n",
       "<tr><td>FSR_Trainable_a9e85ed5</td><td>TERMINATED</td><td>172.26.215.93:6444 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6080</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000116752</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       63.4345  </td><td style=\"text-align: right;\">0.687008</td><td style=\"text-align: right;\">0.382621</td><td style=\"text-align: right;\">0.102008 </td></tr>\n",
       "<tr><td>FSR_Trainable_e8e097f7</td><td>TERMINATED</td><td>172.26.215.93:6622 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__da00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.59209e-05</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       15.1287  </td><td style=\"text-align: right;\">0.686143</td><td style=\"text-align: right;\">0.327867</td><td style=\"text-align: right;\">0.0927403</td></tr>\n",
       "<tr><td>FSR_Trainable_c989bf2b</td><td>TERMINATED</td><td>172.26.215.93:6905 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.03914e-05</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        8.52894 </td><td style=\"text-align: right;\">0.690922</td><td style=\"text-align: right;\">0.330853</td><td style=\"text-align: right;\">0.0937291</td></tr>\n",
       "<tr><td>FSR_Trainable_7a94c05f</td><td>TERMINATED</td><td>172.26.215.93:7223 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__18c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000165147</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.98446 </td><td style=\"text-align: right;\">0.690347</td><td style=\"text-align: right;\">0.341616</td><td style=\"text-align: right;\">0.0946635</td></tr>\n",
       "<tr><td>FSR_Trainable_c058ddce</td><td>TERMINATED</td><td>172.26.215.93:7413 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00015537 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       27.2433  </td><td style=\"text-align: right;\">0.688714</td><td style=\"text-align: right;\">0.373233</td><td style=\"text-align: right;\">0.0981173</td></tr>\n",
       "<tr><td>FSR_Trainable_3839720d</td><td>TERMINATED</td><td>172.26.215.93:7652 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__23c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.08022e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      118.706   </td><td style=\"text-align: right;\">0.67746 </td><td style=\"text-align: right;\">0.348204</td><td style=\"text-align: right;\">0.098733 </td></tr>\n",
       "<tr><td>FSR_Trainable_4ffc8f81</td><td>TERMINATED</td><td>172.26.215.93:7862 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0100</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.04287e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      116.65    </td><td style=\"text-align: right;\">0.681179</td><td style=\"text-align: right;\">0.332525</td><td style=\"text-align: right;\">0.0954524</td></tr>\n",
       "<tr><td>FSR_Trainable_c5867aff</td><td>TERMINATED</td><td>172.26.215.93:8124 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.50966e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.61198 </td><td style=\"text-align: right;\">0.7003  </td><td style=\"text-align: right;\">0.367791</td><td style=\"text-align: right;\">0.11212  </td></tr>\n",
       "<tr><td>FSR_Trainable_dcbc59db</td><td>TERMINATED</td><td>172.26.215.93:8320 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.67584e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.44572 </td><td style=\"text-align: right;\">0.696527</td><td style=\"text-align: right;\">0.356424</td><td style=\"text-align: right;\">0.101547 </td></tr>\n",
       "<tr><td>FSR_Trainable_6bb59630</td><td>TERMINATED</td><td>172.26.215.93:8549 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7c00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.1207e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.43871 </td><td style=\"text-align: right;\">0.69603 </td><td style=\"text-align: right;\">0.3628  </td><td style=\"text-align: right;\">0.101611 </td></tr>\n",
       "<tr><td>FSR_Trainable_e53d7657</td><td>TERMINATED</td><td>172.26.215.93:8777 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        1.10986e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.05507 </td><td style=\"text-align: right;\">0.696169</td><td style=\"text-align: right;\">0.342078</td><td style=\"text-align: right;\">0.096827 </td></tr>\n",
       "<tr><td>FSR_Trainable_67b698a5</td><td>TERMINATED</td><td>172.26.215.93:9006 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        4.17609e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      101.318   </td><td style=\"text-align: right;\">0.689259</td><td style=\"text-align: right;\">0.373099</td><td style=\"text-align: right;\">0.0996082</td></tr>\n",
       "<tr><td>FSR_Trainable_57bbd80f</td><td>TERMINATED</td><td>172.26.215.93:9241 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        3.16691e-05</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        9.90927 </td><td style=\"text-align: right;\">0.692487</td><td style=\"text-align: right;\">0.329414</td><td style=\"text-align: right;\">0.09343  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb97f09f</td><td>TERMINATED</td><td>172.26.215.93:9504 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__47c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.24904e-05</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">      275.698   </td><td style=\"text-align: right;\">0.688128</td><td style=\"text-align: right;\">0.354979</td><td style=\"text-align: right;\">0.0981695</td></tr>\n",
       "<tr><td>FSR_Trainable_754545de</td><td>TERMINATED</td><td>172.26.215.93:9684 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.17116e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      144.888   </td><td style=\"text-align: right;\">0.678567</td><td style=\"text-align: right;\">0.341503</td><td style=\"text-align: right;\">0.0954993</td></tr>\n",
       "<tr><td>FSR_Trainable_bd38f24a</td><td>TERMINATED</td><td>172.26.215.93:9902 </td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.0157e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      148.009   </td><td style=\"text-align: right;\">0.67845 </td><td style=\"text-align: right;\">0.34712 </td><td style=\"text-align: right;\">0.0989498</td></tr>\n",
       "<tr><td>FSR_Trainable_26f78e47</td><td>TERMINATED</td><td>172.26.215.93:10208</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5b00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.99946e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.82283 </td><td style=\"text-align: right;\">0.706823</td><td style=\"text-align: right;\">0.375665</td><td style=\"text-align: right;\">0.104218 </td></tr>\n",
       "<tr><td>FSR_Trainable_93194e63</td><td>TERMINATED</td><td>172.26.215.93:10403</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.95279e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.76379 </td><td style=\"text-align: right;\">0.710222</td><td style=\"text-align: right;\">0.380033</td><td style=\"text-align: right;\">0.0989164</td></tr>\n",
       "<tr><td>FSR_Trainable_95c6210f</td><td>TERMINATED</td><td>172.26.215.93:10630</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.58714e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.07406 </td><td style=\"text-align: right;\">0.729421</td><td style=\"text-align: right;\">0.396483</td><td style=\"text-align: right;\">0.090341 </td></tr>\n",
       "<tr><td>FSR_Trainable_b1964871</td><td>TERMINATED</td><td>172.26.215.93:10858</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.57869e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.06293 </td><td style=\"text-align: right;\">0.734851</td><td style=\"text-align: right;\">0.424845</td><td style=\"text-align: right;\">0.112512 </td></tr>\n",
       "<tr><td>FSR_Trainable_096db0c5</td><td>TERMINATED</td><td>172.26.215.93:11088</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4e40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.02813e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.21653 </td><td style=\"text-align: right;\">0.71041 </td><td style=\"text-align: right;\">0.351559</td><td style=\"text-align: right;\">0.0923458</td></tr>\n",
       "<tr><td>FSR_Trainable_0001f0fa</td><td>TERMINATED</td><td>172.26.215.93:11318</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.76425e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.3535  </td><td style=\"text-align: right;\">0.700016</td><td style=\"text-align: right;\">0.347305</td><td style=\"text-align: right;\">0.0954485</td></tr>\n",
       "<tr><td>FSR_Trainable_cc096752</td><td>TERMINATED</td><td>172.26.215.93:11543</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.72799e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       17.8378  </td><td style=\"text-align: right;\">0.683353</td><td style=\"text-align: right;\">0.346661</td><td style=\"text-align: right;\">0.104535 </td></tr>\n",
       "<tr><td>FSR_Trainable_4998c260</td><td>TERMINATED</td><td>172.26.215.93:11778</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.26027e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.86396 </td><td style=\"text-align: right;\">0.690816</td><td style=\"text-align: right;\">0.350151</td><td style=\"text-align: right;\">0.0948329</td></tr>\n",
       "<tr><td>FSR_Trainable_e8b9d18d</td><td>TERMINATED</td><td>172.26.215.93:11979</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.04682e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.59094 </td><td style=\"text-align: right;\">2.74879 </td><td style=\"text-align: right;\">2.43472 </td><td style=\"text-align: right;\">0.405736 </td></tr>\n",
       "<tr><td>FSR_Trainable_477c79e4</td><td>TERMINATED</td><td>172.26.215.93:12204</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.78017e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.5944  </td><td style=\"text-align: right;\">2.27815 </td><td style=\"text-align: right;\">1.92801 </td><td style=\"text-align: right;\">0.286605 </td></tr>\n",
       "<tr><td>FSR_Trainable_d55d26c1</td><td>TERMINATED</td><td>172.26.215.93:12439</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f140</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        3.98157e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        4.43387 </td><td style=\"text-align: right;\">2.74811 </td><td style=\"text-align: right;\">2.43655 </td><td style=\"text-align: right;\">0.362018 </td></tr>\n",
       "<tr><td>FSR_Trainable_95d591f0</td><td>TERMINATED</td><td>172.26.215.93:12656</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d4c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000261468</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       36.6651  </td><td style=\"text-align: right;\">0.685527</td><td style=\"text-align: right;\">0.345516</td><td style=\"text-align: right;\">0.0991698</td></tr>\n",
       "<tr><td>FSR_Trainable_650765f6</td><td>TERMINATED</td><td>172.26.215.93:12877</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        8.81457e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">      159.483   </td><td style=\"text-align: right;\">0.683295</td><td style=\"text-align: right;\">0.319514</td><td style=\"text-align: right;\">0.096026 </td></tr>\n",
       "<tr><td>FSR_Trainable_2e7b25e1</td><td>TERMINATED</td><td>172.26.215.93:13091</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f600</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        9.25951e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       80.8474  </td><td style=\"text-align: right;\">0.685002</td><td style=\"text-align: right;\">0.312714</td><td style=\"text-align: right;\">0.0918074</td></tr>\n",
       "<tr><td>FSR_Trainable_28d87d6f</td><td>TERMINATED</td><td>172.26.215.93:13365</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000315074</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       84.2606  </td><td style=\"text-align: right;\">0.706498</td><td style=\"text-align: right;\">0.40517 </td><td style=\"text-align: right;\">0.106594 </td></tr>\n",
       "<tr><td>FSR_Trainable_3b5879d2</td><td>TERMINATED</td><td>172.26.215.93:13553</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6b80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.11438e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.05298 </td><td style=\"text-align: right;\">0.696918</td><td style=\"text-align: right;\">0.376475</td><td style=\"text-align: right;\">0.106942 </td></tr>\n",
       "<tr><td>FSR_Trainable_2d25cf1a</td><td>TERMINATED</td><td>172.26.215.93:13779</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.03567e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.12668 </td><td style=\"text-align: right;\">0.689265</td><td style=\"text-align: right;\">0.345748</td><td style=\"text-align: right;\">0.0961393</td></tr>\n",
       "<tr><td>FSR_Trainable_c704d36f</td><td>TERMINATED</td><td>172.26.215.93:14006</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__37c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.68151e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.67832 </td><td style=\"text-align: right;\">0.696494</td><td style=\"text-align: right;\">0.341111</td><td style=\"text-align: right;\">0.0940817</td></tr>\n",
       "<tr><td>FSR_Trainable_f99e21db</td><td>TERMINATED</td><td>172.26.215.93:14228</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.06295e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       50.6498  </td><td style=\"text-align: right;\">0.686842</td><td style=\"text-align: right;\">0.341462</td><td style=\"text-align: right;\">0.0975658</td></tr>\n",
       "<tr><td>FSR_Trainable_10471426</td><td>TERMINATED</td><td>172.26.215.93:14440</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__da80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.4326e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      149.313   </td><td style=\"text-align: right;\">0.676204</td><td style=\"text-align: right;\">0.327839</td><td style=\"text-align: right;\">0.097702 </td></tr>\n",
       "<tr><td>FSR_Trainable_518953d3</td><td>TERMINATED</td><td>172.26.215.93:14711</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.49282e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.17491 </td><td style=\"text-align: right;\">0.706956</td><td style=\"text-align: right;\">0.357299</td><td style=\"text-align: right;\">0.0965995</td></tr>\n",
       "<tr><td>FSR_Trainable_f2951ccc</td><td>TERMINATED</td><td>172.26.215.93:14904</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c600</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.50424e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.86371 </td><td style=\"text-align: right;\">0.697137</td><td style=\"text-align: right;\">0.355148</td><td style=\"text-align: right;\">0.101626 </td></tr>\n",
       "<tr><td>FSR_Trainable_dd647cf3</td><td>TERMINATED</td><td>172.26.215.93:15131</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.42339e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.14424 </td><td style=\"text-align: right;\">0.689379</td><td style=\"text-align: right;\">0.338004</td><td style=\"text-align: right;\">0.094335 </td></tr>\n",
       "<tr><td>FSR_Trainable_bb3b8107</td><td>TERMINATED</td><td>172.26.215.93:15356</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a0c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.10585e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.58725 </td><td style=\"text-align: right;\">0.71561 </td><td style=\"text-align: right;\">0.363001</td><td style=\"text-align: right;\">0.0917417</td></tr>\n",
       "<tr><td>FSR_Trainable_a4520da0</td><td>TERMINATED</td><td>172.26.215.93:15598</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.84567e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      610.579   </td><td style=\"text-align: right;\">0.678976</td><td style=\"text-align: right;\">0.309136</td><td style=\"text-align: right;\">0.0910819</td></tr>\n",
       "<tr><td>FSR_Trainable_a6b2dc23</td><td>TERMINATED</td><td>172.26.215.93:15806</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.83283e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">       47.9901  </td><td style=\"text-align: right;\">0.684573</td><td style=\"text-align: right;\">0.313143</td><td style=\"text-align: right;\">0.0920703</td></tr>\n",
       "<tr><td>FSR_Trainable_3a0adf05</td><td>TERMINATED</td><td>172.26.215.93:16015</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        5.44932e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">       24.5668  </td><td style=\"text-align: right;\">0.685194</td><td style=\"text-align: right;\">0.312783</td><td style=\"text-align: right;\">0.091403 </td></tr>\n",
       "<tr><td>FSR_Trainable_e2dc6632</td><td>TERMINATED</td><td>172.26.215.93:16295</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        5.07942e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      604.019   </td><td style=\"text-align: right;\">0.681025</td><td style=\"text-align: right;\">0.314219</td><td style=\"text-align: right;\">0.0946371</td></tr>\n",
       "<tr><td>FSR_Trainable_c14a090a</td><td>TERMINATED</td><td>172.26.215.93:16473</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__89c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.96591e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">      142.453   </td><td style=\"text-align: right;\">0.686149</td><td style=\"text-align: right;\">0.359613</td><td style=\"text-align: right;\">0.0989166</td></tr>\n",
       "<tr><td>FSR_Trainable_a6d5cddc</td><td>TERMINATED</td><td>172.26.215.93:16755</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.05022e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">      242.996   </td><td style=\"text-align: right;\">0.685743</td><td style=\"text-align: right;\">0.355757</td><td style=\"text-align: right;\">0.0991256</td></tr>\n",
       "<tr><td>FSR_Trainable_6c5ff9e9</td><td>TERMINATED</td><td>172.26.215.93:17015</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.76611e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.02516 </td><td style=\"text-align: right;\">0.689415</td><td style=\"text-align: right;\">0.348869</td><td style=\"text-align: right;\">0.105159 </td></tr>\n",
       "<tr><td>FSR_Trainable_5cd365a4</td><td>TERMINATED</td><td>172.26.215.93:17220</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.0045e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.7123  </td><td style=\"text-align: right;\">0.699606</td><td style=\"text-align: right;\">0.351053</td><td style=\"text-align: right;\">0.0957396</td></tr>\n",
       "<tr><td>FSR_Trainable_f259722f</td><td>TERMINATED</td><td>172.26.215.93:17432</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__53c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.01566e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.6813  </td><td style=\"text-align: right;\">3.2069  </td><td style=\"text-align: right;\">2.85073 </td><td style=\"text-align: right;\">0.440703 </td></tr>\n",
       "<tr><td>FSR_Trainable_f137f0d6</td><td>TERMINATED</td><td>172.26.215.93:17655</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3040</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.16104e-05</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.08116 </td><td style=\"text-align: right;\">0.687457</td><td style=\"text-align: right;\">0.364033</td><td style=\"text-align: right;\">0.104089 </td></tr>\n",
       "<tr><td>FSR_Trainable_ee27e018</td><td>TERMINATED</td><td>172.26.215.93:17892</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__17c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.06833e-05</td><td>sklearn.preproc_c9f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.56937 </td><td style=\"text-align: right;\">0.698303</td><td style=\"text-align: right;\">0.369741</td><td style=\"text-align: right;\">0.0991159</td></tr>\n",
       "<tr><td>FSR_Trainable_e785abf7</td><td>TERMINATED</td><td>172.26.215.93:18105</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bdc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.35467e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       95.2566  </td><td style=\"text-align: right;\">0.676331</td><td style=\"text-align: right;\">0.318072</td><td style=\"text-align: right;\">0.0916696</td></tr>\n",
       "<tr><td>FSR_Trainable_e81dbf53</td><td>TERMINATED</td><td>172.26.215.93:18378</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4300</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        3.05825e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.27192 </td><td style=\"text-align: right;\">0.690913</td><td style=\"text-align: right;\">0.347628</td><td style=\"text-align: right;\">0.0940964</td></tr>\n",
       "<tr><td>FSR_Trainable_20422a55</td><td>TERMINATED</td><td>172.26.215.93:18585</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.40658e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.46349 </td><td style=\"text-align: right;\">0.702879</td><td style=\"text-align: right;\">0.368807</td><td style=\"text-align: right;\">0.0975775</td></tr>\n",
       "<tr><td>FSR_Trainable_f433c207</td><td>TERMINATED</td><td>172.26.215.93:18806</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8300</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.86005e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.14861 </td><td style=\"text-align: right;\">0.6937  </td><td style=\"text-align: right;\">0.342802</td><td style=\"text-align: right;\">0.0943667</td></tr>\n",
       "<tr><td>FSR_Trainable_c1f87b41</td><td>TERMINATED</td><td>172.26.215.93:19031</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ae80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.16161e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       22.7172  </td><td style=\"text-align: right;\">0.683457</td><td style=\"text-align: right;\">0.325004</td><td style=\"text-align: right;\">0.0930291</td></tr>\n",
       "<tr><td>FSR_Trainable_38fe686a</td><td>TERMINATED</td><td>172.26.215.93:19293</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ea00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.78458e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       90.501   </td><td style=\"text-align: right;\">0.681971</td><td style=\"text-align: right;\">0.359487</td><td style=\"text-align: right;\">0.110314 </td></tr>\n",
       "<tr><td>FSR_Trainable_14abb1b6</td><td>TERMINATED</td><td>172.26.215.93:19496</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.31667e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.20641 </td><td style=\"text-align: right;\">0.69607 </td><td style=\"text-align: right;\">0.359101</td><td style=\"text-align: right;\">0.106785 </td></tr>\n",
       "<tr><td>FSR_Trainable_bad073d7</td><td>TERMINATED</td><td>172.26.215.93:19716</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b3c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.35253e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.4932  </td><td style=\"text-align: right;\">0.700042</td><td style=\"text-align: right;\">0.363523</td><td style=\"text-align: right;\">0.103084 </td></tr>\n",
       "<tr><td>FSR_Trainable_ea5fbd0a</td><td>TERMINATED</td><td>172.26.215.93:19942</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b1c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.7643e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.4205  </td><td style=\"text-align: right;\">0.7061  </td><td style=\"text-align: right;\">0.379991</td><td style=\"text-align: right;\">0.0943633</td></tr>\n",
       "<tr><td>FSR_Trainable_dab74113</td><td>TERMINATED</td><td>172.26.215.93:20167</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.74183e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.827611</td><td style=\"text-align: right;\">0.716898</td><td style=\"text-align: right;\">0.392397</td><td style=\"text-align: right;\">0.102979 </td></tr>\n",
       "<tr><td>FSR_Trainable_70a03e16</td><td>TERMINATED</td><td>172.26.215.93:20397</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__db80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.17787e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.888791</td><td style=\"text-align: right;\">0.715739</td><td style=\"text-align: right;\">0.377004</td><td style=\"text-align: right;\">0.104684 </td></tr>\n",
       "<tr><td>FSR_Trainable_bd028dda</td><td>TERMINATED</td><td>172.26.215.93:20720</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f1c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.32216e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.79571 </td><td style=\"text-align: right;\">0.720927</td><td style=\"text-align: right;\">0.374738</td><td style=\"text-align: right;\">0.0998267</td></tr>\n",
       "<tr><td>FSR_Trainable_8df588ab</td><td>TERMINATED</td><td>172.26.215.93:20996</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.45248e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.84231 </td><td style=\"text-align: right;\">0.694193</td><td style=\"text-align: right;\">0.3633  </td><td style=\"text-align: right;\">0.110085 </td></tr>\n",
       "<tr><td>FSR_Trainable_891ae335</td><td>TERMINATED</td><td>172.26.215.93:21223</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.2114e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      107.4     </td><td style=\"text-align: right;\">0.681713</td><td style=\"text-align: right;\">0.333708</td><td style=\"text-align: right;\">0.0969148</td></tr>\n",
       "<tr><td>FSR_Trainable_e1bb9ee7</td><td>TERMINATED</td><td>172.26.215.93:21447</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.23108e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.76376 </td><td style=\"text-align: right;\">0.701611</td><td style=\"text-align: right;\">0.351071</td><td style=\"text-align: right;\">0.0953889</td></tr>\n",
       "<tr><td>FSR_Trainable_143b8858</td><td>TERMINATED</td><td>172.26.215.93:21680</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.23186e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.84476 </td><td style=\"text-align: right;\">0.699819</td><td style=\"text-align: right;\">0.356714</td><td style=\"text-align: right;\">0.0972176</td></tr>\n",
       "<tr><td>FSR_Trainable_08a74e1e</td><td>TERMINATED</td><td>172.26.215.93:21902</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cf00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.52069e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.43254 </td><td style=\"text-align: right;\">3.14734 </td><td style=\"text-align: right;\">2.7164  </td><td style=\"text-align: right;\">0.430053 </td></tr>\n",
       "<tr><td>FSR_Trainable_626ed52c</td><td>TERMINATED</td><td>172.26.215.93:22129</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        3.38363e-05</td><td>sklearn.preproc_cdb0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.44324 </td><td style=\"text-align: right;\">2.90688 </td><td style=\"text-align: right;\">2.5958  </td><td style=\"text-align: right;\">0.390274 </td></tr>\n",
       "<tr><td>FSR_Trainable_1d49ac61</td><td>TERMINATED</td><td>172.26.215.93:22362</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__38c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        4.24359e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.61084 </td><td style=\"text-align: right;\">0.700172</td><td style=\"text-align: right;\">0.371042</td><td style=\"text-align: right;\">0.104796 </td></tr>\n",
       "<tr><td>FSR_Trainable_caec4d1e</td><td>TERMINATED</td><td>172.26.215.93:22582</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        4.70457e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.90635 </td><td style=\"text-align: right;\">0.685871</td><td style=\"text-align: right;\">0.327124</td><td style=\"text-align: right;\">0.0953304</td></tr>\n",
       "<tr><td>FSR_Trainable_42bc5664</td><td>TERMINATED</td><td>172.26.215.93:22813</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.74972e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        4.62039 </td><td style=\"text-align: right;\">0.687566</td><td style=\"text-align: right;\">0.334817</td><td style=\"text-align: right;\">0.0985118</td></tr>\n",
       "<tr><td>FSR_Trainable_ebe4a81c</td><td>TERMINATED</td><td>172.26.215.93:23020</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__aac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.01859e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.29254 </td><td style=\"text-align: right;\">0.694102</td><td style=\"text-align: right;\">0.346181</td><td style=\"text-align: right;\">0.0990032</td></tr>\n",
       "<tr><td>FSR_Trainable_79ecedfa</td><td>TERMINATED</td><td>172.26.215.93:23254</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        7.22352e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">       10.4466  </td><td style=\"text-align: right;\">0.687715</td><td style=\"text-align: right;\">0.318259</td><td style=\"text-align: right;\">0.0953969</td></tr>\n",
       "<tr><td>FSR_Trainable_d6bd2519</td><td>TERMINATED</td><td>172.26.215.93:23475</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3cc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        6.6234e-05 </td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      333.592   </td><td style=\"text-align: right;\">0.680239</td><td style=\"text-align: right;\">0.310606</td><td style=\"text-align: right;\">0.0919962</td></tr>\n",
       "<tr><td>FSR_Trainable_7521c175</td><td>TERMINATED</td><td>172.26.215.93:23704</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7d00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.67841e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       71.7306  </td><td style=\"text-align: right;\">0.682973</td><td style=\"text-align: right;\">0.313549</td><td style=\"text-align: right;\">0.092813 </td></tr>\n",
       "<tr><td>FSR_Trainable_5682ca34</td><td>TERMINATED</td><td>172.26.215.93:23926</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>sklearn.impute._fe60</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.73833e-05</td><td>sklearn.preproc_c990</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">       11.6087  </td><td style=\"text-align: right;\">0.686142</td><td style=\"text-align: right;\">0.318957</td><td style=\"text-align: right;\">0.0956221</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 15:54:19,156\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th><th>node_ip      </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_0001f0fa</td><td>2023-08-07_16-10-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.347305</td><td style=\"text-align: right;\">0.0954485</td><td>172.26.215.93</td><td style=\"text-align: right;\">11318</td><td style=\"text-align: right;\">0.700016</td><td style=\"text-align: right;\">            1.3535  </td><td style=\"text-align: right;\">          1.3535  </td><td style=\"text-align: right;\">      1.3535  </td><td style=\"text-align: right;\"> 1691392202</td><td style=\"text-align: right;\">                   1</td><td>0001f0fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_040aabb1</td><td>2023-08-07_16-00-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.353857</td><td style=\"text-align: right;\">0.0958047</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 4830</td><td style=\"text-align: right;\">0.69464 </td><td style=\"text-align: right;\">            5.26206 </td><td style=\"text-align: right;\">          1.14315 </td><td style=\"text-align: right;\">      5.26206 </td><td style=\"text-align: right;\"> 1691391642</td><td style=\"text-align: right;\">                   4</td><td>040aabb1  </td></tr>\n",
       "<tr><td>FSR_Trainable_08a74e1e</td><td>2023-08-07_16-25-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.7164  </td><td style=\"text-align: right;\">0.430053 </td><td>172.26.215.93</td><td style=\"text-align: right;\">21902</td><td style=\"text-align: right;\">3.14734 </td><td style=\"text-align: right;\">            1.43254 </td><td style=\"text-align: right;\">          1.43254 </td><td style=\"text-align: right;\">      1.43254 </td><td style=\"text-align: right;\"> 1691393100</td><td style=\"text-align: right;\">                   1</td><td>08a74e1e  </td></tr>\n",
       "<tr><td>FSR_Trainable_096db0c5</td><td>2023-08-07_16-09-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.351559</td><td style=\"text-align: right;\">0.0923458</td><td>172.26.215.93</td><td style=\"text-align: right;\">11088</td><td style=\"text-align: right;\">0.71041 </td><td style=\"text-align: right;\">            1.21653 </td><td style=\"text-align: right;\">          1.21653 </td><td style=\"text-align: right;\">      1.21653 </td><td style=\"text-align: right;\"> 1691392186</td><td style=\"text-align: right;\">                   1</td><td>096db0c5  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c822194</td><td>2023-08-07_16-01-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.337937</td><td style=\"text-align: right;\">0.0956393</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 5729</td><td style=\"text-align: right;\">0.692575</td><td style=\"text-align: right;\">            8.03885 </td><td style=\"text-align: right;\">          2.104   </td><td style=\"text-align: right;\">      8.03885 </td><td style=\"text-align: right;\"> 1691391708</td><td style=\"text-align: right;\">                   4</td><td>0c822194  </td></tr>\n",
       "<tr><td>FSR_Trainable_10471426</td><td>2023-08-07_16-16-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.327839</td><td style=\"text-align: right;\">0.097702 </td><td>172.26.215.93</td><td style=\"text-align: right;\">14440</td><td style=\"text-align: right;\">0.676204</td><td style=\"text-align: right;\">          149.313   </td><td style=\"text-align: right;\">          1.66534 </td><td style=\"text-align: right;\">    149.313   </td><td style=\"text-align: right;\"> 1691392568</td><td style=\"text-align: right;\">                 100</td><td>10471426  </td></tr>\n",
       "<tr><td>FSR_Trainable_143b8858</td><td>2023-08-07_16-24-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.356714</td><td style=\"text-align: right;\">0.0972176</td><td>172.26.215.93</td><td style=\"text-align: right;\">21680</td><td style=\"text-align: right;\">0.699819</td><td style=\"text-align: right;\">            1.84476 </td><td style=\"text-align: right;\">          1.84476 </td><td style=\"text-align: right;\">      1.84476 </td><td style=\"text-align: right;\"> 1691393083</td><td style=\"text-align: right;\">                   1</td><td>143b8858  </td></tr>\n",
       "<tr><td>FSR_Trainable_14abb1b6</td><td>2023-08-07_16-22-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.359101</td><td style=\"text-align: right;\">0.106785 </td><td>172.26.215.93</td><td style=\"text-align: right;\">19496</td><td style=\"text-align: right;\">0.69607 </td><td style=\"text-align: right;\">            1.20641 </td><td style=\"text-align: right;\">          1.20641 </td><td style=\"text-align: right;\">      1.20641 </td><td style=\"text-align: right;\"> 1691392937</td><td style=\"text-align: right;\">                   1</td><td>14abb1b6  </td></tr>\n",
       "<tr><td>FSR_Trainable_1bb266e5</td><td>2023-08-07_16-01-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.345517</td><td style=\"text-align: right;\">0.0949297</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 5482</td><td style=\"text-align: right;\">0.698779</td><td style=\"text-align: right;\">            4.99426 </td><td style=\"text-align: right;\">          1.92889 </td><td style=\"text-align: right;\">      4.99426 </td><td style=\"text-align: right;\"> 1691391684</td><td style=\"text-align: right;\">                   2</td><td>1bb266e5  </td></tr>\n",
       "<tr><td>FSR_Trainable_1d49ac61</td><td>2023-08-07_16-25-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.371042</td><td style=\"text-align: right;\">0.104796 </td><td>172.26.215.93</td><td style=\"text-align: right;\">22362</td><td style=\"text-align: right;\">0.700172</td><td style=\"text-align: right;\">            1.61084 </td><td style=\"text-align: right;\">          1.61084 </td><td style=\"text-align: right;\">      1.61084 </td><td style=\"text-align: right;\"> 1691393127</td><td style=\"text-align: right;\">                   1</td><td>1d49ac61  </td></tr>\n",
       "<tr><td>FSR_Trainable_1d8d705d</td><td>2023-08-07_16-03-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.32003 </td><td style=\"text-align: right;\">0.092289 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2038</td><td style=\"text-align: right;\">0.68573 </td><td style=\"text-align: right;\">          479.719   </td><td style=\"text-align: right;\">         13.7563  </td><td style=\"text-align: right;\">    479.719   </td><td style=\"text-align: right;\"> 1691391799</td><td style=\"text-align: right;\">                  32</td><td>1d8d705d  </td></tr>\n",
       "<tr><td>FSR_Trainable_20422a55</td><td>2023-08-07_16-21-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.368807</td><td style=\"text-align: right;\">0.0975775</td><td>172.26.215.93</td><td style=\"text-align: right;\">18585</td><td style=\"text-align: right;\">0.702879</td><td style=\"text-align: right;\">            2.46349 </td><td style=\"text-align: right;\">          2.46349 </td><td style=\"text-align: right;\">      2.46349 </td><td style=\"text-align: right;\"> 1691392863</td><td style=\"text-align: right;\">                   1</td><td>20422a55  </td></tr>\n",
       "<tr><td>FSR_Trainable_26f78e47</td><td>2023-08-07_16-08-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.375665</td><td style=\"text-align: right;\">0.104218 </td><td>172.26.215.93</td><td style=\"text-align: right;\">10208</td><td style=\"text-align: right;\">0.706823</td><td style=\"text-align: right;\">            1.82283 </td><td style=\"text-align: right;\">          1.82283 </td><td style=\"text-align: right;\">      1.82283 </td><td style=\"text-align: right;\"> 1691392116</td><td style=\"text-align: right;\">                   1</td><td>26f78e47  </td></tr>\n",
       "<tr><td>FSR_Trainable_28d87d6f</td><td>2023-08-07_16-13-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.40517 </td><td style=\"text-align: right;\">0.106594 </td><td>172.26.215.93</td><td style=\"text-align: right;\">13365</td><td style=\"text-align: right;\">0.706498</td><td style=\"text-align: right;\">           84.2606  </td><td style=\"text-align: right;\">         10.5843  </td><td style=\"text-align: right;\">     84.2606  </td><td style=\"text-align: right;\"> 1691392407</td><td style=\"text-align: right;\">                   8</td><td>28d87d6f  </td></tr>\n",
       "<tr><td>FSR_Trainable_2d25cf1a</td><td>2023-08-07_16-12-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.345748</td><td style=\"text-align: right;\">0.0961393</td><td>172.26.215.93</td><td style=\"text-align: right;\">13779</td><td style=\"text-align: right;\">0.689265</td><td style=\"text-align: right;\">            3.12668 </td><td style=\"text-align: right;\">          1.44875 </td><td style=\"text-align: right;\">      3.12668 </td><td style=\"text-align: right;\"> 1691392359</td><td style=\"text-align: right;\">                   2</td><td>2d25cf1a  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e373880</td><td>2023-08-07_15-57-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.449656</td><td style=\"text-align: right;\">0.108879 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 3646</td><td style=\"text-align: right;\">0.770788</td><td style=\"text-align: right;\">            2.47794 </td><td style=\"text-align: right;\">          2.47794 </td><td style=\"text-align: right;\">      2.47794 </td><td style=\"text-align: right;\"> 1691391459</td><td style=\"text-align: right;\">                   1</td><td>2e373880  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e7b25e1</td><td>2023-08-07_16-12-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.312714</td><td style=\"text-align: right;\">0.0918074</td><td>172.26.215.93</td><td style=\"text-align: right;\">13091</td><td style=\"text-align: right;\">0.685002</td><td style=\"text-align: right;\">           80.8474  </td><td style=\"text-align: right;\">          9.7433  </td><td style=\"text-align: right;\">     80.8474  </td><td style=\"text-align: right;\"> 1691392372</td><td style=\"text-align: right;\">                   8</td><td>2e7b25e1  </td></tr>\n",
       "<tr><td>FSR_Trainable_35b72e99</td><td>2023-08-07_16-00-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.321643</td><td style=\"text-align: right;\">0.0961289</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 4506</td><td style=\"text-align: right;\">0.683652</td><td style=\"text-align: right;\">          112.946   </td><td style=\"text-align: right;\">          0.974373</td><td style=\"text-align: right;\">    112.946   </td><td style=\"text-align: right;\"> 1691391633</td><td style=\"text-align: right;\">                 100</td><td>35b72e99  </td></tr>\n",
       "<tr><td>FSR_Trainable_3839720d</td><td>2023-08-07_16-07-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.348204</td><td style=\"text-align: right;\">0.098733 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 7652</td><td style=\"text-align: right;\">0.67746 </td><td style=\"text-align: right;\">          118.706   </td><td style=\"text-align: right;\">          0.938259</td><td style=\"text-align: right;\">    118.706   </td><td style=\"text-align: right;\"> 1691392028</td><td style=\"text-align: right;\">                 100</td><td>3839720d  </td></tr>\n",
       "<tr><td>FSR_Trainable_38fe686a</td><td>2023-08-07_16-23-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.359487</td><td style=\"text-align: right;\">0.110314 </td><td>172.26.215.93</td><td style=\"text-align: right;\">19293</td><td style=\"text-align: right;\">0.681971</td><td style=\"text-align: right;\">           90.501   </td><td style=\"text-align: right;\">          1.28788 </td><td style=\"text-align: right;\">     90.501   </td><td style=\"text-align: right;\"> 1691393028</td><td style=\"text-align: right;\">                  64</td><td>38fe686a  </td></tr>\n",
       "<tr><td>FSR_Trainable_3a0adf05</td><td>2023-08-07_16-15-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.312783</td><td style=\"text-align: right;\">0.091403 </td><td>172.26.215.93</td><td style=\"text-align: right;\">16015</td><td style=\"text-align: right;\">0.685194</td><td style=\"text-align: right;\">           24.5668  </td><td style=\"text-align: right;\">          6.0605  </td><td style=\"text-align: right;\">     24.5668  </td><td style=\"text-align: right;\"> 1691392519</td><td style=\"text-align: right;\">                   4</td><td>3a0adf05  </td></tr>\n",
       "<tr><td>FSR_Trainable_3b5879d2</td><td>2023-08-07_16-12-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.376475</td><td style=\"text-align: right;\">0.106942 </td><td>172.26.215.93</td><td style=\"text-align: right;\">13553</td><td style=\"text-align: right;\">0.696918</td><td style=\"text-align: right;\">            2.05298 </td><td style=\"text-align: right;\">          2.05298 </td><td style=\"text-align: right;\">      2.05298 </td><td style=\"text-align: right;\"> 1691392337</td><td style=\"text-align: right;\">                   1</td><td>3b5879d2  </td></tr>\n",
       "<tr><td>FSR_Trainable_42bc5664</td><td>2023-08-07_16-25-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.334817</td><td style=\"text-align: right;\">0.0985118</td><td>172.26.215.93</td><td style=\"text-align: right;\">22813</td><td style=\"text-align: right;\">0.687566</td><td style=\"text-align: right;\">            4.62039 </td><td style=\"text-align: right;\">          2.13963 </td><td style=\"text-align: right;\">      4.62039 </td><td style=\"text-align: right;\"> 1691393159</td><td style=\"text-align: right;\">                   2</td><td>42bc5664  </td></tr>\n",
       "<tr><td>FSR_Trainable_477c79e4</td><td>2023-08-07_16-10-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.92801 </td><td style=\"text-align: right;\">0.286605 </td><td>172.26.215.93</td><td style=\"text-align: right;\">12204</td><td style=\"text-align: right;\">2.27815 </td><td style=\"text-align: right;\">            1.5944  </td><td style=\"text-align: right;\">          1.5944  </td><td style=\"text-align: right;\">      1.5944  </td><td style=\"text-align: right;\"> 1691392249</td><td style=\"text-align: right;\">                   1</td><td>477c79e4  </td></tr>\n",
       "<tr><td>FSR_Trainable_4998c260</td><td>2023-08-07_16-10-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.350151</td><td style=\"text-align: right;\">0.0948329</td><td>172.26.215.93</td><td style=\"text-align: right;\">11778</td><td style=\"text-align: right;\">0.690816</td><td style=\"text-align: right;\">            2.86396 </td><td style=\"text-align: right;\">          1.22114 </td><td style=\"text-align: right;\">      2.86396 </td><td style=\"text-align: right;\"> 1691392234</td><td style=\"text-align: right;\">                   2</td><td>4998c260  </td></tr>\n",
       "<tr><td>FSR_Trainable_4b64ac9b</td><td>2023-08-07_15-58-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.359838</td><td style=\"text-align: right;\">0.0934638</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2716</td><td style=\"text-align: right;\">0.706221</td><td style=\"text-align: right;\">          120.08    </td><td style=\"text-align: right;\">          3.70203 </td><td style=\"text-align: right;\">    120.08    </td><td style=\"text-align: right;\"> 1691391486</td><td style=\"text-align: right;\">                  32</td><td>4b64ac9b  </td></tr>\n",
       "<tr><td>FSR_Trainable_4d593f19</td><td>2023-08-07_15-55-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.359287</td><td style=\"text-align: right;\">0.0904162</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1514</td><td style=\"text-align: right;\">0.724866</td><td style=\"text-align: right;\">           11.7039  </td><td style=\"text-align: right;\">          0.725407</td><td style=\"text-align: right;\">     11.7039  </td><td style=\"text-align: right;\"> 1691391304</td><td style=\"text-align: right;\">                  16</td><td>4d593f19  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ffc8f81</td><td>2023-08-07_16-07-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.332525</td><td style=\"text-align: right;\">0.0954524</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 7862</td><td style=\"text-align: right;\">0.681179</td><td style=\"text-align: right;\">          116.65    </td><td style=\"text-align: right;\">          1.15721 </td><td style=\"text-align: right;\">    116.65    </td><td style=\"text-align: right;\"> 1691392041</td><td style=\"text-align: right;\">                 100</td><td>4ffc8f81  </td></tr>\n",
       "<tr><td>FSR_Trainable_518953d3</td><td>2023-08-07_16-13-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.357299</td><td style=\"text-align: right;\">0.0965995</td><td>172.26.215.93</td><td style=\"text-align: right;\">14711</td><td style=\"text-align: right;\">0.706956</td><td style=\"text-align: right;\">            2.17491 </td><td style=\"text-align: right;\">          2.17491 </td><td style=\"text-align: right;\">      2.17491 </td><td style=\"text-align: right;\"> 1691392421</td><td style=\"text-align: right;\">                   1</td><td>518953d3  </td></tr>\n",
       "<tr><td>FSR_Trainable_5682ca34</td><td>2023-08-07_16-26-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.318957</td><td style=\"text-align: right;\">0.0956221</td><td>172.26.215.93</td><td style=\"text-align: right;\">23926</td><td style=\"text-align: right;\">0.686142</td><td style=\"text-align: right;\">           11.6087  </td><td style=\"text-align: right;\">          5.32846 </td><td style=\"text-align: right;\">     11.6087  </td><td style=\"text-align: right;\"> 1691393215</td><td style=\"text-align: right;\">                   2</td><td>5682ca34  </td></tr>\n",
       "<tr><td>FSR_Trainable_57bbd80f</td><td>2023-08-07_16-07-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.329414</td><td style=\"text-align: right;\">0.09343  </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 9241</td><td style=\"text-align: right;\">0.692487</td><td style=\"text-align: right;\">            9.90927 </td><td style=\"text-align: right;\">          4.17122 </td><td style=\"text-align: right;\">      9.90927 </td><td style=\"text-align: right;\"> 1691392021</td><td style=\"text-align: right;\">                   2</td><td>57bbd80f  </td></tr>\n",
       "<tr><td>FSR_Trainable_5cd365a4</td><td>2023-08-07_16-18-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.351053</td><td style=\"text-align: right;\">0.0957396</td><td>172.26.215.93</td><td style=\"text-align: right;\">17220</td><td style=\"text-align: right;\">0.699606</td><td style=\"text-align: right;\">            1.7123  </td><td style=\"text-align: right;\">          1.7123  </td><td style=\"text-align: right;\">      1.7123  </td><td style=\"text-align: right;\"> 1691392725</td><td style=\"text-align: right;\">                   1</td><td>5cd365a4  </td></tr>\n",
       "<tr><td>FSR_Trainable_5f88c1e0</td><td>2023-08-07_16-01-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.366219</td><td style=\"text-align: right;\">0.0987782</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 5234</td><td style=\"text-align: right;\">0.698099</td><td style=\"text-align: right;\">            3.58748 </td><td style=\"text-align: right;\">          1.59626 </td><td style=\"text-align: right;\">      3.58748 </td><td style=\"text-align: right;\"> 1691391664</td><td style=\"text-align: right;\">                   2</td><td>5f88c1e0  </td></tr>\n",
       "<tr><td>FSR_Trainable_626ed52c</td><td>2023-08-07_16-25-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.5958  </td><td style=\"text-align: right;\">0.390274 </td><td>172.26.215.93</td><td style=\"text-align: right;\">22129</td><td style=\"text-align: right;\">2.90688 </td><td style=\"text-align: right;\">            1.44324 </td><td style=\"text-align: right;\">          1.44324 </td><td style=\"text-align: right;\">      1.44324 </td><td style=\"text-align: right;\"> 1691393115</td><td style=\"text-align: right;\">                   1</td><td>626ed52c  </td></tr>\n",
       "<tr><td>FSR_Trainable_640db335</td><td>2023-08-07_16-06-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.382626</td><td style=\"text-align: right;\">0.100957 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 3416</td><td style=\"text-align: right;\">0.692678</td><td style=\"text-align: right;\">          543.245   </td><td style=\"text-align: right;\">          5.7557  </td><td style=\"text-align: right;\">    543.245   </td><td style=\"text-align: right;\"> 1691391999</td><td style=\"text-align: right;\">                 100</td><td>640db335  </td></tr>\n",
       "<tr><td>FSR_Trainable_650765f6</td><td>2023-08-07_16-14-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.319514</td><td style=\"text-align: right;\">0.096026 </td><td>172.26.215.93</td><td style=\"text-align: right;\">12877</td><td style=\"text-align: right;\">0.683295</td><td style=\"text-align: right;\">          159.483   </td><td style=\"text-align: right;\">          9.33869 </td><td style=\"text-align: right;\">    159.483   </td><td style=\"text-align: right;\"> 1691392443</td><td style=\"text-align: right;\">                  16</td><td>650765f6  </td></tr>\n",
       "<tr><td>FSR_Trainable_67b698a5</td><td>2023-08-07_16-08-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.373099</td><td style=\"text-align: right;\">0.0996082</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 9006</td><td style=\"text-align: right;\">0.689259</td><td style=\"text-align: right;\">          101.318   </td><td style=\"text-align: right;\">          1.52144 </td><td style=\"text-align: right;\">    101.318   </td><td style=\"text-align: right;\"> 1691392103</td><td style=\"text-align: right;\">                  64</td><td>67b698a5  </td></tr>\n",
       "<tr><td>FSR_Trainable_6955b5c7</td><td>2023-08-07_16-03-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.371893</td><td style=\"text-align: right;\">0.100687 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 5025</td><td style=\"text-align: right;\">0.687698</td><td style=\"text-align: right;\">          146.448   </td><td style=\"text-align: right;\">          1.28425 </td><td style=\"text-align: right;\">    146.448   </td><td style=\"text-align: right;\"> 1691391807</td><td style=\"text-align: right;\">                 100</td><td>6955b5c7  </td></tr>\n",
       "<tr><td>FSR_Trainable_69dc294f</td><td>2023-08-07_15-56-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.627795</td><td style=\"text-align: right;\">0.120668 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2964</td><td style=\"text-align: right;\">0.908441</td><td style=\"text-align: right;\">            6.73141 </td><td style=\"text-align: right;\">          6.73141 </td><td style=\"text-align: right;\">      6.73141 </td><td style=\"text-align: right;\"> 1691391405</td><td style=\"text-align: right;\">                   1</td><td>69dc294f  </td></tr>\n",
       "<tr><td>FSR_Trainable_6bb59630</td><td>2023-08-07_16-05-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.3628  </td><td style=\"text-align: right;\">0.101611 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 8549</td><td style=\"text-align: right;\">0.69603 </td><td style=\"text-align: right;\">            1.43871 </td><td style=\"text-align: right;\">          1.43871 </td><td style=\"text-align: right;\">      1.43871 </td><td style=\"text-align: right;\"> 1691391958</td><td style=\"text-align: right;\">                   1</td><td>6bb59630  </td></tr>\n",
       "<tr><td>FSR_Trainable_6c5ff9e9</td><td>2023-08-07_16-18-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.348869</td><td style=\"text-align: right;\">0.105159 </td><td>172.26.215.93</td><td style=\"text-align: right;\">17015</td><td style=\"text-align: right;\">0.689415</td><td style=\"text-align: right;\">            4.02516 </td><td style=\"text-align: right;\">          1.90775 </td><td style=\"text-align: right;\">      4.02516 </td><td style=\"text-align: right;\"> 1691392710</td><td style=\"text-align: right;\">                   2</td><td>6c5ff9e9  </td></tr>\n",
       "<tr><td>FSR_Trainable_70a03e16</td><td>2023-08-07_16-23-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.377004</td><td style=\"text-align: right;\">0.104684 </td><td>172.26.215.93</td><td style=\"text-align: right;\">20397</td><td style=\"text-align: right;\">0.715739</td><td style=\"text-align: right;\">            0.888791</td><td style=\"text-align: right;\">          0.888791</td><td style=\"text-align: right;\">      0.888791</td><td style=\"text-align: right;\"> 1691393002</td><td style=\"text-align: right;\">                   1</td><td>70a03e16  </td></tr>\n",
       "<tr><td>FSR_Trainable_7521c175</td><td>2023-08-07_16-27-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.313549</td><td style=\"text-align: right;\">0.092813 </td><td>172.26.215.93</td><td style=\"text-align: right;\">23704</td><td style=\"text-align: right;\">0.682973</td><td style=\"text-align: right;\">           71.7306  </td><td style=\"text-align: right;\">          3.89851 </td><td style=\"text-align: right;\">     71.7306  </td><td style=\"text-align: right;\"> 1691393266</td><td style=\"text-align: right;\">                  16</td><td>7521c175  </td></tr>\n",
       "<tr><td>FSR_Trainable_754545de</td><td>2023-08-07_16-10-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.341503</td><td style=\"text-align: right;\">0.0954993</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 9684</td><td style=\"text-align: right;\">0.678567</td><td style=\"text-align: right;\">          144.888   </td><td style=\"text-align: right;\">          1.59371 </td><td style=\"text-align: right;\">    144.888   </td><td style=\"text-align: right;\"> 1691392202</td><td style=\"text-align: right;\">                 100</td><td>754545de  </td></tr>\n",
       "<tr><td>FSR_Trainable_7971d6f7</td><td>2023-08-07_15-57-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.3979  </td><td style=\"text-align: right;\">0.0985882</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 3839</td><td style=\"text-align: right;\">0.732806</td><td style=\"text-align: right;\">            3.71625 </td><td style=\"text-align: right;\">          1.74642 </td><td style=\"text-align: right;\">      3.71625 </td><td style=\"text-align: right;\"> 1691391479</td><td style=\"text-align: right;\">                   2</td><td>7971d6f7  </td></tr>\n",
       "<tr><td>FSR_Trainable_79ecedfa</td><td>2023-08-07_16-26-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.318259</td><td style=\"text-align: right;\">0.0953969</td><td>172.26.215.93</td><td style=\"text-align: right;\">23254</td><td style=\"text-align: right;\">0.687715</td><td style=\"text-align: right;\">           10.4466  </td><td style=\"text-align: right;\">          4.85895 </td><td style=\"text-align: right;\">     10.4466  </td><td style=\"text-align: right;\"> 1691393185</td><td style=\"text-align: right;\">                   2</td><td>79ecedfa  </td></tr>\n",
       "<tr><td>FSR_Trainable_7a94c05f</td><td>2023-08-07_16-04-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.341616</td><td style=\"text-align: right;\">0.0946635</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 7223</td><td style=\"text-align: right;\">0.690347</td><td style=\"text-align: right;\">            3.98446 </td><td style=\"text-align: right;\">          0.886682</td><td style=\"text-align: right;\">      3.98446 </td><td style=\"text-align: right;\"> 1691391875</td><td style=\"text-align: right;\">                   4</td><td>7a94c05f  </td></tr>\n",
       "<tr><td>FSR_Trainable_7c041d7c</td><td>2023-08-07_15-57-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.461788</td><td style=\"text-align: right;\">0.107269 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 3194</td><td style=\"text-align: right;\">0.790998</td><td style=\"text-align: right;\">            5.78085 </td><td style=\"text-align: right;\">          5.78085 </td><td style=\"text-align: right;\">      5.78085 </td><td style=\"text-align: right;\"> 1691391426</td><td style=\"text-align: right;\">                   1</td><td>7c041d7c  </td></tr>\n",
       "<tr><td>FSR_Trainable_85bae870</td><td>2023-08-07_16-00-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.320629</td><td style=\"text-align: right;\">0.094771 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 4277</td><td style=\"text-align: right;\">0.68192 </td><td style=\"text-align: right;\">          114.812   </td><td style=\"text-align: right;\">          1.03417 </td><td style=\"text-align: right;\">    114.812   </td><td style=\"text-align: right;\"> 1691391623</td><td style=\"text-align: right;\">                 100</td><td>85bae870  </td></tr>\n",
       "<tr><td>FSR_Trainable_891ae335</td><td>2023-08-07_16-26-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.333708</td><td style=\"text-align: right;\">0.0969148</td><td>172.26.215.93</td><td style=\"text-align: right;\">21223</td><td style=\"text-align: right;\">0.681713</td><td style=\"text-align: right;\">          107.4     </td><td style=\"text-align: right;\">          0.933574</td><td style=\"text-align: right;\">    107.4     </td><td style=\"text-align: right;\"> 1691393177</td><td style=\"text-align: right;\">                 100</td><td>891ae335  </td></tr>\n",
       "<tr><td>FSR_Trainable_8df588ab</td><td>2023-08-07_16-23-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.3633  </td><td style=\"text-align: right;\">0.110085 </td><td>172.26.215.93</td><td style=\"text-align: right;\">20996</td><td style=\"text-align: right;\">0.694193</td><td style=\"text-align: right;\">            1.84231 </td><td style=\"text-align: right;\">          1.84231 </td><td style=\"text-align: right;\">      1.84231 </td><td style=\"text-align: right;\"> 1691393037</td><td style=\"text-align: right;\">                   1</td><td>8df588ab  </td></tr>\n",
       "<tr><td>FSR_Trainable_8eb9515c</td><td>2023-08-07_15-55-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.460227</td><td style=\"text-align: right;\">0.117795 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2449</td><td style=\"text-align: right;\">0.748827</td><td style=\"text-align: right;\">            5.79156 </td><td style=\"text-align: right;\">          5.79156 </td><td style=\"text-align: right;\">      5.79156 </td><td style=\"text-align: right;\"> 1691391345</td><td style=\"text-align: right;\">                   1</td><td>8eb9515c  </td></tr>\n",
       "<tr><td>FSR_Trainable_93194e63</td><td>2023-08-07_16-08-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.380033</td><td style=\"text-align: right;\">0.0989164</td><td>172.26.215.93</td><td style=\"text-align: right;\">10403</td><td style=\"text-align: right;\">0.710222</td><td style=\"text-align: right;\">            1.76379 </td><td style=\"text-align: right;\">          1.76379 </td><td style=\"text-align: right;\">      1.76379 </td><td style=\"text-align: right;\"> 1691392131</td><td style=\"text-align: right;\">                   1</td><td>93194e63  </td></tr>\n",
       "<tr><td>FSR_Trainable_95c6210f</td><td>2023-08-07_16-09-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.396483</td><td style=\"text-align: right;\">0.090341 </td><td>172.26.215.93</td><td style=\"text-align: right;\">10630</td><td style=\"text-align: right;\">0.729421</td><td style=\"text-align: right;\">            1.07406 </td><td style=\"text-align: right;\">          1.07406 </td><td style=\"text-align: right;\">      1.07406 </td><td style=\"text-align: right;\"> 1691392147</td><td style=\"text-align: right;\">                   1</td><td>95c6210f  </td></tr>\n",
       "<tr><td>FSR_Trainable_95d591f0</td><td>2023-08-07_16-11-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.345516</td><td style=\"text-align: right;\">0.0991698</td><td>172.26.215.93</td><td style=\"text-align: right;\">12656</td><td style=\"text-align: right;\">0.685527</td><td style=\"text-align: right;\">           36.6651  </td><td style=\"text-align: right;\">          4.28678 </td><td style=\"text-align: right;\">     36.6651  </td><td style=\"text-align: right;\"> 1691392306</td><td style=\"text-align: right;\">                   8</td><td>95d591f0  </td></tr>\n",
       "<tr><td>FSR_Trainable_9dc25b0b</td><td>2023-08-07_15-58-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.890692</td><td style=\"text-align: right;\">0.248278 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 4086</td><td style=\"text-align: right;\">1.23892 </td><td style=\"text-align: right;\">           10.4021  </td><td style=\"text-align: right;\">         10.4021  </td><td style=\"text-align: right;\">     10.4021  </td><td style=\"text-align: right;\"> 1691391501</td><td style=\"text-align: right;\">                   1</td><td>9dc25b0b  </td></tr>\n",
       "<tr><td>FSR_Trainable_a1ace408</td><td>2023-08-07_15-56-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.418572</td><td style=\"text-align: right;\">0.10345  </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2228</td><td style=\"text-align: right;\">0.720483</td><td style=\"text-align: right;\">           51.9265  </td><td style=\"text-align: right;\">          0.431159</td><td style=\"text-align: right;\">     51.9265  </td><td style=\"text-align: right;\"> 1691391388</td><td style=\"text-align: right;\">                 100</td><td>a1ace408  </td></tr>\n",
       "<tr><td>FSR_Trainable_a4520da0</td><td>2023-08-07_16-25-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.309136</td><td style=\"text-align: right;\">0.0910819</td><td>172.26.215.93</td><td style=\"text-align: right;\">15598</td><td style=\"text-align: right;\">0.678976</td><td style=\"text-align: right;\">          610.579   </td><td style=\"text-align: right;\">          6.01875 </td><td style=\"text-align: right;\">    610.579   </td><td style=\"text-align: right;\"> 1691393104</td><td style=\"text-align: right;\">                 100</td><td>a4520da0  </td></tr>\n",
       "<tr><td>FSR_Trainable_a6b2dc23</td><td>2023-08-07_16-15-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.313143</td><td style=\"text-align: right;\">0.0920703</td><td>172.26.215.93</td><td style=\"text-align: right;\">15806</td><td style=\"text-align: right;\">0.684573</td><td style=\"text-align: right;\">           47.9901  </td><td style=\"text-align: right;\">          5.36463 </td><td style=\"text-align: right;\">     47.9901  </td><td style=\"text-align: right;\"> 1691392531</td><td style=\"text-align: right;\">                   8</td><td>a6b2dc23  </td></tr>\n",
       "<tr><td>FSR_Trainable_a6d5cddc</td><td>2023-08-07_16-20-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.355757</td><td style=\"text-align: right;\">0.0991256</td><td>172.26.215.93</td><td style=\"text-align: right;\">16755</td><td style=\"text-align: right;\">0.685743</td><td style=\"text-align: right;\">          242.996   </td><td style=\"text-align: right;\">         14.2792  </td><td style=\"text-align: right;\">    242.996   </td><td style=\"text-align: right;\"> 1691392829</td><td style=\"text-align: right;\">                  16</td><td>a6d5cddc  </td></tr>\n",
       "<tr><td>FSR_Trainable_a9e85ed5</td><td>2023-08-07_16-04-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.382621</td><td style=\"text-align: right;\">0.102008 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 6444</td><td style=\"text-align: right;\">0.687008</td><td style=\"text-align: right;\">           63.4345  </td><td style=\"text-align: right;\">          1.71253 </td><td style=\"text-align: right;\">     63.4345  </td><td style=\"text-align: right;\"> 1691391880</td><td style=\"text-align: right;\">                  32</td><td>a9e85ed5  </td></tr>\n",
       "<tr><td>FSR_Trainable_ac1e6a66</td><td>2023-08-07_15-57-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.382   </td><td style=\"text-align: right;\">0.0904071</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1270</td><td style=\"text-align: right;\">0.696995</td><td style=\"text-align: right;\">          161.269   </td><td style=\"text-align: right;\">          1.76103 </td><td style=\"text-align: right;\">    161.269   </td><td style=\"text-align: right;\"> 1691391445</td><td style=\"text-align: right;\">                 100</td><td>ac1e6a66  </td></tr>\n",
       "<tr><td>FSR_Trainable_b1964871</td><td>2023-08-07_16-09-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.424845</td><td style=\"text-align: right;\">0.112512 </td><td>172.26.215.93</td><td style=\"text-align: right;\">10858</td><td style=\"text-align: right;\">0.734851</td><td style=\"text-align: right;\">            1.06293 </td><td style=\"text-align: right;\">          1.06293 </td><td style=\"text-align: right;\">      1.06293 </td><td style=\"text-align: right;\"> 1691392169</td><td style=\"text-align: right;\">                   1</td><td>b1964871  </td></tr>\n",
       "<tr><td>FSR_Trainable_bad073d7</td><td>2023-08-07_16-22-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.363523</td><td style=\"text-align: right;\">0.103084 </td><td>172.26.215.93</td><td style=\"text-align: right;\">19716</td><td style=\"text-align: right;\">0.700042</td><td style=\"text-align: right;\">            1.4932  </td><td style=\"text-align: right;\">          1.4932  </td><td style=\"text-align: right;\">      1.4932  </td><td style=\"text-align: right;\"> 1691392953</td><td style=\"text-align: right;\">                   1</td><td>bad073d7  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb3b8107</td><td>2023-08-07_16-14-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.363001</td><td style=\"text-align: right;\">0.0917417</td><td>172.26.215.93</td><td style=\"text-align: right;\">15356</td><td style=\"text-align: right;\">0.71561 </td><td style=\"text-align: right;\">            1.58725 </td><td style=\"text-align: right;\">          1.58725 </td><td style=\"text-align: right;\">      1.58725 </td><td style=\"text-align: right;\"> 1691392462</td><td style=\"text-align: right;\">                   1</td><td>bb3b8107  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb97f09f</td><td>2023-08-07_16-12-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.354979</td><td style=\"text-align: right;\">0.0981695</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 9504</td><td style=\"text-align: right;\">0.688128</td><td style=\"text-align: right;\">          275.698   </td><td style=\"text-align: right;\">          5.2887  </td><td style=\"text-align: right;\">    275.698   </td><td style=\"text-align: right;\"> 1691392324</td><td style=\"text-align: right;\">                  64</td><td>bb97f09f  </td></tr>\n",
       "<tr><td>FSR_Trainable_bd028dda</td><td>2023-08-07_16-23-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.374738</td><td style=\"text-align: right;\">0.0998267</td><td>172.26.215.93</td><td style=\"text-align: right;\">20720</td><td style=\"text-align: right;\">0.720927</td><td style=\"text-align: right;\">            1.79571 </td><td style=\"text-align: right;\">          1.79571 </td><td style=\"text-align: right;\">      1.79571 </td><td style=\"text-align: right;\"> 1691393021</td><td style=\"text-align: right;\">                   1</td><td>bd028dda  </td></tr>\n",
       "<tr><td>FSR_Trainable_bd38f24a</td><td>2023-08-07_16-10-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.34712 </td><td style=\"text-align: right;\">0.0989498</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 9902</td><td style=\"text-align: right;\">0.67845 </td><td style=\"text-align: right;\">          148.009   </td><td style=\"text-align: right;\">          1.84613 </td><td style=\"text-align: right;\">    148.009   </td><td style=\"text-align: right;\"> 1691392216</td><td style=\"text-align: right;\">                 100</td><td>bd38f24a  </td></tr>\n",
       "<tr><td>FSR_Trainable_c058ddce</td><td>2023-08-07_16-05-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.373233</td><td style=\"text-align: right;\">0.0981173</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 7413</td><td style=\"text-align: right;\">0.688714</td><td style=\"text-align: right;\">           27.2433  </td><td style=\"text-align: right;\">          0.919626</td><td style=\"text-align: right;\">     27.2433  </td><td style=\"text-align: right;\"> 1691391914</td><td style=\"text-align: right;\">                  32</td><td>c058ddce  </td></tr>\n",
       "<tr><td>FSR_Trainable_c14a090a</td><td>2023-08-07_16-18-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.359613</td><td style=\"text-align: right;\">0.0989166</td><td>172.26.215.93</td><td style=\"text-align: right;\">16473</td><td style=\"text-align: right;\">0.686149</td><td style=\"text-align: right;\">          142.453   </td><td style=\"text-align: right;\">          8.31012 </td><td style=\"text-align: right;\">    142.453   </td><td style=\"text-align: right;\"> 1691392691</td><td style=\"text-align: right;\">                  16</td><td>c14a090a  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1f87b41</td><td>2023-08-07_16-22-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.325004</td><td style=\"text-align: right;\">0.0930291</td><td>172.26.215.93</td><td style=\"text-align: right;\">19031</td><td style=\"text-align: right;\">0.683457</td><td style=\"text-align: right;\">           22.7172  </td><td style=\"text-align: right;\">          1.49008 </td><td style=\"text-align: right;\">     22.7172  </td><td style=\"text-align: right;\"> 1691392920</td><td style=\"text-align: right;\">                  16</td><td>c1f87b41  </td></tr>\n",
       "<tr><td>FSR_Trainable_c5867aff</td><td>2023-08-07_16-05-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.367791</td><td style=\"text-align: right;\">0.11212  </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 8124</td><td style=\"text-align: right;\">0.7003  </td><td style=\"text-align: right;\">            1.61198 </td><td style=\"text-align: right;\">          1.61198 </td><td style=\"text-align: right;\">      1.61198 </td><td style=\"text-align: right;\"> 1691391927</td><td style=\"text-align: right;\">                   1</td><td>c5867aff  </td></tr>\n",
       "<tr><td>FSR_Trainable_c704d36f</td><td>2023-08-07_16-12-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.341111</td><td style=\"text-align: right;\">0.0940817</td><td>172.26.215.93</td><td style=\"text-align: right;\">14006</td><td style=\"text-align: right;\">0.696494</td><td style=\"text-align: right;\">            1.67832 </td><td style=\"text-align: right;\">          1.67832 </td><td style=\"text-align: right;\">      1.67832 </td><td style=\"text-align: right;\"> 1691392373</td><td style=\"text-align: right;\">                   1</td><td>c704d36f  </td></tr>\n",
       "<tr><td>FSR_Trainable_c926e410</td><td>2023-08-07_15-55-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.752498</td><td style=\"text-align: right;\">0.168617 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1343</td><td style=\"text-align: right;\">0.993285</td><td style=\"text-align: right;\">           34.0262  </td><td style=\"text-align: right;\">         15.7504  </td><td style=\"text-align: right;\">     34.0262  </td><td style=\"text-align: right;\"> 1691391309</td><td style=\"text-align: right;\">                   2</td><td>c926e410  </td></tr>\n",
       "<tr><td>FSR_Trainable_c989bf2b</td><td>2023-08-07_16-04-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.330853</td><td style=\"text-align: right;\">0.0937291</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 6905</td><td style=\"text-align: right;\">0.690922</td><td style=\"text-align: right;\">            8.52894 </td><td style=\"text-align: right;\">          1.7995  </td><td style=\"text-align: right;\">      8.52894 </td><td style=\"text-align: right;\"> 1691391859</td><td style=\"text-align: right;\">                   4</td><td>c989bf2b  </td></tr>\n",
       "<tr><td>FSR_Trainable_caec4d1e</td><td>2023-08-07_16-25-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.327124</td><td style=\"text-align: right;\">0.0953304</td><td>172.26.215.93</td><td style=\"text-align: right;\">22582</td><td style=\"text-align: right;\">0.685871</td><td style=\"text-align: right;\">            2.90635 </td><td style=\"text-align: right;\">          1.34205 </td><td style=\"text-align: right;\">      2.90635 </td><td style=\"text-align: right;\"> 1691393143</td><td style=\"text-align: right;\">                   2</td><td>caec4d1e  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc096752</td><td>2023-08-07_16-10-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.346661</td><td style=\"text-align: right;\">0.104535 </td><td>172.26.215.93</td><td style=\"text-align: right;\">11543</td><td style=\"text-align: right;\">0.683353</td><td style=\"text-align: right;\">           17.8378  </td><td style=\"text-align: right;\">          0.882047</td><td style=\"text-align: right;\">     17.8378  </td><td style=\"text-align: right;\"> 1691392238</td><td style=\"text-align: right;\">                  16</td><td>cc096752  </td></tr>\n",
       "<tr><td>FSR_Trainable_ccd8e300</td><td>2023-08-07_15-55-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.422133</td><td style=\"text-align: right;\">0.103669 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1679</td><td style=\"text-align: right;\">0.746066</td><td style=\"text-align: right;\">           15.5484  </td><td style=\"text-align: right;\">          1.61703 </td><td style=\"text-align: right;\">     15.5484  </td><td style=\"text-align: right;\"> 1691391312</td><td style=\"text-align: right;\">                   8</td><td>ccd8e300  </td></tr>\n",
       "<tr><td>FSR_Trainable_cfdd4deb</td><td>2023-08-07_16-02-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.349186</td><td style=\"text-align: right;\">0.0956282</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 5960</td><td style=\"text-align: right;\">0.693037</td><td style=\"text-align: right;\">            7.22135 </td><td style=\"text-align: right;\">          1.24614 </td><td style=\"text-align: right;\">      7.22135 </td><td style=\"text-align: right;\"> 1691391730</td><td style=\"text-align: right;\">                   4</td><td>cfdd4deb  </td></tr>\n",
       "<tr><td>FSR_Trainable_d55d26c1</td><td>2023-08-07_16-11-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.43655 </td><td style=\"text-align: right;\">0.362018 </td><td>172.26.215.93</td><td style=\"text-align: right;\">12439</td><td style=\"text-align: right;\">2.74811 </td><td style=\"text-align: right;\">            4.43387 </td><td style=\"text-align: right;\">          4.43387 </td><td style=\"text-align: right;\">      4.43387 </td><td style=\"text-align: right;\"> 1691392261</td><td style=\"text-align: right;\">                   1</td><td>d55d26c1  </td></tr>\n",
       "<tr><td>FSR_Trainable_d6bd2519</td><td>2023-08-07_16-32-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.310606</td><td style=\"text-align: right;\">0.0919962</td><td>172.26.215.93</td><td style=\"text-align: right;\">23475</td><td style=\"text-align: right;\">0.680239</td><td style=\"text-align: right;\">          333.592   </td><td style=\"text-align: right;\">          2.88622 </td><td style=\"text-align: right;\">    333.592   </td><td style=\"text-align: right;\"> 1691393521</td><td style=\"text-align: right;\">                 100</td><td>d6bd2519  </td></tr>\n",
       "<tr><td>FSR_Trainable_dab74113</td><td>2023-08-07_16-23-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.392397</td><td style=\"text-align: right;\">0.102979 </td><td>172.26.215.93</td><td style=\"text-align: right;\">20167</td><td style=\"text-align: right;\">0.716898</td><td style=\"text-align: right;\">            0.827611</td><td style=\"text-align: right;\">          0.827611</td><td style=\"text-align: right;\">      0.827611</td><td style=\"text-align: right;\"> 1691392986</td><td style=\"text-align: right;\">                   1</td><td>dab74113  </td></tr>\n",
       "<tr><td>FSR_Trainable_dcbc59db</td><td>2023-08-07_16-05-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.356424</td><td style=\"text-align: right;\">0.101547 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 8320</td><td style=\"text-align: right;\">0.696527</td><td style=\"text-align: right;\">            1.44572 </td><td style=\"text-align: right;\">          1.44572 </td><td style=\"text-align: right;\">      1.44572 </td><td style=\"text-align: right;\"> 1691391943</td><td style=\"text-align: right;\">                   1</td><td>dcbc59db  </td></tr>\n",
       "<tr><td>FSR_Trainable_dd647cf3</td><td>2023-08-07_16-14-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.338004</td><td style=\"text-align: right;\">0.094335 </td><td>172.26.215.93</td><td style=\"text-align: right;\">15131</td><td style=\"text-align: right;\">0.689379</td><td style=\"text-align: right;\">            4.14424 </td><td style=\"text-align: right;\">          2.13022 </td><td style=\"text-align: right;\">      4.14424 </td><td style=\"text-align: right;\"> 1691392457</td><td style=\"text-align: right;\">                   2</td><td>dd647cf3  </td></tr>\n",
       "<tr><td>FSR_Trainable_e1bb9ee7</td><td>2023-08-07_16-24-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.351071</td><td style=\"text-align: right;\">0.0953889</td><td>172.26.215.93</td><td style=\"text-align: right;\">21447</td><td style=\"text-align: right;\">0.701611</td><td style=\"text-align: right;\">            2.76376 </td><td style=\"text-align: right;\">          2.76376 </td><td style=\"text-align: right;\">      2.76376 </td><td style=\"text-align: right;\"> 1691393066</td><td style=\"text-align: right;\">                   1</td><td>e1bb9ee7  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2dc6632</td><td>2023-08-07_16-26-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.314219</td><td style=\"text-align: right;\">0.0946371</td><td>172.26.215.93</td><td style=\"text-align: right;\">16295</td><td style=\"text-align: right;\">0.681025</td><td style=\"text-align: right;\">          604.019   </td><td style=\"text-align: right;\">          6.13526 </td><td style=\"text-align: right;\">    604.019   </td><td style=\"text-align: right;\"> 1691393160</td><td style=\"text-align: right;\">                 100</td><td>e2dc6632  </td></tr>\n",
       "<tr><td>FSR_Trainable_e53d7657</td><td>2023-08-07_16-06-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.342078</td><td style=\"text-align: right;\">0.096827 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 8777</td><td style=\"text-align: right;\">0.696169</td><td style=\"text-align: right;\">            1.05507 </td><td style=\"text-align: right;\">          1.05507 </td><td style=\"text-align: right;\">      1.05507 </td><td style=\"text-align: right;\"> 1691391978</td><td style=\"text-align: right;\">                   1</td><td>e53d7657  </td></tr>\n",
       "<tr><td>FSR_Trainable_e785abf7</td><td>2023-08-07_16-21-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.318072</td><td style=\"text-align: right;\">0.0916696</td><td>172.26.215.93</td><td style=\"text-align: right;\">18105</td><td style=\"text-align: right;\">0.676331</td><td style=\"text-align: right;\">           95.2566  </td><td style=\"text-align: right;\">          0.80503 </td><td style=\"text-align: right;\">     95.2566  </td><td style=\"text-align: right;\"> 1691392905</td><td style=\"text-align: right;\">                 100</td><td>e785abf7  </td></tr>\n",
       "<tr><td>FSR_Trainable_e81dbf53</td><td>2023-08-07_16-20-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.347628</td><td style=\"text-align: right;\">0.0940964</td><td>172.26.215.93</td><td style=\"text-align: right;\">18378</td><td style=\"text-align: right;\">0.690913</td><td style=\"text-align: right;\">            4.27192 </td><td style=\"text-align: right;\">          2.14623 </td><td style=\"text-align: right;\">      4.27192 </td><td style=\"text-align: right;\"> 1691392848</td><td style=\"text-align: right;\">                   2</td><td>e81dbf53  </td></tr>\n",
       "<tr><td>FSR_Trainable_e8b9d18d</td><td>2023-08-07_16-10-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.43472 </td><td style=\"text-align: right;\">0.405736 </td><td>172.26.215.93</td><td style=\"text-align: right;\">11979</td><td style=\"text-align: right;\">2.74879 </td><td style=\"text-align: right;\">            1.59094 </td><td style=\"text-align: right;\">          1.59094 </td><td style=\"text-align: right;\">      1.59094 </td><td style=\"text-align: right;\"> 1691392241</td><td style=\"text-align: right;\">                   1</td><td>e8b9d18d  </td></tr>\n",
       "<tr><td>FSR_Trainable_e8e097f7</td><td>2023-08-07_16-03-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.327867</td><td style=\"text-align: right;\">0.0927403</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 6622</td><td style=\"text-align: right;\">0.686143</td><td style=\"text-align: right;\">           15.1287  </td><td style=\"text-align: right;\">          1.74963 </td><td style=\"text-align: right;\">     15.1287  </td><td style=\"text-align: right;\"> 1691391837</td><td style=\"text-align: right;\">                   8</td><td>e8e097f7  </td></tr>\n",
       "<tr><td>FSR_Trainable_ea5fbd0a</td><td>2023-08-07_16-22-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.379991</td><td style=\"text-align: right;\">0.0943633</td><td>172.26.215.93</td><td style=\"text-align: right;\">19942</td><td style=\"text-align: right;\">0.7061  </td><td style=\"text-align: right;\">            1.4205  </td><td style=\"text-align: right;\">          1.4205  </td><td style=\"text-align: right;\">      1.4205  </td><td style=\"text-align: right;\"> 1691392971</td><td style=\"text-align: right;\">                   1</td><td>ea5fbd0a  </td></tr>\n",
       "<tr><td>FSR_Trainable_ebbdefae</td><td>2023-08-07_16-04-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.334063</td><td style=\"text-align: right;\">0.0991985</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 6185</td><td style=\"text-align: right;\">0.679958</td><td style=\"text-align: right;\">          109.983   </td><td style=\"text-align: right;\">          0.950423</td><td style=\"text-align: right;\">    109.983   </td><td style=\"text-align: right;\"> 1691391860</td><td style=\"text-align: right;\">                 100</td><td>ebbdefae  </td></tr>\n",
       "<tr><td>FSR_Trainable_ebe4a81c</td><td>2023-08-07_16-26-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.346181</td><td style=\"text-align: right;\">0.0990032</td><td>172.26.215.93</td><td style=\"text-align: right;\">23020</td><td style=\"text-align: right;\">0.694102</td><td style=\"text-align: right;\">            3.29254 </td><td style=\"text-align: right;\">          1.42106 </td><td style=\"text-align: right;\">      3.29254 </td><td style=\"text-align: right;\"> 1691393168</td><td style=\"text-align: right;\">                   2</td><td>ebe4a81c  </td></tr>\n",
       "<tr><td>FSR_Trainable_ee27e018</td><td>2023-08-07_16-19-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.369741</td><td style=\"text-align: right;\">0.0991159</td><td>172.26.215.93</td><td style=\"text-align: right;\">17892</td><td style=\"text-align: right;\">0.698303</td><td style=\"text-align: right;\">            1.56937 </td><td style=\"text-align: right;\">          1.56937 </td><td style=\"text-align: right;\">      1.56937 </td><td style=\"text-align: right;\"> 1691392779</td><td style=\"text-align: right;\">                   1</td><td>ee27e018  </td></tr>\n",
       "<tr><td>FSR_Trainable_f137f0d6</td><td>2023-08-07_16-19-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.364033</td><td style=\"text-align: right;\">0.104089 </td><td>172.26.215.93</td><td style=\"text-align: right;\">17655</td><td style=\"text-align: right;\">0.687457</td><td style=\"text-align: right;\">            3.08116 </td><td style=\"text-align: right;\">          1.45121 </td><td style=\"text-align: right;\">      3.08116 </td><td style=\"text-align: right;\"> 1691392765</td><td style=\"text-align: right;\">                   2</td><td>f137f0d6  </td></tr>\n",
       "<tr><td>FSR_Trainable_f259722f</td><td>2023-08-07_16-19-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.85073 </td><td style=\"text-align: right;\">0.440703 </td><td>172.26.215.93</td><td style=\"text-align: right;\">17432</td><td style=\"text-align: right;\">3.2069  </td><td style=\"text-align: right;\">            1.6813  </td><td style=\"text-align: right;\">          1.6813  </td><td style=\"text-align: right;\">      1.6813  </td><td style=\"text-align: right;\"> 1691392743</td><td style=\"text-align: right;\">                   1</td><td>f259722f  </td></tr>\n",
       "<tr><td>FSR_Trainable_f2951ccc</td><td>2023-08-07_16-13-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.355148</td><td style=\"text-align: right;\">0.101626 </td><td>172.26.215.93</td><td style=\"text-align: right;\">14904</td><td style=\"text-align: right;\">0.697137</td><td style=\"text-align: right;\">            1.86371 </td><td style=\"text-align: right;\">          1.86371 </td><td style=\"text-align: right;\">      1.86371 </td><td style=\"text-align: right;\"> 1691392438</td><td style=\"text-align: right;\">                   1</td><td>f2951ccc  </td></tr>\n",
       "<tr><td>FSR_Trainable_f433c207</td><td>2023-08-07_16-21-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.342802</td><td style=\"text-align: right;\">0.0943667</td><td>172.26.215.93</td><td style=\"text-align: right;\">18806</td><td style=\"text-align: right;\">0.6937  </td><td style=\"text-align: right;\">            2.14861 </td><td style=\"text-align: right;\">          2.14861 </td><td style=\"text-align: right;\">      2.14861 </td><td style=\"text-align: right;\"> 1691392880</td><td style=\"text-align: right;\">                   1</td><td>f433c207  </td></tr>\n",
       "<tr><td>FSR_Trainable_f99e21db</td><td>2023-08-07_16-14-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.341462</td><td style=\"text-align: right;\">0.0975658</td><td>172.26.215.93</td><td style=\"text-align: right;\">14228</td><td style=\"text-align: right;\">0.686842</td><td style=\"text-align: right;\">           50.6498  </td><td style=\"text-align: right;\">          1.60932 </td><td style=\"text-align: right;\">     50.6498  </td><td style=\"text-align: right;\"> 1691392445</td><td style=\"text-align: right;\">                  32</td><td>f99e21db  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_ac1e6a66_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-54-19/wandb/run-20230807_155431-ac1e6a66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Syncing run FSR_Trainable_ac1e6a66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac1e6a66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...3)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_c926e410_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-54-25/wandb/run-20230807_155442-c926e410\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Syncing run FSR_Trainable_c926e410\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c926e410\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_4d593f19_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-54-33/wandb/run-20230807_155452-4d593f19\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Syncing run FSR_Trainable_4d593f19\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4d593f19\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_ccd8e300_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-54-45/wandb/run-20230807_155502-ccd8e300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Syncing run FSR_Trainable_ccd8e300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ccd8e300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                      mae ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                     mape ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                     rmse ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▅▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:         time_this_iter_s ▆▄▄▅▃▁▂▃▆▂▅▆▃█▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▅▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                timestamp ▁▃▃▃▄▄▅▅▅▅▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                      mae 0.35929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                     mape 0.09042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                     rmse 0.72487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:       time_since_restore 11.7039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:         time_this_iter_s 0.72541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:             time_total_s 11.7039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:                timestamp 1691391304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: 🚀 View run FSR_Trainable_4d593f19 at: https://wandb.ai/seokjin/FSR-prediction/runs/4d593f19\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1678)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155452-4d593f19/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1513)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155442-c926e410/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                      mae █▇▆▅▄▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                     mape ▁▄▆███▇▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                     rmse █▇▆▅▄▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:       time_since_restore ▁▂▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:         time_this_iter_s ▇▇█▅▂▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:             time_total_s ▁▂▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                timestamp ▁▃▄▅▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                      mae 0.42213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                     mape 0.10367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                     rmse 0.74607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:       time_since_restore 15.54835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:         time_this_iter_s 1.61703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:             time_total_s 15.54835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:                timestamp 1691391312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: 🚀 View run FSR_Trainable_ccd8e300 at: https://wandb.ai/seokjin/FSR-prediction/runs/ccd8e300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155502-ccd8e300/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1862)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_1d8d705d_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-54-55/wandb/run-20230807_155521-1d8d705d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Syncing run FSR_Trainable_1d8d705d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1d8d705d\n",
      "2023-08-07 15:55:30,422\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.787 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:55:30,424\tWARNING util.py:315 -- The `process_trial_result` operation took 3.791 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:55:30,429\tWARNING util.py:315 -- Processing trial results took 3.795 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:55:30,430\tWARNING util.py:315 -- The `process_trial_result` operation took 3.797 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:55:32,431\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.959 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:55:32,432\tWARNING util.py:315 -- The `process_trial_result` operation took 1.961 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:55:32,434\tWARNING util.py:315 -- Processing trial results took 1.963 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:55:32,437\tWARNING util.py:315 -- The `process_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_a1ace408_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-55-14/wandb/run-20230807_155533-a1ace408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Syncing run FSR_Trainable_a1ace408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a1ace408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 15:55:47,166\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.849 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:55:47,170\tWARNING util.py:315 -- The `process_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:55:47,172\tWARNING util.py:315 -- Processing trial results took 1.855 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:55:47,175\tWARNING util.py:315 -- The `process_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_8eb9515c_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-55-25/wandb/run-20230807_155546-8eb9515c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Syncing run FSR_Trainable_8eb9515c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8eb9515c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                      mae 0.46023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                     mape 0.1178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                     rmse 0.74883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:       time_since_restore 5.79156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:         time_this_iter_s 5.79156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:             time_total_s 5.79156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:                timestamp 1691391345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: 🚀 View run FSR_Trainable_8eb9515c at: https://wandb.ai/seokjin/FSR-prediction/runs/8eb9515c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2543)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155546-8eb9515c/logs\n",
      "2023-08-07 15:56:06,247\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.161 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:56:06,252\tWARNING util.py:315 -- The `process_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:56:06,253\tWARNING util.py:315 -- Processing trial results took 2.167 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:56:06,255\tWARNING util.py:315 -- The `process_trial_result` operation took 2.169 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_4b64ac9b_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-55-39/wandb/run-20230807_155606-4b64ac9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Syncing run FSR_Trainable_4b64ac9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4b64ac9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                      mae ▂▁▁▂▃▃▄▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇██████▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                     mape ▄▁▁▁▂▂▃▃▄▄▅▅▆▇▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                     rmse ▃▂▁▁▁▁▂▂▂▃▄▄▅▆▅▄▅▅▆▆▆▇▆▇▇▇▇▇▇▇████▅▇▇█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:         time_this_iter_s █▂▂▁▁▁▂▄▂▂▂▃▃▂▁▁▁▁▁▁▃▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                      mae 0.41857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                     mape 0.10345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                     rmse 0.72048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:       time_since_restore 51.92651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:         time_this_iter_s 0.43116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:             time_total_s 51.92651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:                timestamp 1691391388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: 🚀 View run FSR_Trainable_a1ace408 at: https://wandb.ai/seokjin/FSR-prediction/runs/a1ace408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2320)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155533-a1ace408/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_69dc294f_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_15-55-59/wandb/run-20230807_155646-69dc294f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Syncing run FSR_Trainable_69dc294f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/69dc294f\n",
      "2023-08-07 15:56:48,191\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.193 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:56:48,196\tWARNING util.py:315 -- The `process_trial_result` operation took 2.199 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:56:48,198\tWARNING util.py:315 -- Processing trial results took 2.200 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:56:48,200\tWARNING util.py:315 -- The `process_trial_result` operation took 2.202 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                      mae 0.62779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                     mape 0.12067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                     rmse 0.90844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:       time_since_restore 6.73141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:         time_this_iter_s 6.73141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:             time_total_s 6.73141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:                timestamp 1691391405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: 🚀 View run FSR_Trainable_69dc294f at: https://wandb.ai/seokjin/FSR-prediction/runs/69dc294f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3020)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155646-69dc294f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 15:57:08,570\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:08,575\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:08,576\tWARNING util.py:315 -- Processing trial results took 2.023 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:57:08,578\tWARNING util.py:315 -- The `process_trial_result` operation took 2.025 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_7c041d7c_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-56-39/wandb/run-20230807_155708-7c041d7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Syncing run FSR_Trainable_7c041d7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7c041d7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                      mae 0.46179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                     mape 0.10727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                     rmse 0.791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:       time_since_restore 5.78085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:         time_this_iter_s 5.78085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:             time_total_s 5.78085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:                timestamp 1691391426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: 🚀 View run FSR_Trainable_7c041d7c at: https://wandb.ai/seokjin/FSR-prediction/runs/7c041d7c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3247)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155708-7c041d7c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_640db335_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-57-00/wandb/run-20230807_155727-640db335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Syncing run FSR_Trainable_640db335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/640db335\n",
      "2023-08-07 15:57:28,322\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.443 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:28,325\tWARNING util.py:315 -- The `process_trial_result` operation took 2.448 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:28,328\tWARNING util.py:315 -- Processing trial results took 2.450 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:57:28,329\tWARNING util.py:315 -- The `process_trial_result` operation took 2.452 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                      mae █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                     rmse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:         time_this_iter_s ▆▂▄█▂▄▄▅▃▂▁▃▃▅▃▃▄▅▃▂▅▃▃▃▃▃▃▃▂▃▄▃▃▃▄▄▃▂▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                      mae 0.382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                     mape 0.09041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                     rmse 0.69699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:       time_since_restore 161.26911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:         time_this_iter_s 1.76103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:             time_total_s 161.26911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:                timestamp 1691391445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: 🚀 View run FSR_Trainable_ac1e6a66 at: https://wandb.ai/seokjin/FSR-prediction/runs/ac1e6a66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1342)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155431-ac1e6a66/logs\n",
      "2023-08-07 15:57:42,316\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.510 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:42,323\tWARNING util.py:315 -- The `process_trial_result` operation took 2.517 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:42,327\tWARNING util.py:315 -- Processing trial results took 2.521 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:57:42,330\tWARNING util.py:315 -- The `process_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_2e373880_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-57-19/wandb/run-20230807_155744-2e373880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Syncing run FSR_Trainable_2e373880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e373880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                      mae 0.44966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                     mape 0.10888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                     rmse 0.77079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:       time_since_restore 2.47794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:         time_this_iter_s 2.47794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:             time_total_s 2.47794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:                timestamp 1691391459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: 🚀 View run FSR_Trainable_2e373880 at: https://wandb.ai/seokjin/FSR-prediction/runs/2e373880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3698)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155744-2e373880/logs\n",
      "2023-08-07 15:57:58,100\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.102 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:58,105\tWARNING util.py:315 -- The `process_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:57:58,107\tWARNING util.py:315 -- Processing trial results took 2.110 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:57:58,110\tWARNING util.py:315 -- The `process_trial_result` operation took 2.113 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_7971d6f7_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-57-37/wandb/run-20230807_155801-7971d6f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Syncing run FSR_Trainable_7971d6f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7971d6f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                      mae 0.3979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                     mape 0.09859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                     rmse 0.73281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:       time_since_restore 3.71625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:         time_this_iter_s 1.74642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:             time_total_s 3.71625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:                timestamp 1691391479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: 🚀 View run FSR_Trainable_7971d6f7 at: https://wandb.ai/seokjin/FSR-prediction/runs/7971d6f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3923)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155801-7971d6f7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:                      mae █▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:                     mape ██▇▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:                     rmse █▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:         time_this_iter_s ▆▂▃▂▁▁▂▂▁▁▃▄█▁▂▄▂▂▁▂▅▄▂▂▃▃▃▃▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155606-4b64ac9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2775)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_9dc25b0b_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-57-54/wandb/run-20230807_155818-9dc25b0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Syncing run FSR_Trainable_9dc25b0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9dc25b0b\n",
      "2023-08-07 15:58:23,269\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:58:23,273\tWARNING util.py:315 -- The `process_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:58:23,276\tWARNING util.py:315 -- Processing trial results took 2.064 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:58:23,279\tWARNING util.py:315 -- The `process_trial_result` operation took 2.067 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 15:58:27,125\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:58:27,129\tWARNING util.py:315 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:58:27,131\tWARNING util.py:315 -- Processing trial results took 1.981 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:58:27,133\tWARNING util.py:315 -- The `process_trial_result` operation took 1.984 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                      mae 0.89069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                     mape 0.24828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                     rmse 1.23892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:       time_since_restore 10.40205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:         time_this_iter_s 10.40205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:             time_total_s 10.40205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:                timestamp 1691391501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: 🚀 View run FSR_Trainable_9dc25b0b at: https://wandb.ai/seokjin/FSR-prediction/runs/9dc25b0b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4160)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155818-9dc25b0b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_85bae870_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-58-10/wandb/run-20230807_155830-85bae870\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Syncing run FSR_Trainable_85bae870\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/85bae870\n",
      "2023-08-07 15:58:39,805\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:58:39,810\tWARNING util.py:315 -- The `process_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-08-07 15:58:39,812\tWARNING util.py:315 -- Processing trial results took 1.918 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 15:58:39,814\tWARNING util.py:315 -- The `process_trial_result` operation took 1.920 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_35b72e99_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-58-23/wandb/run-20230807_155843-35b72e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Syncing run FSR_Trainable_35b72e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/35b72e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                      mae ███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                     mape ██▇▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                     rmse ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:         time_this_iter_s ▆▃▁▂▂▃▂▂▂▁▂▁▁▁▂▁▁▁▁▃▂▃▄▃▄▃▄█▂▁▂▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                      mae 0.32063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                     mape 0.09477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                     rmse 0.68192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:       time_since_restore 114.81183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:         time_this_iter_s 1.03417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:             time_total_s 114.81183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:                timestamp 1691391623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: 🚀 View run FSR_Trainable_85bae870 at: https://wandb.ai/seokjin/FSR-prediction/runs/85bae870\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4375)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155830-85bae870/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                      mae ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                     mape ███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                     rmse ██▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:         time_this_iter_s ▄▄▂▂▂▁▁▁▂▂▁▂▁▂▁▃▂▂▅▄▄▄▃█▃▁▂▁▂▂▂▂▁▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                      mae 0.32164\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                     mape 0.09613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                     rmse 0.68365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:       time_since_restore 112.94619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:         time_this_iter_s 0.97437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:             time_total_s 112.94619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:                timestamp 1691391633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: 🚀 View run FSR_Trainable_35b72e99 at: https://wandb.ai/seokjin/FSR-prediction/runs/35b72e99\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4594)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155843-35b72e99/logs\n",
      "2023-08-07 16:00:38,779\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.199 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:00:38,783\tWARNING util.py:315 -- The `process_trial_result` operation took 2.204 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:00:38,791\tWARNING util.py:315 -- Processing trial results took 2.212 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:00:38,794\tWARNING util.py:315 -- The `process_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_040aabb1_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_15-58-36/wandb/run-20230807_160042-040aabb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Syncing run FSR_Trainable_040aabb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/040aabb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:         time_this_iter_s █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                      mae 0.35386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                     mape 0.0958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                     rmse 0.69464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:       time_since_restore 5.26206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:         time_this_iter_s 1.14315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:             time_total_s 5.26206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:                timestamp 1691391642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: 🚀 View run FSR_Trainable_040aabb1 at: https://wandb.ai/seokjin/FSR-prediction/runs/040aabb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=4889)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160042-040aabb1/logs\n",
      "2023-08-07 16:00:51,481\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.190 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:00:51,486\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:00:51,488\tWARNING util.py:315 -- Processing trial results took 2.198 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:00:51,491\tWARNING util.py:315 -- The `process_trial_result` operation took 2.200 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_6955b5c7_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-00-34/wandb/run-20230807_160054-6955b5c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Syncing run FSR_Trainable_6955b5c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6955b5c7\n",
      "2023-08-07 16:01:03,005\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:01:03,010\tWARNING util.py:315 -- The `process_trial_result` operation took 2.419 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:01:03,012\tWARNING util.py:315 -- Processing trial results took 2.421 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:01:03,014\tWARNING util.py:315 -- The `process_trial_result` operation took 2.423 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_5f88c1e0_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-00-47/wandb/run-20230807_160106-5f88c1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Syncing run FSR_Trainable_5f88c1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5f88c1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                      mae 0.36622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                     mape 0.09878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                     rmse 0.6981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:       time_since_restore 3.58748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:         time_this_iter_s 1.59626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:             time_total_s 3.58748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:                timestamp 1691391664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: 🚀 View run FSR_Trainable_5f88c1e0 at: https://wandb.ai/seokjin/FSR-prediction/runs/5f88c1e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5332)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160106-5f88c1e0/logs\n",
      "2023-08-07 16:01:22,332\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.850 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:01:22,338\tWARNING util.py:315 -- The `process_trial_result` operation took 2.857 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:01:22,340\tWARNING util.py:315 -- Processing trial results took 2.860 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:01:22,343\tWARNING util.py:315 -- The `process_trial_result` operation took 2.862 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_1bb266e5_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-00-58/wandb/run-20230807_160125-1bb266e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Syncing run FSR_Trainable_1bb266e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1bb266e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                      mae 0.34552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                     mape 0.09493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                     rmse 0.69878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:       time_since_restore 4.99426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:         time_this_iter_s 1.92889\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:             time_total_s 4.99426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:                timestamp 1691391684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: 🚀 View run FSR_Trainable_1bb266e5 at: https://wandb.ai/seokjin/FSR-prediction/runs/1bb266e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5558)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160125-1bb266e5/logs\n",
      "2023-08-07 16:01:42,141\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.901 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:01:42,147\tWARNING util.py:315 -- The `process_trial_result` operation took 2.909 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:01:42,150\tWARNING util.py:315 -- Processing trial results took 2.911 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:01:42,152\tWARNING util.py:315 -- The `process_trial_result` operation took 2.913 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_0c822194_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-01-16/wandb/run-20230807_160145-0c822194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Syncing run FSR_Trainable_0c822194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0c822194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:         time_this_iter_s █▄▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                      mae 0.33794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                     mape 0.09564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                     rmse 0.69257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:       time_since_restore 8.03885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:         time_this_iter_s 2.104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:             time_total_s 8.03885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:                timestamp 1691391708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: 🚀 View run FSR_Trainable_0c822194 at: https://wandb.ai/seokjin/FSR-prediction/runs/0c822194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5786)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160145-0c822194/logs\n",
      "2023-08-07 16:02:05,513\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.333 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:02:05,516\tWARNING util.py:315 -- The `process_trial_result` operation took 3.336 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:02:05,523\tWARNING util.py:315 -- Processing trial results took 3.343 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:02:05,526\tWARNING util.py:315 -- The `process_trial_result` operation took 3.346 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_cfdd4deb_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-01-37/wandb/run-20230807_160210-cfdd4deb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Syncing run FSR_Trainable_cfdd4deb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cfdd4deb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:         time_this_iter_s █▆▇▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                      mae 0.34919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                     mape 0.09563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                     rmse 0.69304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:       time_since_restore 7.22135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:         time_this_iter_s 1.24614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:             time_total_s 7.22135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:                timestamp 1691391730\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: 🚀 View run FSR_Trainable_cfdd4deb at: https://wandb.ai/seokjin/FSR-prediction/runs/cfdd4deb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6014)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160210-cfdd4deb/logs\n",
      "2023-08-07 16:02:25,512\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.317 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:02:25,515\tWARNING util.py:315 -- The `process_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:02:25,517\tWARNING util.py:315 -- Processing trial results took 2.323 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:02:25,518\tWARNING util.py:315 -- The `process_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_ebbdefae_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-02-00/wandb/run-20230807_160229-ebbdefae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Syncing run FSR_Trainable_ebbdefae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ebbdefae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                      mae ███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                     mape ███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                     rmse ███▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:         time_this_iter_s ▅▃▄▂▂▃▅▂▄▄▄▃▂▂▁▁▄█▄▁▂▂▃▄█▇▆▄▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                      mae 0.32003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                     mape 0.09229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                     rmse 0.68573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:       time_since_restore 479.71918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:         time_this_iter_s 13.75631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:             time_total_s 479.71918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:                timestamp 1691391799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: 🚀 View run FSR_Trainable_1d8d705d at: https://wandb.ai/seokjin/FSR-prediction/runs/1d8d705d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2108)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155521-1d8d705d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                      mae ▃▂▂▂▂▁▁▁▁▂▃▃▄▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                     mape ▂▂▂▂▁▁▁▁▁▁▂▃▃▄▄▅▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                     rmse ▇▆▆▆▅▅▅▄▅▆▆▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:         time_this_iter_s ▄▂▂▃▂▃▁█▃▁▂▄▅▂▁▂▆▇▂▁▁▃▄▁▁▁▁▂▂▂▂▁▁▂▁▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                      mae 0.37189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                     mape 0.10069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                     rmse 0.6877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:       time_since_restore 146.44777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:         time_this_iter_s 1.28425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:             time_total_s 146.44777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:                timestamp 1691391807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: 🚀 View run FSR_Trainable_6955b5c7 at: https://wandb.ai/seokjin/FSR-prediction/runs/6955b5c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=5116)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160054-6955b5c7/logs\n",
      "2023-08-07 16:03:34,574\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.026 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:03:34,578\tWARNING util.py:315 -- The `process_trial_result` operation took 2.030 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:03:34,580\tWARNING util.py:315 -- Processing trial results took 2.032 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:03:34,582\tWARNING util.py:315 -- The `process_trial_result` operation took 2.035 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_a9e85ed5_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-02-21/wandb/run-20230807_160336-a9e85ed5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Syncing run FSR_Trainable_a9e85ed5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9e85ed5\n",
      "2023-08-07 16:03:44,952\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:03:44,955\tWARNING util.py:315 -- The `process_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:03:44,958\tWARNING util.py:315 -- Processing trial results took 1.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:03:44,961\tWARNING util.py:315 -- The `process_trial_result` operation took 1.887 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_e8e097f7_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-03-29/wandb/run-20230807_160347-e8e097f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Syncing run FSR_Trainable_e8e097f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e8e097f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                      mae █▇▆▅▄▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                     mape █▇▆▅▄▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                     rmse █▇▆▅▄▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:         time_this_iter_s █▄▃▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                timestamp ▁▃▃▅▅▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                      mae 0.32787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                     mape 0.09274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                     rmse 0.68614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:       time_since_restore 15.12868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:         time_this_iter_s 1.74963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:             time_total_s 15.12868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:                timestamp 1691391837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: 🚀 View run FSR_Trainable_e8e097f7 at: https://wandb.ai/seokjin/FSR-prediction/runs/e8e097f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6725)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160347-e8e097f7/logs\n",
      "2023-08-07 16:04:13,706\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:13,710\tWARNING util.py:315 -- The `process_trial_result` operation took 2.008 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:13,712\tWARNING util.py:315 -- Processing trial results took 2.010 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:04:13,714\tWARNING util.py:315 -- The `process_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_c989bf2b_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-03-40/wandb/run-20230807_160416-c989bf2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Syncing run FSR_Trainable_c989bf2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c989bf2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                      mae █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:         time_this_iter_s █▅▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                      mae 0.33085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                     mape 0.09373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                     rmse 0.69092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:       time_since_restore 8.52894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:         time_this_iter_s 1.7995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:             time_total_s 8.52894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:                timestamp 1691391859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: 🚀 View run FSR_Trainable_c989bf2b at: https://wandb.ai/seokjin/FSR-prediction/runs/c989bf2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7038)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160416-c989bf2b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:                      mae ███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:                     mape ███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:                     rmse ████▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:         time_this_iter_s █▆▃▂▃▂▃▂▃▂▂▃▁▁▂▃▂▂▂▂▂▂▁▁▅▃▂▆▄▂▂▂▂▂▂▃▃▄█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: 🚀 View run FSR_Trainable_ebbdefae at: https://wandb.ai/seokjin/FSR-prediction/runs/ebbdefae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160229-ebbdefae/logs\n",
      "2023-08-07 16:04:32,859\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.114 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:32,862\tWARNING util.py:315 -- The `process_trial_result` operation took 2.118 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:32,865\tWARNING util.py:315 -- Processing trial results took 2.121 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:04:32,868\tWARNING util.py:315 -- The `process_trial_result` operation took 2.124 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6242)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_7a94c05f_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-04-09/wandb/run-20230807_160436-7a94c05f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Syncing run FSR_Trainable_7a94c05f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7a94c05f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:         time_this_iter_s █▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                      mae 0.34162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                     mape 0.09466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                     rmse 0.69035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:       time_since_restore 3.98446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:         time_this_iter_s 0.88668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:             time_total_s 3.98446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:                timestamp 1691391875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: 🚀 View run FSR_Trainable_7a94c05f at: https://wandb.ai/seokjin/FSR-prediction/runs/7a94c05f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160436-7a94c05f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7284)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:                      mae ▃▂▂▂▁▁▁▁▂▂▃▄▄▅▆▆▆▇▇▇▇▇▇▇▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:                     mape ▃▃▂▂▂▁▁▁▂▃▃▄▄▅▆▆▇▇▇▇▇▇▇▇████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:                     rmse ▇▇▆▆▆▅▅▄▂▁▁▁▂▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:         time_this_iter_s █▅▄▃▇▆▄▄▄▃▃▃▄▆▃▃▃▆▃▅▆▄▂▂▂▁▁▄▆▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:                timestamp ▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160336-a9e85ed5/logs\n",
      "2023-08-07 16:04:45,171\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.696 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:45,177\tWARNING util.py:315 -- The `process_trial_result` operation took 2.703 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:45,185\tWARNING util.py:315 -- Processing trial results took 2.711 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:04:45,188\tWARNING util.py:315 -- The `process_trial_result` operation took 2.714 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=6507)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_c058ddce_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-04-29/wandb/run-20230807_160448-c058ddce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Syncing run FSR_Trainable_c058ddce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c058ddce\n",
      "2023-08-07 16:04:57,252\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:57,258\tWARNING util.py:315 -- The `process_trial_result` operation took 1.807 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:04:57,260\tWARNING util.py:315 -- Processing trial results took 1.809 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:04:57,261\tWARNING util.py:315 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_3839720d_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-04-41/wandb/run-20230807_160500-3839720d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Syncing run FSR_Trainable_3839720d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3839720d\n",
      "2023-08-07 16:05:10,148\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:05:10,151\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:05:10,156\tWARNING util.py:315 -- Processing trial results took 2.084 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:05:10,158\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_4ffc8f81_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-04-53/wandb/run-20230807_160513-4ffc8f81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Syncing run FSR_Trainable_4ffc8f81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ffc8f81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                      mae ▃▃▂▂▂▂▁▁▁▁▁▁▁▁▂▂▂▃▃▄▄▅▅▆▆▆▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                     mape ▄▄▃▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▂▂▃▄▄▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                     rmse ▇▆▆▆▅▅▅▄▄▄▃▃▂▂▁▁▁▁▂▃▃▄▄▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:         time_this_iter_s ▆█▃▃▂▂▁▂▁▁▁▁▆▄▄▄▅▄▄▂▄▂▃▃▄▅▄▅▄▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                      mae 0.37323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                     mape 0.09812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                     rmse 0.68871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:       time_since_restore 27.24326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:         time_this_iter_s 0.91963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:             time_total_s 27.24326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:                timestamp 1691391914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: 🚀 View run FSR_Trainable_c058ddce at: https://wandb.ai/seokjin/FSR-prediction/runs/c058ddce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7506)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160448-c058ddce/logs\n",
      "2023-08-07 16:05:29,787\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.001 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:05:29,792\tWARNING util.py:315 -- The `process_trial_result` operation took 2.007 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:05:29,794\tWARNING util.py:315 -- Processing trial results took 2.008 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:05:29,796\tWARNING util.py:315 -- The `process_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_c5867aff_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-05-06/wandb/run-20230807_160532-c5867aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Syncing run FSR_Trainable_c5867aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c5867aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                      mae 0.36779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                     mape 0.11212\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                     rmse 0.7003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:       time_since_restore 1.61198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:         time_this_iter_s 1.61198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:             time_total_s 1.61198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:                timestamp 1691391927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: 🚀 View run FSR_Trainable_c5867aff at: https://wandb.ai/seokjin/FSR-prediction/runs/c5867aff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8180)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160532-c5867aff/logs\n",
      "2023-08-07 16:05:45,104\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.069 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:05:45,109\tWARNING util.py:315 -- The `process_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:05:45,111\tWARNING util.py:315 -- Processing trial results took 2.077 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:05:45,114\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_dcbc59db_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-05-26/wandb/run-20230807_160548-dcbc59db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Syncing run FSR_Trainable_dcbc59db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dcbc59db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                      mae 0.35642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                     mape 0.10155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                     rmse 0.69653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:       time_since_restore 1.44572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:         time_this_iter_s 1.44572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:             time_total_s 1.44572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:                timestamp 1691391943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: 🚀 View run FSR_Trainable_dcbc59db at: https://wandb.ai/seokjin/FSR-prediction/runs/dcbc59db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8408)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160548-dcbc59db/logs\n",
      "2023-08-07 16:06:01,758\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.995 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:01,764\tWARNING util.py:315 -- The `process_trial_result` operation took 3.002 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:01,768\tWARNING util.py:315 -- Processing trial results took 3.006 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:06:01,772\tWARNING util.py:315 -- The `process_trial_result` operation took 3.010 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_6bb59630_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-05-41/wandb/run-20230807_160607-6bb59630\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Syncing run FSR_Trainable_6bb59630\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6bb59630\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                      mae 0.3628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                     mape 0.10161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                     rmse 0.69603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:       time_since_restore 1.43871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:         time_this_iter_s 1.43871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:             time_total_s 1.43871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:                timestamp 1691391958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: 🚀 View run FSR_Trainable_6bb59630 at: https://wandb.ai/seokjin/FSR-prediction/runs/6bb59630\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8636)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160607-6bb59630/logs\n",
      "2023-08-07 16:06:21,060\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.587 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:21,065\tWARNING util.py:315 -- The `process_trial_result` operation took 2.593 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:21,069\tWARNING util.py:315 -- Processing trial results took 2.597 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:06:21,071\tWARNING util.py:315 -- The `process_trial_result` operation took 2.599 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_e53d7657_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-05-57/wandb/run-20230807_160624-e53d7657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Syncing run FSR_Trainable_e53d7657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e53d7657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                      mae 0.34208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                     mape 0.09683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                     rmse 0.69617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:       time_since_restore 1.05507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:         time_this_iter_s 1.05507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:             time_total_s 1.05507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:                timestamp 1691391978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: 🚀 View run FSR_Trainable_e53d7657 at: https://wandb.ai/seokjin/FSR-prediction/runs/e53d7657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=8867)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160624-e53d7657/logs\n",
      "2023-08-07 16:06:38,439\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:38,443\tWARNING util.py:315 -- The `process_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:38,445\tWARNING util.py:315 -- Processing trial results took 2.021 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:06:38,447\tWARNING util.py:315 -- The `process_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_67b698a5_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-06-17/wandb/run-20230807_160641-67b698a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Syncing run FSR_Trainable_67b698a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/67b698a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                      mae ▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▄▅▅▅▅▅▅▅▅▆▇▇▇███████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                     mape ▂▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▅▆▆▆▅▅▅▅▆▆▇▇▇▇▇█████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                     rmse ▄▄▄▃▃▃▃▃▃▃▃▂▂▁▁▁▂▂▂▂▂▁▁▁▂▃▅▆▇▇▇█████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:         time_this_iter_s ▅▃▃▁▃▂▂▂▂▆▆▁▂▁▂▃▄▂▇█▂▂▂▁▂▁▂▁▂▂▁▁▁▂▁▂▃▅▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                      mae 0.38263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                     mape 0.10096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                     rmse 0.69268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:       time_since_restore 543.2451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:         time_this_iter_s 5.7557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:             time_total_s 543.2451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:                timestamp 1691391999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: 🚀 View run FSR_Trainable_640db335 at: https://wandb.ai/seokjin/FSR-prediction/runs/640db335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3473)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_155727-640db335/logs\n",
      "2023-08-07 16:06:57,696\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:57,698\tWARNING util.py:315 -- The `process_trial_result` operation took 1.829 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:06:57,702\tWARNING util.py:315 -- Processing trial results took 1.833 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:06:57,704\tWARNING util.py:315 -- The `process_trial_result` operation took 1.836 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_57bbd80f_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-06-34/wandb/run-20230807_160657-57bbd80f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Syncing run FSR_Trainable_57bbd80f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/57bbd80f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                      mae 0.32941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                     mape 0.09343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                     rmse 0.69249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:       time_since_restore 9.90927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:         time_this_iter_s 4.17122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:             time_total_s 9.90927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:                timestamp 1691392021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: 🚀 View run FSR_Trainable_57bbd80f at: https://wandb.ai/seokjin/FSR-prediction/runs/57bbd80f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9325)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160657-57bbd80f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                      mae ▇▆▆▆▅▅▄▄▄▃▃▂▂▁▁▁▂▂▃▃▃▄▄▄▄▅▅▅▆▆▇▇▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                     mape ▅▄▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁▂▂▂▂▃▃▄▄▄▅▅▆▆▇▇▇███████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                     rmse ██▇▇▆▆▅▅▅▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:         time_this_iter_s █▃▂▂▃▃▁▂▂▂▄▂▂▂▁▄▄▃▂▂▄▅▅▃▂▅▃▄▃▆▃▃▁▂▆▆▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                      mae 0.3482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                     mape 0.09873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                     rmse 0.67746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:       time_since_restore 118.70619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:         time_this_iter_s 0.93826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:             time_total_s 118.70619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:                timestamp 1691392028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: 🚀 View run FSR_Trainable_3839720d at: https://wandb.ai/seokjin/FSR-prediction/runs/3839720d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7733)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160500-3839720d/logs\n",
      "2023-08-07 16:07:19,054\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:07:19,058\tWARNING util.py:315 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:07:19,059\tWARNING util.py:315 -- Processing trial results took 1.904 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:07:19,061\tWARNING util.py:315 -- The `process_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_bb97f09f_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-06-50/wandb/run-20230807_160719-bb97f09f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Syncing run FSR_Trainable_bb97f09f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb97f09f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                      mae ██▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                     mape ██▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▄▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                     rmse ██▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:         time_this_iter_s █▅▃▄▄▄▄▄▄▄▂▄▅▄▄▄▆▆▇▆▃▆▅▅▅█▅▄▃▃██▄▄▃▄▁▁▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                      mae 0.33253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                     mape 0.09545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                     rmse 0.68118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:       time_since_restore 116.65009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:         time_this_iter_s 1.15721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:             time_total_s 116.65009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:                timestamp 1691392041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: 🚀 View run FSR_Trainable_4ffc8f81 at: https://wandb.ai/seokjin/FSR-prediction/runs/4ffc8f81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=7945)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160513-4ffc8f81/logs\n",
      "2023-08-07 16:07:27,293\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:07:27,296\tWARNING util.py:315 -- The `process_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:07:27,299\tWARNING util.py:315 -- Processing trial results took 2.222 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:07:27,301\tWARNING util.py:315 -- The `process_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_754545de_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-07-12/wandb/run-20230807_160730-754545de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Syncing run FSR_Trainable_754545de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/754545de\n",
      "2023-08-07 16:07:37,982\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.761 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:07:37,985\tWARNING util.py:315 -- The `process_trial_result` operation took 1.765 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:07:37,987\tWARNING util.py:315 -- Processing trial results took 1.767 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:07:37,988\tWARNING util.py:315 -- The `process_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_bd38f24a_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-07-23/wandb/run-20230807_160741-bd38f24a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Syncing run FSR_Trainable_bd38f24a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bd38f24a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                      mae ▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                     mape ▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                     rmse ▆▆▅▅▅▅▅▄▄▃▃▃▂▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▄▄▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:         time_this_iter_s █▄▃▂▂▄▇▃▃▃▂▃▂▁▅▃▅▃▄▃▃▄▄▃▃▃▃▂▄▃▃▂▃▂▂▂▃▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                      mae 0.3731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                     mape 0.09961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                     rmse 0.68926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:       time_since_restore 101.31756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:         time_this_iter_s 1.52144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:             time_total_s 101.31756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:                timestamp 1691392103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: 🚀 View run FSR_Trainable_67b698a5 at: https://wandb.ai/seokjin/FSR-prediction/runs/67b698a5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9095)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160641-67b698a5/logs\n",
      "2023-08-07 16:08:38,347\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:08:38,352\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:08:38,354\tWARNING util.py:315 -- Processing trial results took 1.979 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:08:38,357\tWARNING util.py:315 -- The `process_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_26f78e47_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-07-34/wandb/run-20230807_160841-26f78e47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Syncing run FSR_Trainable_26f78e47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/26f78e47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                      mae 0.37567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                     mape 0.10422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                     rmse 0.70682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:       time_since_restore 1.82283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:         time_this_iter_s 1.82283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:             time_total_s 1.82283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:                timestamp 1691392116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: 🚀 View run FSR_Trainable_26f78e47 at: https://wandb.ai/seokjin/FSR-prediction/runs/26f78e47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10264)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160841-26f78e47/logs\n",
      "2023-08-07 16:08:53,721\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:08:53,724\tWARNING util.py:315 -- The `process_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:08:53,726\tWARNING util.py:315 -- Processing trial results took 1.956 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:08:53,729\tWARNING util.py:315 -- The `process_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_93194e63_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-08-34/wandb/run-20230807_160856-93194e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Syncing run FSR_Trainable_93194e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/93194e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                      mae 0.38003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                     mape 0.09892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                     rmse 0.71022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:       time_since_restore 1.76379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:         time_this_iter_s 1.76379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:             time_total_s 1.76379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:                timestamp 1691392131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: 🚀 View run FSR_Trainable_93194e63 at: https://wandb.ai/seokjin/FSR-prediction/runs/93194e63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10491)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160856-93194e63/logs\n",
      "2023-08-07 16:09:09,877\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:09:09,880\tWARNING util.py:315 -- The `process_trial_result` operation took 2.219 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:09:09,881\tWARNING util.py:315 -- Processing trial results took 2.220 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:09:09,882\tWARNING util.py:315 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_95c6210f_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-08-50/wandb/run-20230807_160913-95c6210f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Syncing run FSR_Trainable_95c6210f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/95c6210f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                      mae 0.39648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                     mape 0.09034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                     rmse 0.72942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:       time_since_restore 1.07406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:         time_this_iter_s 1.07406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:             time_total_s 1.07406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:                timestamp 1691392147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: 🚀 View run FSR_Trainable_95c6210f at: https://wandb.ai/seokjin/FSR-prediction/runs/95c6210f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10720)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160913-95c6210f/logs\n",
      "2023-08-07 16:09:32,216\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.098 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:09:32,220\tWARNING util.py:315 -- The `process_trial_result` operation took 3.103 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:09:32,224\tWARNING util.py:315 -- Processing trial results took 3.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:09:32,226\tWARNING util.py:315 -- The `process_trial_result` operation took 3.109 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_b1964871_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-09-06/wandb/run-20230807_160936-b1964871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Syncing run FSR_Trainable_b1964871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b1964871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                      mae 0.42485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                     mape 0.11251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                     rmse 0.73485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:       time_since_restore 1.06293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:         time_this_iter_s 1.06293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:             time_total_s 1.06293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:                timestamp 1691392169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: 🚀 View run FSR_Trainable_b1964871 at: https://wandb.ai/seokjin/FSR-prediction/runs/b1964871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10948)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160936-b1964871/logs\n",
      "2023-08-07 16:09:48,699\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.927 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:09:48,703\tWARNING util.py:315 -- The `process_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:09:48,705\tWARNING util.py:315 -- Processing trial results took 1.934 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:09:48,706\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_096db0c5_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-09-28/wandb/run-20230807_160951-096db0c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Syncing run FSR_Trainable_096db0c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/096db0c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                      mae 0.35156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                     mape 0.09235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                     rmse 0.71041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:       time_since_restore 1.21653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:         time_this_iter_s 1.21653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:             time_total_s 1.21653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:                timestamp 1691392186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: 🚀 View run FSR_Trainable_096db0c5 at: https://wandb.ai/seokjin/FSR-prediction/runs/096db0c5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11177)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160951-096db0c5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 16:10:05,887\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.897 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:05,890\tWARNING util.py:315 -- The `process_trial_result` operation took 2.901 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:05,893\tWARNING util.py:315 -- Processing trial results took 2.904 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:10:05,908\tWARNING util.py:315 -- The `process_trial_result` operation took 2.919 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                      mae ▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▂▃▃▄▄▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                     mape ▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▂▂▃▃▄▄▅▆▆▆▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                     rmse ██████▇▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:         time_this_iter_s ▄▂▂▂▂▁▁▁▃▂▁▂▁▂▁▂▂▂▃▂▂▂▁▂▄▂▃▄▃▃▇█▇▃▂▁▂▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                      mae 0.3415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                     mape 0.0955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                     rmse 0.67857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:       time_since_restore 144.88848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:         time_this_iter_s 1.59371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:             time_total_s 144.88848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:                timestamp 1691392202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: 🚀 View run FSR_Trainable_754545de at: https://wandb.ai/seokjin/FSR-prediction/runs/754545de\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9786)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160730-754545de/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_0001f0fa_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-09-45/wandb/run-20230807_161009-0001f0fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Syncing run FSR_Trainable_0001f0fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0001f0fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                      mae 0.34731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                     mape 0.09545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                     rmse 0.70002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:       time_since_restore 1.3535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:         time_this_iter_s 1.3535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:             time_total_s 1.3535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:                timestamp 1691392202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: 🚀 View run FSR_Trainable_0001f0fa at: https://wandb.ai/seokjin/FSR-prediction/runs/0001f0fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11405)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161009-0001f0fa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 16:10:21,177\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.565 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:21,179\tWARNING util.py:315 -- The `process_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:21,182\tWARNING util.py:315 -- Processing trial results took 2.570 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:10:21,183\tWARNING util.py:315 -- The `process_trial_result` operation took 2.572 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:                      mae ▇▇▆▆▆▆▅▅▅▄▄▃▃▂▂▁▁▁▂▂▃▃▄▄▄▅▅▆▆▆▇▇▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:                     mape ██▇▇▇▇▆▆▅▅▅▄▄▃▂▁▁▁▂▂▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:                     rmse ██▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:         time_this_iter_s ▅▂▁▂▁▁▂▂▂▂▂▁▂▂▁▂▂▄▂▂▂▃▂▁▄▄▃▆█▄▃▃▁▂▂▃▄▄▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=10006)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160741-bd38f24a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_cc096752_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-10-01/wandb/run-20230807_161024-cc096752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Syncing run FSR_Trainable_cc096752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc096752\n",
      "2023-08-07 16:10:33,082\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.103 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:33,086\tWARNING util.py:315 -- The `process_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:33,089\tWARNING util.py:315 -- Processing trial results took 2.111 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:10:33,091\tWARNING util.py:315 -- The `process_trial_result` operation took 2.113 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_4998c260_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-10-16/wandb/run-20230807_161036-4998c260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Syncing run FSR_Trainable_4998c260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4998c260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                      mae 0.35015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                     mape 0.09483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                     rmse 0.69082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:       time_since_restore 2.86396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:         time_this_iter_s 1.22114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:             time_total_s 2.86396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:                timestamp 1691392234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: 🚀 View run FSR_Trainable_4998c260 at: https://wandb.ai/seokjin/FSR-prediction/runs/4998c260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11865)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161036-4998c260/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:                      mae █▇▇▆▆▅▅▄▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:                     mape █▇▇▆▆▅▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:                     rmse █▇▇▆▆▅▅▄▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:       time_since_restore ▁▂▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:         time_this_iter_s █▃▂▁▂▁▁▁▂▃▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:             time_total_s ▁▂▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:                timestamp ▁▂▃▃▃▄▄▅▅▅▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161024-cc096752/logs\n",
      "2023-08-07 16:10:43,066\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:43,071\tWARNING util.py:315 -- The `process_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:43,074\tWARNING util.py:315 -- Processing trial results took 1.883 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:10:43,079\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_e8b9d18d_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-10-29/wandb/run-20230807_161045-e8b9d18d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Syncing run FSR_Trainable_e8b9d18d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e8b9d18d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=11640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                      mae 2.43472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                     mape 0.40574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                     rmse 2.74879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:       time_since_restore 1.59094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:         time_this_iter_s 1.59094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:             time_total_s 1.59094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:                timestamp 1691392241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: 🚀 View run FSR_Trainable_e8b9d18d at: https://wandb.ai/seokjin/FSR-prediction/runs/e8b9d18d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161045-e8b9d18d/logs\n",
      "2023-08-07 16:10:51,981\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:51,987\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:10:51,989\tWARNING util.py:315 -- Processing trial results took 2.088 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:10:51,990\tWARNING util.py:315 -- The `process_trial_result` operation took 2.090 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12086)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_477c79e4_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-10-39/wandb/run-20230807_161054-477c79e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Syncing run FSR_Trainable_477c79e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/477c79e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                      mae 1.92801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                     mape 0.2866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                     rmse 2.27815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:       time_since_restore 1.5944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:         time_this_iter_s 1.5944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:             time_total_s 1.5944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:                timestamp 1691392249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: 🚀 View run FSR_Trainable_477c79e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/477c79e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161054-477c79e4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12318)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 16:11:03,758\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:03,763\tWARNING util.py:315 -- The `process_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:03,765\tWARNING util.py:315 -- Processing trial results took 1.921 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:11:03,767\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_d55d26c1_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-10-48/wandb/run-20230807_161103-d55d26c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Syncing run FSR_Trainable_d55d26c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d55d26c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                      mae 2.43655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                     mape 0.36202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                     rmse 2.74811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:       time_since_restore 4.43387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:         time_this_iter_s 4.43387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:             time_total_s 4.43387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:                timestamp 1691392261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: 🚀 View run FSR_Trainable_d55d26c1 at: https://wandb.ai/seokjin/FSR-prediction/runs/d55d26c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12541)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161103-d55d26c1/logs\n",
      "2023-08-07 16:11:12,749\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:12,753\tWARNING util.py:315 -- The `process_trial_result` operation took 1.729 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:12,755\tWARNING util.py:315 -- Processing trial results took 1.731 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:11:12,757\tWARNING util.py:315 -- The `process_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_95d591f0_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-10-57/wandb/run-20230807_161112-95d591f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Syncing run FSR_Trainable_95d591f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/95d591f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_650765f6_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-11-06/wandb/run-20230807_161123-650765f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Syncing run FSR_Trainable_650765f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/650765f6\n",
      "2023-08-07 16:11:27,739\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.743 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:27,741\tWARNING util.py:315 -- The `process_trial_result` operation took 1.746 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:27,744\tWARNING util.py:315 -- Processing trial results took 1.749 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:11:27,745\tWARNING util.py:315 -- The `process_trial_result` operation took 1.750 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_2e7b25e1_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-11-16/wandb/run-20230807_161135-2e7b25e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Syncing run FSR_Trainable_2e7b25e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e7b25e1\n",
      "2023-08-07 16:11:40,783\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:40,785\tWARNING util.py:315 -- The `process_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:11:40,787\tWARNING util.py:315 -- Processing trial results took 2.086 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:11:40,790\tWARNING util.py:315 -- The `process_trial_result` operation took 2.089 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                      mae ▃▃▃▂▂▁▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                     mape ▆▆▅▄▃▁▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                     rmse ▁▁▁▁▁▁▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:         time_this_iter_s ▅▁█▄█▇▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                      mae 0.34552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                     mape 0.09917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                     rmse 0.68553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:       time_since_restore 36.66506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:         time_this_iter_s 4.28678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:             time_total_s 36.66506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:                timestamp 1691392306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: 🚀 View run FSR_Trainable_95d591f0 at: https://wandb.ai/seokjin/FSR-prediction/runs/95d591f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12762)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161112-95d591f0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_28d87d6f_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-11-27/wandb/run-20230807_161206-28d87d6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Syncing run FSR_Trainable_28d87d6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/28d87d6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                      mae ▂▂▂▂▂▁▁▁▁▂▃▄▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                     mape ▂▂▁▁▁▁▁▁▁▂▄▅▇████▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                     rmse ▅▅▄▄▄▄▄▃▂▁▂▃▆▇▇▆▆▅▄▄▃▃▃▃▃▃▄▄▅▅▅▆▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:         time_this_iter_s ▅▄▄▄▃▃▃▃▃▃▄▃▃▃▄▅▅█▅▇▃▃▅▄▄▃▄▃▁▂▁▃▂▂▅▃▄▄▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                      mae 0.35498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                     mape 0.09817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                     rmse 0.68813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:       time_since_restore 275.69791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:         time_this_iter_s 5.2887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:             time_total_s 275.69791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:                timestamp 1691392324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: 🚀 View run FSR_Trainable_bb97f09f at: https://wandb.ai/seokjin/FSR-prediction/runs/bb97f09f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=9565)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_160719-bb97f09f/logs\n",
      "2023-08-07 16:12:11,933\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:11,935\tWARNING util.py:315 -- The `process_trial_result` operation took 2.002 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:11,939\tWARNING util.py:315 -- Processing trial results took 2.006 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:12:11,941\tWARNING util.py:315 -- The `process_trial_result` operation took 2.007 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:20,579\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.640 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:20,585\tWARNING util.py:315 -- The `process_trial_result` operation took 2.647 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:20,588\tWARNING util.py:315 -- Processing trial results took 2.650 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:12:20,592\tWARNING util.py:315 -- The `process_trial_result` operation took 2.653 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_3b5879d2_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-11-58/wandb/run-20230807_161224-3b5879d2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Syncing run FSR_Trainable_3b5879d2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3b5879d2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                      mae 0.37648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                     mape 0.10694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                     rmse 0.69692\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:       time_since_restore 2.05298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:         time_this_iter_s 2.05298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:             time_total_s 2.05298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:                timestamp 1691392337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: 🚀 View run FSR_Trainable_3b5879d2 at: https://wandb.ai/seokjin/FSR-prediction/runs/3b5879d2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13642)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161224-3b5879d2/logs\n",
      "2023-08-07 16:12:37,981\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.731 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:37,985\tWARNING util.py:315 -- The `process_trial_result` operation took 2.737 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:37,990\tWARNING util.py:315 -- Processing trial results took 2.741 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:12:37,991\tWARNING util.py:315 -- The `process_trial_result` operation took 2.743 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_2d25cf1a_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-12-15/wandb/run-20230807_161241-2d25cf1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Syncing run FSR_Trainable_2d25cf1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2d25cf1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                      mae 0.34575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                     mape 0.09614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                     rmse 0.68927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:       time_since_restore 3.12668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:         time_this_iter_s 1.44875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:             time_total_s 3.12668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:                timestamp 1691392359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: 🚀 View run FSR_Trainable_2d25cf1a at: https://wandb.ai/seokjin/FSR-prediction/runs/2d25cf1a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13865)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161241-2d25cf1a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 16:12:55,431\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.244 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:55,435\tWARNING util.py:315 -- The `process_trial_result` operation took 2.249 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:12:55,437\tWARNING util.py:315 -- Processing trial results took 2.251 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:12:55,439\tWARNING util.py:315 -- The `process_trial_result` operation took 2.253 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                      mae █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                     mape █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                     rmse █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:         time_this_iter_s █▁▃▅▄▆▆▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                      mae 0.31271\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                     mape 0.09181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                     rmse 0.685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:       time_since_restore 80.84743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:         time_this_iter_s 9.7433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:             time_total_s 80.84743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:                timestamp 1691392372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: 🚀 View run FSR_Trainable_2e7b25e1 at: https://wandb.ai/seokjin/FSR-prediction/runs/2e7b25e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13187)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161135-2e7b25e1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_c704d36f_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-12-33/wandb/run-20230807_161259-c704d36f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Syncing run FSR_Trainable_c704d36f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c704d36f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                      mae 0.34111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                     mape 0.09408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                     rmse 0.69649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:       time_since_restore 1.67832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:         time_this_iter_s 1.67832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:             time_total_s 1.67832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:                timestamp 1691392373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: 🚀 View run FSR_Trainable_c704d36f at: https://wandb.ai/seokjin/FSR-prediction/runs/c704d36f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14089)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161259-c704d36f/logs\n",
      "2023-08-07 16:13:10,193\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.490 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:13:10,197\tWARNING util.py:315 -- The `process_trial_result` operation took 2.496 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:13:10,200\tWARNING util.py:315 -- Processing trial results took 2.498 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:13:10,202\tWARNING util.py:315 -- The `process_trial_result` operation took 2.500 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_f99e21db_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-12-51/wandb/run-20230807_161313-f99e21db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: Syncing run FSR_Trainable_f99e21db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f99e21db\n",
      "2023-08-07 16:13:23,199\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.447 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:13:23,204\tWARNING util.py:315 -- The `process_trial_result` operation took 2.453 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:13:23,206\tWARNING util.py:315 -- Processing trial results took 2.455 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:13:23,209\tWARNING util.py:315 -- The `process_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_10471426_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-13-05/wandb/run-20230807_161326-10471426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Syncing run FSR_Trainable_10471426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/10471426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                      mae ▁▄▄▄▄▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                     mape ▁▂▅▅▄▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                     rmse ▃▁▂▂▁▂▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:         time_this_iter_s █▁▅▅▁▁▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                      mae 0.40517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                     mape 0.10659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                     rmse 0.7065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:       time_since_restore 84.26065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:         time_this_iter_s 10.58428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:             time_total_s 84.26065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:                timestamp 1691392407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: 🚀 View run FSR_Trainable_28d87d6f at: https://wandb.ai/seokjin/FSR-prediction/runs/28d87d6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=13418)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161206-28d87d6f/logs\n",
      "2023-08-07 16:13:43,547\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.868 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:13:43,549\tWARNING util.py:315 -- The `process_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:13:43,554\tWARNING util.py:315 -- Processing trial results took 1.876 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:13:43,556\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_518953d3_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-13-18/wandb/run-20230807_161346-518953d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Syncing run FSR_Trainable_518953d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/518953d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                      mae 0.3573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                     mape 0.0966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                     rmse 0.70696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:       time_since_restore 2.17491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:         time_this_iter_s 2.17491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:             time_total_s 2.17491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:                timestamp 1691392421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: 🚀 View run FSR_Trainable_518953d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/518953d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14766)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161346-518953d3/logs\n",
      "2023-08-07 16:14:00,231\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:00,237\tWARNING util.py:315 -- The `process_trial_result` operation took 2.072 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:00,238\tWARNING util.py:315 -- Processing trial results took 2.073 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:14:00,240\tWARNING util.py:315 -- The `process_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_f2951ccc_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-13-39/wandb/run-20230807_161403-f2951ccc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Syncing run FSR_Trainable_f2951ccc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f2951ccc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                      mae █▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                     mape █▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                     rmse █▇▇▆▅▅▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:         time_this_iter_s ▄▆▁▃▆▁█▄▄▅▂▇▅▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                      mae 0.31951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                     mape 0.09603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                     rmse 0.68329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:       time_since_restore 159.48312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:         time_this_iter_s 9.33869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:             time_total_s 159.48312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:                timestamp 1691392443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: 🚀 View run FSR_Trainable_650765f6 at: https://wandb.ai/seokjin/FSR-prediction/runs/650765f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=12977)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161123-650765f6/logs\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)3 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:                      mae ▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▂▃▄▄▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:                     mape ▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:                     rmse ▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▃▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:         time_this_iter_s █▅▇▅▅▃▇▄▆▄▆▄▃▃▃▁▃▂▄▇▆▆▅▄▅▃▂▃▆▅▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-08-07 16:14:15,128\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:15,132\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:15,134\tWARNING util.py:315 -- Processing trial results took 2.070 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:14:15,136\tWARNING util.py:315 -- The `process_trial_result` operation took 2.073 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14327)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: iterations_since_restore 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                      mae 0.35515\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                     mape 0.10163\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                     rmse 0.69714\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:       time_since_restore 1.86371\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:         time_this_iter_s 1.86371\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:             time_total_s 1.86371\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:                timestamp 1691392438\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb:       training_iteration 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: 🚀 View run FSR_Trainable_f2951ccc at: https://wandb.ai/seokjin/FSR-prediction/runs/f2951ccc\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14994)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161403-f2951ccc/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_dd647cf3_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-13-56/wandb/run-20230807_161417-dd647cf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Syncing run FSR_Trainable_dd647cf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dd647cf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15243)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161417-dd647cf3/logs\n",
      "2023-08-07 16:14:24,082\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.715 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:24,086\tWARNING util.py:315 -- The `process_trial_result` operation took 1.720 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:24,089\tWARNING util.py:315 -- Processing trial results took 1.723 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:14:24,092\tWARNING util.py:315 -- The `process_trial_result` operation took 1.725 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_bb3b8107_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-14-11/wandb/run-20230807_161426-bb3b8107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Syncing run FSR_Trainable_bb3b8107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb3b8107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                      mae 0.363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                     mape 0.09174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                     rmse 0.71561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:       time_since_restore 1.58725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:         time_this_iter_s 1.58725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:             time_total_s 1.58725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:                timestamp 1691392462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: 🚀 View run FSR_Trainable_bb3b8107 at: https://wandb.ai/seokjin/FSR-prediction/runs/bb3b8107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15460)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161426-bb3b8107/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 16:14:37,757\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.620 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:37,759\tWARNING util.py:315 -- The `process_trial_result` operation took 1.623 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:37,760\tWARNING util.py:315 -- Processing trial results took 1.625 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:14:37,761\tWARNING util.py:315 -- The `process_trial_result` operation took 1.626 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_a4520da0_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-14-20/wandb/run-20230807_161437-a4520da0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Syncing run FSR_Trainable_a4520da0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a4520da0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 16:14:49,264\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.403 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:49,268\tWARNING util.py:315 -- The `process_trial_result` operation took 2.408 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:14:49,269\tWARNING util.py:315 -- Processing trial results took 2.409 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:14:49,271\tWARNING util.py:315 -- The `process_trial_result` operation took 2.411 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_a6b2dc23_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-14-30/wandb/run-20230807_161448-a6b2dc23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Syncing run FSR_Trainable_a6b2dc23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a6b2dc23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 16:15:01,681\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.320 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:15:01,686\tWARNING util.py:315 -- The `process_trial_result` operation took 2.327 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:15:01,688\tWARNING util.py:315 -- Processing trial results took 2.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:15:01,692\tWARNING util.py:315 -- The `process_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_3a0adf05_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-14-40/wandb/run-20230807_161500-3a0adf05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Syncing run FSR_Trainable_3a0adf05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3a0adf05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                      mae █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                     mape █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                     rmse █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:         time_this_iter_s █▁▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                      mae 0.31278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                     mape 0.0914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                     rmse 0.68519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:       time_since_restore 24.56682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:         time_this_iter_s 6.0605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:             time_total_s 24.56682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:                timestamp 1691392519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: 🚀 View run FSR_Trainable_3a0adf05 at: https://wandb.ai/seokjin/FSR-prediction/runs/3a0adf05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16112)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161500-3a0adf05/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                      mae █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                     mape █▆▅▄▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                     rmse █▇▅▄▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:         time_this_iter_s █▇▄▃▄▅▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                      mae 0.31314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                     mape 0.09207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                     rmse 0.68457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:       time_since_restore 47.99014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:         time_this_iter_s 5.36463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:             time_total_s 47.99014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:                timestamp 1691392531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: 🚀 View run FSR_Trainable_a6b2dc23 at: https://wandb.ai/seokjin/FSR-prediction/runs/a6b2dc23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15902)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161448-a6b2dc23/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_e2dc6632_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-14-52/wandb/run-20230807_161540-e2dc6632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Syncing run FSR_Trainable_e2dc6632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2dc6632\n",
      "2023-08-07 16:15:42,400\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.185 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:15:42,406\tWARNING util.py:315 -- The `process_trial_result` operation took 3.192 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:15:42,408\tWARNING util.py:315 -- Processing trial results took 3.195 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:15:42,413\tWARNING util.py:315 -- The `process_trial_result` operation took 3.199 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_c14a090a_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-15-32/wandb/run-20230807_161555-c14a090a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Syncing run FSR_Trainable_c14a090a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c14a090a\n",
      "2023-08-07 16:15:58,841\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:15:58,842\tWARNING util.py:315 -- The `process_trial_result` operation took 2.046 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:15:58,843\tWARNING util.py:315 -- Processing trial results took 2.047 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:15:58,844\tWARNING util.py:315 -- The `process_trial_result` operation took 2.048 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                      mae ██▇▇▇▇▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                     mape ██▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                     rmse ███▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:         time_this_iter_s █▄▄▄▃▆▄▄▄▆▂▁▄▄▁▃▁▁▄▃▂▅▄▆▅▃▅▄▄▄▄▃▇▅▅▇▆▄▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                      mae 0.32784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                     mape 0.0977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                     rmse 0.6762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:       time_since_restore 149.31279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:         time_this_iter_s 1.66534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:             time_total_s 149.31279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:                timestamp 1691392568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: 🚀 View run FSR_Trainable_10471426 at: https://wandb.ai/seokjin/FSR-prediction/runs/10471426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=14537)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161326-10471426/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_a6d5cddc_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-15-46/wandb/run-20230807_161632-a6d5cddc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Syncing run FSR_Trainable_a6d5cddc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a6d5cddc\n",
      "2023-08-07 16:16:42,532\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.611 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:16:42,534\tWARNING util.py:315 -- The `process_trial_result` operation took 2.614 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:16:42,537\tWARNING util.py:315 -- Processing trial results took 2.616 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:16:42,540\tWARNING util.py:315 -- The `process_trial_result` operation took 2.619 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                      mae ▂▁▁▁▁▁▂▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                     mape ▃▃▂▂▁▁▃▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                     rmse ▆▆▆▆▅▄▁▃▃▃▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:         time_this_iter_s █▃▃▃▅▂▂▇▃▃▂▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                      mae 0.35961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                     mape 0.09892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                     rmse 0.68615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:       time_since_restore 142.45279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:         time_this_iter_s 8.31012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:             time_total_s 142.45279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:                timestamp 1691392691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: 🚀 View run FSR_Trainable_c14a090a at: https://wandb.ai/seokjin/FSR-prediction/runs/c14a090a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16575)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161555-c14a090a/logs\n",
      "2023-08-07 16:18:28,862\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.507 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:18:28,865\tWARNING util.py:315 -- The `process_trial_result` operation took 2.512 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:18:28,867\tWARNING util.py:315 -- Processing trial results took 2.514 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:18:28,868\tWARNING util.py:315 -- The `process_trial_result` operation took 2.515 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_6c5ff9e9_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-16-22/wandb/run-20230807_161832-6c5ff9e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Syncing run FSR_Trainable_6c5ff9e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6c5ff9e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                      mae 0.34887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                     mape 0.10516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                     rmse 0.68941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:       time_since_restore 4.02516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:         time_this_iter_s 1.90775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:             time_total_s 4.02516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:                timestamp 1691392710\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: 🚀 View run FSR_Trainable_6c5ff9e9 at: https://wandb.ai/seokjin/FSR-prediction/runs/6c5ff9e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17070)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161832-6c5ff9e9/logs\n",
      "2023-08-07 16:18:48,450\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.008 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:18:48,459\tWARNING util.py:315 -- The `process_trial_result` operation took 3.018 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:18:48,462\tWARNING util.py:315 -- Processing trial results took 3.021 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:18:48,464\tWARNING util.py:315 -- The `process_trial_result` operation took 3.023 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_5cd365a4_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-18-24/wandb/run-20230807_161852-5cd365a4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Syncing run FSR_Trainable_5cd365a4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cd365a4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)4 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                      mae 0.35105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                     mape 0.09574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                     rmse 0.69961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:       time_since_restore 1.7123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:         time_this_iter_s 1.7123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:             time_total_s 1.7123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:                timestamp 1691392725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: 🚀 View run FSR_Trainable_5cd365a4 at: https://wandb.ai/seokjin/FSR-prediction/runs/5cd365a4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17294)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161852-5cd365a4/logs\n",
      "2023-08-07 16:19:06,271\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.680 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:19:06,279\tWARNING util.py:315 -- The `process_trial_result` operation took 2.688 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:19:06,282\tWARNING util.py:315 -- Processing trial results took 2.691 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:19:06,285\tWARNING util.py:315 -- The `process_trial_result` operation took 2.694 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_f259722f_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-18-43/wandb/run-20230807_161910-f259722f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Syncing run FSR_Trainable_f259722f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f259722f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                      mae 2.85073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                     mape 0.4407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                     rmse 3.2069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:       time_since_restore 1.6813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:         time_this_iter_s 1.6813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:             time_total_s 1.6813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:                timestamp 1691392743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: 🚀 View run FSR_Trainable_f259722f at: https://wandb.ai/seokjin/FSR-prediction/runs/f259722f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17518)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161910-f259722f/logs\n",
      "2023-08-07 16:19:23,654\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.579 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:19:23,658\tWARNING util.py:315 -- The `process_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:19:23,660\tWARNING util.py:315 -- Processing trial results took 2.586 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:19:23,662\tWARNING util.py:315 -- The `process_trial_result` operation took 2.588 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_f137f0d6_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-19-02/wandb/run-20230807_161927-f137f0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Syncing run FSR_Trainable_f137f0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f137f0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                      mae 0.36403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                     mape 0.10409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                     rmse 0.68746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:       time_since_restore 3.08116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:         time_this_iter_s 1.45121\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:             time_total_s 3.08116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:                timestamp 1691392765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: 🚀 View run FSR_Trainable_f137f0d6 at: https://wandb.ai/seokjin/FSR-prediction/runs/f137f0d6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17740)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161927-f137f0d6/logs\n",
      "2023-08-07 16:19:42,849\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.420 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:19:42,852\tWARNING util.py:315 -- The `process_trial_result` operation took 3.424 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:19:42,854\tWARNING util.py:315 -- Processing trial results took 3.426 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:19:42,856\tWARNING util.py:315 -- The `process_trial_result` operation took 3.428 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_ee27e018_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-19-19/wandb/run-20230807_161946-ee27e018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Syncing run FSR_Trainable_ee27e018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ee27e018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                      mae 0.36974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                     mape 0.09912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                     rmse 0.6983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:       time_since_restore 1.56937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:         time_this_iter_s 1.56937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:             time_total_s 1.56937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:                timestamp 1691392779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: 🚀 View run FSR_Trainable_ee27e018 at: https://wandb.ai/seokjin/FSR-prediction/runs/ee27e018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=17967)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161946-ee27e018/logs\n",
      "2023-08-07 16:20:00,793\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.829 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:20:00,795\tWARNING util.py:315 -- The `process_trial_result` operation took 2.832 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:20:00,800\tWARNING util.py:315 -- Processing trial results took 2.837 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:20:00,802\tWARNING util.py:315 -- The `process_trial_result` operation took 2.839 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_e785abf7_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-19-37/wandb/run-20230807_162004-e785abf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Syncing run FSR_Trainable_e785abf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e785abf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                      mae ▂▂▂▁▁▁▁▄▆▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                     mape ▃▃▂▂▂▁▁▅███▇▇▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                     rmse ▂▂▂▂▂▂▁▃▇█████▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:         time_this_iter_s █▃▇▂▁▃▂▂▄▄▄▁▄▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                      mae 0.35576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                     mape 0.09913\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                     rmse 0.68574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:       time_since_restore 242.99614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:         time_this_iter_s 14.27924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:             time_total_s 242.99614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:                timestamp 1691392829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: 🚀 View run FSR_Trainable_a6d5cddc at: https://wandb.ai/seokjin/FSR-prediction/runs/a6d5cddc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16809)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161632-a6d5cddc/logs\n",
      "2023-08-07 16:20:46,382\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:20:46,386\tWARNING util.py:315 -- The `process_trial_result` operation took 1.904 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:20:46,387\tWARNING util.py:315 -- Processing trial results took 1.905 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:20:46,388\tWARNING util.py:315 -- The `process_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_e81dbf53_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-19-56/wandb/run-20230807_162049-e81dbf53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Syncing run FSR_Trainable_e81dbf53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e81dbf53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                      mae 0.34763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                     mape 0.0941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                     rmse 0.69091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:       time_since_restore 4.27192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:         time_this_iter_s 2.14623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:             time_total_s 4.27192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:                timestamp 1691392848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: 🚀 View run FSR_Trainable_e81dbf53 at: https://wandb.ai/seokjin/FSR-prediction/runs/e81dbf53\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18434)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162049-e81dbf53/logs\n",
      "2023-08-07 16:21:05,673\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.158 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:21:05,679\tWARNING util.py:315 -- The `process_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:21:05,681\tWARNING util.py:315 -- Processing trial results took 2.167 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:21:05,683\tWARNING util.py:315 -- The `process_trial_result` operation took 2.168 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_20422a55_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-20-42/wandb/run-20230807_162108-20422a55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Syncing run FSR_Trainable_20422a55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/20422a55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                      mae 0.36881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                     mape 0.09758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                     rmse 0.70288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:       time_since_restore 2.46349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:         time_this_iter_s 2.46349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:             time_total_s 2.46349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:                timestamp 1691392863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: 🚀 View run FSR_Trainable_20422a55 at: https://wandb.ai/seokjin/FSR-prediction/runs/20422a55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18666)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162108-20422a55/logs\n",
      "2023-08-07 16:21:22,490\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:21:22,497\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:21:22,499\tWARNING util.py:315 -- Processing trial results took 2.230 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:21:22,501\tWARNING util.py:315 -- The `process_trial_result` operation took 2.232 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_f433c207_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-21-01/wandb/run-20230807_162125-f433c207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Syncing run FSR_Trainable_f433c207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f433c207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                      mae 0.3428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                     mape 0.09437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                     rmse 0.6937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:       time_since_restore 2.14861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:         time_this_iter_s 2.14861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:             time_total_s 2.14861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:                timestamp 1691392880\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: 🚀 View run FSR_Trainable_f433c207 at: https://wandb.ai/seokjin/FSR-prediction/runs/f433c207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18892)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162125-f433c207/logs\n",
      "2023-08-07 16:21:38,459\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:21:38,462\tWARNING util.py:315 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:21:38,465\tWARNING util.py:315 -- Processing trial results took 2.065 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:21:38,467\tWARNING util.py:315 -- The `process_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_c1f87b41_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-21-18/wandb/run-20230807_162141-c1f87b41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Syncing run FSR_Trainable_c1f87b41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1f87b41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                      mae ██▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                     mape ██▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                     rmse ███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:         time_this_iter_s █▄▆▂▁▂▂▃▃▁▂▄▅▆▃▄▃▃▂▂▃▂▃▂▄▅▃▄▁▂▄▅▂▂▂▁▅▅▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                      mae 0.31807\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                     mape 0.09167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                     rmse 0.67633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:       time_since_restore 95.2566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:         time_this_iter_s 0.80503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:             time_total_s 95.2566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:                timestamp 1691392905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: 🚀 View run FSR_Trainable_e785abf7 at: https://wandb.ai/seokjin/FSR-prediction/runs/e785abf7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=18192)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162004-e785abf7/logs\n",
      "2023-08-07 16:22:02,210\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.307 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:02,214\tWARNING util.py:315 -- The `process_trial_result` operation took 2.311 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:02,217\tWARNING util.py:315 -- Processing trial results took 2.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:22:02,220\tWARNING util.py:315 -- The `process_trial_result` operation took 2.318 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)4 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                      mae ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                     mape ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                     rmse ██▇▇▆▆▅▅▄▄▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:         time_this_iter_s █▆▅▅▅▃█▄▁▅▅▄▄▁▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                      mae 0.325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                     mape 0.09303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                     rmse 0.68346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:       time_since_restore 22.71724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:         time_this_iter_s 1.49008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:             time_total_s 22.71724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:                timestamp 1691392920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: 🚀 View run FSR_Trainable_c1f87b41 at: https://wandb.ai/seokjin/FSR-prediction/runs/c1f87b41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19120)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162141-c1f87b41/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_38fe686a_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-21-34/wandb/run-20230807_162206-38fe686a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Syncing run FSR_Trainable_38fe686a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/38fe686a\n",
      "2023-08-07 16:22:19,185\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.172 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:19,191\tWARNING util.py:315 -- The `process_trial_result` operation took 2.179 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:19,193\tWARNING util.py:315 -- Processing trial results took 2.181 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:22:19,194\tWARNING util.py:315 -- The `process_trial_result` operation took 2.183 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_14abb1b6_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-21-58/wandb/run-20230807_162222-14abb1b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Syncing run FSR_Trainable_14abb1b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/14abb1b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                      mae 0.3591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                     mape 0.10679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                     rmse 0.69607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:       time_since_restore 1.20641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:         time_this_iter_s 1.20641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:             time_total_s 1.20641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:                timestamp 1691392937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: 🚀 View run FSR_Trainable_14abb1b6 at: https://wandb.ai/seokjin/FSR-prediction/runs/14abb1b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19577)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162222-14abb1b6/logs\n",
      "2023-08-07 16:22:36,620\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.823 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:36,626\tWARNING util.py:315 -- The `process_trial_result` operation took 2.830 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:36,628\tWARNING util.py:315 -- Processing trial results took 2.833 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:22:36,630\tWARNING util.py:315 -- The `process_trial_result` operation took 2.835 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_bad073d7_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-22-15/wandb/run-20230807_162240-bad073d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Syncing run FSR_Trainable_bad073d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bad073d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                      mae 0.36352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                     mape 0.10308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                     rmse 0.70004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:       time_since_restore 1.4932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:         time_this_iter_s 1.4932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:             time_total_s 1.4932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:                timestamp 1691392953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: 🚀 View run FSR_Trainable_bad073d7 at: https://wandb.ai/seokjin/FSR-prediction/runs/bad073d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19804)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162240-bad073d7/logs\n",
      "2023-08-07 16:22:53,370\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.309 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:53,374\tWARNING util.py:315 -- The `process_trial_result` operation took 2.314 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:22:53,375\tWARNING util.py:315 -- Processing trial results took 2.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:22:53,377\tWARNING util.py:315 -- The `process_trial_result` operation took 2.317 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_ea5fbd0a_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-22-32/wandb/run-20230807_162256-ea5fbd0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Syncing run FSR_Trainable_ea5fbd0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ea5fbd0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                      mae 0.37999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                     mape 0.09436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                     rmse 0.7061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:       time_since_restore 1.4205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:         time_this_iter_s 1.4205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:             time_total_s 1.4205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:                timestamp 1691392971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: 🚀 View run FSR_Trainable_ea5fbd0a at: https://wandb.ai/seokjin/FSR-prediction/runs/ea5fbd0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20027)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162256-ea5fbd0a/logs\n",
      "2023-08-07 16:23:08,900\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.520 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:08,905\tWARNING util.py:315 -- The `process_trial_result` operation took 2.526 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:08,907\tWARNING util.py:315 -- Processing trial results took 2.528 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:23:08,908\tWARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_dab74113_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-22-49/wandb/run-20230807_162312-dab74113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Syncing run FSR_Trainable_dab74113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dab74113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                      mae 0.3924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                     mape 0.10298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                     rmse 0.7169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:       time_since_restore 0.82761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:         time_this_iter_s 0.82761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:             time_total_s 0.82761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:                timestamp 1691392986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: 🚀 View run FSR_Trainable_dab74113 at: https://wandb.ai/seokjin/FSR-prediction/runs/dab74113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20256)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162312-dab74113/logs\n",
      "2023-08-07 16:23:26,124\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.574 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:26,130\tWARNING util.py:315 -- The `process_trial_result` operation took 3.584 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:26,136\tWARNING util.py:315 -- Processing trial results took 3.590 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:23:26,138\tWARNING util.py:315 -- The `process_trial_result` operation took 3.593 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_70a03e16_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-23-05/wandb/run-20230807_162330-70a03e16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Syncing run FSR_Trainable_70a03e16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/70a03e16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                      mae 0.377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                     mape 0.10468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                     rmse 0.71574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:       time_since_restore 0.88879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:         time_this_iter_s 0.88879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:             time_total_s 0.88879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:                timestamp 1691393002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: 🚀 View run FSR_Trainable_70a03e16 at: https://wandb.ai/seokjin/FSR-prediction/runs/70a03e16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20482)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162330-70a03e16/logs\n",
      "2023-08-07 16:23:44,082\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.341 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:44,089\tWARNING util.py:315 -- The `process_trial_result` operation took 2.349 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:44,091\tWARNING util.py:315 -- Processing trial results took 2.351 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:23:44,093\tWARNING util.py:315 -- The `process_trial_result` operation took 2.353 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_bd028dda_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-23-21/wandb/run-20230807_162347-bd028dda\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Syncing run FSR_Trainable_bd028dda\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bd028dda\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                      mae ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                     mape ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                     rmse ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:         time_this_iter_s ▅▅▆▃▁▁▁▂▄▂▂▁▂▅▄▂▂▁▂▄▃▂▃▁▁▂▂▂▂▂▂▅█▃▅▂▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                      mae 0.35949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                     mape 0.11031\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                     rmse 0.68197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:       time_since_restore 90.50099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:         time_this_iter_s 1.28788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:             time_total_s 90.50099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:                timestamp 1691393028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: 🚀 View run FSR_Trainable_38fe686a at: https://wandb.ai/seokjin/FSR-prediction/runs/38fe686a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162206-38fe686a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162347-bd028dda/logs\n",
      "2023-08-07 16:23:59,879\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.272 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:59,885\tWARNING util.py:315 -- The `process_trial_result` operation took 2.279 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:23:59,888\tWARNING util.py:315 -- Processing trial results took 2.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:23:59,889\tWARNING util.py:315 -- The `process_trial_result` operation took 2.283 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=19348)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=20851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_8df588ab_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-23-40/wandb/run-20230807_162403-8df588ab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Syncing run FSR_Trainable_8df588ab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8df588ab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                      mae 0.3633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                     mape 0.11009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                     rmse 0.69419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:       time_since_restore 1.84231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:         time_this_iter_s 1.84231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:             time_total_s 1.84231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:                timestamp 1691393037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: 🚀 View run FSR_Trainable_8df588ab at: https://wandb.ai/seokjin/FSR-prediction/runs/8df588ab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162403-8df588ab/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21092)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-07 16:24:13,268\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.697 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:24:13,272\tWARNING util.py:315 -- The `process_trial_result` operation took 2.701 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:24:13,274\tWARNING util.py:315 -- Processing trial results took 2.704 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:24:13,276\tWARNING util.py:315 -- The `process_trial_result` operation took 2.705 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_891ae335_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-23-55/wandb/run-20230807_162417-891ae335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Syncing run FSR_Trainable_891ae335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/891ae335\n",
      "2023-08-07 16:24:28,475\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:24:28,484\tWARNING util.py:315 -- The `process_trial_result` operation took 1.985 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:24:28,487\tWARNING util.py:315 -- Processing trial results took 1.988 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:24:28,489\tWARNING util.py:315 -- The `process_trial_result` operation took 1.990 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_e1bb9ee7_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-24-09/wandb/run-20230807_162430-e1bb9ee7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Syncing run FSR_Trainable_e1bb9ee7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e1bb9ee7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                      mae 0.35107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                     mape 0.09539\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                     rmse 0.70161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:       time_since_restore 2.76376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:         time_this_iter_s 2.76376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:             time_total_s 2.76376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:                timestamp 1691393066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: 🚀 View run FSR_Trainable_e1bb9ee7 at: https://wandb.ai/seokjin/FSR-prediction/runs/e1bb9ee7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21534)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162430-e1bb9ee7/logs\n",
      "2023-08-07 16:24:45,317\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:24:45,322\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:24:45,326\tWARNING util.py:315 -- Processing trial results took 2.232 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:24:45,328\tWARNING util.py:315 -- The `process_trial_result` operation took 2.234 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_143b8858_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-24-23/wandb/run-20230807_162449-143b8858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Syncing run FSR_Trainable_143b8858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/143b8858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                      mae 0.35671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                     mape 0.09722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                     rmse 0.69982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:       time_since_restore 1.84476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:         time_this_iter_s 1.84476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:             time_total_s 1.84476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:                timestamp 1691393083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: 🚀 View run FSR_Trainable_143b8858 at: https://wandb.ai/seokjin/FSR-prediction/runs/143b8858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21763)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162449-143b8858/logs\n",
      "2023-08-07 16:25:02,675\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.360 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:02,679\tWARNING util.py:315 -- The `process_trial_result` operation took 2.365 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:02,681\tWARNING util.py:315 -- Processing trial results took 2.368 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:25:02,684\tWARNING util.py:315 -- The `process_trial_result` operation took 2.370 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_08a74e1e_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-24-41/wandb/run-20230807_162506-08a74e1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Syncing run FSR_Trainable_08a74e1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/08a74e1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)8 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                      mae █▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                     mape ██▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                     rmse ███████▇▇▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:         time_this_iter_s ▂▃▂▄▅█▅▅▄▆▄▄▅▄▃▃▅▄▄▇▄▃▄▅▃▃▂▄▅▄▄▂▃▄▃▁▃▃▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                      mae 0.30914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                     mape 0.09108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                     rmse 0.67898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:       time_since_restore 610.57933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:         time_this_iter_s 6.01875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:             time_total_s 610.57933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:                timestamp 1691393104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: 🚀 View run FSR_Trainable_a4520da0 at: https://wandb.ai/seokjin/FSR-prediction/runs/a4520da0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=15686)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161437-a4520da0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162506-08a74e1e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162506-08a74e1e/logs\n",
      "2023-08-07 16:25:16,840\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:16,845\tWARNING util.py:315 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:16,848\tWARNING util.py:315 -- Processing trial results took 1.735 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:25:16,849\tWARNING util.py:315 -- The `process_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21988)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_626ed52c_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-24-58/wandb/run-20230807_162520-626ed52c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Syncing run FSR_Trainable_626ed52c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/626ed52c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                      mae 2.5958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                     mape 0.39027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                     rmse 2.90688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:       time_since_restore 1.44324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:         time_this_iter_s 1.44324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:             time_total_s 1.44324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:                timestamp 1691393115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: 🚀 View run FSR_Trainable_626ed52c at: https://wandb.ai/seokjin/FSR-prediction/runs/626ed52c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162520-626ed52c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22230)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-07 16:25:29,525\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:29,529\tWARNING util.py:315 -- The `process_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:29,530\tWARNING util.py:315 -- Processing trial results took 1.984 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:25:29,531\tWARNING util.py:315 -- The `process_trial_result` operation took 1.986 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_1d49ac61_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-25-13/wandb/run-20230807_162532-1d49ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Syncing run FSR_Trainable_1d49ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1d49ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                      mae 0.37104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                     mape 0.1048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                     rmse 0.70017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:       time_since_restore 1.61084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:         time_this_iter_s 1.61084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:             time_total_s 1.61084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:                timestamp 1691393127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: 🚀 View run FSR_Trainable_1d49ac61 at: https://wandb.ai/seokjin/FSR-prediction/runs/1d49ac61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22453)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162532-1d49ac61/logs\n",
      "2023-08-07 16:25:42,497\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.235 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:42,503\tWARNING util.py:315 -- The `process_trial_result` operation took 2.241 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:42,505\tWARNING util.py:315 -- Processing trial results took 2.244 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:25:42,506\tWARNING util.py:315 -- The `process_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_caec4d1e_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-25-26/wandb/run-20230807_162545-caec4d1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Syncing run FSR_Trainable_caec4d1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/caec4d1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                      mae 0.32712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                     mape 0.09533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                     rmse 0.68587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:       time_since_restore 2.90635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:         time_this_iter_s 1.34205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:             time_total_s 2.90635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:                timestamp 1691393143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: 🚀 View run FSR_Trainable_caec4d1e at: https://wandb.ai/seokjin/FSR-prediction/runs/caec4d1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22680)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162545-caec4d1e/logs\n",
      "2023-08-07 16:25:57,410\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.445 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:57,415\tWARNING util.py:315 -- The `process_trial_result` operation took 2.452 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:25:57,417\tWARNING util.py:315 -- Processing trial results took 2.453 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:25:57,418\tWARNING util.py:315 -- The `process_trial_result` operation took 2.455 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_42bc5664_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-25-38/wandb/run-20230807_162600-42bc5664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Syncing run FSR_Trainable_42bc5664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/42bc5664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                      mae █▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                     mape ██▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                     rmse ██▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:         time_this_iter_s ▆▇▃▆▄█▄▄▄▄▄▃▃▄▃▅▅▂▃▄▃▄▃▃▆▄▂▃▃▆▃▂▄▃▃▃▁▁▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                      mae 0.31422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                     mape 0.09464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                     rmse 0.68103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:       time_since_restore 604.01876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:         time_this_iter_s 6.13526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:             time_total_s 604.01876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:                timestamp 1691393160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: 🚀 View run FSR_Trainable_e2dc6632 at: https://wandb.ai/seokjin/FSR-prediction/runs/e2dc6632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=16354)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_161540-e2dc6632/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-08-07 16:26:07,475\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:07,479\tWARNING util.py:315 -- The `process_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:07,481\tWARNING util.py:315 -- Processing trial results took 1.885 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:26:07,482\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=22906)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_ebe4a81c_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-25-52/wandb/run-20230807_162610-ebe4a81c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Syncing run FSR_Trainable_ebe4a81c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ebe4a81c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                      mae 0.34618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                     mape 0.099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                     rmse 0.6941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:       time_since_restore 3.29254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:         time_this_iter_s 1.42106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:             time_total_s 3.29254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:                timestamp 1691393168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: 🚀 View run FSR_Trainable_ebe4a81c at: https://wandb.ai/seokjin/FSR-prediction/runs/ebe4a81c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162610-ebe4a81c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23132)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 16:26:20,210\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.663 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:20,214\tWARNING util.py:315 -- The `process_trial_result` operation took 1.667 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:20,215\tWARNING util.py:315 -- Processing trial results took 1.669 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:26:20,217\tWARNING util.py:315 -- The `process_trial_result` operation took 1.670 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_79ecedfa_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-26-03/wandb/run-20230807_162619-79ecedfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Syncing run FSR_Trainable_79ecedfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/79ecedfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                      mae ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                     mape ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                     rmse ███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:         time_this_iter_s █▅▃▃▆▅▅▄▃▆█▅▅▄▆▄▂▃▂▄▄▃▂▄▅▃▂▆▅▄▃▃▆▃▁▆▁▂▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                      mae 0.33371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                     mape 0.09691\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                     rmse 0.68171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:       time_since_restore 107.39985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:         time_this_iter_s 0.93357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:             time_total_s 107.39985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:                timestamp 1691393177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: 🚀 View run FSR_Trainable_891ae335 at: https://wandb.ai/seokjin/FSR-prediction/runs/891ae335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=21315)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162417-891ae335/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                      mae 0.31826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                     mape 0.0954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                     rmse 0.68772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:       time_since_restore 10.44657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:         time_this_iter_s 4.85895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:             time_total_s 10.44657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:                timestamp 1691393185\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: 🚀 View run FSR_Trainable_79ecedfa at: https://wandb.ai/seokjin/FSR-prediction/runs/79ecedfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23360)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162619-79ecedfa/logs\n",
      "2023-08-07 16:26:29,628\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:29,631\tWARNING util.py:315 -- The `process_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:29,633\tWARNING util.py:315 -- Processing trial results took 1.785 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:26:29,635\tWARNING util.py:315 -- The `process_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_d6bd2519_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-26-13/wandb/run-20230807_162628-d6bd2519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Syncing run FSR_Trainable_d6bd2519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d6bd2519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_7521c175_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_16-26-22/wandb/run-20230807_162638-7521c175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Syncing run FSR_Trainable_7521c175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7521c175\n",
      "2023-08-07 16:26:39,302\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.963 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:39,307\tWARNING util.py:315 -- The `process_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:39,309\tWARNING util.py:315 -- Processing trial results took 1.970 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:26:39,312\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 16:26:50,056\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.065 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:50,061\tWARNING util.py:315 -- The `process_trial_result` operation took 2.071 s, which may be a performance bottleneck.\n",
      "2023-08-07 16:26:50,063\tWARNING util.py:315 -- Processing trial results took 2.072 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 16:26:50,064\tWARNING util.py:315 -- The `process_trial_result` operation took 2.074 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_15-54-13/FSR_Trainable_5682ca34_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-08-07_16-26-31/wandb/run-20230807_162649-5682ca34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Syncing run FSR_Trainable_5682ca34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5682ca34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                      mae 0.31896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                     mape 0.09562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                     rmse 0.68614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:       time_since_restore 11.60873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:         time_this_iter_s 5.32846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:             time_total_s 11.60873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:                timestamp 1691393215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: 🚀 View run FSR_Trainable_5682ca34 at: https://wandb.ai/seokjin/FSR-prediction/runs/5682ca34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=24021)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162649-5682ca34/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)2 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                      mae █▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                     mape █▇▇▆▅▅▄▄▄▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                     rmse █▇▆▆▅▅▄▄▃▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:         time_this_iter_s ███▇▄▂▂▂▃▁▂▂▃▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                      mae 0.31355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                     mape 0.09281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                     rmse 0.68297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:       time_since_restore 71.73058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:         time_this_iter_s 3.89851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:             time_total_s 71.73058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:                timestamp 1691393266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: 🚀 View run FSR_Trainable_7521c175 at: https://wandb.ai/seokjin/FSR-prediction/runs/7521c175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23813)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162638-7521c175/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                      mae █▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                     mape █▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                     rmse ███▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:         time_this_iter_s █▇█▄▅▄▄▄▁▄▁▂▁▁▁▅▂▁▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                      mae 0.31061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                     mape 0.092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                     rmse 0.68024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:       time_since_restore 333.59194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:         time_this_iter_s 2.88622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:             time_total_s 333.59194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:                timestamp 1691393521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: 🚀 View run FSR_Trainable_d6bd2519 at: https://wandb.ai/seokjin/FSR-prediction/runs/d6bd2519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=23591)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_162628-d6bd2519/logs\n",
      "2023-08-07 16:32:05,372\tINFO tune.py:1111 -- Total run time: 2266.31 seconds (2262.11 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
