{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-08-10_22-35-32/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-08-10_22-35-32\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "177.361\n",
    "\n",
    "0.974"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-10 22:35:32,849] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='metric',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 22:35:35,008\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-08-10 22:35:37,100\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-10 22:53:55</td></tr>\n",
       "<tr><td>Running for: </td><td>00:18:18.39        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.1/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.6499779892368805 | Iter 32.000: -0.6500586216997876 | Iter 16.000: -0.6587064691641351 | Iter 8.000: -0.6750845848345232 | Iter 4.000: -0.6952105339721033 | Iter 2.000: -0.7298993528586186 | Iter 1.000: -0.9068396734477406<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model        </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  trmse_force</th><th style=\"text-align: right;\">  tmape_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_e8eecf5d</td><td>TERMINATED</td><td>172.26.215.93:133787</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_8280</td><td>[&#x27;force&#x27;, &#x27;x_co_3dc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00337958 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       50.3775  </td><td style=\"text-align: right;\">    0.319028</td><td style=\"text-align: right;\">     0.184989</td><td style=\"text-align: right;\">  6.54144e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_e7a6ae12</td><td>TERMINATED</td><td>172.26.215.93:133857</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_3b40</td><td>[&#x27;force&#x27;, &#x27;x_co_8100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0540357  </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.488214</td><td style=\"text-align: right;\">    7.4843  </td><td style=\"text-align: right;\">     6.32459 </td><td style=\"text-align: right;\">  9.60595e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_49e42cfa</td><td>TERMINATED</td><td>172.26.215.93:134030</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_0bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_97c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00177504 </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.88604 </td><td style=\"text-align: right;\">    2.91372 </td><td style=\"text-align: right;\">     1.88128 </td><td style=\"text-align: right;\">160.311      </td></tr>\n",
       "<tr><td>FSR_Trainable_93d1b467</td><td>TERMINATED</td><td>172.26.215.93:134204</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_bcc0</td><td>[&#x27;force&#x27;, &#x27;x_co_9040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.69789e-05</td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.822738</td><td style=\"text-align: right;\">    4.03443 </td><td style=\"text-align: right;\">     2.11771 </td><td style=\"text-align: right;\">130.83       </td></tr>\n",
       "<tr><td>FSR_Trainable_919a9d49</td><td>TERMINATED</td><td>172.26.215.93:134525</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_2d80</td><td>[&#x27;force&#x27;, &#x27;x_co_1300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00130045 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       32.8018  </td><td style=\"text-align: right;\">    0.317279</td><td style=\"text-align: right;\">     0.181444</td><td style=\"text-align: right;\">  6.24981e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ad928d07</td><td>TERMINATED</td><td>172.26.215.93:134759</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_ae40</td><td>[&#x27;force&#x27;, &#x27;x_co_7100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00549221 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.93682 </td><td style=\"text-align: right;\">    0.533957</td><td style=\"text-align: right;\">     0.313474</td><td style=\"text-align: right;\">  1.1883e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_25fed8e4</td><td>TERMINATED</td><td>172.26.215.93:134974</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_1980</td><td>[&#x27;force&#x27;, &#x27;x_co_a200</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00880349 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.91159 </td><td style=\"text-align: right;\">    0.414766</td><td style=\"text-align: right;\">     0.238062</td><td style=\"text-align: right;\">  8.18955e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_8484e1ff</td><td>TERMINATED</td><td>172.26.215.93:135202</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_a380</td><td>[&#x27;force&#x27;, &#x27;x_co_3ac0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000414958</td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.584313</td><td style=\"text-align: right;\">    3.70513 </td><td style=\"text-align: right;\">     2.22787 </td><td style=\"text-align: right;\"> 35.0568     </td></tr>\n",
       "<tr><td>FSR_Trainable_9f4f8ce1</td><td>TERMINATED</td><td>172.26.215.93:135441</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_2000</td><td>[&#x27;force&#x27;, &#x27;x_co_a9c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.05477e-05</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.34856 </td><td style=\"text-align: right;\">    1.15354 </td><td style=\"text-align: right;\">     0.546361</td><td style=\"text-align: right;\">  3.14033e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_5fe67775</td><td>TERMINATED</td><td>172.26.215.93:135675</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_b0c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ad80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.40425e-05</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.38844 </td><td style=\"text-align: right;\">    0.573491</td><td style=\"text-align: right;\">     0.363057</td><td style=\"text-align: right;\">  7.64382e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_2258fe3e</td><td>TERMINATED</td><td>172.26.215.93:135741</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_8800</td><td>[&#x27;force&#x27;, &#x27;x_co_d240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.95394e-05</td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.5482  </td><td style=\"text-align: right;\">    5.84881 </td><td style=\"text-align: right;\">     7.06361 </td><td style=\"text-align: right;\">  7.07247e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_0af22b82</td><td>TERMINATED</td><td>172.26.215.93:135944</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_a380</td><td>[&#x27;force&#x27;, &#x27;x_co_e180</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000417467</td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.684115</td><td style=\"text-align: right;\">    8.14161 </td><td style=\"text-align: right;\">     8.01112 </td><td style=\"text-align: right;\">  5.23546e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_acb158e3</td><td>TERMINATED</td><td>172.26.215.93:136256</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_9080</td><td>[&#x27;force&#x27;, &#x27;x_co_d300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000585481</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        2.96278 </td><td style=\"text-align: right;\">    0.347566</td><td style=\"text-align: right;\">     0.204606</td><td style=\"text-align: right;\">  6.59632e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_f14e2192</td><td>TERMINATED</td><td>172.26.215.93:136349</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_ee40</td><td>[&#x27;force&#x27;, &#x27;x_co_3b80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00158637 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.25394 </td><td style=\"text-align: right;\">    0.358448</td><td style=\"text-align: right;\">     0.201613</td><td style=\"text-align: right;\">  8.07087e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_14fdb345</td><td>TERMINATED</td><td>172.26.215.93:136666</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_ecc0</td><td>[&#x27;force&#x27;, &#x27;x_co_5ec0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00246293 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.77932 </td><td style=\"text-align: right;\">    0.349352</td><td style=\"text-align: right;\">     0.195873</td><td style=\"text-align: right;\">  7.93542e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_c4789d59</td><td>TERMINATED</td><td>172.26.215.93:136758</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_2ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_dbc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00937553 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.82286 </td><td style=\"text-align: right;\">    0.340595</td><td style=\"text-align: right;\">     0.216244</td><td style=\"text-align: right;\">  6.75648e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a862fb48</td><td>TERMINATED</td><td>172.26.215.93:136939</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_60c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0152383  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        6.39885 </td><td style=\"text-align: right;\">    0.329724</td><td style=\"text-align: right;\">     0.195767</td><td style=\"text-align: right;\">  6.68889e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_5ea11c97</td><td>TERMINATED</td><td>172.26.215.93:137122</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_3540</td><td>[&#x27;force&#x27;, &#x27;x_co_fd40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0273726  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.366137</td><td style=\"text-align: right;\">    0.888336</td><td style=\"text-align: right;\">     0.472833</td><td style=\"text-align: right;\">  1.99937e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_f980a2be</td><td>TERMINATED</td><td>172.26.215.93:137305</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_1700</td><td>[&#x27;force&#x27;, &#x27;x_co_5f80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0235334  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.963896</td><td style=\"text-align: right;\">    0.419895</td><td style=\"text-align: right;\">     0.296973</td><td style=\"text-align: right;\">  5.76815e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_254f3e5c</td><td>TERMINATED</td><td>172.26.215.93:137624</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4640</td><td>[&#x27;force&#x27;, &#x27;x_co_8800</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0667963  </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.705323</td><td style=\"text-align: right;\">   19.4382  </td><td style=\"text-align: right;\">    18.9198  </td><td style=\"text-align: right;\">  3.60056e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cf7c2dca</td><td>TERMINATED</td><td>172.26.215.93:137715</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_63c0</td><td>[&#x27;force&#x27;, &#x27;x_co_7e00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00416027 </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.55694 </td><td style=\"text-align: right;\">    2.51609 </td><td style=\"text-align: right;\">     1.55927 </td><td style=\"text-align: right;\"> 93.7999     </td></tr>\n",
       "<tr><td>FSR_Trainable_5cf78066</td><td>TERMINATED</td><td>172.26.215.93:138024</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_e640</td><td>[&#x27;force&#x27;, &#x27;x_co_bd00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00381223 </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.19538 </td><td style=\"text-align: right;\">    2.92916 </td><td style=\"text-align: right;\">     1.80521 </td><td style=\"text-align: right;\"> 98.6173     </td></tr>\n",
       "<tr><td>FSR_Trainable_59e6f3f0</td><td>TERMINATED</td><td>172.26.215.93:138115</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_9cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_6f80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000624328</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.04026 </td><td style=\"text-align: right;\">    0.421475</td><td style=\"text-align: right;\">     0.255425</td><td style=\"text-align: right;\">  8.11015e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ce197d2c</td><td>TERMINATED</td><td>172.26.215.93:138427</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_30c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ce40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000668245</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.57269 </td><td style=\"text-align: right;\">    0.418113</td><td style=\"text-align: right;\">     0.248504</td><td style=\"text-align: right;\">  8.37014e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b58979bc</td><td>TERMINATED</td><td>172.26.215.93:138520</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_ee80</td><td>[&#x27;force&#x27;, &#x27;x_co_8980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00100372 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.42368 </td><td style=\"text-align: right;\">    0.364292</td><td style=\"text-align: right;\">     0.202556</td><td style=\"text-align: right;\">  7.72928e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_1ebbb40f</td><td>TERMINATED</td><td>172.26.215.93:138685</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_e900</td><td>[&#x27;force&#x27;, &#x27;x_co_8f40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00129833 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.90946 </td><td style=\"text-align: right;\">    0.386363</td><td style=\"text-align: right;\">     0.233321</td><td style=\"text-align: right;\">  7.44711e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_e5f907e0</td><td>TERMINATED</td><td>172.26.215.93:139023</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_3080</td><td>[&#x27;force&#x27;, &#x27;x_co_30c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000308049</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.647535</td><td style=\"text-align: right;\">    0.710586</td><td style=\"text-align: right;\">     0.387461</td><td style=\"text-align: right;\">  1.36357e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_81cb7e1f</td><td>TERMINATED</td><td>172.26.215.93:139117</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_9200</td><td>[&#x27;force&#x27;, &#x27;x_co_a380</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000247475</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.774022</td><td style=\"text-align: right;\">    0.851651</td><td style=\"text-align: right;\">     0.522028</td><td style=\"text-align: right;\">  1.15133e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_0d1de52d</td><td>TERMINATED</td><td>172.26.215.93:139298</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_d780</td><td>[&#x27;force&#x27;, &#x27;x_co_3bc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000227116</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.736976</td><td style=\"text-align: right;\">    0.657647</td><td style=\"text-align: right;\">     0.411442</td><td style=\"text-align: right;\">  9.59173e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_76d84eb0</td><td>TERMINATED</td><td>172.26.215.93:139614</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_d040</td><td>[&#x27;force&#x27;, &#x27;x_co_f440</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00269858 </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.757781</td><td style=\"text-align: right;\">    5.28483 </td><td style=\"text-align: right;\">     5.15582 </td><td style=\"text-align: right;\">  6.31828e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_93091046</td><td>TERMINATED</td><td>172.26.215.93:139708</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_f1c0</td><td>[&#x27;force&#x27;, &#x27;x_co_dfc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00170716 </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.903354</td><td style=\"text-align: right;\">    5.46604 </td><td style=\"text-align: right;\">     6.41255 </td><td style=\"text-align: right;\">  1.50892e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_b2b7e672</td><td>TERMINATED</td><td>172.26.215.93:140019</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_ca40</td><td>[&#x27;force&#x27;, &#x27;x_co_e2c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0979274  </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.852096</td><td style=\"text-align: right;\">   17.6182  </td><td style=\"text-align: right;\">    18.7695  </td><td style=\"text-align: right;\">  1.85545e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_eaf9fef7</td><td>TERMINATED</td><td>172.26.215.93:140111</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_5a80</td><td>[&#x27;force&#x27;, &#x27;x_co_6d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00980744 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.87148 </td><td style=\"text-align: right;\">    0.343855</td><td style=\"text-align: right;\">     0.204099</td><td style=\"text-align: right;\">  7.20953e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_dd88466f</td><td>TERMINATED</td><td>172.26.215.93:140418</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_edc0</td><td>[&#x27;force&#x27;, &#x27;x_co_4700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00830362 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.00614 </td><td style=\"text-align: right;\">    0.346598</td><td style=\"text-align: right;\">     0.205687</td><td style=\"text-align: right;\">  7.37672e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a78b6bca</td><td>TERMINATED</td><td>172.26.215.93:140519</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_f4c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5340</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00101197 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       27.7193  </td><td style=\"text-align: right;\">    0.31229 </td><td style=\"text-align: right;\">     0.181337</td><td style=\"text-align: right;\">  6.09804e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b149d862</td><td>TERMINATED</td><td>172.26.215.93:140833</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4f00</td><td>[&#x27;force&#x27;, &#x27;x_co_4500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00254078 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       28.3259  </td><td style=\"text-align: right;\">    0.321119</td><td style=\"text-align: right;\">     0.188292</td><td style=\"text-align: right;\">  6.18326e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_0350f91f</td><td>TERMINATED</td><td>172.26.215.93:140923</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6940</td><td>[&#x27;force&#x27;, &#x27;x_co_d440</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.019961   </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.64916 </td><td style=\"text-align: right;\">    0.349622</td><td style=\"text-align: right;\">     0.21898 </td><td style=\"text-align: right;\">  6.92473e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_13115552</td><td>TERMINATED</td><td>172.26.215.93:141231</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_3340</td><td>[&#x27;force&#x27;, &#x27;x_co_4740</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00108653 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       23.9775  </td><td style=\"text-align: right;\">    0.315835</td><td style=\"text-align: right;\">     0.1845  </td><td style=\"text-align: right;\">  6.14835e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_1a2e7ffb</td><td>TERMINATED</td><td>172.26.215.93:141461</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4640</td><td>[&#x27;force&#x27;, &#x27;x_co_0900</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00253253 </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.724885</td><td style=\"text-align: right;\">    2.61905 </td><td style=\"text-align: right;\">     2.04099 </td><td style=\"text-align: right;\">139.198      </td></tr>\n",
       "<tr><td>FSR_Trainable_e78b1941</td><td>TERMINATED</td><td>172.26.215.93:141693</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_7b40</td><td>[&#x27;force&#x27;, &#x27;x_co_d540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00268841 </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.451035</td><td style=\"text-align: right;\">    2.9867  </td><td style=\"text-align: right;\">     1.69679 </td><td style=\"text-align: right;\"> 71.7809     </td></tr>\n",
       "<tr><td>FSR_Trainable_0be5760d</td><td>TERMINATED</td><td>172.26.215.93:141931</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_ca80</td><td>[&#x27;force&#x27;, &#x27;x_co_7f40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00102132 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.907253</td><td style=\"text-align: right;\">    0.53555 </td><td style=\"text-align: right;\">     0.300552</td><td style=\"text-align: right;\">  1.00784e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d3a0f5db</td><td>TERMINATED</td><td>172.26.215.93:142164</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_7800</td><td>[&#x27;force&#x27;, &#x27;x_co_6cc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0010812  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.627033</td><td style=\"text-align: right;\">    1.35502 </td><td style=\"text-align: right;\">     0.676181</td><td style=\"text-align: right;\">  2.92206e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d1a09ab3</td><td>TERMINATED</td><td>172.26.215.93:142257</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_7600</td><td>[&#x27;force&#x27;, &#x27;x_co_5680</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00484143 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       34.343   </td><td style=\"text-align: right;\">    0.255965</td><td style=\"text-align: right;\">     0.164258</td><td style=\"text-align: right;\">  4.01086e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a0f7a6e6</td><td>TERMINATED</td><td>172.26.215.93:142568</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6100</td><td>[&#x27;force&#x27;, &#x27;x_co_f540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00526384 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.04387 </td><td style=\"text-align: right;\">    0.3469  </td><td style=\"text-align: right;\">     0.197056</td><td style=\"text-align: right;\">  7.69476e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_1ec667ce</td><td>TERMINATED</td><td>172.26.215.93:142790</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_2d80</td><td>[&#x27;force&#x27;, &#x27;x_co_26c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00189098 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       11.4561  </td><td style=\"text-align: right;\">    0.347941</td><td style=\"text-align: right;\">     0.201366</td><td style=\"text-align: right;\">  7.69562e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_eeaeafba</td><td>TERMINATED</td><td>172.26.215.93:143015</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_9f80</td><td>[&#x27;force&#x27;, &#x27;x_co_9d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00183553 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.591333</td><td style=\"text-align: right;\">    0.844224</td><td style=\"text-align: right;\">     0.503292</td><td style=\"text-align: right;\">  1.21193e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_5456a8ec</td><td>TERMINATED</td><td>172.26.215.93:143232</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6700</td><td>[&#x27;force&#x27;, &#x27;x_co_7040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00156212 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.439428</td><td style=\"text-align: right;\">    0.670276</td><td style=\"text-align: right;\">     0.396337</td><td style=\"text-align: right;\">  1.14859e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_6670c862</td><td>TERMINATED</td><td>172.26.215.93:143471</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6d80</td><td>[&#x27;force&#x27;, &#x27;x_co_7500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000786806</td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.685786</td><td style=\"text-align: right;\">    0.97625 </td><td style=\"text-align: right;\">     0.512925</td><td style=\"text-align: right;\">  1.95033e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_70ee40e6</td><td>TERMINATED</td><td>172.26.215.93:143704</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_f880</td><td>[&#x27;force&#x27;, &#x27;x_co_c780</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00346034 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       37.6831  </td><td style=\"text-align: right;\">    0.241022</td><td style=\"text-align: right;\">     0.15935 </td><td style=\"text-align: right;\">  3.42975e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_56638e36</td><td>TERMINATED</td><td>172.26.215.93:143790</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6240</td><td>[&#x27;force&#x27;, &#x27;x_co_0d80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0053511  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       49.1302  </td><td style=\"text-align: right;\">    0.261525</td><td style=\"text-align: right;\">     0.164101</td><td style=\"text-align: right;\">  4.33272e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_64391342</td><td>TERMINATED</td><td>172.26.215.93:143972</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_2740</td><td>[&#x27;force&#x27;, &#x27;x_co_0140</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00454412 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       49.6819  </td><td style=\"text-align: right;\">    0.262675</td><td style=\"text-align: right;\">     0.167402</td><td style=\"text-align: right;\">  4.22141e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_7b47a99c</td><td>TERMINATED</td><td>172.26.215.93:144277</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_d900</td><td>[&#x27;force&#x27;, &#x27;x_co_ce00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00501025 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       49.1998  </td><td style=\"text-align: right;\">    0.255925</td><td style=\"text-align: right;\">     0.163424</td><td style=\"text-align: right;\">  4.11198e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_eb36545b</td><td>TERMINATED</td><td>172.26.215.93:144579</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_7640</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00420608 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       57.6334  </td><td style=\"text-align: right;\">    0.251745</td><td style=\"text-align: right;\">     0.162546</td><td style=\"text-align: right;\">  3.91845e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fc7a34d4</td><td>TERMINATED</td><td>172.26.215.93:144821</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_7ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_4d40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00587257 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       47.0138  </td><td style=\"text-align: right;\">    0.241827</td><td style=\"text-align: right;\">     0.161135</td><td style=\"text-align: right;\">  3.4109e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_d9e1a2f5</td><td>TERMINATED</td><td>172.26.215.93:145024</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_54c0</td><td>[&#x27;force&#x27;, &#x27;x_co_4140</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00425884 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       56.016   </td><td style=\"text-align: right;\">    0.256098</td><td style=\"text-align: right;\">     0.163062</td><td style=\"text-align: right;\">  4.13304e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_3f75ed82</td><td>TERMINATED</td><td>172.26.215.93:145260</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_e940</td><td>[&#x27;force&#x27;, &#x27;x_co_71c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00602572 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       14.6935  </td><td style=\"text-align: right;\">    0.360171</td><td style=\"text-align: right;\">     0.190774</td><td style=\"text-align: right;\">  8.25917e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_cce353eb</td><td>TERMINATED</td><td>172.26.215.93:145545</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_9300</td><td>[&#x27;force&#x27;, &#x27;x_co_9c80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0051514  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       18.7362  </td><td style=\"text-align: right;\">    0.305875</td><td style=\"text-align: right;\">     0.173623</td><td style=\"text-align: right;\">  6.24124e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_89c82371</td><td>TERMINATED</td><td>172.26.215.93:145762</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_edc0</td><td>[&#x27;force&#x27;, &#x27;x_co_f980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00427254 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       31.0618  </td><td style=\"text-align: right;\">    0.240194</td><td style=\"text-align: right;\">     0.161055</td><td style=\"text-align: right;\">  3.357e+14  </td></tr>\n",
       "<tr><td>FSR_Trainable_52a2c1ef</td><td>TERMINATED</td><td>172.26.215.93:145982</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_c180</td><td>[&#x27;force&#x27;, &#x27;x_co_cd40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0034993  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.930013</td><td style=\"text-align: right;\">    0.3981  </td><td style=\"text-align: right;\">     0.22491 </td><td style=\"text-align: right;\">  8.02442e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b58f0ea2</td><td>TERMINATED</td><td>172.26.215.93:146232</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_7bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_55c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0033679  </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.433   </td><td style=\"text-align: right;\">    2.00205 </td><td style=\"text-align: right;\">     1.30838 </td><td style=\"text-align: right;\"> 83.4958     </td></tr>\n",
       "<tr><td>FSR_Trainable_2c7af959</td><td>TERMINATED</td><td>172.26.215.93:146458</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_ee00</td><td>[&#x27;force&#x27;, &#x27;x_co_7700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0133078  </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.534642</td><td style=\"text-align: right;\">    3.16228 </td><td style=\"text-align: right;\">     1.77846 </td><td style=\"text-align: right;\"> 91.4654     </td></tr>\n",
       "<tr><td>FSR_Trainable_30ee5665</td><td>TERMINATED</td><td>172.26.215.93:146686</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_bd40</td><td>[&#x27;force&#x27;, &#x27;x_co_9080</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0130711  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        7.18847 </td><td style=\"text-align: right;\">    0.313118</td><td style=\"text-align: right;\">     0.178914</td><td style=\"text-align: right;\">  6.60823e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_730fdf81</td><td>TERMINATED</td><td>172.26.215.93:146921</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_5100</td><td>[&#x27;force&#x27;, &#x27;x_co_d500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00635622 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       42.0472  </td><td style=\"text-align: right;\">    0.245876</td><td style=\"text-align: right;\">     0.163005</td><td style=\"text-align: right;\">  3.51725e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_852e9620</td><td>TERMINATED</td><td>172.26.215.93:147149</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_79c0</td><td>[&#x27;force&#x27;, &#x27;x_co_f700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00664711 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       44.9481  </td><td style=\"text-align: right;\">    0.245156</td><td style=\"text-align: right;\">     0.16408 </td><td style=\"text-align: right;\">  3.43276e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_a991e120</td><td>TERMINATED</td><td>172.26.215.93:147223</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_9980</td><td>[&#x27;force&#x27;, &#x27;x_co_a780</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00821424 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.38481 </td><td style=\"text-align: right;\">    0.479074</td><td style=\"text-align: right;\">     0.318465</td><td style=\"text-align: right;\">  8.75022e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_f90c403c</td><td>TERMINATED</td><td>172.26.215.93:147543</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4e00</td><td>[&#x27;force&#x27;, &#x27;x_co_4580</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00674346 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.490505</td><td style=\"text-align: right;\">    0.608424</td><td style=\"text-align: right;\">     0.331394</td><td style=\"text-align: right;\">  1.31323e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_3dc99959</td><td>TERMINATED</td><td>172.26.215.93:147775</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4440</td><td>[&#x27;force&#x27;, &#x27;x_co_7000</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00698435 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       36.7484  </td><td style=\"text-align: right;\">    0.248936</td><td style=\"text-align: right;\">     0.165075</td><td style=\"text-align: right;\">  3.56676e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fbb4fbf5</td><td>TERMINATED</td><td>172.26.215.93:148009</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_e300</td><td>[&#x27;force&#x27;, &#x27;x_co_c540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0069854  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        8.63157 </td><td style=\"text-align: right;\">    0.305532</td><td style=\"text-align: right;\">     0.17475 </td><td style=\"text-align: right;\">  6.30818e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_545d0224</td><td>TERMINATED</td><td>172.26.215.93:148245</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_1640</td><td>[&#x27;force&#x27;, &#x27;x_co_2840</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0035595  </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.408051</td><td style=\"text-align: right;\">    4.9288  </td><td style=\"text-align: right;\">     4.75151 </td><td style=\"text-align: right;\">  4.18619e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_89d84316</td><td>TERMINATED</td><td>172.26.215.93:148481</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4a00</td><td>[&#x27;force&#x27;, &#x27;x_co_6a00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0104234  </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.501491</td><td style=\"text-align: right;\">    6.12996 </td><td style=\"text-align: right;\">     6.55424 </td><td style=\"text-align: right;\">  2.71307e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_549e2a1c</td><td>TERMINATED</td><td>172.26.215.93:148713</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_f000</td><td>[&#x27;force&#x27;, &#x27;x_co_4a80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0111336  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.778097</td><td style=\"text-align: right;\">    0.519264</td><td style=\"text-align: right;\">     0.273988</td><td style=\"text-align: right;\">  1.24529e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_223f2432</td><td>TERMINATED</td><td>172.26.215.93:148950</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_bd40</td><td>[&#x27;force&#x27;, &#x27;x_co_92c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0033239  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.17405 </td><td style=\"text-align: right;\">    0.520413</td><td style=\"text-align: right;\">     0.304263</td><td style=\"text-align: right;\">  1.1232e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_00cc9211</td><td>TERMINATED</td><td>172.26.215.93:149034</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_15c0</td><td>[&#x27;force&#x27;, &#x27;x_co_24c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00808837 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       24.4484  </td><td style=\"text-align: right;\">    0.252381</td><td style=\"text-align: right;\">     0.165807</td><td style=\"text-align: right;\">  3.71883e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_622e1df8</td><td>TERMINATED</td><td>172.26.215.93:149214</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_3d80</td><td>[&#x27;force&#x27;, &#x27;x_co_1880</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0085335  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       39.7737  </td><td style=\"text-align: right;\">    0.250445</td><td style=\"text-align: right;\">     0.164728</td><td style=\"text-align: right;\">  3.67421e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_116c5420</td><td>TERMINATED</td><td>172.26.215.93:149398</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_7b00</td><td>[&#x27;force&#x27;, &#x27;x_co_abc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00722702 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       41.3902  </td><td style=\"text-align: right;\">    0.247729</td><td style=\"text-align: right;\">     0.163763</td><td style=\"text-align: right;\">  3.5731e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_bf2ad8fb</td><td>TERMINATED</td><td>172.26.215.93:149705</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_60c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00697547 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       41.805   </td><td style=\"text-align: right;\">    0.247949</td><td style=\"text-align: right;\">     0.164878</td><td style=\"text-align: right;\">  3.51057e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_8b481b07</td><td>TERMINATED</td><td>172.26.215.93:149979</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_52c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00721304 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       40.4853  </td><td style=\"text-align: right;\">    0.247705</td><td style=\"text-align: right;\">     0.164754</td><td style=\"text-align: right;\">  3.51748e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_405740e0</td><td>TERMINATED</td><td>172.26.215.93:150244</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_fe40</td><td>[&#x27;force&#x27;, &#x27;x_co_4080</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00722282 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       35.9579  </td><td style=\"text-align: right;\">    0.248116</td><td style=\"text-align: right;\">     0.164688</td><td style=\"text-align: right;\">  3.55493e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b6a00942</td><td>TERMINATED</td><td>172.26.215.93:150449</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_f880</td><td>[&#x27;force&#x27;, &#x27;x_co_5500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0161167  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.471463</td><td style=\"text-align: right;\">    0.579856</td><td style=\"text-align: right;\">     0.364161</td><td style=\"text-align: right;\">  9.65605e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fe4d7d4d</td><td>TERMINATED</td><td>172.26.215.93:150682</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_4e00</td><td>[&#x27;force&#x27;, &#x27;x_co_4480</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0170608  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.578387</td><td style=\"text-align: right;\">    0.520988</td><td style=\"text-align: right;\">     0.277754</td><td style=\"text-align: right;\">  1.14029e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_72ac5990</td><td>TERMINATED</td><td>172.26.215.93:150925</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_d540</td><td>[&#x27;force&#x27;, &#x27;x_co_ce80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0116727  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.576678</td><td style=\"text-align: right;\">    0.779547</td><td style=\"text-align: right;\">     0.405904</td><td style=\"text-align: right;\">  1.96711e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_292a4961</td><td>TERMINATED</td><td>172.26.215.93:151147</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_cf80</td><td>[&#x27;force&#x27;, &#x27;x_co_fec0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0112717  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.702361</td><td style=\"text-align: right;\">    0.511275</td><td style=\"text-align: right;\">     0.29128 </td><td style=\"text-align: right;\">  1.13102e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_2ae0cce2</td><td>TERMINATED</td><td>172.26.215.93:151386</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_32c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1600</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00966615 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.27937 </td><td style=\"text-align: right;\">    0.376048</td><td style=\"text-align: right;\">     0.217081</td><td style=\"text-align: right;\">  7.74329e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_352e843d</td><td>TERMINATED</td><td>172.26.215.93:151476</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_aa40</td><td>[&#x27;force&#x27;, &#x27;x_co_5e00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00645396 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.41834 </td><td style=\"text-align: right;\">    0.394007</td><td style=\"text-align: right;\">     0.226139</td><td style=\"text-align: right;\">  7.89839e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_cb45896d</td><td>TERMINATED</td><td>172.26.215.93:151791</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_82c0</td><td>[&#x27;force&#x27;, &#x27;x_co_2e00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00603037 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       36.9193  </td><td style=\"text-align: right;\">    0.247248</td><td style=\"text-align: right;\">     0.1645  </td><td style=\"text-align: right;\">  3.49845e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_6f44ace7</td><td>TERMINATED</td><td>172.26.215.93:151882</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_bf00</td><td>[&#x27;force&#x27;, &#x27;x_co_8f80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0273811  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.26757 </td><td style=\"text-align: right;\">    0.307673</td><td style=\"text-align: right;\">     0.183089</td><td style=\"text-align: right;\">  6.14238e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_b898cfb7</td><td>TERMINATED</td><td>172.26.215.93:152190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_8a40</td><td>[&#x27;force&#x27;, &#x27;x_co_0b40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.004261   </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       37.2504  </td><td style=\"text-align: right;\">    0.239777</td><td style=\"text-align: right;\">     0.160933</td><td style=\"text-align: right;\">  3.30727e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_7a48eded</td><td>TERMINATED</td><td>172.26.215.93:152414</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_c340</td><td>[&#x27;force&#x27;, &#x27;x_co_6740</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00411356 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.31403 </td><td style=\"text-align: right;\">    0.389072</td><td style=\"text-align: right;\">     0.232314</td><td style=\"text-align: right;\">  8.08108e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_9da4ea97</td><td>TERMINATED</td><td>172.26.215.93:152642</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_71c0</td><td>[&#x27;force&#x27;, &#x27;x_co_9ec0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00386291 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.771869</td><td style=\"text-align: right;\">    0.397854</td><td style=\"text-align: right;\">     0.235932</td><td style=\"text-align: right;\">  7.44809e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ee2a2b18</td><td>TERMINATED</td><td>172.26.215.93:152875</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_fcc0</td><td>[&#x27;force&#x27;, &#x27;x_co_4300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00576027 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       33.039   </td><td style=\"text-align: right;\">    0.244417</td><td style=\"text-align: right;\">     0.162756</td><td style=\"text-align: right;\">  3.46297e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_970091e8</td><td>TERMINATED</td><td>172.26.215.93:153117</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_3a40</td><td>[&#x27;force&#x27;, &#x27;x_co_02c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00226669 </td><td>sklearn.preproc_4d50</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.977356</td><td style=\"text-align: right;\">    3.09082 </td><td style=\"text-align: right;\">     1.9833  </td><td style=\"text-align: right;\"> 59.8207     </td></tr>\n",
       "<tr><td>FSR_Trainable_63e20809</td><td>TERMINATED</td><td>172.26.215.93:153346</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_09c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0f00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00211564 </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.486954</td><td style=\"text-align: right;\">    5.37073 </td><td style=\"text-align: right;\">     5.68165 </td><td style=\"text-align: right;\">  3.04694e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_d194dbfc</td><td>TERMINATED</td><td>172.26.215.93:153574</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>median</td><td>[&#x27;FSR_for_force_00c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ebc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00298606 </td><td>sklearn.preproc_4ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.475096</td><td style=\"text-align: right;\">    6.77678 </td><td style=\"text-align: right;\">     7.07258 </td><td style=\"text-align: right;\">  4.96014e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_ef2e7036</td><td>TERMINATED</td><td>172.26.215.93:153801</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_3540</td><td>[&#x27;force&#x27;, &#x27;x_co_01c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0058628  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       34.5384  </td><td style=\"text-align: right;\">    0.244747</td><td style=\"text-align: right;\">     0.163764</td><td style=\"text-align: right;\">  3.41153e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_7aceb6ac</td><td>TERMINATED</td><td>172.26.215.93:154038</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_dc40</td><td>[&#x27;force&#x27;, &#x27;x_co_f000</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0051604  </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       36.8204  </td><td style=\"text-align: right;\">    0.245982</td><td style=\"text-align: right;\">     0.164032</td><td style=\"text-align: right;\">  3.4561e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_7745b095</td><td>TERMINATED</td><td>172.26.215.93:154254</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_3e80</td><td>[&#x27;force&#x27;, &#x27;x_co_2240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00458965 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.2232  </td><td style=\"text-align: right;\">    0.355282</td><td style=\"text-align: right;\">     0.231056</td><td style=\"text-align: right;\">  6.17303e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_2b727f4f</td><td>TERMINATED</td><td>172.26.215.93:154472</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_0080</td><td>[&#x27;force&#x27;, &#x27;x_co_8280</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00455637 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.20457 </td><td style=\"text-align: right;\">    0.361235</td><td style=\"text-align: right;\">     0.211936</td><td style=\"text-align: right;\">  7.08432e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_ffa11e7b</td><td>TERMINATED</td><td>172.26.215.93:154705</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_6740</td><td>[&#x27;force&#x27;, &#x27;x_co_6e80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00569028 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.575132</td><td style=\"text-align: right;\">    0.609707</td><td style=\"text-align: right;\">     0.319747</td><td style=\"text-align: right;\">  1.49662e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_17ad2c46</td><td>TERMINATED</td><td>172.26.215.93:154948</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_f940</td><td>[&#x27;force&#x27;, &#x27;x_co_dd00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00270947 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.453102</td><td style=\"text-align: right;\">    0.973435</td><td style=\"text-align: right;\">     0.566717</td><td style=\"text-align: right;\">  1.54423e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_64ab2554</td><td>TERMINATED</td><td>172.26.215.93:155175</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4f30</td><td>sklearn.impute._7d70</td><td>mean  </td><td>[&#x27;FSR_for_force_de80</td><td>[&#x27;force&#x27;, &#x27;x_co_dd40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00291282 </td><td>sklearn.preproc_4cf0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        4.88313 </td><td style=\"text-align: right;\">    0.289596</td><td style=\"text-align: right;\">     0.186454</td><td style=\"text-align: right;\">  4.94046e+14</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 22:35:37,153\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  mae_coord</th><th style=\"text-align: right;\">  mae_force</th><th style=\"text-align: right;\">  mape_coord</th><th style=\"text-align: right;\">  mape_force</th><th style=\"text-align: right;\">   metric</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">  rmse_coord</th><th style=\"text-align: right;\">  rmse_force</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  tmae_coord</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">   tmape_coord</th><th style=\"text-align: right;\">  tmape_force</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  trmse_coord</th><th style=\"text-align: right;\">  trmse_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_00cc9211</td><td>2023-08-10_22-48-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    4.46879</td><td style=\"text-align: right;\">    632.662</td><td style=\"text-align: right;\">     1.11428</td><td style=\"text-align: right;\"> 8.32014e+17</td><td style=\"text-align: right;\"> 0.65263 </td><td>172.26.215.93</td><td style=\"text-align: right;\">149034</td><td style=\"text-align: right;\">     2.43054</td><td style=\"text-align: right;\">     438.157</td><td style=\"text-align: right;\">           24.4484  </td><td style=\"text-align: right;\">          0.631907</td><td style=\"text-align: right;\">     24.4484  </td><td style=\"text-align: right;\"> 1691675331</td><td style=\"text-align: right;\">    0.95916 </td><td style=\"text-align: right;\">    0.252381</td><td style=\"text-align: right;\">   8.01392e+13</td><td style=\"text-align: right;\">  3.71883e+14</td><td style=\"text-align: right;\">                  64</td><td>00cc9211  </td><td style=\"text-align: right;\">     0.486823</td><td style=\"text-align: right;\">     0.165807</td></tr>\n",
       "<tr><td>FSR_Trainable_0350f91f</td><td>2023-08-10_22-40-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.50522</td><td style=\"text-align: right;\">    766.778</td><td style=\"text-align: right;\">     1.36742</td><td style=\"text-align: right;\"> 1.24086e+18</td><td style=\"text-align: right;\"> 0.742581</td><td>172.26.215.93</td><td style=\"text-align: right;\">140923</td><td style=\"text-align: right;\">     2.71   </td><td style=\"text-align: right;\">     526.43 </td><td style=\"text-align: right;\">            2.64916 </td><td style=\"text-align: right;\">          0.563669</td><td style=\"text-align: right;\">      2.64916 </td><td style=\"text-align: right;\"> 1691674853</td><td style=\"text-align: right;\">    1.13541 </td><td style=\"text-align: right;\">    0.349622</td><td style=\"text-align: right;\">   8.29604e+13</td><td style=\"text-align: right;\">  6.92473e+14</td><td style=\"text-align: right;\">                   4</td><td>0350f91f  </td><td style=\"text-align: right;\">     0.523601</td><td style=\"text-align: right;\">     0.21898 </td></tr>\n",
       "<tr><td>FSR_Trainable_0af22b82</td><td>2023-08-10_22-37-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    7.16495</td><td style=\"text-align: right;\">   1381.82 </td><td style=\"text-align: right;\">     1.54647</td><td style=\"text-align: right;\"> 1.34095e+17</td><td style=\"text-align: right;\">56.2364  </td><td>172.26.215.93</td><td style=\"text-align: right;\">135944</td><td style=\"text-align: right;\">     3.73784</td><td style=\"text-align: right;\">    1192.11 </td><td style=\"text-align: right;\">            0.684115</td><td style=\"text-align: right;\">          0.684115</td><td style=\"text-align: right;\">      0.684115</td><td style=\"text-align: right;\"> 1691674649</td><td style=\"text-align: right;\">   48.2198  </td><td style=\"text-align: right;\">    8.14161 </td><td style=\"text-align: right;\">   1.36862e+16</td><td style=\"text-align: right;\">  5.23546e+15</td><td style=\"text-align: right;\">                   1</td><td>0af22b82  </td><td style=\"text-align: right;\">    48.2253  </td><td style=\"text-align: right;\">     8.01112 </td></tr>\n",
       "<tr><td>FSR_Trainable_0be5760d</td><td>2023-08-10_22-41-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">   19.1085 </td><td style=\"text-align: right;\">   1335.66 </td><td style=\"text-align: right;\">     3.30393</td><td style=\"text-align: right;\"> 1.86761e+18</td><td style=\"text-align: right;\"> 1.66398 </td><td>172.26.215.93</td><td style=\"text-align: right;\">141931</td><td style=\"text-align: right;\">     6.75541</td><td style=\"text-align: right;\">     937.552</td><td style=\"text-align: right;\">            0.907253</td><td style=\"text-align: right;\">          0.307523</td><td style=\"text-align: right;\">      0.907253</td><td style=\"text-align: right;\"> 1691674896</td><td style=\"text-align: right;\">    4.06787 </td><td style=\"text-align: right;\">    0.53555 </td><td style=\"text-align: right;\">   5.60158e+13</td><td style=\"text-align: right;\">  1.00784e+15</td><td style=\"text-align: right;\">                   2</td><td>0be5760d  </td><td style=\"text-align: right;\">     1.36343 </td><td style=\"text-align: right;\">     0.300552</td></tr>\n",
       "<tr><td>FSR_Trainable_0d1de52d</td><td>2023-08-10_22-39-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   23.8971 </td><td style=\"text-align: right;\">   1894.99 </td><td style=\"text-align: right;\">     3.68499</td><td style=\"text-align: right;\"> 2.24457e+18</td><td style=\"text-align: right;\"> 2.22064 </td><td>172.26.215.93</td><td style=\"text-align: right;\">139298</td><td style=\"text-align: right;\">     7.90143</td><td style=\"text-align: right;\">    1361.63 </td><td style=\"text-align: right;\">            0.736976</td><td style=\"text-align: right;\">          0.736976</td><td style=\"text-align: right;\">      0.736976</td><td style=\"text-align: right;\"> 1691674780</td><td style=\"text-align: right;\">    5.54185 </td><td style=\"text-align: right;\">    0.657647</td><td style=\"text-align: right;\">   3.75797e+13</td><td style=\"text-align: right;\">  9.59173e+14</td><td style=\"text-align: right;\">                   1</td><td>0d1de52d  </td><td style=\"text-align: right;\">     1.8092  </td><td style=\"text-align: right;\">     0.411442</td></tr>\n",
       "<tr><td>FSR_Trainable_116c5420</td><td>2023-08-10_22-49-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.42188</td><td style=\"text-align: right;\">    627.988</td><td style=\"text-align: right;\">     1.10052</td><td style=\"text-align: right;\"> 8.18704e+17</td><td style=\"text-align: right;\"> 0.649558</td><td>172.26.215.93</td><td style=\"text-align: right;\">149398</td><td style=\"text-align: right;\">     2.41949</td><td style=\"text-align: right;\">     436.472</td><td style=\"text-align: right;\">           41.3902  </td><td style=\"text-align: right;\">          0.463696</td><td style=\"text-align: right;\">     41.3902  </td><td style=\"text-align: right;\"> 1691675365</td><td style=\"text-align: right;\">    0.950661</td><td style=\"text-align: right;\">    0.247729</td><td style=\"text-align: right;\">   7.90436e+13</td><td style=\"text-align: right;\">  3.5731e+14 </td><td style=\"text-align: right;\">                 100</td><td>116c5420  </td><td style=\"text-align: right;\">     0.485795</td><td style=\"text-align: right;\">     0.163763</td></tr>\n",
       "<tr><td>FSR_Trainable_13115552</td><td>2023-08-10_22-41-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.42585</td><td style=\"text-align: right;\">    739.719</td><td style=\"text-align: right;\">     1.17501</td><td style=\"text-align: right;\"> 1.23462e+18</td><td style=\"text-align: right;\"> 0.696955</td><td>172.26.215.93</td><td style=\"text-align: right;\">141231</td><td style=\"text-align: right;\">     2.60987</td><td style=\"text-align: right;\">     475.263</td><td style=\"text-align: right;\">           23.9775  </td><td style=\"text-align: right;\">          0.265702</td><td style=\"text-align: right;\">     23.9775  </td><td style=\"text-align: right;\"> 1691674892</td><td style=\"text-align: right;\">    1.13926 </td><td style=\"text-align: right;\">    0.315835</td><td style=\"text-align: right;\">   7.36685e+13</td><td style=\"text-align: right;\">  6.14835e+14</td><td style=\"text-align: right;\">                  64</td><td>13115552  </td><td style=\"text-align: right;\">     0.512455</td><td style=\"text-align: right;\">     0.1845  </td></tr>\n",
       "<tr><td>FSR_Trainable_14fdb345</td><td>2023-08-10_22-38-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.89484</td><td style=\"text-align: right;\">    749.007</td><td style=\"text-align: right;\">     1.37012</td><td style=\"text-align: right;\"> 1.38367e+18</td><td style=\"text-align: right;\"> 0.721609</td><td>172.26.215.93</td><td style=\"text-align: right;\">136666</td><td style=\"text-align: right;\">     2.69268</td><td style=\"text-align: right;\">     451.087</td><td style=\"text-align: right;\">            3.77932 </td><td style=\"text-align: right;\">          0.337322</td><td style=\"text-align: right;\">      3.77932 </td><td style=\"text-align: right;\"> 1691674680</td><td style=\"text-align: right;\">    1.2227  </td><td style=\"text-align: right;\">    0.349352</td><td style=\"text-align: right;\">   6.42736e+13</td><td style=\"text-align: right;\">  7.93542e+14</td><td style=\"text-align: right;\">                   8</td><td>14fdb345  </td><td style=\"text-align: right;\">     0.525736</td><td style=\"text-align: right;\">     0.195873</td></tr>\n",
       "<tr><td>FSR_Trainable_17ad2c46</td><td>2023-08-10_22-53-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   22.9792 </td><td style=\"text-align: right;\">   2687.17 </td><td style=\"text-align: right;\">     3.99761</td><td style=\"text-align: right;\"> 3.50714e+18</td><td style=\"text-align: right;\"> 2.4335  </td><td>172.26.215.93</td><td style=\"text-align: right;\">154948</td><td style=\"text-align: right;\">     8.11512</td><td style=\"text-align: right;\">    1751.45 </td><td style=\"text-align: right;\">            0.453102</td><td style=\"text-align: right;\">          0.453102</td><td style=\"text-align: right;\">      0.453102</td><td style=\"text-align: right;\"> 1691675618</td><td style=\"text-align: right;\">    5.25306 </td><td style=\"text-align: right;\">    0.973435</td><td style=\"text-align: right;\">   4.69889e+13</td><td style=\"text-align: right;\">  1.54423e+15</td><td style=\"text-align: right;\">                   1</td><td>17ad2c46  </td><td style=\"text-align: right;\">     1.86678 </td><td style=\"text-align: right;\">     0.566717</td></tr>\n",
       "<tr><td>FSR_Trainable_1a2e7ffb</td><td>2023-08-10_22-41-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    5.50955</td><td style=\"text-align: right;\">    844.833</td><td style=\"text-align: right;\">     1.25407</td><td style=\"text-align: right;\"> 7.98399e+08</td><td style=\"text-align: right;\"> 7.33338 </td><td>172.26.215.93</td><td style=\"text-align: right;\">141461</td><td style=\"text-align: right;\">     3.02712</td><td style=\"text-align: right;\">     597.814</td><td style=\"text-align: right;\">            0.724885</td><td style=\"text-align: right;\">          0.724885</td><td style=\"text-align: right;\">      0.724885</td><td style=\"text-align: right;\"> 1691674873</td><td style=\"text-align: right;\">    9.81804 </td><td style=\"text-align: right;\">    2.61905 </td><td style=\"text-align: right;\">   4.30316e+15</td><td style=\"text-align: right;\">139.198      </td><td style=\"text-align: right;\">                   1</td><td>1a2e7ffb  </td><td style=\"text-align: right;\">     5.29239 </td><td style=\"text-align: right;\">     2.04099 </td></tr>\n",
       "<tr><td>FSR_Trainable_1ebbb40f</td><td>2023-08-10_22-39-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.79112</td><td style=\"text-align: right;\">    989.509</td><td style=\"text-align: right;\">     1.47549</td><td style=\"text-align: right;\"> 1.73559e+18</td><td style=\"text-align: right;\"> 0.749813</td><td>172.26.215.93</td><td style=\"text-align: right;\">138685</td><td style=\"text-align: right;\">     2.61693</td><td style=\"text-align: right;\">     685.266</td><td style=\"text-align: right;\">            1.90946 </td><td style=\"text-align: right;\">          0.350572</td><td style=\"text-align: right;\">      1.90946 </td><td style=\"text-align: right;\"> 1691674761</td><td style=\"text-align: right;\">    1.22042 </td><td style=\"text-align: right;\">    0.386363</td><td style=\"text-align: right;\">   7.73354e+13</td><td style=\"text-align: right;\">  7.44711e+14</td><td style=\"text-align: right;\">                   4</td><td>1ebbb40f  </td><td style=\"text-align: right;\">     0.516492</td><td style=\"text-align: right;\">     0.233321</td></tr>\n",
       "<tr><td>FSR_Trainable_1ec667ce</td><td>2023-08-10_22-42-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    5.53427</td><td style=\"text-align: right;\">    753.382</td><td style=\"text-align: right;\">     1.32363</td><td style=\"text-align: right;\"> 1.34244e+18</td><td style=\"text-align: right;\"> 0.71179 </td><td>172.26.215.93</td><td style=\"text-align: right;\">142790</td><td style=\"text-align: right;\">     2.54827</td><td style=\"text-align: right;\">     478.687</td><td style=\"text-align: right;\">           11.4561  </td><td style=\"text-align: right;\">          0.740808</td><td style=\"text-align: right;\">     11.4561  </td><td style=\"text-align: right;\"> 1691674944</td><td style=\"text-align: right;\">    1.17498 </td><td style=\"text-align: right;\">    0.347941</td><td style=\"text-align: right;\">   8.35949e+13</td><td style=\"text-align: right;\">  7.69562e+14</td><td style=\"text-align: right;\">                  16</td><td>1ec667ce  </td><td style=\"text-align: right;\">     0.510424</td><td style=\"text-align: right;\">     0.201366</td></tr>\n",
       "<tr><td>FSR_Trainable_223f2432</td><td>2023-08-10_22-48-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    4.74409</td><td style=\"text-align: right;\">   1419.73 </td><td style=\"text-align: right;\">     1.22264</td><td style=\"text-align: right;\"> 2.81884e+18</td><td style=\"text-align: right;\"> 0.807872</td><td>172.26.215.93</td><td style=\"text-align: right;\">148950</td><td style=\"text-align: right;\">     2.43657</td><td style=\"text-align: right;\">     940.767</td><td style=\"text-align: right;\">            1.17405 </td><td style=\"text-align: right;\">          0.404697</td><td style=\"text-align: right;\">      1.17405 </td><td style=\"text-align: right;\"> 1691675291</td><td style=\"text-align: right;\">    1.0431  </td><td style=\"text-align: right;\">    0.520413</td><td style=\"text-align: right;\">   8.2433e+13 </td><td style=\"text-align: right;\">  1.1232e+15 </td><td style=\"text-align: right;\">                   2</td><td>223f2432  </td><td style=\"text-align: right;\">     0.503608</td><td style=\"text-align: right;\">     0.304263</td></tr>\n",
       "<tr><td>FSR_Trainable_2258fe3e</td><td>2023-08-10_22-37-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.79019</td><td style=\"text-align: right;\">   1103.38 </td><td style=\"text-align: right;\">     1.11746</td><td style=\"text-align: right;\"> 2.06996e+16</td><td style=\"text-align: right;\">40.3029  </td><td>172.26.215.93</td><td style=\"text-align: right;\">135741</td><td style=\"text-align: right;\">     2.3701 </td><td style=\"text-align: right;\">     946.436</td><td style=\"text-align: right;\">            1.5482  </td><td style=\"text-align: right;\">          1.5482  </td><td style=\"text-align: right;\">      1.5482  </td><td style=\"text-align: right;\"> 1691674643</td><td style=\"text-align: right;\">   27.7787  </td><td style=\"text-align: right;\">    5.84881 </td><td style=\"text-align: right;\">   1.38264e+15</td><td style=\"text-align: right;\">  7.07247e+14</td><td style=\"text-align: right;\">                   1</td><td>2258fe3e  </td><td style=\"text-align: right;\">    33.2393  </td><td style=\"text-align: right;\">     7.06361 </td></tr>\n",
       "<tr><td>FSR_Trainable_254f3e5c</td><td>2023-08-10_22-38-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   15.4421 </td><td style=\"text-align: right;\">   8405.19 </td><td style=\"text-align: right;\">     2.9923 </td><td style=\"text-align: right;\"> 6.53955e+17</td><td style=\"text-align: right;\">59.4367  </td><td>172.26.215.93</td><td style=\"text-align: right;\">137624</td><td style=\"text-align: right;\">    11.9181 </td><td style=\"text-align: right;\">   14548.9  </td><td style=\"text-align: right;\">            0.705323</td><td style=\"text-align: right;\">          0.705323</td><td style=\"text-align: right;\">      0.705323</td><td style=\"text-align: right;\"> 1691674711</td><td style=\"text-align: right;\">   51.5569  </td><td style=\"text-align: right;\">   19.4382  </td><td style=\"text-align: right;\">   4.22259e+16</td><td style=\"text-align: right;\">  3.60056e+16</td><td style=\"text-align: right;\">                   1</td><td>254f3e5c  </td><td style=\"text-align: right;\">    40.5169  </td><td style=\"text-align: right;\">    18.9198  </td></tr>\n",
       "<tr><td>FSR_Trainable_25fed8e4</td><td>2023-08-10_22-36-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    7.30059</td><td style=\"text-align: right;\">   1001.79 </td><td style=\"text-align: right;\">     1.42859</td><td style=\"text-align: right;\"> 1.70773e+18</td><td style=\"text-align: right;\"> 0.856586</td><td>172.26.215.93</td><td style=\"text-align: right;\">134974</td><td style=\"text-align: right;\">     3.03115</td><td style=\"text-align: right;\">     638.517</td><td style=\"text-align: right;\">            2.91159 </td><td style=\"text-align: right;\">          0.571605</td><td style=\"text-align: right;\">      2.91159 </td><td style=\"text-align: right;\"> 1691674607</td><td style=\"text-align: right;\">    1.58409 </td><td style=\"text-align: right;\">    0.414766</td><td style=\"text-align: right;\">   7.53007e+13</td><td style=\"text-align: right;\">  8.18955e+14</td><td style=\"text-align: right;\">                   4</td><td>25fed8e4  </td><td style=\"text-align: right;\">     0.618523</td><td style=\"text-align: right;\">     0.238062</td></tr>\n",
       "<tr><td>FSR_Trainable_292a4961</td><td>2023-08-10_22-50-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   14.9111 </td><td style=\"text-align: right;\">   1406.5  </td><td style=\"text-align: right;\">     2.53813</td><td style=\"text-align: right;\"> 3.00955e+18</td><td style=\"text-align: right;\"> 1.47468 </td><td>172.26.215.93</td><td style=\"text-align: right;\">151147</td><td style=\"text-align: right;\">     5.21231</td><td style=\"text-align: right;\">     932.579</td><td style=\"text-align: right;\">            0.702361</td><td style=\"text-align: right;\">          0.702361</td><td style=\"text-align: right;\">      0.702361</td><td style=\"text-align: right;\"> 1691675422</td><td style=\"text-align: right;\">    3.39262 </td><td style=\"text-align: right;\">    0.511275</td><td style=\"text-align: right;\">   6.73967e+13</td><td style=\"text-align: right;\">  1.13102e+15</td><td style=\"text-align: right;\">                   1</td><td>292a4961  </td><td style=\"text-align: right;\">     1.1834  </td><td style=\"text-align: right;\">     0.29128 </td></tr>\n",
       "<tr><td>FSR_Trainable_2ae0cce2</td><td>2023-08-10_22-50-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    6.64063</td><td style=\"text-align: right;\">    881.874</td><td style=\"text-align: right;\">     1.38804</td><td style=\"text-align: right;\"> 1.64901e+18</td><td style=\"text-align: right;\"> 0.789401</td><td>172.26.215.93</td><td style=\"text-align: right;\">151386</td><td style=\"text-align: right;\">     2.93014</td><td style=\"text-align: right;\">     537.595</td><td style=\"text-align: right;\">            1.27937 </td><td style=\"text-align: right;\">          0.538842</td><td style=\"text-align: right;\">      1.27937 </td><td style=\"text-align: right;\"> 1691675436</td><td style=\"text-align: right;\">    1.35995 </td><td style=\"text-align: right;\">    0.376048</td><td style=\"text-align: right;\">   7.48195e+13</td><td style=\"text-align: right;\">  7.74329e+14</td><td style=\"text-align: right;\">                   2</td><td>2ae0cce2  </td><td style=\"text-align: right;\">     0.57232 </td><td style=\"text-align: right;\">     0.217081</td></tr>\n",
       "<tr><td>FSR_Trainable_2b727f4f</td><td>2023-08-10_22-53-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    5.56732</td><td style=\"text-align: right;\">    945.732</td><td style=\"text-align: right;\">     1.35103</td><td style=\"text-align: right;\"> 1.71575e+18</td><td style=\"text-align: right;\"> 0.73825 </td><td>172.26.215.93</td><td style=\"text-align: right;\">154472</td><td style=\"text-align: right;\">     2.62621</td><td style=\"text-align: right;\">     639.181</td><td style=\"text-align: right;\">            1.20457 </td><td style=\"text-align: right;\">          0.648268</td><td style=\"text-align: right;\">      1.20457 </td><td style=\"text-align: right;\"> 1691675594</td><td style=\"text-align: right;\">    1.2004  </td><td style=\"text-align: right;\">    0.361235</td><td style=\"text-align: right;\">   8.33788e+13</td><td style=\"text-align: right;\">  7.08432e+14</td><td style=\"text-align: right;\">                   2</td><td>2b727f4f  </td><td style=\"text-align: right;\">     0.526314</td><td style=\"text-align: right;\">     0.211936</td></tr>\n",
       "<tr><td>FSR_Trainable_2c7af959</td><td>2023-08-10_22-45-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.61699</td><td style=\"text-align: right;\">   1332.98 </td><td style=\"text-align: right;\">     1.19437</td><td style=\"text-align: right;\"> 7.25784e+08</td><td style=\"text-align: right;\"> 6.41673 </td><td>172.26.215.93</td><td style=\"text-align: right;\">146458</td><td style=\"text-align: right;\">     2.49679</td><td style=\"text-align: right;\">     942.731</td><td style=\"text-align: right;\">            0.534642</td><td style=\"text-align: right;\">          0.534642</td><td style=\"text-align: right;\">      0.534642</td><td style=\"text-align: right;\"> 1691675156</td><td style=\"text-align: right;\">    8.42284 </td><td style=\"text-align: right;\">    3.16228 </td><td style=\"text-align: right;\">   1.88544e+15</td><td style=\"text-align: right;\"> 91.4654     </td><td style=\"text-align: right;\">                   1</td><td>2c7af959  </td><td style=\"text-align: right;\">     4.63827 </td><td style=\"text-align: right;\">     1.77846 </td></tr>\n",
       "<tr><td>FSR_Trainable_30ee5665</td><td>2023-08-10_22-46-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    5.24146</td><td style=\"text-align: right;\">    692.173</td><td style=\"text-align: right;\">     1.28288</td><td style=\"text-align: right;\"> 1.1582e+18 </td><td style=\"text-align: right;\"> 0.678663</td><td>172.26.215.93</td><td style=\"text-align: right;\">146686</td><td style=\"text-align: right;\">     2.48981</td><td style=\"text-align: right;\">     429.876</td><td style=\"text-align: right;\">            7.18847 </td><td style=\"text-align: right;\">          0.321271</td><td style=\"text-align: right;\">      7.18847 </td><td style=\"text-align: right;\"> 1691675176</td><td style=\"text-align: right;\">    1.11612 </td><td style=\"text-align: right;\">    0.313118</td><td style=\"text-align: right;\">   8.12413e+13</td><td style=\"text-align: right;\">  6.60823e+14</td><td style=\"text-align: right;\">                  16</td><td>30ee5665  </td><td style=\"text-align: right;\">     0.499749</td><td style=\"text-align: right;\">     0.178914</td></tr>\n",
       "<tr><td>FSR_Trainable_352e843d</td><td>2023-08-10_22-50-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    6.6401 </td><td style=\"text-align: right;\">    897.054</td><td style=\"text-align: right;\">     1.29754</td><td style=\"text-align: right;\"> 1.54932e+18</td><td style=\"text-align: right;\"> 0.803788</td><td>172.26.215.93</td><td style=\"text-align: right;\">151476</td><td style=\"text-align: right;\">     2.90689</td><td style=\"text-align: right;\">     564.935</td><td style=\"text-align: right;\">            1.41834 </td><td style=\"text-align: right;\">          0.467187</td><td style=\"text-align: right;\">      1.41834 </td><td style=\"text-align: right;\"> 1691675443</td><td style=\"text-align: right;\">    1.39896 </td><td style=\"text-align: right;\">    0.394007</td><td style=\"text-align: right;\">   7.77729e+13</td><td style=\"text-align: right;\">  7.89839e+14</td><td style=\"text-align: right;\">                   2</td><td>352e843d  </td><td style=\"text-align: right;\">     0.577649</td><td style=\"text-align: right;\">     0.226139</td></tr>\n",
       "<tr><td>FSR_Trainable_3dc99959</td><td>2023-08-10_22-47-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.42422</td><td style=\"text-align: right;\">    631.239</td><td style=\"text-align: right;\">     1.1121 </td><td style=\"text-align: right;\"> 8.18104e+17</td><td style=\"text-align: right;\"> 0.649945</td><td>172.26.215.93</td><td style=\"text-align: right;\">147775</td><td style=\"text-align: right;\">     2.42132</td><td style=\"text-align: right;\">     439.459</td><td style=\"text-align: right;\">           36.7484  </td><td style=\"text-align: right;\">          0.218629</td><td style=\"text-align: right;\">     36.7484  </td><td style=\"text-align: right;\"> 1691675279</td><td style=\"text-align: right;\">    0.949158</td><td style=\"text-align: right;\">    0.248936</td><td style=\"text-align: right;\">   7.96442e+13</td><td style=\"text-align: right;\">  3.56676e+14</td><td style=\"text-align: right;\">                 100</td><td>3dc99959  </td><td style=\"text-align: right;\">     0.48487 </td><td style=\"text-align: right;\">     0.165075</td></tr>\n",
       "<tr><td>FSR_Trainable_3f75ed82</td><td>2023-08-10_22-44-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    5.01799</td><td style=\"text-align: right;\">    809.303</td><td style=\"text-align: right;\">     1.2331 </td><td style=\"text-align: right;\"> 1.56197e+18</td><td style=\"text-align: right;\"> 0.683126</td><td>172.26.215.93</td><td style=\"text-align: right;\">145260</td><td style=\"text-align: right;\">     2.45356</td><td style=\"text-align: right;\">     466.028</td><td style=\"text-align: right;\">           14.6935  </td><td style=\"text-align: right;\">          0.635844</td><td style=\"text-align: right;\">     14.6935  </td><td style=\"text-align: right;\"> 1691675095</td><td style=\"text-align: right;\">    1.07194 </td><td style=\"text-align: right;\">    0.360171</td><td style=\"text-align: right;\">   7.80554e+13</td><td style=\"text-align: right;\">  8.25917e+14</td><td style=\"text-align: right;\">                  16</td><td>3f75ed82  </td><td style=\"text-align: right;\">     0.492352</td><td style=\"text-align: right;\">     0.190774</td></tr>\n",
       "<tr><td>FSR_Trainable_405740e0</td><td>2023-08-10_22-50-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.40042</td><td style=\"text-align: right;\">    628.717</td><td style=\"text-align: right;\">     1.09974</td><td style=\"text-align: right;\"> 8.16056e+17</td><td style=\"text-align: right;\"> 0.64909 </td><td>172.26.215.93</td><td style=\"text-align: right;\">150244</td><td style=\"text-align: right;\">     2.41252</td><td style=\"text-align: right;\">     437.431</td><td style=\"text-align: right;\">           35.9579  </td><td style=\"text-align: right;\">          0.259389</td><td style=\"text-align: right;\">     35.9579  </td><td style=\"text-align: right;\"> 1691675421</td><td style=\"text-align: right;\">    0.946362</td><td style=\"text-align: right;\">    0.248116</td><td style=\"text-align: right;\">   7.95413e+13</td><td style=\"text-align: right;\">  3.55493e+14</td><td style=\"text-align: right;\">                 100</td><td>405740e0  </td><td style=\"text-align: right;\">     0.484401</td><td style=\"text-align: right;\">     0.164688</td></tr>\n",
       "<tr><td>FSR_Trainable_49e42cfa</td><td>2023-08-10_22-36-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    5.28867</td><td style=\"text-align: right;\">   1052.22 </td><td style=\"text-align: right;\">     1.22498</td><td style=\"text-align: right;\"> 7.55748e+08</td><td style=\"text-align: right;\"> 6.79089 </td><td>172.26.215.93</td><td style=\"text-align: right;\">134030</td><td style=\"text-align: right;\">     2.80626</td><td style=\"text-align: right;\">     710.227</td><td style=\"text-align: right;\">            1.88604 </td><td style=\"text-align: right;\">          0.84487 </td><td style=\"text-align: right;\">      1.88604 </td><td style=\"text-align: right;\"> 1691674562</td><td style=\"text-align: right;\">    9.41496 </td><td style=\"text-align: right;\">    2.91372 </td><td style=\"text-align: right;\">   3.69229e+15</td><td style=\"text-align: right;\">160.311      </td><td style=\"text-align: right;\">                   2</td><td>49e42cfa  </td><td style=\"text-align: right;\">     4.90961 </td><td style=\"text-align: right;\">     1.88128 </td></tr>\n",
       "<tr><td>FSR_Trainable_52a2c1ef</td><td>2023-08-10_22-45-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    6.92257</td><td style=\"text-align: right;\">   1108.49 </td><td style=\"text-align: right;\">     1.55319</td><td style=\"text-align: right;\"> 2.02703e+18</td><td style=\"text-align: right;\"> 0.813033</td><td>172.26.215.93</td><td style=\"text-align: right;\">145982</td><td style=\"text-align: right;\">     2.92705</td><td style=\"text-align: right;\">     744.953</td><td style=\"text-align: right;\">            0.930013</td><td style=\"text-align: right;\">          0.361137</td><td style=\"text-align: right;\">      0.930013</td><td style=\"text-align: right;\"> 1691675139</td><td style=\"text-align: right;\">    1.49262 </td><td style=\"text-align: right;\">    0.3981  </td><td style=\"text-align: right;\">   8.77066e+13</td><td style=\"text-align: right;\">  8.02442e+14</td><td style=\"text-align: right;\">                   2</td><td>52a2c1ef  </td><td style=\"text-align: right;\">     0.588123</td><td style=\"text-align: right;\">     0.22491 </td></tr>\n",
       "<tr><td>FSR_Trainable_5456a8ec</td><td>2023-08-10_22-42-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   25.9768 </td><td style=\"text-align: right;\">   1874.95 </td><td style=\"text-align: right;\">     3.96173</td><td style=\"text-align: right;\"> 3.1854e+18 </td><td style=\"text-align: right;\"> 2.20226 </td><td>172.26.215.93</td><td style=\"text-align: right;\">143232</td><td style=\"text-align: right;\">     8.43509</td><td style=\"text-align: right;\">    1180.49 </td><td style=\"text-align: right;\">            0.439428</td><td style=\"text-align: right;\">          0.439428</td><td style=\"text-align: right;\">      0.439428</td><td style=\"text-align: right;\"> 1691674950</td><td style=\"text-align: right;\">    5.71801 </td><td style=\"text-align: right;\">    0.670276</td><td style=\"text-align: right;\">   1.24387e+13</td><td style=\"text-align: right;\">  1.14859e+15</td><td style=\"text-align: right;\">                   1</td><td>5456a8ec  </td><td style=\"text-align: right;\">     1.80592 </td><td style=\"text-align: right;\">     0.396337</td></tr>\n",
       "<tr><td>FSR_Trainable_545d0224</td><td>2023-08-10_22-47-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.21449</td><td style=\"text-align: right;\">    895.87 </td><td style=\"text-align: right;\">     1.12446</td><td style=\"text-align: right;\"> 1.04075e+17</td><td style=\"text-align: right;\">38.0877  </td><td>172.26.215.93</td><td style=\"text-align: right;\">148245</td><td style=\"text-align: right;\">     2.37158</td><td style=\"text-align: right;\">     714.506</td><td style=\"text-align: right;\">            0.408051</td><td style=\"text-align: right;\">          0.408051</td><td style=\"text-align: right;\">      0.408051</td><td style=\"text-align: right;\"> 1691675257</td><td style=\"text-align: right;\">   28.6768  </td><td style=\"text-align: right;\">    4.9288  </td><td style=\"text-align: right;\">   4.31397e+15</td><td style=\"text-align: right;\">  4.18619e+15</td><td style=\"text-align: right;\">                   1</td><td>545d0224  </td><td style=\"text-align: right;\">    33.3362  </td><td style=\"text-align: right;\">     4.75151 </td></tr>\n",
       "<tr><td>FSR_Trainable_549e2a1c</td><td>2023-08-10_22-48-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   16.1533 </td><td style=\"text-align: right;\">   1395.68 </td><td style=\"text-align: right;\">     2.71449</td><td style=\"text-align: right;\"> 3.12496e+18</td><td style=\"text-align: right;\"> 1.53748 </td><td>172.26.215.93</td><td style=\"text-align: right;\">148713</td><td style=\"text-align: right;\">     6.09244</td><td style=\"text-align: right;\">     854.646</td><td style=\"text-align: right;\">            0.778097</td><td style=\"text-align: right;\">          0.778097</td><td style=\"text-align: right;\">      0.778097</td><td style=\"text-align: right;\"> 1691675280</td><td style=\"text-align: right;\">    3.43236 </td><td style=\"text-align: right;\">    0.519264</td><td style=\"text-align: right;\">   7.03815e+13</td><td style=\"text-align: right;\">  1.24529e+15</td><td style=\"text-align: right;\">                   1</td><td>549e2a1c  </td><td style=\"text-align: right;\">     1.26349 </td><td style=\"text-align: right;\">     0.273988</td></tr>\n",
       "<tr><td>FSR_Trainable_56638e36</td><td>2023-08-10_22-43-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.6591 </td><td style=\"text-align: right;\">    639.304</td><td style=\"text-align: right;\">     1.11294</td><td style=\"text-align: right;\"> 9.16272e+17</td><td style=\"text-align: right;\"> 0.654956</td><td>172.26.215.93</td><td style=\"text-align: right;\">143790</td><td style=\"text-align: right;\">     2.45344</td><td style=\"text-align: right;\">     430.938</td><td style=\"text-align: right;\">           49.1302  </td><td style=\"text-align: right;\">          0.48204 </td><td style=\"text-align: right;\">     49.1302  </td><td style=\"text-align: right;\"> 1691675036</td><td style=\"text-align: right;\">    0.999647</td><td style=\"text-align: right;\">    0.261525</td><td style=\"text-align: right;\">   7.79273e+13</td><td style=\"text-align: right;\">  4.33272e+14</td><td style=\"text-align: right;\">                 100</td><td>56638e36  </td><td style=\"text-align: right;\">     0.490855</td><td style=\"text-align: right;\">     0.164101</td></tr>\n",
       "<tr><td>FSR_Trainable_59e6f3f0</td><td>2023-08-10_22-38-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.30506</td><td style=\"text-align: right;\">   1163.57 </td><td style=\"text-align: right;\">     1.16772</td><td style=\"text-align: right;\"> 2.12775e+18</td><td style=\"text-align: right;\"> 0.766974</td><td>172.26.215.93</td><td style=\"text-align: right;\">138115</td><td style=\"text-align: right;\">     2.56221</td><td style=\"text-align: right;\">     792.852</td><td style=\"text-align: right;\">            2.04026 </td><td style=\"text-align: right;\">          0.464194</td><td style=\"text-align: right;\">      2.04026 </td><td style=\"text-align: right;\"> 1691674737</td><td style=\"text-align: right;\">    1.15572 </td><td style=\"text-align: right;\">    0.421475</td><td style=\"text-align: right;\">   7.36004e+13</td><td style=\"text-align: right;\">  8.11015e+14</td><td style=\"text-align: right;\">                   4</td><td>59e6f3f0  </td><td style=\"text-align: right;\">     0.511549</td><td style=\"text-align: right;\">     0.255425</td></tr>\n",
       "<tr><td>FSR_Trainable_5cf78066</td><td>2023-08-10_22-38-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    5.65886</td><td style=\"text-align: right;\">   1155.94 </td><td style=\"text-align: right;\">     1.47502</td><td style=\"text-align: right;\"> 9.46127e+08</td><td style=\"text-align: right;\"> 6.78401 </td><td>172.26.215.93</td><td style=\"text-align: right;\">138024</td><td style=\"text-align: right;\">     2.90161</td><td style=\"text-align: right;\">     878.1  </td><td style=\"text-align: right;\">            1.19538 </td><td style=\"text-align: right;\">          1.19538 </td><td style=\"text-align: right;\">      1.19538 </td><td style=\"text-align: right;\"> 1691674728</td><td style=\"text-align: right;\">    9.68512 </td><td style=\"text-align: right;\">    2.92916 </td><td style=\"text-align: right;\"> 886.605      </td><td style=\"text-align: right;\"> 98.6173     </td><td style=\"text-align: right;\">                   1</td><td>5cf78066  </td><td style=\"text-align: right;\">     4.9788  </td><td style=\"text-align: right;\">     1.80521 </td></tr>\n",
       "<tr><td>FSR_Trainable_5ea11c97</td><td>2023-08-10_22-38-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   22.0969 </td><td style=\"text-align: right;\">   1923.21 </td><td style=\"text-align: right;\">     4.14092</td><td style=\"text-align: right;\"> 3.5938e+18 </td><td style=\"text-align: right;\"> 2.12622 </td><td>172.26.215.93</td><td style=\"text-align: right;\">137122</td><td style=\"text-align: right;\">     7.28772</td><td style=\"text-align: right;\">    1108.06 </td><td style=\"text-align: right;\">            0.366137</td><td style=\"text-align: right;\">          0.366137</td><td style=\"text-align: right;\">      0.366137</td><td style=\"text-align: right;\"> 1691674695</td><td style=\"text-align: right;\">    5.0407  </td><td style=\"text-align: right;\">    0.888336</td><td style=\"text-align: right;\">   4.42134e+13</td><td style=\"text-align: right;\">  1.99937e+15</td><td style=\"text-align: right;\">                   1</td><td>5ea11c97  </td><td style=\"text-align: right;\">     1.65339 </td><td style=\"text-align: right;\">     0.472833</td></tr>\n",
       "<tr><td>FSR_Trainable_5fe67775</td><td>2023-08-10_22-37-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">   24.1605 </td><td style=\"text-align: right;\">   1536.57 </td><td style=\"text-align: right;\">     3.97913</td><td style=\"text-align: right;\"> 1.61099e+18</td><td style=\"text-align: right;\"> 2.05987 </td><td>172.26.215.93</td><td style=\"text-align: right;\">135675</td><td style=\"text-align: right;\">     7.65234</td><td style=\"text-align: right;\">    1128.07 </td><td style=\"text-align: right;\">            1.38844 </td><td style=\"text-align: right;\">          0.487181</td><td style=\"text-align: right;\">      1.38844 </td><td style=\"text-align: right;\"> 1691674637</td><td style=\"text-align: right;\">    5.42529 </td><td style=\"text-align: right;\">    0.573491</td><td style=\"text-align: right;\">   2.26883e+13</td><td style=\"text-align: right;\">  7.64382e+14</td><td style=\"text-align: right;\">                   2</td><td>5fe67775  </td><td style=\"text-align: right;\">     1.69682 </td><td style=\"text-align: right;\">     0.363057</td></tr>\n",
       "<tr><td>FSR_Trainable_622e1df8</td><td>2023-08-10_22-49-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.44215</td><td style=\"text-align: right;\">    629.873</td><td style=\"text-align: right;\">     1.10093</td><td style=\"text-align: right;\"> 8.27417e+17</td><td style=\"text-align: right;\"> 0.650804</td><td>172.26.215.93</td><td style=\"text-align: right;\">149214</td><td style=\"text-align: right;\">     2.42421</td><td style=\"text-align: right;\">     436.326</td><td style=\"text-align: right;\">           39.7737  </td><td style=\"text-align: right;\">          0.456445</td><td style=\"text-align: right;\">     39.7737  </td><td style=\"text-align: right;\"> 1691675358</td><td style=\"text-align: right;\">    0.954721</td><td style=\"text-align: right;\">    0.250445</td><td style=\"text-align: right;\">   7.89679e+13</td><td style=\"text-align: right;\">  3.67421e+14</td><td style=\"text-align: right;\">                 100</td><td>622e1df8  </td><td style=\"text-align: right;\">     0.486076</td><td style=\"text-align: right;\">     0.164728</td></tr>\n",
       "<tr><td>FSR_Trainable_63e20809</td><td>2023-08-10_22-52-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.51279</td><td style=\"text-align: right;\">   1158.26 </td><td style=\"text-align: right;\">     1.28684</td><td style=\"text-align: right;\"> 7.52997e+16</td><td style=\"text-align: right;\">39.3376  </td><td>172.26.215.93</td><td style=\"text-align: right;\">153346</td><td style=\"text-align: right;\">     2.54964</td><td style=\"text-align: right;\">    1016.29 </td><td style=\"text-align: right;\">            0.486954</td><td style=\"text-align: right;\">          0.486954</td><td style=\"text-align: right;\">      0.486954</td><td style=\"text-align: right;\"> 1691675534</td><td style=\"text-align: right;\">   29.3029  </td><td style=\"text-align: right;\">    5.37073 </td><td style=\"text-align: right;\">   4.37863e+15</td><td style=\"text-align: right;\">  3.04694e+15</td><td style=\"text-align: right;\">                   1</td><td>63e20809  </td><td style=\"text-align: right;\">    33.6559  </td><td style=\"text-align: right;\">     5.68165 </td></tr>\n",
       "<tr><td>FSR_Trainable_64391342</td><td>2023-08-10_22-44-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.54596</td><td style=\"text-align: right;\">    644.141</td><td style=\"text-align: right;\">     1.10837</td><td style=\"text-align: right;\"> 8.97832e+17</td><td style=\"text-align: right;\"> 0.6533  </td><td>172.26.215.93</td><td style=\"text-align: right;\">143972</td><td style=\"text-align: right;\">     2.42711</td><td style=\"text-align: right;\">     437.442</td><td style=\"text-align: right;\">           49.6819  </td><td style=\"text-align: right;\">          0.425775</td><td style=\"text-align: right;\">     49.6819  </td><td style=\"text-align: right;\"> 1691675043</td><td style=\"text-align: right;\">    0.977171</td><td style=\"text-align: right;\">    0.262675</td><td style=\"text-align: right;\">   7.84408e+13</td><td style=\"text-align: right;\">  4.22141e+14</td><td style=\"text-align: right;\">                 100</td><td>64391342  </td><td style=\"text-align: right;\">     0.485898</td><td style=\"text-align: right;\">     0.167402</td></tr>\n",
       "<tr><td>FSR_Trainable_64ab2554</td><td>2023-08-10_22-53-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    4.33015</td><td style=\"text-align: right;\">    771.195</td><td style=\"text-align: right;\">     1.1446 </td><td style=\"text-align: right;\"> 1.23556e+18</td><td style=\"text-align: right;\"> 0.660548</td><td>172.26.215.93</td><td style=\"text-align: right;\">155175</td><td style=\"text-align: right;\">     2.36006</td><td style=\"text-align: right;\">     557.099</td><td style=\"text-align: right;\">            4.88313 </td><td style=\"text-align: right;\">          0.206396</td><td style=\"text-align: right;\">      4.88313 </td><td style=\"text-align: right;\"> 1691675635</td><td style=\"text-align: right;\">    0.934107</td><td style=\"text-align: right;\">    0.289596</td><td style=\"text-align: right;\">   7.9593e+13 </td><td style=\"text-align: right;\">  4.94046e+14</td><td style=\"text-align: right;\">                  16</td><td>64ab2554  </td><td style=\"text-align: right;\">     0.474094</td><td style=\"text-align: right;\">     0.186454</td></tr>\n",
       "<tr><td>FSR_Trainable_6670c862</td><td>2023-08-10_22-42-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   25.933  </td><td style=\"text-align: right;\">   2582.6  </td><td style=\"text-align: right;\">     4.33372</td><td style=\"text-align: right;\"> 3.87772e+18</td><td style=\"text-align: right;\"> 2.4116  </td><td>172.26.215.93</td><td style=\"text-align: right;\">143471</td><td style=\"text-align: right;\">     8.91215</td><td style=\"text-align: right;\">    1662.95 </td><td style=\"text-align: right;\">            0.685786</td><td style=\"text-align: right;\">          0.685786</td><td style=\"text-align: right;\">      0.685786</td><td style=\"text-align: right;\"> 1691674959</td><td style=\"text-align: right;\">    5.73739 </td><td style=\"text-align: right;\">    0.97625 </td><td style=\"text-align: right;\">   2.82518e+13</td><td style=\"text-align: right;\">  1.95033e+15</td><td style=\"text-align: right;\">                   1</td><td>6670c862  </td><td style=\"text-align: right;\">     1.89867 </td><td style=\"text-align: right;\">     0.512925</td></tr>\n",
       "<tr><td>FSR_Trainable_6f44ace7</td><td>2023-08-10_22-51-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.07079</td><td style=\"text-align: right;\">    672.208</td><td style=\"text-align: right;\">     1.22629</td><td style=\"text-align: right;\"> 1.05787e+18</td><td style=\"text-align: right;\"> 0.680168</td><td>172.26.215.93</td><td style=\"text-align: right;\">151882</td><td style=\"text-align: right;\">     2.49162</td><td style=\"text-align: right;\">     433.004</td><td style=\"text-align: right;\">            3.26757 </td><td style=\"text-align: right;\">          0.37256 </td><td style=\"text-align: right;\">      3.26757 </td><td style=\"text-align: right;\"> 1691675463</td><td style=\"text-align: right;\">    1.0738  </td><td style=\"text-align: right;\">    0.307673</td><td style=\"text-align: right;\">   7.86571e+13</td><td style=\"text-align: right;\">  6.14238e+14</td><td style=\"text-align: right;\">                   8</td><td>6f44ace7  </td><td style=\"text-align: right;\">     0.497079</td><td style=\"text-align: right;\">     0.183089</td></tr>\n",
       "<tr><td>FSR_Trainable_70ee40e6</td><td>2023-08-10_22-43-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.32762</td><td style=\"text-align: right;\">    621.43 </td><td style=\"text-align: right;\">     1.11361</td><td style=\"text-align: right;\"> 8.08103e+17</td><td style=\"text-align: right;\"> 0.640633</td><td>172.26.215.93</td><td style=\"text-align: right;\">143704</td><td style=\"text-align: right;\">     2.38734</td><td style=\"text-align: right;\">     434.444</td><td style=\"text-align: right;\">           37.6831  </td><td style=\"text-align: right;\">          0.510098</td><td style=\"text-align: right;\">     37.6831  </td><td style=\"text-align: right;\"> 1691675018</td><td style=\"text-align: right;\">    0.93302 </td><td style=\"text-align: right;\">    0.241022</td><td style=\"text-align: right;\">   7.86967e+13</td><td style=\"text-align: right;\">  3.42975e+14</td><td style=\"text-align: right;\">                 100</td><td>70ee40e6  </td><td style=\"text-align: right;\">     0.481283</td><td style=\"text-align: right;\">     0.15935 </td></tr>\n",
       "<tr><td>FSR_Trainable_72ac5990</td><td>2023-08-10_22-50-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   21.0023 </td><td style=\"text-align: right;\">   1830.98 </td><td style=\"text-align: right;\">     3.36066</td><td style=\"text-align: right;\"> 4.15851e+18</td><td style=\"text-align: right;\"> 1.92191 </td><td>172.26.215.93</td><td style=\"text-align: right;\">150925</td><td style=\"text-align: right;\">     7.09398</td><td style=\"text-align: right;\">    1022.62 </td><td style=\"text-align: right;\">            0.576678</td><td style=\"text-align: right;\">          0.576678</td><td style=\"text-align: right;\">      0.576678</td><td style=\"text-align: right;\"> 1691675410</td><td style=\"text-align: right;\">    4.55974 </td><td style=\"text-align: right;\">    0.779547</td><td style=\"text-align: right;\">   5.70072e+13</td><td style=\"text-align: right;\">  1.96711e+15</td><td style=\"text-align: right;\">                   1</td><td>72ac5990  </td><td style=\"text-align: right;\">     1.51601 </td><td style=\"text-align: right;\">     0.405904</td></tr>\n",
       "<tr><td>FSR_Trainable_730fdf81</td><td>2023-08-10_22-47-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.40033</td><td style=\"text-align: right;\">    627.018</td><td style=\"text-align: right;\">     1.10618</td><td style=\"text-align: right;\"> 8.15693e+17</td><td style=\"text-align: right;\"> 0.647642</td><td>172.26.215.93</td><td style=\"text-align: right;\">146921</td><td style=\"text-align: right;\">     2.41338</td><td style=\"text-align: right;\">     436.837</td><td style=\"text-align: right;\">           42.0472  </td><td style=\"text-align: right;\">          0.479107</td><td style=\"text-align: right;\">     42.0472  </td><td style=\"text-align: right;\"> 1691675236</td><td style=\"text-align: right;\">    0.945919</td><td style=\"text-align: right;\">    0.245876</td><td style=\"text-align: right;\">   7.92545e+13</td><td style=\"text-align: right;\">  3.51725e+14</td><td style=\"text-align: right;\">                 100</td><td>730fdf81  </td><td style=\"text-align: right;\">     0.484637</td><td style=\"text-align: right;\">     0.163005</td></tr>\n",
       "<tr><td>FSR_Trainable_76d84eb0</td><td>2023-08-10_22-39-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.84673</td><td style=\"text-align: right;\">   1033.09 </td><td style=\"text-align: right;\">     1.18108</td><td style=\"text-align: right;\"> 1.76836e+17</td><td style=\"text-align: right;\">39.0866  </td><td>172.26.215.93</td><td style=\"text-align: right;\">139614</td><td style=\"text-align: right;\">     2.54798</td><td style=\"text-align: right;\">     921.407</td><td style=\"text-align: right;\">            0.757781</td><td style=\"text-align: right;\">          0.757781</td><td style=\"text-align: right;\">      0.757781</td><td style=\"text-align: right;\"> 1691674789</td><td style=\"text-align: right;\">   30.1848  </td><td style=\"text-align: right;\">    5.28483 </td><td style=\"text-align: right;\">   5.92319e+15</td><td style=\"text-align: right;\">  6.31828e+15</td><td style=\"text-align: right;\">                   1</td><td>76d84eb0  </td><td style=\"text-align: right;\">    33.9307  </td><td style=\"text-align: right;\">     5.15582 </td></tr>\n",
       "<tr><td>FSR_Trainable_7745b095</td><td>2023-08-10_22-53-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    5.32362</td><td style=\"text-align: right;\">    965.973</td><td style=\"text-align: right;\">     1.28006</td><td style=\"text-align: right;\"> 1.57245e+18</td><td style=\"text-align: right;\"> 0.747998</td><td>172.26.215.93</td><td style=\"text-align: right;\">154254</td><td style=\"text-align: right;\">     2.50137</td><td style=\"text-align: right;\">     711.203</td><td style=\"text-align: right;\">            1.2232  </td><td style=\"text-align: right;\">          0.536931</td><td style=\"text-align: right;\">      1.2232  </td><td style=\"text-align: right;\"> 1691675581</td><td style=\"text-align: right;\">    1.17064 </td><td style=\"text-align: right;\">    0.355282</td><td style=\"text-align: right;\">   8.76904e+13</td><td style=\"text-align: right;\">  6.17303e+14</td><td style=\"text-align: right;\">                   2</td><td>7745b095  </td><td style=\"text-align: right;\">     0.516941</td><td style=\"text-align: right;\">     0.231056</td></tr>\n",
       "<tr><td>FSR_Trainable_7a48eded</td><td>2023-08-10_22-51-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    5.1202 </td><td style=\"text-align: right;\">   1000.29 </td><td style=\"text-align: right;\">     1.32434</td><td style=\"text-align: right;\"> 1.8399e+18 </td><td style=\"text-align: right;\"> 0.744186</td><td>172.26.215.93</td><td style=\"text-align: right;\">152414</td><td style=\"text-align: right;\">     2.49798</td><td style=\"text-align: right;\">     711.85 </td><td style=\"text-align: right;\">            1.31403 </td><td style=\"text-align: right;\">          0.636227</td><td style=\"text-align: right;\">      1.31403 </td><td style=\"text-align: right;\"> 1691675485</td><td style=\"text-align: right;\">    1.12801 </td><td style=\"text-align: right;\">    0.389072</td><td style=\"text-align: right;\">   9.08252e+13</td><td style=\"text-align: right;\">  8.08108e+14</td><td style=\"text-align: right;\">                   2</td><td>7a48eded  </td><td style=\"text-align: right;\">     0.511872</td><td style=\"text-align: right;\">     0.232314</td></tr>\n",
       "<tr><td>FSR_Trainable_7aceb6ac</td><td>2023-08-10_22-53-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.36811</td><td style=\"text-align: right;\">    628.735</td><td style=\"text-align: right;\">     1.09296</td><td style=\"text-align: right;\"> 8.08114e+17</td><td style=\"text-align: right;\"> 0.64752 </td><td>172.26.215.93</td><td style=\"text-align: right;\">154038</td><td style=\"text-align: right;\">     2.40041</td><td style=\"text-align: right;\">     439.022</td><td style=\"text-align: right;\">           36.8204  </td><td style=\"text-align: right;\">          0.305108</td><td style=\"text-align: right;\">     36.8204  </td><td style=\"text-align: right;\"> 1691675618</td><td style=\"text-align: right;\">    0.941269</td><td style=\"text-align: right;\">    0.245982</td><td style=\"text-align: right;\">   7.88682e+13</td><td style=\"text-align: right;\">  3.4561e+14 </td><td style=\"text-align: right;\">                 100</td><td>7aceb6ac  </td><td style=\"text-align: right;\">     0.483488</td><td style=\"text-align: right;\">     0.164032</td></tr>\n",
       "<tr><td>FSR_Trainable_7b47a99c</td><td>2023-08-10_22-44-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.64607</td><td style=\"text-align: right;\">    637.471</td><td style=\"text-align: right;\">     1.11598</td><td style=\"text-align: right;\"> 9.12679e+17</td><td style=\"text-align: right;\"> 0.653452</td><td>172.26.215.93</td><td style=\"text-align: right;\">144277</td><td style=\"text-align: right;\">     2.45232</td><td style=\"text-align: right;\">     432.059</td><td style=\"text-align: right;\">           49.1998  </td><td style=\"text-align: right;\">          0.57707 </td><td style=\"text-align: right;\">     49.1998  </td><td style=\"text-align: right;\"> 1691675057</td><td style=\"text-align: right;\">    0.997232</td><td style=\"text-align: right;\">    0.255925</td><td style=\"text-align: right;\">   7.77917e+13</td><td style=\"text-align: right;\">  4.11198e+14</td><td style=\"text-align: right;\">                 100</td><td>7b47a99c  </td><td style=\"text-align: right;\">     0.490028</td><td style=\"text-align: right;\">     0.163424</td></tr>\n",
       "<tr><td>FSR_Trainable_81cb7e1f</td><td>2023-08-10_22-39-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   25.9573 </td><td style=\"text-align: right;\">   2531.96 </td><td style=\"text-align: right;\">     4.31706</td><td style=\"text-align: right;\"> 2.83933e+18</td><td style=\"text-align: right;\"> 2.35822 </td><td>172.26.215.93</td><td style=\"text-align: right;\">139117</td><td style=\"text-align: right;\">     8.82089</td><td style=\"text-align: right;\">    1871.94 </td><td style=\"text-align: right;\">            0.774022</td><td style=\"text-align: right;\">          0.774022</td><td style=\"text-align: right;\">      0.774022</td><td style=\"text-align: right;\"> 1691674773</td><td style=\"text-align: right;\">    5.62615 </td><td style=\"text-align: right;\">    0.851651</td><td style=\"text-align: right;\">   3.04171e+13</td><td style=\"text-align: right;\">  1.15133e+15</td><td style=\"text-align: right;\">                   1</td><td>81cb7e1f  </td><td style=\"text-align: right;\">     1.8362  </td><td style=\"text-align: right;\">     0.522028</td></tr>\n",
       "<tr><td>FSR_Trainable_8484e1ff</td><td>2023-08-10_22-36-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.99288</td><td style=\"text-align: right;\">   1522.18 </td><td style=\"text-align: right;\">     1.32335</td><td style=\"text-align: right;\"> 6.13815e+08</td><td style=\"text-align: right;\"> 6.7358  </td><td>172.26.215.93</td><td style=\"text-align: right;\">135202</td><td style=\"text-align: right;\">     2.50165</td><td style=\"text-align: right;\">    1005.23 </td><td style=\"text-align: right;\">            0.584313</td><td style=\"text-align: right;\">          0.584313</td><td style=\"text-align: right;\">      0.584313</td><td style=\"text-align: right;\"> 1691674615</td><td style=\"text-align: right;\">    8.82356 </td><td style=\"text-align: right;\">    3.70513 </td><td style=\"text-align: right;\">1049.57       </td><td style=\"text-align: right;\"> 35.0568     </td><td style=\"text-align: right;\">                   1</td><td>8484e1ff  </td><td style=\"text-align: right;\">     4.50793 </td><td style=\"text-align: right;\">     2.22787 </td></tr>\n",
       "<tr><td>FSR_Trainable_852e9620</td><td>2023-08-10_22-47-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.40522</td><td style=\"text-align: right;\">    626.539</td><td style=\"text-align: right;\">     1.10912</td><td style=\"text-align: right;\"> 8.02049e+17</td><td style=\"text-align: right;\"> 0.648784</td><td>172.26.215.93</td><td style=\"text-align: right;\">147149</td><td style=\"text-align: right;\">     2.41549</td><td style=\"text-align: right;\">     438.5  </td><td style=\"text-align: right;\">           44.9481  </td><td style=\"text-align: right;\">          0.419256</td><td style=\"text-align: right;\">     44.9481  </td><td style=\"text-align: right;\"> 1691675251</td><td style=\"text-align: right;\">    0.946187</td><td style=\"text-align: right;\">    0.245156</td><td style=\"text-align: right;\">   7.89791e+13</td><td style=\"text-align: right;\">  3.43276e+14</td><td style=\"text-align: right;\">                 100</td><td>852e9620  </td><td style=\"text-align: right;\">     0.484704</td><td style=\"text-align: right;\">     0.16408 </td></tr>\n",
       "<tr><td>FSR_Trainable_89c82371</td><td>2023-08-10_22-46-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.34487</td><td style=\"text-align: right;\">    621.937</td><td style=\"text-align: right;\">     1.09901</td><td style=\"text-align: right;\"> 8.05839e+17</td><td style=\"text-align: right;\"> 0.64309 </td><td>172.26.215.93</td><td style=\"text-align: right;\">145762</td><td style=\"text-align: right;\">     2.39466</td><td style=\"text-align: right;\">     437.145</td><td style=\"text-align: right;\">           31.0618  </td><td style=\"text-align: right;\">          0.269032</td><td style=\"text-align: right;\">     31.0618  </td><td style=\"text-align: right;\"> 1691675166</td><td style=\"text-align: right;\">    0.936156</td><td style=\"text-align: right;\">    0.240194</td><td style=\"text-align: right;\">   7.9191e+13 </td><td style=\"text-align: right;\">  3.357e+14  </td><td style=\"text-align: right;\">                 100</td><td>89c82371  </td><td style=\"text-align: right;\">     0.482035</td><td style=\"text-align: right;\">     0.161055</td></tr>\n",
       "<tr><td>FSR_Trainable_89d84316</td><td>2023-08-10_22-47-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    5.01432</td><td style=\"text-align: right;\">   1185.91 </td><td style=\"text-align: right;\">     1.31499</td><td style=\"text-align: right;\"> 7.85634e+16</td><td style=\"text-align: right;\">40.143   </td><td>172.26.215.93</td><td style=\"text-align: right;\">148481</td><td style=\"text-align: right;\">     2.66955</td><td style=\"text-align: right;\">     991.409</td><td style=\"text-align: right;\">            0.501491</td><td style=\"text-align: right;\">          0.501491</td><td style=\"text-align: right;\">      0.501491</td><td style=\"text-align: right;\"> 1691675270</td><td style=\"text-align: right;\">   29.9513  </td><td style=\"text-align: right;\">    6.12996 </td><td style=\"text-align: right;\">   8.81176e+15</td><td style=\"text-align: right;\">  2.71307e+15</td><td style=\"text-align: right;\">                   1</td><td>89d84316  </td><td style=\"text-align: right;\">    33.5888  </td><td style=\"text-align: right;\">     6.55424 </td></tr>\n",
       "<tr><td>FSR_Trainable_8b481b07</td><td>2023-08-10_22-49-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.42568</td><td style=\"text-align: right;\">    629.104</td><td style=\"text-align: right;\">     1.11656</td><td style=\"text-align: right;\"> 8.1095e+17 </td><td style=\"text-align: right;\"> 0.650201</td><td>172.26.215.93</td><td style=\"text-align: right;\">149979</td><td style=\"text-align: right;\">     2.42332</td><td style=\"text-align: right;\">     438.511</td><td style=\"text-align: right;\">           40.4853  </td><td style=\"text-align: right;\">          0.435439</td><td style=\"text-align: right;\">     40.4853  </td><td style=\"text-align: right;\"> 1691675397</td><td style=\"text-align: right;\">    0.94976 </td><td style=\"text-align: right;\">    0.247705</td><td style=\"text-align: right;\">   8.00129e+13</td><td style=\"text-align: right;\">  3.51748e+14</td><td style=\"text-align: right;\">                 100</td><td>8b481b07  </td><td style=\"text-align: right;\">     0.485447</td><td style=\"text-align: right;\">     0.164754</td></tr>\n",
       "<tr><td>FSR_Trainable_919a9d49</td><td>2023-08-10_22-37-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.42788</td><td style=\"text-align: right;\">    746.903</td><td style=\"text-align: right;\">     1.13914</td><td style=\"text-align: right;\"> 1.24464e+18</td><td style=\"text-align: right;\"> 0.696029</td><td>172.26.215.93</td><td style=\"text-align: right;\">134525</td><td style=\"text-align: right;\">     2.6347 </td><td style=\"text-align: right;\">     472.418</td><td style=\"text-align: right;\">           32.8018  </td><td style=\"text-align: right;\">          0.450702</td><td style=\"text-align: right;\">     32.8018  </td><td style=\"text-align: right;\"> 1691674620</td><td style=\"text-align: right;\">    1.1535  </td><td style=\"text-align: right;\">    0.317279</td><td style=\"text-align: right;\">   6.55474e+13</td><td style=\"text-align: right;\">  6.24981e+14</td><td style=\"text-align: right;\">                  64</td><td>919a9d49  </td><td style=\"text-align: right;\">     0.514585</td><td style=\"text-align: right;\">     0.181444</td></tr>\n",
       "<tr><td>FSR_Trainable_93091046</td><td>2023-08-10_22-39-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    3.98616</td><td style=\"text-align: right;\">   1066.01 </td><td style=\"text-align: right;\">     1.12   </td><td style=\"text-align: right;\"> 4.23624e+16</td><td style=\"text-align: right;\">39.7355  </td><td>172.26.215.93</td><td style=\"text-align: right;\">139708</td><td style=\"text-align: right;\">     2.43781</td><td style=\"text-align: right;\">     952.232</td><td style=\"text-align: right;\">            0.903354</td><td style=\"text-align: right;\">          0.903354</td><td style=\"text-align: right;\">      0.903354</td><td style=\"text-align: right;\"> 1691674796</td><td style=\"text-align: right;\">   28.04    </td><td style=\"text-align: right;\">    5.46604 </td><td style=\"text-align: right;\">   1.96962e+15</td><td style=\"text-align: right;\">  1.50892e+15</td><td style=\"text-align: right;\">                   1</td><td>93091046  </td><td style=\"text-align: right;\">    33.323   </td><td style=\"text-align: right;\">     6.41255 </td></tr>\n",
       "<tr><td>FSR_Trainable_93d1b467</td><td>2023-08-10_22-36-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    4.45678</td><td style=\"text-align: right;\">   1617.7  </td><td style=\"text-align: right;\">     1.17678</td><td style=\"text-align: right;\"> 1.06805e+09</td><td style=\"text-align: right;\"> 6.66054 </td><td>172.26.215.93</td><td style=\"text-align: right;\">134204</td><td style=\"text-align: right;\">     2.46383</td><td style=\"text-align: right;\">    1029.39 </td><td style=\"text-align: right;\">            0.822738</td><td style=\"text-align: right;\">          0.398867</td><td style=\"text-align: right;\">      0.822738</td><td style=\"text-align: right;\"> 1691674568</td><td style=\"text-align: right;\">    8.24482 </td><td style=\"text-align: right;\">    4.03443 </td><td style=\"text-align: right;\"> 101.935      </td><td style=\"text-align: right;\">130.83       </td><td style=\"text-align: right;\">                   2</td><td>93d1b467  </td><td style=\"text-align: right;\">     4.54283 </td><td style=\"text-align: right;\">     2.11771 </td></tr>\n",
       "<tr><td>FSR_Trainable_970091e8</td><td>2023-08-10_22-52-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.25008</td><td style=\"text-align: right;\">   1338.76 </td><td style=\"text-align: right;\">     1.32694</td><td style=\"text-align: right;\"> 7.02675e+08</td><td style=\"text-align: right;\"> 6.5132  </td><td>172.26.215.93</td><td style=\"text-align: right;\">153117</td><td style=\"text-align: right;\">     2.42622</td><td style=\"text-align: right;\">     938.195</td><td style=\"text-align: right;\">            0.977356</td><td style=\"text-align: right;\">          0.977356</td><td style=\"text-align: right;\">      0.977356</td><td style=\"text-align: right;\"> 1691675522</td><td style=\"text-align: right;\">    7.99132 </td><td style=\"text-align: right;\">    3.09082 </td><td style=\"text-align: right;\">   1.68509e+15</td><td style=\"text-align: right;\"> 59.8207     </td><td style=\"text-align: right;\">                   1</td><td>970091e8  </td><td style=\"text-align: right;\">     4.5299  </td><td style=\"text-align: right;\">     1.9833  </td></tr>\n",
       "<tr><td>FSR_Trainable_9da4ea97</td><td>2023-08-10_22-51-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    7.77661</td><td style=\"text-align: right;\">   1085.76 </td><td style=\"text-align: right;\">     1.71589</td><td style=\"text-align: right;\"> 1.88558e+18</td><td style=\"text-align: right;\"> 0.932863</td><td>172.26.215.93</td><td style=\"text-align: right;\">152642</td><td style=\"text-align: right;\">     3.14229</td><td style=\"text-align: right;\">     746.524</td><td style=\"text-align: right;\">            0.771869</td><td style=\"text-align: right;\">          0.771869</td><td style=\"text-align: right;\">      0.771869</td><td style=\"text-align: right;\"> 1691675496</td><td style=\"text-align: right;\">    1.79029 </td><td style=\"text-align: right;\">    0.397854</td><td style=\"text-align: right;\">   8.99146e+13</td><td style=\"text-align: right;\">  7.44809e+14</td><td style=\"text-align: right;\">                   1</td><td>9da4ea97  </td><td style=\"text-align: right;\">     0.69693 </td><td style=\"text-align: right;\">     0.235932</td></tr>\n",
       "<tr><td>FSR_Trainable_9f4f8ce1</td><td>2023-08-10_22-37-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">   27.7862 </td><td style=\"text-align: right;\">   2609.17 </td><td style=\"text-align: right;\">     4.39553</td><td style=\"text-align: right;\"> 5.70035e+18</td><td style=\"text-align: right;\"> 2.5846  </td><td>172.26.215.93</td><td style=\"text-align: right;\">135441</td><td style=\"text-align: right;\">     9.36968</td><td style=\"text-align: right;\">    1456.89 </td><td style=\"text-align: right;\">            1.34856 </td><td style=\"text-align: right;\">          0.46306 </td><td style=\"text-align: right;\">      1.34856 </td><td style=\"text-align: right;\"> 1691674628</td><td style=\"text-align: right;\">    6.12614 </td><td style=\"text-align: right;\">    1.15354 </td><td style=\"text-align: right;\">   3.38057e+13</td><td style=\"text-align: right;\">  3.14033e+15</td><td style=\"text-align: right;\">                   2</td><td>9f4f8ce1  </td><td style=\"text-align: right;\">     2.03824 </td><td style=\"text-align: right;\">     0.546361</td></tr>\n",
       "<tr><td>FSR_Trainable_a0f7a6e6</td><td>2023-08-10_22-42-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.63774</td><td style=\"text-align: right;\">    739.461</td><td style=\"text-align: right;\">     1.39265</td><td style=\"text-align: right;\"> 1.3175e+18 </td><td style=\"text-align: right;\"> 0.711067</td><td>172.26.215.93</td><td style=\"text-align: right;\">142568</td><td style=\"text-align: right;\">     2.60101</td><td style=\"text-align: right;\">     451.077</td><td style=\"text-align: right;\">            3.04387 </td><td style=\"text-align: right;\">          0.311718</td><td style=\"text-align: right;\">      3.04387 </td><td style=\"text-align: right;\"> 1691674923</td><td style=\"text-align: right;\">    1.17186 </td><td style=\"text-align: right;\">    0.3469  </td><td style=\"text-align: right;\">   7.82107e+13</td><td style=\"text-align: right;\">  7.69476e+14</td><td style=\"text-align: right;\">                   8</td><td>a0f7a6e6  </td><td style=\"text-align: right;\">     0.514011</td><td style=\"text-align: right;\">     0.197056</td></tr>\n",
       "<tr><td>FSR_Trainable_a78b6bca</td><td>2023-08-10_22-41-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.39161</td><td style=\"text-align: right;\">    725.665</td><td style=\"text-align: right;\">     1.17721</td><td style=\"text-align: right;\"> 1.20032e+18</td><td style=\"text-align: right;\"> 0.691772</td><td>172.26.215.93</td><td style=\"text-align: right;\">140519</td><td style=\"text-align: right;\">     2.60626</td><td style=\"text-align: right;\">     463.466</td><td style=\"text-align: right;\">           27.7193  </td><td style=\"text-align: right;\">          0.431659</td><td style=\"text-align: right;\">     27.7193  </td><td style=\"text-align: right;\"> 1691674867</td><td style=\"text-align: right;\">    1.13042 </td><td style=\"text-align: right;\">    0.31229 </td><td style=\"text-align: right;\">   7.49665e+13</td><td style=\"text-align: right;\">  6.09804e+14</td><td style=\"text-align: right;\">                  64</td><td>a78b6bca  </td><td style=\"text-align: right;\">     0.510435</td><td style=\"text-align: right;\">     0.181337</td></tr>\n",
       "<tr><td>FSR_Trainable_a862fb48</td><td>2023-08-10_22-38-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    5.39478</td><td style=\"text-align: right;\">    731.27 </td><td style=\"text-align: right;\">     1.44713</td><td style=\"text-align: right;\"> 1.19385e+18</td><td style=\"text-align: right;\"> 0.708052</td><td>172.26.215.93</td><td style=\"text-align: right;\">136939</td><td style=\"text-align: right;\">     2.6187 </td><td style=\"text-align: right;\">     478.202</td><td style=\"text-align: right;\">            6.39885 </td><td style=\"text-align: right;\">          0.455781</td><td style=\"text-align: right;\">      6.39885 </td><td style=\"text-align: right;\"> 1691674699</td><td style=\"text-align: right;\">    1.13352 </td><td style=\"text-align: right;\">    0.329724</td><td style=\"text-align: right;\">   8.76008e+13</td><td style=\"text-align: right;\">  6.68889e+14</td><td style=\"text-align: right;\">                  16</td><td>a862fb48  </td><td style=\"text-align: right;\">     0.512285</td><td style=\"text-align: right;\">     0.195767</td></tr>\n",
       "<tr><td>FSR_Trainable_a991e120</td><td>2023-08-10_22-46-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   18.0186 </td><td style=\"text-align: right;\">   1349.59 </td><td style=\"text-align: right;\">     2.83161</td><td style=\"text-align: right;\"> 2.39022e+18</td><td style=\"text-align: right;\"> 1.62443 </td><td>172.26.215.93</td><td style=\"text-align: right;\">147223</td><td style=\"text-align: right;\">     6.6166 </td><td style=\"text-align: right;\">     950.909</td><td style=\"text-align: right;\">            1.38481 </td><td style=\"text-align: right;\">          1.38481 </td><td style=\"text-align: right;\">      1.38481 </td><td style=\"text-align: right;\"> 1691675199</td><td style=\"text-align: right;\">    3.79553 </td><td style=\"text-align: right;\">    0.479074</td><td style=\"text-align: right;\">   5.137e+13  </td><td style=\"text-align: right;\">  8.75022e+14</td><td style=\"text-align: right;\">                   1</td><td>a991e120  </td><td style=\"text-align: right;\">     1.30597 </td><td style=\"text-align: right;\">     0.318465</td></tr>\n",
       "<tr><td>FSR_Trainable_acb158e3</td><td>2023-08-10_22-37-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    4.94516</td><td style=\"text-align: right;\">    895.236</td><td style=\"text-align: right;\">     1.12163</td><td style=\"text-align: right;\"> 1.52717e+18</td><td style=\"text-align: right;\"> 0.705083</td><td>172.26.215.93</td><td style=\"text-align: right;\">136256</td><td style=\"text-align: right;\">     2.51705</td><td style=\"text-align: right;\">     603.034</td><td style=\"text-align: right;\">            2.96278 </td><td style=\"text-align: right;\">          0.277822</td><td style=\"text-align: right;\">      2.96278 </td><td style=\"text-align: right;\"> 1691674663</td><td style=\"text-align: right;\">    1.07027 </td><td style=\"text-align: right;\">    0.347566</td><td style=\"text-align: right;\">   7.25247e+13</td><td style=\"text-align: right;\">  6.59632e+14</td><td style=\"text-align: right;\">                   8</td><td>acb158e3  </td><td style=\"text-align: right;\">     0.500478</td><td style=\"text-align: right;\">     0.204606</td></tr>\n",
       "<tr><td>FSR_Trainable_ad928d07</td><td>2023-08-10_22-36-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">   12.3036 </td><td style=\"text-align: right;\">   1494.07 </td><td style=\"text-align: right;\">     2.28183</td><td style=\"text-align: right;\"> 3.15337e+18</td><td style=\"text-align: right;\"> 1.323   </td><td>172.26.215.93</td><td style=\"text-align: right;\">134759</td><td style=\"text-align: right;\">     4.89646</td><td style=\"text-align: right;\">    1029    </td><td style=\"text-align: right;\">            1.93682 </td><td style=\"text-align: right;\">          0.444412</td><td style=\"text-align: right;\">      1.93682 </td><td style=\"text-align: right;\"> 1691674593</td><td style=\"text-align: right;\">    2.68663 </td><td style=\"text-align: right;\">    0.533957</td><td style=\"text-align: right;\">   8.19612e+13</td><td style=\"text-align: right;\">  1.1883e+15 </td><td style=\"text-align: right;\">                   4</td><td>ad928d07  </td><td style=\"text-align: right;\">     1.00952 </td><td style=\"text-align: right;\">     0.313474</td></tr>\n",
       "<tr><td>FSR_Trainable_b149d862</td><td>2023-08-10_22-41-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    5.27316</td><td style=\"text-align: right;\">    761.433</td><td style=\"text-align: right;\">     1.16927</td><td style=\"text-align: right;\"> 1.26248e+18</td><td style=\"text-align: right;\"> 0.697652</td><td>172.26.215.93</td><td style=\"text-align: right;\">140833</td><td style=\"text-align: right;\">     2.57123</td><td style=\"text-align: right;\">     490.56 </td><td style=\"text-align: right;\">           28.3259  </td><td style=\"text-align: right;\">          0.367339</td><td style=\"text-align: right;\">     28.3259  </td><td style=\"text-align: right;\"> 1691674878</td><td style=\"text-align: right;\">    1.12084 </td><td style=\"text-align: right;\">    0.321119</td><td style=\"text-align: right;\">   7.85049e+13</td><td style=\"text-align: right;\">  6.18326e+14</td><td style=\"text-align: right;\">                  64</td><td>b149d862  </td><td style=\"text-align: right;\">     0.50936 </td><td style=\"text-align: right;\">     0.188292</td></tr>\n",
       "<tr><td>FSR_Trainable_b2b7e672</td><td>2023-08-10_22-40-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   14.7312 </td><td style=\"text-align: right;\">   3958.99 </td><td style=\"text-align: right;\">     2.71464</td><td style=\"text-align: right;\"> 4.76694e+17</td><td style=\"text-align: right;\">56.5973  </td><td>172.26.215.93</td><td style=\"text-align: right;\">140019</td><td style=\"text-align: right;\">    11.4781 </td><td style=\"text-align: right;\">    4269.69 </td><td style=\"text-align: right;\">            0.852096</td><td style=\"text-align: right;\">          0.852096</td><td style=\"text-align: right;\">      0.852096</td><td style=\"text-align: right;\"> 1691674805</td><td style=\"text-align: right;\">   44.9546  </td><td style=\"text-align: right;\">   17.6182  </td><td style=\"text-align: right;\">   1.94089e+16</td><td style=\"text-align: right;\">  1.85545e+16</td><td style=\"text-align: right;\">                   1</td><td>b2b7e672  </td><td style=\"text-align: right;\">    37.8278  </td><td style=\"text-align: right;\">    18.7695  </td></tr>\n",
       "<tr><td>FSR_Trainable_b58979bc</td><td>2023-08-10_22-39-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.33226</td><td style=\"text-align: right;\">    866.171</td><td style=\"text-align: right;\">     1.22298</td><td style=\"text-align: right;\"> 1.5665e+18 </td><td style=\"text-align: right;\"> 0.712544</td><td>172.26.215.93</td><td style=\"text-align: right;\">138520</td><td style=\"text-align: right;\">     2.59747</td><td style=\"text-align: right;\">     562.043</td><td style=\"text-align: right;\">            3.42368 </td><td style=\"text-align: right;\">          0.310376</td><td style=\"text-align: right;\">      3.42368 </td><td style=\"text-align: right;\"> 1691674755</td><td style=\"text-align: right;\">    1.14102 </td><td style=\"text-align: right;\">    0.364292</td><td style=\"text-align: right;\">   7.14172e+13</td><td style=\"text-align: right;\">  7.72928e+14</td><td style=\"text-align: right;\">                   8</td><td>b58979bc  </td><td style=\"text-align: right;\">     0.509988</td><td style=\"text-align: right;\">     0.202556</td></tr>\n",
       "<tr><td>FSR_Trainable_b58f0ea2</td><td>2023-08-10_22-45-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.49744</td><td style=\"text-align: right;\">    859.8  </td><td style=\"text-align: right;\">     1.26469</td><td style=\"text-align: right;\"> 3.93832e+08</td><td style=\"text-align: right;\"> 5.80134 </td><td>172.26.215.93</td><td style=\"text-align: right;\">146232</td><td style=\"text-align: right;\">     2.41121</td><td style=\"text-align: right;\">     616.691</td><td style=\"text-align: right;\">            0.433   </td><td style=\"text-align: right;\">          0.433   </td><td style=\"text-align: right;\">      0.433   </td><td style=\"text-align: right;\"> 1691675146</td><td style=\"text-align: right;\">    8.29159 </td><td style=\"text-align: right;\">    2.00205 </td><td style=\"text-align: right;\">   2.34875e+15</td><td style=\"text-align: right;\"> 83.4958     </td><td style=\"text-align: right;\">                   1</td><td>b58f0ea2  </td><td style=\"text-align: right;\">     4.49296 </td><td style=\"text-align: right;\">     1.30838 </td></tr>\n",
       "<tr><td>FSR_Trainable_b6a00942</td><td>2023-08-10_22-49-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   16.9306 </td><td style=\"text-align: right;\">   1673.09 </td><td style=\"text-align: right;\">     2.83456</td><td style=\"text-align: right;\"> 2.09481e+18</td><td style=\"text-align: right;\"> 1.63559 </td><td>172.26.215.93</td><td style=\"text-align: right;\">150449</td><td style=\"text-align: right;\">     5.90696</td><td style=\"text-align: right;\">    1337.79 </td><td style=\"text-align: right;\">            0.471463</td><td style=\"text-align: right;\">          0.471463</td><td style=\"text-align: right;\">      0.471463</td><td style=\"text-align: right;\"> 1691675386</td><td style=\"text-align: right;\">    3.7315  </td><td style=\"text-align: right;\">    0.579856</td><td style=\"text-align: right;\">   3.78034e+13</td><td style=\"text-align: right;\">  9.65605e+14</td><td style=\"text-align: right;\">                   1</td><td>b6a00942  </td><td style=\"text-align: right;\">     1.27143 </td><td style=\"text-align: right;\">     0.364161</td></tr>\n",
       "<tr><td>FSR_Trainable_b898cfb7</td><td>2023-08-10_22-52-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.33443</td><td style=\"text-align: right;\">    620.672</td><td style=\"text-align: right;\">     1.09445</td><td style=\"text-align: right;\"> 7.87413e+17</td><td style=\"text-align: right;\"> 0.641356</td><td>172.26.215.93</td><td style=\"text-align: right;\">152190</td><td style=\"text-align: right;\">     2.38934</td><td style=\"text-align: right;\">     437.166</td><td style=\"text-align: right;\">           37.2504  </td><td style=\"text-align: right;\">          0.338286</td><td style=\"text-align: right;\">     37.2504  </td><td style=\"text-align: right;\"> 1691675521</td><td style=\"text-align: right;\">    0.932832</td><td style=\"text-align: right;\">    0.239777</td><td style=\"text-align: right;\">   7.91666e+13</td><td style=\"text-align: right;\">  3.30727e+14</td><td style=\"text-align: right;\">                 100</td><td>b898cfb7  </td><td style=\"text-align: right;\">     0.480423</td><td style=\"text-align: right;\">     0.160933</td></tr>\n",
       "<tr><td>FSR_Trainable_bf2ad8fb</td><td>2023-08-10_22-49-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.39906</td><td style=\"text-align: right;\">    632.199</td><td style=\"text-align: right;\">     1.09632</td><td style=\"text-align: right;\"> 8.14862e+17</td><td style=\"text-align: right;\"> 0.648845</td><td>172.26.215.93</td><td style=\"text-align: right;\">149705</td><td style=\"text-align: right;\">     2.41195</td><td style=\"text-align: right;\">     440.746</td><td style=\"text-align: right;\">           41.805   </td><td style=\"text-align: right;\">          0.367896</td><td style=\"text-align: right;\">     41.805   </td><td style=\"text-align: right;\"> 1691675379</td><td style=\"text-align: right;\">    0.945422</td><td style=\"text-align: right;\">    0.247949</td><td style=\"text-align: right;\">   7.87218e+13</td><td style=\"text-align: right;\">  3.51057e+14</td><td style=\"text-align: right;\">                 100</td><td>bf2ad8fb  </td><td style=\"text-align: right;\">     0.483967</td><td style=\"text-align: right;\">     0.164878</td></tr>\n",
       "<tr><td>FSR_Trainable_c4789d59</td><td>2023-08-10_22-38-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.65926</td><td style=\"text-align: right;\">    753.474</td><td style=\"text-align: right;\">     1.37543</td><td style=\"text-align: right;\"> 1.24777e+18</td><td style=\"text-align: right;\"> 0.749388</td><td>172.26.215.93</td><td style=\"text-align: right;\">136758</td><td style=\"text-align: right;\">     2.76458</td><td style=\"text-align: right;\">     511.887</td><td style=\"text-align: right;\">            2.82286 </td><td style=\"text-align: right;\">          0.561648</td><td style=\"text-align: right;\">      2.82286 </td><td style=\"text-align: right;\"> 1691674686</td><td style=\"text-align: right;\">    1.16729 </td><td style=\"text-align: right;\">    0.340595</td><td style=\"text-align: right;\">   8.12927e+13</td><td style=\"text-align: right;\">  6.75648e+14</td><td style=\"text-align: right;\">                   4</td><td>c4789d59  </td><td style=\"text-align: right;\">     0.533144</td><td style=\"text-align: right;\">     0.216244</td></tr>\n",
       "<tr><td>FSR_Trainable_cb45896d</td><td>2023-08-10_22-51-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.41471</td><td style=\"text-align: right;\">    630.646</td><td style=\"text-align: right;\">     1.11806</td><td style=\"text-align: right;\"> 8.11089e+17</td><td style=\"text-align: right;\"> 0.649328</td><td>172.26.215.93</td><td style=\"text-align: right;\">151791</td><td style=\"text-align: right;\">     2.41886</td><td style=\"text-align: right;\">     440.701</td><td style=\"text-align: right;\">           36.9193  </td><td style=\"text-align: right;\">          0.344986</td><td style=\"text-align: right;\">     36.9193  </td><td style=\"text-align: right;\"> 1691675503</td><td style=\"text-align: right;\">    0.947472</td><td style=\"text-align: right;\">    0.247248</td><td style=\"text-align: right;\">   7.9639e+13 </td><td style=\"text-align: right;\">  3.49845e+14</td><td style=\"text-align: right;\">                 100</td><td>cb45896d  </td><td style=\"text-align: right;\">     0.484828</td><td style=\"text-align: right;\">     0.1645  </td></tr>\n",
       "<tr><td>FSR_Trainable_cce353eb</td><td>2023-08-10_22-45-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    4.83476</td><td style=\"text-align: right;\">    709.449</td><td style=\"text-align: right;\">     1.15529</td><td style=\"text-align: right;\"> 1.221e+18  </td><td style=\"text-align: right;\"> 0.665492</td><td>172.26.215.93</td><td style=\"text-align: right;\">145545</td><td style=\"text-align: right;\">     2.45179</td><td style=\"text-align: right;\">     440.597</td><td style=\"text-align: right;\">           18.7362  </td><td style=\"text-align: right;\">          0.447677</td><td style=\"text-align: right;\">     18.7362  </td><td style=\"text-align: right;\"> 1691675134</td><td style=\"text-align: right;\">    1.04132 </td><td style=\"text-align: right;\">    0.305875</td><td style=\"text-align: right;\">   7.90951e+13</td><td style=\"text-align: right;\">  6.24124e+14</td><td style=\"text-align: right;\">                  32</td><td>cce353eb  </td><td style=\"text-align: right;\">     0.491869</td><td style=\"text-align: right;\">     0.173623</td></tr>\n",
       "<tr><td>FSR_Trainable_ce197d2c</td><td>2023-08-10_22-39-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    5.17169</td><td style=\"text-align: right;\">   1113.57 </td><td style=\"text-align: right;\">     1.1634 </td><td style=\"text-align: right;\"> 2.09757e+18</td><td style=\"text-align: right;\"> 0.756679</td><td>172.26.215.93</td><td style=\"text-align: right;\">138427</td><td style=\"text-align: right;\">     2.54651</td><td style=\"text-align: right;\">     736.214</td><td style=\"text-align: right;\">            1.57269 </td><td style=\"text-align: right;\">          0.321633</td><td style=\"text-align: right;\">      1.57269 </td><td style=\"text-align: right;\"> 1691674746</td><td style=\"text-align: right;\">    1.11838 </td><td style=\"text-align: right;\">    0.418113</td><td style=\"text-align: right;\">   7.46289e+13</td><td style=\"text-align: right;\">  8.37014e+14</td><td style=\"text-align: right;\">                   4</td><td>ce197d2c  </td><td style=\"text-align: right;\">     0.508175</td><td style=\"text-align: right;\">     0.248504</td></tr>\n",
       "<tr><td>FSR_Trainable_cf7c2dca</td><td>2023-08-10_22-38-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.76847</td><td style=\"text-align: right;\">   1074.4  </td><td style=\"text-align: right;\">     1.14994</td><td style=\"text-align: right;\"> 5.16964e+08</td><td style=\"text-align: right;\"> 6.21098 </td><td>172.26.215.93</td><td style=\"text-align: right;\">137715</td><td style=\"text-align: right;\">     2.65519</td><td style=\"text-align: right;\">     752.095</td><td style=\"text-align: right;\">            1.55694 </td><td style=\"text-align: right;\">          1.55694 </td><td style=\"text-align: right;\">      1.55694 </td><td style=\"text-align: right;\"> 1691674720</td><td style=\"text-align: right;\">    8.50584 </td><td style=\"text-align: right;\">    2.51609 </td><td style=\"text-align: right;\"> 218.369      </td><td style=\"text-align: right;\"> 93.7999     </td><td style=\"text-align: right;\">                   1</td><td>cf7c2dca  </td><td style=\"text-align: right;\">     4.65171 </td><td style=\"text-align: right;\">     1.55927 </td></tr>\n",
       "<tr><td>FSR_Trainable_d194dbfc</td><td>2023-08-10_22-52-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    5.38326</td><td style=\"text-align: right;\">   1515.69 </td><td style=\"text-align: right;\">     1.37738</td><td style=\"text-align: right;\"> 1.15733e+17</td><td style=\"text-align: right;\">55.182   </td><td>172.26.215.93</td><td style=\"text-align: right;\">153574</td><td style=\"text-align: right;\">     2.73269</td><td style=\"text-align: right;\">    1554.13 </td><td style=\"text-align: right;\">            0.475096</td><td style=\"text-align: right;\">          0.475096</td><td style=\"text-align: right;\">      0.475096</td><td style=\"text-align: right;\"> 1691675545</td><td style=\"text-align: right;\">   46.0437  </td><td style=\"text-align: right;\">    6.77678 </td><td style=\"text-align: right;\">   8.59909e+15</td><td style=\"text-align: right;\">  4.96014e+15</td><td style=\"text-align: right;\">                   1</td><td>d194dbfc  </td><td style=\"text-align: right;\">    48.1094  </td><td style=\"text-align: right;\">     7.07258 </td></tr>\n",
       "<tr><td>FSR_Trainable_d1a09ab3</td><td>2023-08-10_22-42-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.49321</td><td style=\"text-align: right;\">    636.669</td><td style=\"text-align: right;\">     1.10098</td><td style=\"text-align: right;\"> 8.81126e+17</td><td style=\"text-align: right;\"> 0.650639</td><td>172.26.215.93</td><td style=\"text-align: right;\">142257</td><td style=\"text-align: right;\">     2.43309</td><td style=\"text-align: right;\">     434.533</td><td style=\"text-align: right;\">           34.343   </td><td style=\"text-align: right;\">          0.333658</td><td style=\"text-align: right;\">     34.343   </td><td style=\"text-align: right;\"> 1691674955</td><td style=\"text-align: right;\">    0.963834</td><td style=\"text-align: right;\">    0.255965</td><td style=\"text-align: right;\">   7.84486e+13</td><td style=\"text-align: right;\">  4.01086e+14</td><td style=\"text-align: right;\">                 100</td><td>d1a09ab3  </td><td style=\"text-align: right;\">     0.486381</td><td style=\"text-align: right;\">     0.164258</td></tr>\n",
       "<tr><td>FSR_Trainable_d3a0f5db</td><td>2023-08-10_22-41-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   25.465  </td><td style=\"text-align: right;\">   2776.89 </td><td style=\"text-align: right;\">     3.7559 </td><td style=\"text-align: right;\"> 4.65167e+18</td><td style=\"text-align: right;\"> 2.50554 </td><td>172.26.215.93</td><td style=\"text-align: right;\">142164</td><td style=\"text-align: right;\">     8.79376</td><td style=\"text-align: right;\">    1524.66 </td><td style=\"text-align: right;\">            0.627033</td><td style=\"text-align: right;\">          0.627033</td><td style=\"text-align: right;\">      0.627033</td><td style=\"text-align: right;\"> 1691674901</td><td style=\"text-align: right;\">    5.4903  </td><td style=\"text-align: right;\">    1.35502 </td><td style=\"text-align: right;\">   3.77928e+13</td><td style=\"text-align: right;\">  2.92206e+15</td><td style=\"text-align: right;\">                   1</td><td>d3a0f5db  </td><td style=\"text-align: right;\">     1.82936 </td><td style=\"text-align: right;\">     0.676181</td></tr>\n",
       "<tr><td>FSR_Trainable_d9e1a2f5</td><td>2023-08-10_22-45-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.56081</td><td style=\"text-align: right;\">    633.6  </td><td style=\"text-align: right;\">     1.10373</td><td style=\"text-align: right;\"> 8.94983e+17</td><td style=\"text-align: right;\"> 0.649722</td><td>172.26.215.93</td><td style=\"text-align: right;\">145024</td><td style=\"text-align: right;\">     2.42884</td><td style=\"text-align: right;\">     430.124</td><td style=\"text-align: right;\">           56.016   </td><td style=\"text-align: right;\">          0.559976</td><td style=\"text-align: right;\">     56.016   </td><td style=\"text-align: right;\"> 1691675133</td><td style=\"text-align: right;\">    0.981399</td><td style=\"text-align: right;\">    0.256098</td><td style=\"text-align: right;\">   7.76696e+13</td><td style=\"text-align: right;\">  4.13304e+14</td><td style=\"text-align: right;\">                 100</td><td>d9e1a2f5  </td><td style=\"text-align: right;\">     0.48666 </td><td style=\"text-align: right;\">     0.163062</td></tr>\n",
       "<tr><td>FSR_Trainable_dd88466f</td><td>2023-08-10_22-40-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.40423</td><td style=\"text-align: right;\">    738.104</td><td style=\"text-align: right;\">     1.35239</td><td style=\"text-align: right;\"> 1.25692e+18</td><td style=\"text-align: right;\"> 0.721691</td><td>172.26.215.93</td><td style=\"text-align: right;\">140418</td><td style=\"text-align: right;\">     2.65606</td><td style=\"text-align: right;\">     473.314</td><td style=\"text-align: right;\">            4.00614 </td><td style=\"text-align: right;\">          0.344062</td><td style=\"text-align: right;\">      4.00614 </td><td style=\"text-align: right;\"> 1691674827</td><td style=\"text-align: right;\">    1.12338 </td><td style=\"text-align: right;\">    0.346598</td><td style=\"text-align: right;\">   8.29721e+13</td><td style=\"text-align: right;\">  7.37672e+14</td><td style=\"text-align: right;\">                   8</td><td>dd88466f  </td><td style=\"text-align: right;\">     0.516003</td><td style=\"text-align: right;\">     0.205687</td></tr>\n",
       "<tr><td>FSR_Trainable_e5f907e0</td><td>2023-08-10_22-39-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   26.0915 </td><td style=\"text-align: right;\">   1834.83 </td><td style=\"text-align: right;\">     4.22638</td><td style=\"text-align: right;\"> 2.95423e+18</td><td style=\"text-align: right;\"> 2.26562 </td><td>172.26.215.93</td><td style=\"text-align: right;\">139023</td><td style=\"text-align: right;\">     8.87808</td><td style=\"text-align: right;\">    1149.32 </td><td style=\"text-align: right;\">            0.647535</td><td style=\"text-align: right;\">          0.647535</td><td style=\"text-align: right;\">      0.647535</td><td style=\"text-align: right;\"> 1691674766</td><td style=\"text-align: right;\">    5.69152 </td><td style=\"text-align: right;\">    0.710586</td><td style=\"text-align: right;\">   1.48651e+13</td><td style=\"text-align: right;\">  1.36357e+15</td><td style=\"text-align: right;\">                   1</td><td>e5f907e0  </td><td style=\"text-align: right;\">     1.87816 </td><td style=\"text-align: right;\">     0.387461</td></tr>\n",
       "<tr><td>FSR_Trainable_e78b1941</td><td>2023-08-10_22-41-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    4.32008</td><td style=\"text-align: right;\">   1153.73 </td><td style=\"text-align: right;\">     1.13267</td><td style=\"text-align: right;\"> 6.65182e+08</td><td style=\"text-align: right;\"> 6.16284 </td><td>172.26.215.93</td><td style=\"text-align: right;\">141693</td><td style=\"text-align: right;\">     2.37491</td><td style=\"text-align: right;\">     799.436</td><td style=\"text-align: right;\">            0.451035</td><td style=\"text-align: right;\">          0.451035</td><td style=\"text-align: right;\">      0.451035</td><td style=\"text-align: right;\"> 1691674884</td><td style=\"text-align: right;\">    8.03439 </td><td style=\"text-align: right;\">    2.9867  </td><td style=\"text-align: right;\">   2.14763e+15</td><td style=\"text-align: right;\"> 71.7809     </td><td style=\"text-align: right;\">                   1</td><td>e78b1941  </td><td style=\"text-align: right;\">     4.46604 </td><td style=\"text-align: right;\">     1.69679 </td></tr>\n",
       "<tr><td>FSR_Trainable_e7a6ae12</td><td>2023-08-10_22-35-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    5.31655</td><td style=\"text-align: right;\">   1532.29 </td><td style=\"text-align: right;\">     1.47213</td><td style=\"text-align: right;\"> 2.4871e+17 </td><td style=\"text-align: right;\">40.0977  </td><td>172.26.215.93</td><td style=\"text-align: right;\">133857</td><td style=\"text-align: right;\">     2.85655</td><td style=\"text-align: right;\">    1287.94 </td><td style=\"text-align: right;\">            0.488214</td><td style=\"text-align: right;\">          0.488214</td><td style=\"text-align: right;\">      0.488214</td><td style=\"text-align: right;\"> 1691674550</td><td style=\"text-align: right;\">   30.5703  </td><td style=\"text-align: right;\">    7.4843  </td><td style=\"text-align: right;\">   6.11728e+15</td><td style=\"text-align: right;\">  9.60595e+15</td><td style=\"text-align: right;\">                   1</td><td>e7a6ae12  </td><td style=\"text-align: right;\">    33.7731  </td><td style=\"text-align: right;\">     6.32459 </td></tr>\n",
       "<tr><td>FSR_Trainable_e8eecf5d</td><td>2023-08-10_22-36-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    5.45012</td><td style=\"text-align: right;\">    734.37 </td><td style=\"text-align: right;\">     1.33696</td><td style=\"text-align: right;\"> 1.28171e+18</td><td style=\"text-align: right;\"> 0.699551</td><td>172.26.215.93</td><td style=\"text-align: right;\">133787</td><td style=\"text-align: right;\">     2.6341 </td><td style=\"text-align: right;\">     467.343</td><td style=\"text-align: right;\">           50.3775  </td><td style=\"text-align: right;\">          0.537098</td><td style=\"text-align: right;\">     50.3775  </td><td style=\"text-align: right;\"> 1691674611</td><td style=\"text-align: right;\">    1.12887 </td><td style=\"text-align: right;\">    0.319028</td><td style=\"text-align: right;\">   7.87739e+13</td><td style=\"text-align: right;\">  6.54144e+14</td><td style=\"text-align: right;\">                 100</td><td>e8eecf5d  </td><td style=\"text-align: right;\">     0.514563</td><td style=\"text-align: right;\">     0.184989</td></tr>\n",
       "<tr><td>FSR_Trainable_eaf9fef7</td><td>2023-08-10_22-40-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.3601 </td><td style=\"text-align: right;\">    739.237</td><td style=\"text-align: right;\">     1.37099</td><td style=\"text-align: right;\"> 1.24394e+18</td><td style=\"text-align: right;\"> 0.721247</td><td>172.26.215.93</td><td style=\"text-align: right;\">140111</td><td style=\"text-align: right;\">     2.66076</td><td style=\"text-align: right;\">     476.899</td><td style=\"text-align: right;\">            3.87148 </td><td style=\"text-align: right;\">          0.357303</td><td style=\"text-align: right;\">      3.87148 </td><td style=\"text-align: right;\"> 1691674817</td><td style=\"text-align: right;\">    1.11984 </td><td style=\"text-align: right;\">    0.343855</td><td style=\"text-align: right;\">   8.41399e+13</td><td style=\"text-align: right;\">  7.20953e+14</td><td style=\"text-align: right;\">                   8</td><td>eaf9fef7  </td><td style=\"text-align: right;\">     0.517148</td><td style=\"text-align: right;\">     0.204099</td></tr>\n",
       "<tr><td>FSR_Trainable_eb36545b</td><td>2023-08-10_22-45-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.51735</td><td style=\"text-align: right;\">    631.823</td><td style=\"text-align: right;\">     1.09751</td><td style=\"text-align: right;\"> 8.7056e+17 </td><td style=\"text-align: right;\"> 0.64872 </td><td>172.26.215.93</td><td style=\"text-align: right;\">144579</td><td style=\"text-align: right;\">     2.4238 </td><td style=\"text-align: right;\">     434.197</td><td style=\"text-align: right;\">           57.6334  </td><td style=\"text-align: right;\">          0.483055</td><td style=\"text-align: right;\">     57.6334  </td><td style=\"text-align: right;\"> 1691675102</td><td style=\"text-align: right;\">    0.970685</td><td style=\"text-align: right;\">    0.251745</td><td style=\"text-align: right;\">   7.79554e+13</td><td style=\"text-align: right;\">  3.91845e+14</td><td style=\"text-align: right;\">                 100</td><td>eb36545b  </td><td style=\"text-align: right;\">     0.486174</td><td style=\"text-align: right;\">     0.162546</td></tr>\n",
       "<tr><td>FSR_Trainable_ee2a2b18</td><td>2023-08-10_22-52-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.37119</td><td style=\"text-align: right;\">    623.652</td><td style=\"text-align: right;\">     1.09633</td><td style=\"text-align: right;\"> 8.03778e+17</td><td style=\"text-align: right;\"> 0.64622 </td><td>172.26.215.93</td><td style=\"text-align: right;\">152875</td><td style=\"text-align: right;\">     2.402  </td><td style=\"text-align: right;\">     435.708</td><td style=\"text-align: right;\">           33.039   </td><td style=\"text-align: right;\">          0.204951</td><td style=\"text-align: right;\">     33.039   </td><td style=\"text-align: right;\"> 1691675555</td><td style=\"text-align: right;\">    0.941558</td><td style=\"text-align: right;\">    0.244417</td><td style=\"text-align: right;\">   8.02049e+13</td><td style=\"text-align: right;\">  3.46297e+14</td><td style=\"text-align: right;\">                 100</td><td>ee2a2b18  </td><td style=\"text-align: right;\">     0.483464</td><td style=\"text-align: right;\">     0.162756</td></tr>\n",
       "<tr><td>FSR_Trainable_eeaeafba</td><td>2023-08-10_22-42-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   26.865  </td><td style=\"text-align: right;\">   2491.72 </td><td style=\"text-align: right;\">     4.18648</td><td style=\"text-align: right;\"> 2.95166e+18</td><td style=\"text-align: right;\"> 2.36036 </td><td>172.26.215.93</td><td style=\"text-align: right;\">143015</td><td style=\"text-align: right;\">     8.67393</td><td style=\"text-align: right;\">    1738.77 </td><td style=\"text-align: right;\">            0.591333</td><td style=\"text-align: right;\">          0.591333</td><td style=\"text-align: right;\">      0.591333</td><td style=\"text-align: right;\"> 1691674940</td><td style=\"text-align: right;\">    5.89475 </td><td style=\"text-align: right;\">    0.844224</td><td style=\"text-align: right;\">   1.89289e+13</td><td style=\"text-align: right;\">  1.21193e+15</td><td style=\"text-align: right;\">                   1</td><td>eeaeafba  </td><td style=\"text-align: right;\">     1.85707 </td><td style=\"text-align: right;\">     0.503292</td></tr>\n",
       "<tr><td>FSR_Trainable_ef2e7036</td><td>2023-08-10_22-53-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.36888</td><td style=\"text-align: right;\">    626.958</td><td style=\"text-align: right;\">     1.09293</td><td style=\"text-align: right;\"> 8.01898e+17</td><td style=\"text-align: right;\"> 0.64684 </td><td>172.26.215.93</td><td style=\"text-align: right;\">153801</td><td style=\"text-align: right;\">     2.40183</td><td style=\"text-align: right;\">     438.541</td><td style=\"text-align: right;\">           34.5384  </td><td style=\"text-align: right;\">          0.423738</td><td style=\"text-align: right;\">     34.5384  </td><td style=\"text-align: right;\"> 1691675605</td><td style=\"text-align: right;\">    0.941063</td><td style=\"text-align: right;\">    0.244747</td><td style=\"text-align: right;\">   7.84005e+13</td><td style=\"text-align: right;\">  3.41153e+14</td><td style=\"text-align: right;\">                 100</td><td>ef2e7036  </td><td style=\"text-align: right;\">     0.483076</td><td style=\"text-align: right;\">     0.163764</td></tr>\n",
       "<tr><td>FSR_Trainable_f14e2192</td><td>2023-08-10_22-37-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    5.81763</td><td style=\"text-align: right;\">    776.185</td><td style=\"text-align: right;\">     1.41411</td><td style=\"text-align: right;\"> 1.43131e+18</td><td style=\"text-align: right;\"> 0.721107</td><td>172.26.215.93</td><td style=\"text-align: right;\">136349</td><td style=\"text-align: right;\">     2.65711</td><td style=\"text-align: right;\">     471.137</td><td style=\"text-align: right;\">            4.25394 </td><td style=\"text-align: right;\">          0.406008</td><td style=\"text-align: right;\">      4.25394 </td><td style=\"text-align: right;\"> 1691674671</td><td style=\"text-align: right;\">    1.20704 </td><td style=\"text-align: right;\">    0.358448</td><td style=\"text-align: right;\">   6.67502e+13</td><td style=\"text-align: right;\">  8.07087e+14</td><td style=\"text-align: right;\">                   8</td><td>f14e2192  </td><td style=\"text-align: right;\">     0.519494</td><td style=\"text-align: right;\">     0.201613</td></tr>\n",
       "<tr><td>FSR_Trainable_f90c403c</td><td>2023-08-10_22-46-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   15.5135 </td><td style=\"text-align: right;\">   1563.67 </td><td style=\"text-align: right;\">     2.62226</td><td style=\"text-align: right;\"> 3.13801e+18</td><td style=\"text-align: right;\"> 1.57069 </td><td>172.26.215.93</td><td style=\"text-align: right;\">147543</td><td style=\"text-align: right;\">     5.92873</td><td style=\"text-align: right;\">     939.836</td><td style=\"text-align: right;\">            0.490505</td><td style=\"text-align: right;\">          0.490505</td><td style=\"text-align: right;\">      0.490505</td><td style=\"text-align: right;\"> 1691675213</td><td style=\"text-align: right;\">    3.35937 </td><td style=\"text-align: right;\">    0.608424</td><td style=\"text-align: right;\">   5.44103e+13</td><td style=\"text-align: right;\">  1.31323e+15</td><td style=\"text-align: right;\">                   1</td><td>f90c403c  </td><td style=\"text-align: right;\">     1.23929 </td><td style=\"text-align: right;\">     0.331394</td></tr>\n",
       "<tr><td>FSR_Trainable_f980a2be</td><td>2023-08-10_22-38-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">   10.7897 </td><td style=\"text-align: right;\">   1171.27 </td><td style=\"text-align: right;\">     1.93651</td><td style=\"text-align: right;\"> 1.47008e+18</td><td style=\"text-align: right;\"> 1.14429 </td><td>172.26.215.93</td><td style=\"text-align: right;\">137305</td><td style=\"text-align: right;\">     3.93773</td><td style=\"text-align: right;\">     913.772</td><td style=\"text-align: right;\">            0.963896</td><td style=\"text-align: right;\">          0.303136</td><td style=\"text-align: right;\">      0.963896</td><td style=\"text-align: right;\"> 1691674705</td><td style=\"text-align: right;\">    2.40857 </td><td style=\"text-align: right;\">    0.419895</td><td style=\"text-align: right;\">   5.16516e+13</td><td style=\"text-align: right;\">  5.76815e+14</td><td style=\"text-align: right;\">                   2</td><td>f980a2be  </td><td style=\"text-align: right;\">     0.847316</td><td style=\"text-align: right;\">     0.296973</td></tr>\n",
       "<tr><td>FSR_Trainable_fbb4fbf5</td><td>2023-08-10_22-47-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    4.97612</td><td style=\"text-align: right;\">    687.861</td><td style=\"text-align: right;\">     1.20811</td><td style=\"text-align: right;\"> 1.15449e+18</td><td style=\"text-align: right;\"> 0.668707</td><td>172.26.215.93</td><td style=\"text-align: right;\">148009</td><td style=\"text-align: right;\">     2.46361</td><td style=\"text-align: right;\">     429.656</td><td style=\"text-align: right;\">            8.63157 </td><td style=\"text-align: right;\">          0.344055</td><td style=\"text-align: right;\">      8.63157 </td><td style=\"text-align: right;\"> 1691675255</td><td style=\"text-align: right;\">    1.06649 </td><td style=\"text-align: right;\">    0.305532</td><td style=\"text-align: right;\">   8.17921e+13</td><td style=\"text-align: right;\">  6.30818e+14</td><td style=\"text-align: right;\">                  16</td><td>fbb4fbf5  </td><td style=\"text-align: right;\">     0.493957</td><td style=\"text-align: right;\">     0.17475 </td></tr>\n",
       "<tr><td>FSR_Trainable_fc7a34d4</td><td>2023-08-10_22-45-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    4.39381</td><td style=\"text-align: right;\">    620.636</td><td style=\"text-align: right;\">     1.11467</td><td style=\"text-align: right;\"> 8.00049e+17</td><td style=\"text-align: right;\"> 0.645696</td><td>172.26.215.93</td><td style=\"text-align: right;\">144821</td><td style=\"text-align: right;\">     2.41182</td><td style=\"text-align: right;\">     434.214</td><td style=\"text-align: right;\">           47.0138  </td><td style=\"text-align: right;\">          0.443665</td><td style=\"text-align: right;\">     47.0138  </td><td style=\"text-align: right;\"> 1691675108</td><td style=\"text-align: right;\">    0.944754</td><td style=\"text-align: right;\">    0.241827</td><td style=\"text-align: right;\">   7.88331e+13</td><td style=\"text-align: right;\">  3.4109e+14 </td><td style=\"text-align: right;\">                 100</td><td>fc7a34d4  </td><td style=\"text-align: right;\">     0.484561</td><td style=\"text-align: right;\">     0.161135</td></tr>\n",
       "<tr><td>FSR_Trainable_fe4d7d4d</td><td>2023-08-10_22-49-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   15.8018 </td><td style=\"text-align: right;\">   1278.9  </td><td style=\"text-align: right;\">     2.6493 </td><td style=\"text-align: right;\"> 2.24196e+18</td><td style=\"text-align: right;\"> 1.51781 </td><td>172.26.215.93</td><td style=\"text-align: right;\">150682</td><td style=\"text-align: right;\">     5.67161</td><td style=\"text-align: right;\">     828.543</td><td style=\"text-align: right;\">            0.578387</td><td style=\"text-align: right;\">          0.578387</td><td style=\"text-align: right;\">      0.578387</td><td style=\"text-align: right;\"> 1691675399</td><td style=\"text-align: right;\">    3.53713 </td><td style=\"text-align: right;\">    0.520988</td><td style=\"text-align: right;\">   6.29717e+13</td><td style=\"text-align: right;\">  1.14029e+15</td><td style=\"text-align: right;\">                   1</td><td>fe4d7d4d  </td><td style=\"text-align: right;\">     1.24006 </td><td style=\"text-align: right;\">     0.277754</td></tr>\n",
       "<tr><td>FSR_Trainable_ffa11e7b</td><td>2023-08-10_22-53-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   19.1177 </td><td style=\"text-align: right;\">   1487.5  </td><td style=\"text-align: right;\">     3.15246</td><td style=\"text-align: right;\"> 3.35848e+18</td><td style=\"text-align: right;\"> 1.81723 </td><td>172.26.215.93</td><td style=\"text-align: right;\">154705</td><td style=\"text-align: right;\">     6.48324</td><td style=\"text-align: right;\">     895.215</td><td style=\"text-align: right;\">            0.575132</td><td style=\"text-align: right;\">          0.575132</td><td style=\"text-align: right;\">      0.575132</td><td style=\"text-align: right;\"> 1691675605</td><td style=\"text-align: right;\">    4.44931 </td><td style=\"text-align: right;\">    0.609707</td><td style=\"text-align: right;\">   5.11626e+13</td><td style=\"text-align: right;\">  1.49662e+15</td><td style=\"text-align: right;\">                   1</td><td>ffa11e7b  </td><td style=\"text-align: right;\">     1.49749 </td><td style=\"text-align: right;\">     0.319747</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_e8eecf5d_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-35-37/wandb/run-20230810_223548-e8eecf5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Syncing run FSR_Trainable_e8eecf5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e8eecf5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_e7a6ae12_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-35-43/wandb/run-20230810_223556-e7a6ae12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Syncing run FSR_Trainable_e7a6ae12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e7a6ae12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                mae_coord 5.31655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                mae_force 1532.29183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               mape_coord 1.47213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               mape_force 2.487103273483704e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                   metric 40.09765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               rmse_coord 2.85655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               rmse_force 1287.94205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:       time_since_restore 0.48821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:         time_this_iter_s 0.48821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:             time_total_s 0.48821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:                timestamp 1691674550\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               tmae_coord 30.57035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:               tmae_force 7.4843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              tmape_coord 6117279040349646.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              tmape_force 9605946279082580.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              trmse_coord 33.77306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:              trmse_force 6.32459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb:  View run FSR_Trainable_e7a6ae12 at: https://wandb.ai/seokjin/FSR-prediction/runs/e7a6ae12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134029)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223556-e7a6ae12/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_49e42cfa_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-35-50/wandb/run-20230810_223604-49e42cfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Syncing run FSR_Trainable_49e42cfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/49e42cfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                mae_coord 5.28867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                mae_force 1052.22418\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               mape_coord 1.22498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               mape_force 755747951.27978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                   metric 6.79089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               rmse_coord 2.80626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               rmse_force 710.22663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:       time_since_restore 1.88604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:         time_this_iter_s 0.84487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:             time_total_s 1.88604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:                timestamp 1691674562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               tmae_coord 9.41496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:               tmae_force 2.91372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              tmape_coord 3692286168757693.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              tmape_force 160.31079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              trmse_coord 4.90961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:              trmse_force 1.88128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb:  View run FSR_Trainable_49e42cfa at: https://wandb.ai/seokjin/FSR-prediction/runs/49e42cfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134202)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223604-49e42cfa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_93d1b467_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-35-57/wandb/run-20230810_223611-93d1b467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Syncing run FSR_Trainable_93d1b467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/93d1b467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                mae_coord 4.45678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                mae_force 1617.7025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               mape_coord 1.17678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               mape_force 1068051905.2359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                   metric 6.66054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               rmse_coord 2.46383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               rmse_force 1029.38592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:       time_since_restore 0.82274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:         time_this_iter_s 0.39887\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:             time_total_s 0.82274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:                timestamp 1691674568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               tmae_coord 8.24482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:               tmae_force 4.03443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              tmape_coord 101.93499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              tmape_force 130.83048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              trmse_coord 4.54283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:              trmse_force 2.11771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb:  View run FSR_Trainable_93d1b467 at: https://wandb.ai/seokjin/FSR-prediction/runs/93d1b467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134391)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223611-93d1b467/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_919a9d49_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-36-05/wandb/run-20230810_223625-919a9d49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Syncing run FSR_Trainable_919a9d49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/919a9d49\n",
      "2023-08-10 22:36:32,071\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.573 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:36:32,074\tWARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:36:32,078\tWARNING util.py:315 -- Processing trial results took 2.581 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:36:32,082\tWARNING util.py:315 -- The `process_trial_result` operation took 2.585 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_ad928d07_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-36-16/wandb/run-20230810_223635-ad928d07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Syncing run FSR_Trainable_ad928d07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad928d07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                mae_coord 12.30362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                mae_force 1494.07282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               mape_coord 2.28183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               mape_force 3.1533728087931976e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                   metric 1.323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               rmse_coord 4.89646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               rmse_force 1028.99641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:       time_since_restore 1.93682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:         time_this_iter_s 0.44441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:             time_total_s 1.93682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:                timestamp 1691674593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               tmae_coord 2.68663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:               tmae_force 0.53396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              tmape_coord 81961153343057.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              tmape_force 1188302631563074.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              trmse_coord 1.00952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:              trmse_force 0.31347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb:  View run FSR_Trainable_ad928d07 at: https://wandb.ai/seokjin/FSR-prediction/runs/ad928d07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134843)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223635-ad928d07/logs\n",
      "2023-08-10 22:36:45,288\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.604 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:36:45,292\tWARNING util.py:315 -- The `process_trial_result` operation took 2.609 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:36:45,295\tWARNING util.py:315 -- Processing trial results took 2.612 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:36:45,297\tWARNING util.py:315 -- The `process_trial_result` operation took 2.614 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_25fed8e4_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-36-28/wandb/run-20230810_223649-25fed8e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Syncing run FSR_Trainable_25fed8e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/25fed8e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: \\ 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                mae_coord 7.30059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                mae_force 1001.792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               mape_coord 1.42859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               mape_force 1.7077270997361044e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                   metric 0.85659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               rmse_coord 3.03115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               rmse_force 638.51731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:       time_since_restore 2.91159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:         time_this_iter_s 0.57161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:             time_total_s 2.91159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:                timestamp 1691674607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               tmae_coord 1.58409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:               tmae_force 0.41477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              tmape_coord 75300657926697.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              tmape_force 818955220405286.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              trmse_coord 0.61852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:              trmse_force 0.23806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb:  View run FSR_Trainable_25fed8e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/25fed8e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135065)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223649-25fed8e4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223548-e8eecf5d/logs\n",
      "2023-08-10 22:36:58,305\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.524 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:36:58,310\tWARNING util.py:315 -- The `process_trial_result` operation took 2.531 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:36:58,314\tWARNING util.py:315 -- Processing trial results took 2.534 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:36:58,316\tWARNING util.py:315 -- The `process_trial_result` operation took 2.536 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=133856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_8484e1ff_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-36-41/wandb/run-20230810_223701-8484e1ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Syncing run FSR_Trainable_8484e1ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8484e1ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                mae_coord 5.42788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                mae_force 746.90306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               mape_coord 1.13914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               mape_force 1.2446444352829443e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                   metric 0.69603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               rmse_coord 2.6347\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               rmse_force 472.41777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:       time_since_restore 32.8018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:         time_this_iter_s 0.4507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:             time_total_s 32.8018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:                timestamp 1691674620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               tmae_coord 1.1535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:               tmae_force 0.31728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              tmape_coord 65547431349650.24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              tmape_force 624981198155516.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              trmse_coord 0.51458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:              trmse_force 0.18144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb:  View run FSR_Trainable_919a9d49 at: https://wandb.ai/seokjin/FSR-prediction/runs/919a9d49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=134622)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223625-919a9d49/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223701-8484e1ff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2023-08-10 22:37:07,822\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:07,826\tWARNING util.py:315 -- The `process_trial_result` operation took 1.928 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:07,827\tWARNING util.py:315 -- Processing trial results took 1.929 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:37:07,828\tWARNING util.py:315 -- The `process_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135308)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_9f4f8ce1_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-10_22-36-55/wandb/run-20230810_223713-9f4f8ce1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Syncing run FSR_Trainable_9f4f8ce1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9f4f8ce1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:37:17,193\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.494 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:17,197\tWARNING util.py:315 -- The `process_trial_result` operation took 2.498 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:17,199\tWARNING util.py:315 -- Processing trial results took 2.500 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:37:17,203\tWARNING util.py:315 -- The `process_trial_result` operation took 2.504 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: | 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: / 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                mae_coord 27.78615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                mae_force 2609.16956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               mape_coord 4.39553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               mape_force 5.700348899756827e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                   metric 2.5846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               rmse_coord 9.36968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               rmse_force 1456.88744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:       time_since_restore 1.34856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:         time_this_iter_s 0.46306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:             time_total_s 1.34856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:                timestamp 1691674628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               tmae_coord 6.12614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:               tmae_force 1.15354\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              tmape_coord 33805708323156.297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              tmape_force 3140326351683376.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              trmse_coord 2.03824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:              trmse_force 0.54636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb:  View run FSR_Trainable_9f4f8ce1 at: https://wandb.ai/seokjin/FSR-prediction/runs/9f4f8ce1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135543)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223713-9f4f8ce1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_5fe67775_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-37-05/wandb/run-20230810_223719-5fe67775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Syncing run FSR_Trainable_5fe67775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5fe67775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb:              tmape_force \n",
      "2023-08-10 22:37:25,151\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.064 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:25,156\tWARNING util.py:315 -- The `process_trial_result` operation took 2.070 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:25,158\tWARNING util.py:315 -- Processing trial results took 2.072 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:37:25,161\tWARNING util.py:315 -- The `process_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135740)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_2258fe3e_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-37-13/wandb/run-20230810_223727-2258fe3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Syncing run FSR_Trainable_2258fe3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2258fe3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:37:32,100\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:32,102\tWARNING util.py:315 -- The `process_trial_result` operation took 2.292 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:32,105\tWARNING util.py:315 -- Processing trial results took 2.295 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:37:32,106\tWARNING util.py:315 -- The `process_trial_result` operation took 2.296 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                mae_coord 3.79019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                mae_force 1103.38468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               mape_coord 1.11746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               mape_force 2.069959506707562e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                   metric 40.30293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               rmse_coord 2.3701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               rmse_force 946.43627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:       time_since_restore 1.5482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:         time_this_iter_s 1.5482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:             time_total_s 1.5482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:                timestamp 1691674643\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               tmae_coord 27.77872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:               tmae_force 5.84881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              tmape_coord 1382642656352195.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              tmape_force 707247222791327.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              trmse_coord 33.23932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:              trmse_force 7.06361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb:  View run FSR_Trainable_2258fe3e at: https://wandb.ai/seokjin/FSR-prediction/runs/2258fe3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223727-2258fe3e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=135943)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_0af22b82_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-37-21/wandb/run-20230810_223734-0af22b82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Syncing run FSR_Trainable_0af22b82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0af22b82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                mae_coord 7.16495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                mae_force 1381.81906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               mape_coord 1.54647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               mape_force 1.3409499079221813e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                   metric 56.23641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               rmse_coord 3.73784\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               rmse_force 1192.10886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:       time_since_restore 0.68412\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:         time_this_iter_s 0.68412\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:             time_total_s 0.68412\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:                timestamp 1691674649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               tmae_coord 48.21982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:               tmae_force 8.14161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              tmape_coord 1.3686150132060558e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              tmape_force 5235457662800961.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              trmse_coord 48.22529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:              trmse_force 8.01112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb:  View run FSR_Trainable_0af22b82 at: https://wandb.ai/seokjin/FSR-prediction/runs/0af22b82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136126)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223734-0af22b82/logs\n",
      "2023-08-10 22:37:40,961\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.340 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:40,967\tWARNING util.py:315 -- The `process_trial_result` operation took 2.347 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:40,969\tWARNING util.py:315 -- Processing trial results took 2.349 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:37:40,973\tWARNING util.py:315 -- The `process_trial_result` operation took 2.352 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_acb158e3_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-37-29/wandb/run-20230810_223743-acb158e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Syncing run FSR_Trainable_acb158e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/acb158e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:37:47,781\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.993 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:47,783\tWARNING util.py:315 -- The `process_trial_result` operation took 1.996 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:47,785\tWARNING util.py:315 -- Processing trial results took 1.998 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:37:47,786\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                mae_coord 4.94516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                mae_force 895.23583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               mape_coord 1.12163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               mape_force 1.5271662712021038e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                   metric 0.70508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               rmse_coord 2.51705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               rmse_force 603.03425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:       time_since_restore 2.96278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:         time_this_iter_s 0.27782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:             time_total_s 2.96278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:                timestamp 1691674663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               tmae_coord 1.07027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:               tmae_force 0.34757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              tmape_coord 72524684982021.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              tmape_force 659631784384520.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              trmse_coord 0.50048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:              trmse_force 0.20461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb:  View run FSR_Trainable_acb158e3 at: https://wandb.ai/seokjin/FSR-prediction/runs/acb158e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136348)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223743-acb158e3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_f14e2192_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-37-37/wandb/run-20230810_223750-f14e2192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Syncing run FSR_Trainable_f14e2192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f14e2192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                mae_coord 5.81763\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                mae_force 776.18524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               mape_coord 1.41411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               mape_force 1.4313051927526326e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                   metric 0.72111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               rmse_coord 2.65711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               rmse_force 471.13676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:       time_since_restore 4.25394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:         time_this_iter_s 0.40601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:             time_total_s 4.25394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:                timestamp 1691674671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               tmae_coord 1.20704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:               tmae_force 0.35845\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              tmape_coord 66750215343126.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              tmape_force 807087177460325.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              trmse_coord 0.51949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:              trmse_force 0.20161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb:  View run FSR_Trainable_f14e2192 at: https://wandb.ai/seokjin/FSR-prediction/runs/f14e2192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136532)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223750-f14e2192/logs\n",
      "2023-08-10 22:37:57,685\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:57,689\tWARNING util.py:315 -- The `process_trial_result` operation took 2.113 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:37:57,693\tWARNING util.py:315 -- Processing trial results took 2.116 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:37:57,696\tWARNING util.py:315 -- The `process_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_14fdb345_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-37-44/wandb/run-20230810_223800-14fdb345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Syncing run FSR_Trainable_14fdb345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/14fdb345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:38:04,741\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:04,743\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:04,745\tWARNING util.py:315 -- Processing trial results took 2.024 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:04,750\tWARNING util.py:315 -- The `process_trial_result` operation took 2.028 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                mae_coord 5.89484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                mae_force 749.00664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               mape_coord 1.37012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               mape_force 1.3836683303678612e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                   metric 0.72161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               rmse_coord 2.69268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               rmse_force 451.08694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:       time_since_restore 3.77932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:         time_this_iter_s 0.33732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:             time_total_s 3.77932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:                timestamp 1691674680\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               tmae_coord 1.2227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:               tmae_force 0.34935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              tmape_coord 64273621801314.76\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              tmape_force 793541590868002.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              trmse_coord 0.52574\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:              trmse_force 0.19587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb:  View run FSR_Trainable_14fdb345 at: https://wandb.ai/seokjin/FSR-prediction/runs/14fdb345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136757)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223800-14fdb345/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_c4789d59_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-37-54/wandb/run-20230810_223807-c4789d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Syncing run FSR_Trainable_c4789d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4789d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:38:11,346\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:11,348\tWARNING util.py:315 -- The `process_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:11,353\tWARNING util.py:315 -- Processing trial results took 2.064 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:11,355\tWARNING util.py:315 -- The `process_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                mae_coord 5.65926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                mae_force 753.47443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               mape_coord 1.37543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               mape_force 1.2477731369560384e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                   metric 0.74939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               rmse_coord 2.76458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               rmse_force 511.88689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:       time_since_restore 2.82286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:         time_this_iter_s 0.56165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:             time_total_s 2.82286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:                timestamp 1691674686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               tmae_coord 1.16729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:               tmae_force 0.3406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              tmape_coord 81292678197559.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              tmape_force 675647792773923.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              trmse_coord 0.53314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:              trmse_force 0.21624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb:  View run FSR_Trainable_c4789d59 at: https://wandb.ai/seokjin/FSR-prediction/runs/c4789d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=136938)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223807-c4789d59/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_a862fb48_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-01/wandb/run-20230810_223813-a862fb48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Syncing run FSR_Trainable_a862fb48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a862fb48\n",
      "2023-08-10 22:38:18,394\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.518 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:18,401\tWARNING util.py:315 -- The `process_trial_result` operation took 2.526 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:18,403\tWARNING util.py:315 -- Processing trial results took 2.529 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:18,405\tWARNING util.py:315 -- The `process_trial_result` operation took 2.531 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_5ea11c97_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-08/wandb/run-20230810_223821-5ea11c97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Syncing run FSR_Trainable_5ea11c97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ea11c97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                mae_coord 5.39478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                mae_force 731.26972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               mape_coord 1.44713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               mape_force 1.193854347490437e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                   metric 0.70805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               rmse_coord 2.6187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               rmse_force 478.20187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:       time_since_restore 6.39885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:         time_this_iter_s 0.45578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:             time_total_s 6.39885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:                timestamp 1691674699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               tmae_coord 1.13352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:               tmae_force 0.32972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              tmape_coord 87600772050685.11\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              tmape_force 668889357542654.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              trmse_coord 0.51228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:              trmse_force 0.19577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb:  View run FSR_Trainable_a862fb48 at: https://wandb.ai/seokjin/FSR-prediction/runs/a862fb48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137121)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223813-a862fb48/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:38:25,310\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.004 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:25,314\tWARNING util.py:315 -- The `process_trial_result` operation took 2.008 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:25,315\tWARNING util.py:315 -- Processing trial results took 2.010 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:25,317\tWARNING util.py:315 -- The `process_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_f980a2be_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-15/wandb/run-20230810_223827-f980a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Syncing run FSR_Trainable_f980a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f980a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137304)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                mae_coord 10.78969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                mae_force 1171.27279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               mape_coord 1.93651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               mape_force 1.4700794909543286e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                   metric 1.14429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               rmse_coord 3.93773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               rmse_force 913.77152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:       time_since_restore 0.9639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:         time_this_iter_s 0.30314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:             time_total_s 0.9639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:                timestamp 1691674705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               tmae_coord 2.40857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:               tmae_force 0.4199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              tmape_coord 51651646321513.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              tmape_force 576815186578678.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              trmse_coord 0.84732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:              trmse_force 0.29697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb:  View run FSR_Trainable_f980a2be at: https://wandb.ai/seokjin/FSR-prediction/runs/f980a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223827-f980a2be/logs\n",
      "2023-08-10 22:38:34,274\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.494 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:34,279\tWARNING util.py:315 -- The `process_trial_result` operation took 2.500 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:34,282\tWARNING util.py:315 -- Processing trial results took 2.503 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:34,284\tWARNING util.py:315 -- The `process_trial_result` operation took 2.505 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137486)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_254f3e5c_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-22/wandb/run-20230810_223836-254f3e5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Syncing run FSR_Trainable_254f3e5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/254f3e5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-08-10 22:38:41,797\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.750 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:41,802\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:41,804\tWARNING util.py:315 -- Processing trial results took 1.758 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:41,805\tWARNING util.py:315 -- The `process_trial_result` operation took 1.759 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                mae_coord 15.44213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                mae_force 8405.19007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               mape_coord 2.9923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               mape_force 6.53955310211838e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                   metric 59.43671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               rmse_coord 11.91814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               rmse_force 14548.90986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:       time_since_restore 0.70532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:         time_this_iter_s 0.70532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:             time_total_s 0.70532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:                timestamp 1691674711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               tmae_coord 51.55694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:               tmae_force 19.43822\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              tmape_coord 4.222589329069812e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              tmape_force 3.6005577473424716e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              trmse_coord 40.51693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:              trmse_force 18.91978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb:  View run FSR_Trainable_254f3e5c at: https://wandb.ai/seokjin/FSR-prediction/runs/254f3e5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137714)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223836-254f3e5c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_cf7c2dca_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-31/wandb/run-20230810_223844-cf7c2dca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Syncing run FSR_Trainable_cf7c2dca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cf7c2dca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:38:50,458\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.849 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:50,464\tWARNING util.py:315 -- The `process_trial_result` operation took 1.855 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:50,466\tWARNING util.py:315 -- Processing trial results took 1.858 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:50,469\tWARNING util.py:315 -- The `process_trial_result` operation took 1.860 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=137893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_5cf78066_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-38/wandb/run-20230810_223852-5cf78066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Syncing run FSR_Trainable_5cf78066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5cf78066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:38:56,604\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.001 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:56,607\tWARNING util.py:315 -- The `process_trial_result` operation took 2.005 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:38:56,609\tWARNING util.py:315 -- Processing trial results took 2.007 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:38:56,611\tWARNING util.py:315 -- The `process_trial_result` operation took 2.009 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                mae_coord 5.65886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                mae_force 1155.9432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               mape_coord 1.47502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               mape_force 946127203.92418\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                   metric 6.78401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               rmse_coord 2.90161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               rmse_force 878.10047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:       time_since_restore 1.19538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:         time_this_iter_s 1.19538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:             time_total_s 1.19538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:                timestamp 1691674728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               tmae_coord 9.68512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:               tmae_force 2.92916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              tmape_coord 886.60477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              tmape_force 98.61725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              trmse_coord 4.9788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:              trmse_force 1.80521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb:  View run FSR_Trainable_5cf78066 at: https://wandb.ai/seokjin/FSR-prediction/runs/5cf78066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223852-5cf78066/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138114)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_59e6f3f0_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-47/wandb/run-20230810_223859-59e6f3f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Syncing run FSR_Trainable_59e6f3f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/59e6f3f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                mae_coord 5.30506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                mae_force 1163.56861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               mape_coord 1.16772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               mape_force 2.127747984691505e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                   metric 0.76697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               rmse_coord 2.56221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               rmse_force 792.85158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:       time_since_restore 2.04026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:         time_this_iter_s 0.46419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:             time_total_s 2.04026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:                timestamp 1691674737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               tmae_coord 1.15572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:               tmae_force 0.42147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              tmape_coord 73600409364800.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              tmape_force 811015141893167.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              trmse_coord 0.51155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:              trmse_force 0.25542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb:  View run FSR_Trainable_59e6f3f0 at: https://wandb.ai/seokjin/FSR-prediction/runs/59e6f3f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138294)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223859-59e6f3f0/logs\n",
      "2023-08-10 22:39:05,636\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.186 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:05,641\tWARNING util.py:315 -- The `process_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:05,643\tWARNING util.py:315 -- Processing trial results took 2.193 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:05,646\tWARNING util.py:315 -- The `process_trial_result` operation took 2.197 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "wandb: | Waiting for wandb.init()...519)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_ce197d2c_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-38-53/wandb/run-20230810_223908-ce197d2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Syncing run FSR_Trainable_ce197d2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ce197d2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:39:12,969\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.811 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:12,974\tWARNING util.py:315 -- The `process_trial_result` operation took 2.816 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:12,975\tWARNING util.py:315 -- Processing trial results took 2.818 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:12,977\tWARNING util.py:315 -- The `process_trial_result` operation took 2.819 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                mae_coord 5.17169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                mae_force 1113.56873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               mape_coord 1.1634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               mape_force 2.0975677918012815e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                   metric 0.75668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               rmse_coord 2.54651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               rmse_force 736.21399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:       time_since_restore 1.57269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:         time_this_iter_s 0.32163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:             time_total_s 1.57269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:                timestamp 1691674746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               tmae_coord 1.11838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:               tmae_force 0.41811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              tmape_coord 74628920883537.45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              tmape_force 837013597516830.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              trmse_coord 0.50817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:              trmse_force 0.2485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb:  View run FSR_Trainable_ce197d2c at: https://wandb.ai/seokjin/FSR-prediction/runs/ce197d2c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138519)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223908-ce197d2c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_b58979bc_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-02/wandb/run-20230810_223915-b58979bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Syncing run FSR_Trainable_b58979bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b58979bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:39:20,561\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.112 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:20,565\tWARNING util.py:315 -- The `process_trial_result` operation took 2.116 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:20,567\tWARNING util.py:315 -- Processing trial results took 2.119 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:20,569\tWARNING util.py:315 -- The `process_trial_result` operation took 2.120 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_1ebbb40f_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-09/wandb/run-20230810_223923-1ebbb40f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Syncing run FSR_Trainable_1ebbb40f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1ebbb40f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:39:29,259\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.398 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:29,263\tWARNING util.py:315 -- The `process_trial_result` operation took 2.403 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:29,266\tWARNING util.py:315 -- Processing trial results took 2.406 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:29,270\tWARNING util.py:315 -- The `process_trial_result` operation took 2.410 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                mae_coord 5.79112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                mae_force 989.50884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               mape_coord 1.47549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               mape_force 1.735590016415834e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                   metric 0.74981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               rmse_coord 2.61693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               rmse_force 685.26564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:       time_since_restore 1.90946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:         time_this_iter_s 0.35057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:             time_total_s 1.90946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:                timestamp 1691674761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               tmae_coord 1.22042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:               tmae_force 0.38636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              tmape_coord 77335383200625.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              tmape_force 744710649566272.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              trmse_coord 0.51649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:              trmse_force 0.23332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb:  View run FSR_Trainable_1ebbb40f at: https://wandb.ai/seokjin/FSR-prediction/runs/1ebbb40f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=138893)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223923-1ebbb40f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_e5f907e0_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-17/wandb/run-20230810_223931-e5f907e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Syncing run FSR_Trainable_e5f907e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e5f907e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:39:36,291\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:36,297\tWARNING util.py:315 -- The `process_trial_result` operation took 2.498 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:36,300\tWARNING util.py:315 -- Processing trial results took 2.500 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:36,302\tWARNING util.py:315 -- The `process_trial_result` operation took 2.503 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139116)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223931-e5f907e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_81cb7e1f_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-26/wandb/run-20230810_223938-81cb7e1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Syncing run FSR_Trainable_81cb7e1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/81cb7e1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:39:43,185\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:43,197\tWARNING util.py:315 -- The `process_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:43,200\tWARNING util.py:315 -- Processing trial results took 2.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:43,206\tWARNING util.py:315 -- The `process_trial_result` operation took 2.254 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_0d1de52d_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-33/wandb/run-20230810_223945-0d1de52d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Syncing run FSR_Trainable_0d1de52d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0d1de52d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                mae_coord 25.95727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                mae_force 2531.96437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               mape_coord 4.31706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               mape_force 2.839330271941839e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                   metric 2.35822\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               rmse_coord 8.82089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               rmse_force 1871.93601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:       time_since_restore 0.77402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:         time_this_iter_s 0.77402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:             time_total_s 0.77402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:                timestamp 1691674773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               tmae_coord 5.62615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:               tmae_force 0.85165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              tmape_coord 30417065536405.926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              tmape_force 1151330175781166.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              trmse_coord 1.8362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:              trmse_force 0.52203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb:  View run FSR_Trainable_81cb7e1f at: https://wandb.ai/seokjin/FSR-prediction/runs/81cb7e1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139297)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223938-81cb7e1f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:39:52,442\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.518 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:52,448\tWARNING util.py:315 -- The `process_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:52,450\tWARNING util.py:315 -- Processing trial results took 2.527 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:52,452\tWARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139482)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_76d84eb0_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-40/wandb/run-20230810_223954-76d84eb0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Syncing run FSR_Trainable_76d84eb0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/76d84eb0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:39:58,855\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:58,857\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:39:58,859\tWARNING util.py:315 -- Processing trial results took 1.911 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:39:58,861\tWARNING util.py:315 -- The `process_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                mae_coord 4.84673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                mae_force 1033.08865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               mape_coord 1.18108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               mape_force 1.768364616428746e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                   metric 39.08657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               rmse_coord 2.54798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               rmse_force 921.40719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:       time_since_restore 0.75778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:         time_this_iter_s 0.75778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:             time_total_s 0.75778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:                timestamp 1691674789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               tmae_coord 30.18484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:               tmae_force 5.28483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              tmape_coord 5923188597651015.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              tmape_force 6318278597511470.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              trmse_coord 33.93075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:              trmse_force 5.15582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb:  View run FSR_Trainable_76d84eb0 at: https://wandb.ai/seokjin/FSR-prediction/runs/76d84eb0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_223954-76d84eb0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139707)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_93091046_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-49/wandb/run-20230810_224001-93091046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Syncing run FSR_Trainable_93091046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/93091046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224001-93091046/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=139888)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-10 22:40:07,528\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.270 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:07,533\tWARNING util.py:315 -- The `process_trial_result` operation took 2.276 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:07,535\tWARNING util.py:315 -- Processing trial results took 2.278 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:40:07,536\tWARNING util.py:315 -- The `process_trial_result` operation took 2.279 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_b2b7e672_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-39-56/wandb/run-20230810_224013-b2b7e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Syncing run FSR_Trainable_b2b7e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b2b7e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-08-10 22:40:14,177\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:14,180\tWARNING util.py:315 -- The `process_trial_result` operation took 2.043 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:14,183\tWARNING util.py:315 -- Processing trial results took 2.046 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:40:14,186\tWARNING util.py:315 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                mae_coord 14.73124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                mae_force 3958.98583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               mape_coord 2.71464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               mape_force 4.766935836677504e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                   metric 56.59728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               rmse_coord 11.4781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               rmse_force 4269.69035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:       time_since_restore 0.8521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:         time_this_iter_s 0.8521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:             time_total_s 0.8521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:                timestamp 1691674805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               tmae_coord 44.95455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:               tmae_force 17.61815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              tmape_coord 1.9408865515634828e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              tmape_force 1.855446884372297e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              trmse_coord 37.82781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:              trmse_force 18.76947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb:  View run FSR_Trainable_b2b7e672 at: https://wandb.ai/seokjin/FSR-prediction/runs/b2b7e672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140110)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224013-b2b7e672/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224013-b2b7e672/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224016-eaf9fef7/logs\n",
      "2023-08-10 22:40:24,116\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.519 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:24,118\tWARNING util.py:315 -- The `process_trial_result` operation took 2.522 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:24,120\tWARNING util.py:315 -- Processing trial results took 2.524 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:40:24,122\tWARNING util.py:315 -- The `process_trial_result` operation took 2.526 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_dd88466f_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-40-11/wandb/run-20230810_224026-dd88466f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Syncing run FSR_Trainable_dd88466f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dd88466f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:40:31,314\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:31,318\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:31,319\tWARNING util.py:315 -- Processing trial results took 1.976 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:40:31,321\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                mae_coord 5.40423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                mae_force 738.1036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               mape_coord 1.35239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               mape_force 1.2569232336168238e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                   metric 0.72169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               rmse_coord 2.65606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               rmse_force 473.31359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:       time_since_restore 4.00614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:         time_this_iter_s 0.34406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:             time_total_s 4.00614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:                timestamp 1691674827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               tmae_coord 1.12338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:               tmae_force 0.3466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              tmape_coord 82972052758441.92\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              tmape_force 737671988794432.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              trmse_coord 0.516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:              trmse_force 0.20569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb:  View run FSR_Trainable_dd88466f at: https://wandb.ai/seokjin/FSR-prediction/runs/dd88466f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224026-dd88466f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140518)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_a78b6bca_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-40-20/wandb/run-20230810_224034-a78b6bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Syncing run FSR_Trainable_a78b6bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a78b6bca\n",
      "2023-08-10 22:40:41,737\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.713 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:41,744\tWARNING util.py:315 -- The `process_trial_result` operation took 2.721 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:41,746\tWARNING util.py:315 -- Processing trial results took 2.723 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:40:41,748\tWARNING util.py:315 -- The `process_trial_result` operation took 2.725 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_b149d862_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-40-28/wandb/run-20230810_224044-b149d862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Syncing run FSR_Trainable_b149d862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b149d862\n",
      "2023-08-10 22:40:51,006\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.496 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:51,010\tWARNING util.py:315 -- The `process_trial_result` operation took 2.501 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:40:51,012\tWARNING util.py:315 -- Processing trial results took 2.503 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:40:51,017\tWARNING util.py:315 -- The `process_trial_result` operation took 2.508 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_0350f91f_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-40-38/wandb/run-20230810_224054-0350f91f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Syncing run FSR_Trainable_0350f91f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0350f91f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                mae_coord 5.50522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                mae_force 766.77815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               mape_coord 1.36742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               mape_force 1.2408585488425006e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                   metric 0.74258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               rmse_coord 2.71\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               rmse_force 526.42958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:       time_since_restore 2.64916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:         time_this_iter_s 0.56367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:             time_total_s 2.64916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:                timestamp 1691674853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               tmae_coord 1.13541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:               tmae_force 0.34962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              tmape_coord 82960368725083.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              tmape_force 692472846755554.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              trmse_coord 0.5236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:              trmse_force 0.21898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb:  View run FSR_Trainable_0350f91f at: https://wandb.ai/seokjin/FSR-prediction/runs/0350f91f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141098)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224054-0350f91f/logs\n",
      "2023-08-10 22:41:03,591\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.308 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:03,593\tWARNING util.py:315 -- The `process_trial_result` operation took 2.312 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:03,596\tWARNING util.py:315 -- Processing trial results took 2.314 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:41:03,598\tWARNING util.py:315 -- The `process_trial_result` operation took 2.316 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_13115552_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-40-47/wandb/run-20230810_224109-13115552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: Syncing run FSR_Trainable_13115552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/13115552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                mae_coord 5.39161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                mae_force 725.66491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               mape_coord 1.17721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               mape_force 1.2003195221137224e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                   metric 0.69177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               rmse_coord 2.60626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               rmse_force 463.46561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:       time_since_restore 27.71931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:         time_this_iter_s 0.43166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:             time_total_s 27.71931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:                timestamp 1691674867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               tmae_coord 1.13042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:               tmae_force 0.31229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              tmape_coord 74966517520583.14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              tmape_force 609803699144937.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              trmse_coord 0.51043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:              trmse_force 0.18134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb:  View run FSR_Trainable_a78b6bca at: https://wandb.ai/seokjin/FSR-prediction/runs/a78b6bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140701)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224034-a78b6bca/logs\n",
      "2023-08-10 22:41:15,567\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:15,571\tWARNING util.py:315 -- The `process_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:15,580\tWARNING util.py:315 -- Processing trial results took 2.025 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:41:15,582\tWARNING util.py:315 -- The `process_trial_result` operation took 2.028 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_1a2e7ffb_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-41-00/wandb/run-20230810_224118-1a2e7ffb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Syncing run FSR_Trainable_1a2e7ffb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a2e7ffb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                mae_coord 5.27316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                mae_force 761.43254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               mape_coord 1.16927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               mape_force 1.2624788298266752e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                   metric 0.69765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               rmse_coord 2.57123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               rmse_force 490.56024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:       time_since_restore 28.3259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:         time_this_iter_s 0.36734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:             time_total_s 28.3259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:                timestamp 1691674878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               tmae_coord 1.12084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:               tmae_force 0.32112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              tmape_coord 78504944856690.86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              tmape_force 618325818183593.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              trmse_coord 0.50936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:              trmse_force 0.18829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb:  View run FSR_Trainable_b149d862 at: https://wandb.ai/seokjin/FSR-prediction/runs/b149d862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=140922)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224044-b149d862/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb:              trmse_force \n",
      "2023-08-10 22:41:26,365\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:26,369\tWARNING util.py:315 -- The `process_trial_result` operation took 2.256 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:26,372\tWARNING util.py:315 -- Processing trial results took 2.259 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:41:26,373\tWARNING util.py:315 -- The `process_trial_result` operation took 2.261 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141564)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_e78b1941_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-41-12/wandb/run-20230810_224129-e78b1941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Syncing run FSR_Trainable_e78b1941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e78b1941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                mae_coord 4.32008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                mae_force 1153.73167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               mape_coord 1.13267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               mape_force 665182463.21531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                   metric 6.16284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               rmse_coord 2.37491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               rmse_force 799.43623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:       time_since_restore 0.45104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:         time_this_iter_s 0.45104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:             time_total_s 0.45104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:                timestamp 1691674884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               tmae_coord 8.03439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:               tmae_force 2.9867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              tmape_coord 2147628021881186.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              tmape_force 71.78088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              trmse_coord 4.46604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:              trmse_force 1.69679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb:  View run FSR_Trainable_e78b1941 at: https://wandb.ai/seokjin/FSR-prediction/runs/e78b1941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224129-e78b1941/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141797)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:41:35,842\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.013 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:35,845\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:35,847\tWARNING util.py:315 -- Processing trial results took 2.019 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:41:35,849\tWARNING util.py:315 -- The `process_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=141328)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_0be5760d_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-41-23/wandb/run-20230810_224138-0be5760d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Syncing run FSR_Trainable_0be5760d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0be5760d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:41:44,263\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.422 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:44,268\tWARNING util.py:315 -- The `process_trial_result` operation took 2.428 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:44,270\tWARNING util.py:315 -- Processing trial results took 2.430 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:41:44,272\tWARNING util.py:315 -- The `process_trial_result` operation took 2.432 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                mae_coord 19.10849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                mae_force 1335.65974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               mape_coord 3.30393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               mape_force 1.867607871827527e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                   metric 1.66398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               rmse_coord 6.75541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               rmse_force 937.55155\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:       time_since_restore 0.90725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:         time_this_iter_s 0.30752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:             time_total_s 0.90725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:                timestamp 1691674896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               tmae_coord 4.06787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:               tmae_force 0.53555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              tmape_coord 56015774474363.04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              tmape_force 1007839726697251.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              trmse_coord 1.36343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:              trmse_force 0.30055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb:  View run FSR_Trainable_0be5760d at: https://wandb.ai/seokjin/FSR-prediction/runs/0be5760d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224138-0be5760d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142026)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_d3a0f5db_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-41-33/wandb/run-20230810_224146-d3a0f5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Syncing run FSR_Trainable_d3a0f5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d3a0f5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:41:50,522\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:50,524\tWARNING util.py:315 -- The `process_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:41:50,526\tWARNING util.py:315 -- Processing trial results took 2.061 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:41:50,527\tWARNING util.py:315 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                mae_coord 25.46502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                mae_force 2776.88569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               mape_coord 3.7559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               mape_force 4.65166820714124e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                   metric 2.50554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               rmse_coord 8.79376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               rmse_force 1524.66459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:       time_since_restore 0.62703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:         time_this_iter_s 0.62703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:             time_total_s 0.62703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:                timestamp 1691674901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               tmae_coord 5.4903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:               tmae_force 1.35502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              tmape_coord 37792778557492.25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              tmape_force 2922064331864176.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              trmse_coord 1.82936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:              trmse_force 0.67618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb:  View run FSR_Trainable_d3a0f5db at: https://wandb.ai/seokjin/FSR-prediction/runs/d3a0f5db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224146-d3a0f5db/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142256)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_d1a09ab3_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-41-41/wandb/run-20230810_224153-d1a09ab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Syncing run FSR_Trainable_d1a09ab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d1a09ab3\n",
      "2023-08-10 22:42:00,447\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:00,449\tWARNING util.py:315 -- The `process_trial_result` operation took 2.240 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:00,452\tWARNING util.py:315 -- Processing trial results took 2.242 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:42:00,457\tWARNING util.py:315 -- The `process_trial_result` operation took 2.247 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_a0f7a6e6_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-41-47/wandb/run-20230810_224203-a0f7a6e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Syncing run FSR_Trainable_a0f7a6e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a0f7a6e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                mae_coord 5.63774\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                mae_force 739.46078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               mape_coord 1.39265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               mape_force 1.31750297011094e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                   metric 0.71107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               rmse_coord 2.60101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               rmse_force 451.07743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:       time_since_restore 3.04387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:         time_this_iter_s 0.31172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:             time_total_s 3.04387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:                timestamp 1691674923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               tmae_coord 1.17186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:               tmae_force 0.3469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              tmape_coord 78210708495865.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              tmape_force 769475657584761.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              trmse_coord 0.51401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:              trmse_force 0.19706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb:  View run FSR_Trainable_a0f7a6e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/a0f7a6e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142658)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224203-a0f7a6e6/logs\n",
      "2023-08-10 22:42:11,272\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.898 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:11,275\tWARNING util.py:315 -- The `process_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:11,279\tWARNING util.py:315 -- Processing trial results took 1.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:42:11,281\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_1ec667ce_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-41-57/wandb/run-20230810_224214-1ec667ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Syncing run FSR_Trainable_1ec667ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1ec667ce\n",
      "2023-08-10 22:42:22,455\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:22,459\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:22,460\tWARNING util.py:315 -- Processing trial results took 2.084 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:42:22,464\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_eeaeafba_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-42-08/wandb/run-20230810_224225-eeaeafba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Syncing run FSR_Trainable_eeaeafba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eeaeafba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                mae_coord 5.53427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                mae_force 753.38242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               mape_coord 1.32363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               mape_force 1.342436057905852e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                   metric 0.71179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               rmse_coord 2.54827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               rmse_force 478.68702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:       time_since_restore 11.45615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:         time_this_iter_s 0.74081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:             time_total_s 11.45615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:                timestamp 1691674944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               tmae_coord 1.17498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:               tmae_force 0.34794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              tmape_coord 83594939999366.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              tmape_force 769562324930750.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              trmse_coord 0.51042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:              trmse_force 0.20137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb:  View run FSR_Trainable_1ec667ce at: https://wandb.ai/seokjin/FSR-prediction/runs/1ec667ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142881)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224214-1ec667ce/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb:  View run FSR_Trainable_eeaeafba at: https://wandb.ai/seokjin/FSR-prediction/runs/eeaeafba\n",
      "2023-08-10 22:42:32,581\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:32,585\tWARNING util.py:315 -- The `process_trial_result` operation took 2.202 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:32,587\tWARNING util.py:315 -- Processing trial results took 2.203 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:42:32,588\tWARNING util.py:315 -- The `process_trial_result` operation took 2.205 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_5456a8ec_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-42-19/wandb/run-20230810_224235-5456a8ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Syncing run FSR_Trainable_5456a8ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5456a8ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                mae_coord 4.49321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                mae_force 636.66863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               mape_coord 1.10098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               mape_force 8.81125843790776e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                   metric 0.65064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               rmse_coord 2.43309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               rmse_force 434.53322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:       time_since_restore 34.34298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:         time_this_iter_s 0.33366\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:             time_total_s 34.34298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:                timestamp 1691674955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               tmae_coord 0.96383\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:               tmae_force 0.25597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              tmape_coord 78448628520692.39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              tmape_force 401086087348293.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              trmse_coord 0.48638\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:              trmse_force 0.16426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb:  View run FSR_Trainable_d1a09ab3 at: https://wandb.ai/seokjin/FSR-prediction/runs/d1a09ab3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142435)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224153-d1a09ab3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224235-5456a8ec/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2023-08-10 22:42:41,723\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.004 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:41,728\tWARNING util.py:315 -- The `process_trial_result` operation took 2.010 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:41,730\tWARNING util.py:315 -- Processing trial results took 2.012 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:42:41,732\tWARNING util.py:315 -- The `process_trial_result` operation took 2.014 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_6670c862_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-42-30/wandb/run-20230810_224243-6670c862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Syncing run FSR_Trainable_6670c862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6670c862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:42:50,058\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.382 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:50,062\tWARNING util.py:315 -- The `process_trial_result` operation took 2.386 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:50,064\tWARNING util.py:315 -- Processing trial results took 2.389 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:42:50,066\tWARNING util.py:315 -- The `process_trial_result` operation took 2.390 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                mae_coord 25.93305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                mae_force 2582.6018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               mape_coord 4.33372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               mape_force 3.8777164160692296e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                   metric 2.4116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               rmse_coord 8.91215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               rmse_force 1662.95076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:       time_since_restore 0.68579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:         time_this_iter_s 0.68579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:             time_total_s 0.68579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:                timestamp 1691674959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               tmae_coord 5.73739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:               tmae_force 0.97625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              tmape_coord 28251778877064.645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              tmape_force 1950329425577526.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              trmse_coord 1.89867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:              trmse_force 0.51293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb:  View run FSR_Trainable_6670c862 at: https://wandb.ai/seokjin/FSR-prediction/runs/6670c862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143568)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224243-6670c862/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_70ee40e6_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-42-39/wandb/run-20230810_224252-70ee40e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Syncing run FSR_Trainable_70ee40e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/70ee40e6\n",
      "2023-08-10 22:42:57,567\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.426 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:57,573\tWARNING util.py:315 -- The `process_trial_result` operation took 2.433 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:42:57,575\tWARNING util.py:315 -- Processing trial results took 2.435 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:42:57,577\tWARNING util.py:315 -- The `process_trial_result` operation took 2.437 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_56638e36_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-42-47/wandb/run-20230810_224300-56638e36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Syncing run FSR_Trainable_56638e36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/56638e36\n",
      "2023-08-10 22:43:05,883\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.150 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:43:05,884\tWARNING util.py:315 -- The `process_trial_result` operation took 2.152 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:43:05,889\tWARNING util.py:315 -- Processing trial results took 2.157 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:43:05,894\tWARNING util.py:315 -- The `process_trial_result` operation took 2.162 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_64391342_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-42-54/wandb/run-20230810_224309-64391342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Syncing run FSR_Trainable_64391342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/64391342\n",
      "2023-08-10 22:43:19,480\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:43:19,482\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:43:19,484\tWARNING util.py:315 -- Processing trial results took 2.189 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:43:19,486\tWARNING util.py:315 -- The `process_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_7b47a99c_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-43-03/wandb/run-20230810_224323-7b47a99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Syncing run FSR_Trainable_7b47a99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b47a99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                mae_coord 4.32762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                mae_force 621.42985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               mape_coord 1.11361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               mape_force 8.081026611730673e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                   metric 0.64063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               rmse_coord 2.38734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               rmse_force 434.44367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:       time_since_restore 37.68306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:         time_this_iter_s 0.5101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:             time_total_s 37.68306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:                timestamp 1691675018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               tmae_coord 0.93302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:               tmae_force 0.24102\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              tmape_coord 78696726325343.31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              tmape_force 342974624308245.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              trmse_coord 0.48128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:              trmse_force 0.15935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb:  View run FSR_Trainable_70ee40e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/70ee40e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143789)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224252-70ee40e6/logs\n",
      "2023-08-10 22:43:53,568\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.076 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:43:53,570\tWARNING util.py:315 -- The `process_trial_result` operation took 2.080 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:43:53,572\tWARNING util.py:315 -- Processing trial results took 2.081 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:43:53,577\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_eb36545b_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-43-16/wandb/run-20230810_224357-eb36545b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Syncing run FSR_Trainable_eb36545b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb36545b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                mae_coord 4.6591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                mae_force 639.30361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               mape_coord 1.11294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               mape_force 9.162722116955921e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                   metric 0.65496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               rmse_coord 2.45344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               rmse_force 430.9381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:       time_since_restore 49.13016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:         time_this_iter_s 0.48204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:             time_total_s 49.13016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:                timestamp 1691675036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               tmae_coord 0.99965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:               tmae_force 0.26153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              tmape_coord 77927258848675.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              tmape_force 433271793745505.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              trmse_coord 0.49085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:              trmse_force 0.1641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb:  View run FSR_Trainable_56638e36 at: https://wandb.ai/seokjin/FSR-prediction/runs/56638e36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143971)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224300-56638e36/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                mae_coord 4.54596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                mae_force 644.14107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               mape_coord 1.10837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               mape_force 8.978324584179084e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                   metric 0.6533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               rmse_coord 2.42711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               rmse_force 437.44166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:       time_since_restore 49.68192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:         time_this_iter_s 0.42577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:             time_total_s 49.68192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:                timestamp 1691675043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               tmae_coord 0.97717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:               tmae_force 0.26268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              tmape_coord 78440771096576.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              tmape_force 422141125279674.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              trmse_coord 0.4859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:              trmse_force 0.1674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb:  View run FSR_Trainable_64391342 at: https://wandb.ai/seokjin/FSR-prediction/runs/64391342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144148)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224309-64391342/logs\n",
      "2023-08-10 22:44:11,619\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.574 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:44:11,622\tWARNING util.py:315 -- The `process_trial_result` operation took 2.578 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:44:11,625\tWARNING util.py:315 -- Processing trial results took 2.581 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:44:11,628\tWARNING util.py:315 -- The `process_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_fc7a34d4_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-43-50/wandb/run-20230810_224415-fc7a34d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Syncing run FSR_Trainable_fc7a34d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fc7a34d4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                mae_coord 4.64607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                mae_force 637.47085\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               mape_coord 1.11598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               mape_force 9.126789794286377e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                   metric 0.65345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               rmse_coord 2.45232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               rmse_force 432.05866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:       time_since_restore 49.19976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:         time_this_iter_s 0.57707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:             time_total_s 49.19976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:                timestamp 1691675057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               tmae_coord 0.99723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:               tmae_force 0.25592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              tmape_coord 77791673224058.36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              tmape_force 411197993624900.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              trmse_coord 0.49003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:              trmse_force 0.16342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb:  View run FSR_Trainable_7b47a99c at: https://wandb.ai/seokjin/FSR-prediction/runs/7b47a99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144371)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224323-7b47a99c/logs\n",
      "2023-08-10 22:44:25,758\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.516 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:44:25,760\tWARNING util.py:315 -- The `process_trial_result` operation took 2.520 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:44:25,771\tWARNING util.py:315 -- Processing trial results took 2.531 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:44:25,772\tWARNING util.py:315 -- The `process_trial_result` operation took 2.532 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_d9e1a2f5_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-44-08/wandb/run-20230810_224429-d9e1a2f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Syncing run FSR_Trainable_d9e1a2f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d9e1a2f5\n",
      "2023-08-10 22:44:39,816\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.112 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:44:39,819\tWARNING util.py:315 -- The `process_trial_result` operation took 2.115 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:44:39,822\tWARNING util.py:315 -- Processing trial results took 2.118 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:44:39,823\tWARNING util.py:315 -- The `process_trial_result` operation took 2.120 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_3f75ed82_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-44-22/wandb/run-20230810_224445-3f75ed82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Syncing run FSR_Trainable_3f75ed82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3f75ed82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                mae_coord 5.01799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                mae_force 809.30303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               mape_coord 1.2331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               mape_force 1.5619668112586557e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                   metric 0.68313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               rmse_coord 2.45356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               rmse_force 466.02836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:       time_since_restore 14.69353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:         time_this_iter_s 0.63584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:             time_total_s 14.69353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:                timestamp 1691675095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               tmae_coord 1.07194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:               tmae_force 0.36017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              tmape_coord 78055414917180.83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              tmape_force 825917275477540.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              trmse_coord 0.49235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:              trmse_force 0.19077\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb:  View run FSR_Trainable_3f75ed82 at: https://wandb.ai/seokjin/FSR-prediction/runs/3f75ed82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145351)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224445-3f75ed82/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                mae_coord 4.51735\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                mae_force 631.82333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               mape_coord 1.09751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               mape_force 8.705602438291779e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                   metric 0.64872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               rmse_coord 2.4238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               rmse_force 434.19711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:       time_since_restore 57.63341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:         time_this_iter_s 0.48305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:             time_total_s 57.63341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:                timestamp 1691675102\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               tmae_coord 0.97069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:               tmae_force 0.25174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              tmape_coord 77955410252260.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              tmape_force 391845191981686.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              trmse_coord 0.48617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:              trmse_force 0.16255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb:  View run FSR_Trainable_eb36545b at: https://wandb.ai/seokjin/FSR-prediction/runs/eb36545b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144644)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224357-eb36545b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb:              trmse_coord \n",
      "2023-08-10 22:45:13,217\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.533 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:13,221\tWARNING util.py:315 -- The `process_trial_result` operation took 2.538 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:13,223\tWARNING util.py:315 -- Processing trial results took 2.539 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:45:13,226\tWARNING util.py:315 -- The `process_trial_result` operation took 2.542 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_cce353eb_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-44-37/wandb/run-20230810_224516-cce353eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Syncing run FSR_Trainable_cce353eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cce353eb\n",
      "2023-08-10 22:45:24,969\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.408 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:24,971\tWARNING util.py:315 -- The `process_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:24,976\tWARNING util.py:315 -- Processing trial results took 2.416 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:45:24,979\tWARNING util.py:315 -- The `process_trial_result` operation took 2.419 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_89c82371_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-45-09/wandb/run-20230810_224531-89c82371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Syncing run FSR_Trainable_89c82371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/89c82371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                mae_coord 4.56081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                mae_force 633.60035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               mape_coord 1.10373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               mape_force 8.94982578960767e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                   metric 0.64972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               rmse_coord 2.42884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               rmse_force 430.12408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:       time_since_restore 56.01595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:         time_this_iter_s 0.55998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:             time_total_s 56.01595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:                timestamp 1691675133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               tmae_coord 0.9814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:               tmae_force 0.2561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              tmape_coord 77669610387136.58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              tmape_force 413303640059437.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              trmse_coord 0.48666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:              trmse_force 0.16306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb:  View run FSR_Trainable_d9e1a2f5 at: https://wandb.ai/seokjin/FSR-prediction/runs/d9e1a2f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145126)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224429-d9e1a2f5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224516-cce353eb/logs\n",
      "2023-08-10 22:45:38,633\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.447 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:38,636\tWARNING util.py:315 -- The `process_trial_result` operation took 2.453 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:38,638\tWARNING util.py:315 -- Processing trial results took 2.455 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:45:38,642\tWARNING util.py:315 -- The `process_trial_result` operation took 2.459 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_52a2c1ef_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-45-22/wandb/run-20230810_224541-52a2c1ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Syncing run FSR_Trainable_52a2c1ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/52a2c1ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145626)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                mae_coord 6.92257\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                mae_force 1108.49012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               mape_coord 1.55319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               mape_force 2.0270289816711127e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                   metric 0.81303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               rmse_coord 2.92705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               rmse_force 744.95322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:       time_since_restore 0.93001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:         time_this_iter_s 0.36114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:             time_total_s 0.93001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:                timestamp 1691675139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               tmae_coord 1.49262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:               tmae_force 0.3981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              tmape_coord 87706638361285.23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              tmape_force 802442186000824.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              trmse_coord 0.58812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:              trmse_force 0.22491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb:  View run FSR_Trainable_52a2c1ef at: https://wandb.ai/seokjin/FSR-prediction/runs/52a2c1ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224541-52a2c1ef/logs\n",
      "2023-08-10 22:45:48,622\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:48,626\tWARNING util.py:315 -- The `process_trial_result` operation took 2.163 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:48,627\tWARNING util.py:315 -- Processing trial results took 2.165 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:45:48,628\tWARNING util.py:315 -- The `process_trial_result` operation took 2.166 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146085)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_b58f0ea2_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-45-35/wandb/run-20230810_224551-b58f0ea2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Syncing run FSR_Trainable_b58f0ea2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b58f0ea2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:45:59,019\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.680 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:59,024\tWARNING util.py:315 -- The `process_trial_result` operation took 2.685 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:45:59,029\tWARNING util.py:315 -- Processing trial results took 2.690 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:45:59,032\tWARNING util.py:315 -- The `process_trial_result` operation took 2.693 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                mae_coord 4.49744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                mae_force 859.80009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               mape_coord 1.26469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               mape_force 393832466.61852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                   metric 5.80134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               rmse_coord 2.41121\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               rmse_force 616.69052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:       time_since_restore 0.433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:         time_this_iter_s 0.433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:             time_total_s 0.433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:                timestamp 1691675146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               tmae_coord 8.29159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:               tmae_force 2.00205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              tmape_coord 2348753214251664.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              tmape_force 83.49579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              trmse_coord 4.49296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:              trmse_force 1.30838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb:  View run FSR_Trainable_b58f0ea2 at: https://wandb.ai/seokjin/FSR-prediction/runs/b58f0ea2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146324)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224551-b58f0ea2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_2c7af959_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-45-46/wandb/run-20230810_224601-2c7af959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Syncing run FSR_Trainable_2c7af959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2c7af959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                mae_coord 4.61699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                mae_force 1332.98036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               mape_coord 1.19437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               mape_force 725783987.64523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                   metric 6.41673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               rmse_coord 2.49679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               rmse_force 942.73078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:       time_since_restore 0.53464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:         time_this_iter_s 0.53464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:             time_total_s 0.53464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:                timestamp 1691675156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               tmae_coord 8.42284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:               tmae_force 3.16228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              tmape_coord 1885443092008942.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              tmape_force 91.46543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              trmse_coord 4.63827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:              trmse_force 1.77846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb:  View run FSR_Trainable_2c7af959 at: https://wandb.ai/seokjin/FSR-prediction/runs/2c7af959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146549)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224601-2c7af959/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:46:09,862\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.135 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:09,864\tWARNING util.py:315 -- The `process_trial_result` operation took 2.139 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:09,866\tWARNING util.py:315 -- Processing trial results took 2.140 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:46:09,867\tWARNING util.py:315 -- The `process_trial_result` operation took 2.141 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224531-89c82371/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145851)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_30ee5665_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-45-55/wandb/run-20230810_224612-30ee5665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Syncing run FSR_Trainable_30ee5665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/30ee5665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                mae_coord 5.24146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                mae_force 692.17301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               mape_coord 1.28288\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               mape_force 1.1581962989853622e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                   metric 0.67866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               rmse_coord 2.48981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               rmse_force 429.87613\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:       time_since_restore 7.18847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:         time_this_iter_s 0.32127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:             time_total_s 7.18847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:                timestamp 1691675176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               tmae_coord 1.11612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:               tmae_force 0.31312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              tmape_coord 81241253914028.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              tmape_force 660822786345539.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              trmse_coord 0.49975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:              trmse_force 0.17891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb:  View run FSR_Trainable_30ee5665 at: https://wandb.ai/seokjin/FSR-prediction/runs/30ee5665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146783)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224612-30ee5665/logs\n",
      "2023-08-10 22:46:20,380\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.111 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:20,387\tWARNING util.py:315 -- The `process_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:20,389\tWARNING util.py:315 -- Processing trial results took 2.121 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:46:20,392\tWARNING util.py:315 -- The `process_trial_result` operation took 2.124 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_730fdf81_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-46-06/wandb/run-20230810_224626-730fdf81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Syncing run FSR_Trainable_730fdf81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/730fdf81\n",
      "2023-08-10 22:46:30,915\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.716 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:30,920\tWARNING util.py:315 -- The `process_trial_result` operation took 2.723 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:30,924\tWARNING util.py:315 -- Processing trial results took 2.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:46:30,925\tWARNING util.py:315 -- The `process_trial_result` operation took 2.728 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_852e9620_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-46-17/wandb/run-20230810_224634-852e9620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Syncing run FSR_Trainable_852e9620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/852e9620\n",
      "2023-08-10 22:46:42,425\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.590 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:42,430\tWARNING util.py:315 -- The `process_trial_result` operation took 2.598 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:42,431\tWARNING util.py:315 -- Processing trial results took 2.599 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:46:42,433\tWARNING util.py:315 -- The `process_trial_result` operation took 2.601 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_a991e120_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-46-27/wandb/run-20230810_224646-a991e120\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Syncing run FSR_Trainable_a991e120\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a991e120\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                mae_coord 18.01861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                mae_force 1349.59211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               mape_coord 2.83161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               mape_force 2.3902215082614666e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                   metric 1.62443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               rmse_coord 6.6166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               rmse_force 950.90901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:       time_since_restore 1.38481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:         time_this_iter_s 1.38481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:             time_total_s 1.38481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:                timestamp 1691675199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               tmae_coord 3.79553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:               tmae_force 0.47907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              tmape_coord 51370003192004.35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              tmape_force 875021628712813.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              trmse_coord 1.30597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:              trmse_force 0.31846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb:  View run FSR_Trainable_a991e120 at: https://wandb.ai/seokjin/FSR-prediction/runs/a991e120\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147413)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224646-a991e120/logs\n",
      "2023-08-10 22:46:55,782\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.627 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:55,788\tWARNING util.py:315 -- The `process_trial_result` operation took 2.635 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:46:55,790\tWARNING util.py:315 -- Processing trial results took 2.636 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:46:55,791\tWARNING util.py:315 -- The `process_trial_result` operation took 2.637 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_f90c403c_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-46-38/wandb/run-20230810_224702-f90c403c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Syncing run FSR_Trainable_f90c403c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f90c403c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                mae_coord 15.51346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                mae_force 1563.67384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               mape_coord 2.62226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               mape_force 3.1380129765200876e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                   metric 1.57069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               rmse_coord 5.92873\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               rmse_force 939.83555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:       time_since_restore 0.49051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:         time_this_iter_s 0.49051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:             time_total_s 0.49051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:                timestamp 1691675213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               tmae_coord 3.35937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:               tmae_force 0.60842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              tmape_coord 54410270623780.12\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              tmape_force 1313227466735063.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              trmse_coord 1.23929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:              trmse_force 0.33139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb:  View run FSR_Trainable_f90c403c at: https://wandb.ai/seokjin/FSR-prediction/runs/f90c403c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147644)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224702-f90c403c/logs\n",
      "2023-08-10 22:47:11,961\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.726 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:11,965\tWARNING util.py:315 -- The `process_trial_result` operation took 3.731 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:11,966\tWARNING util.py:315 -- Processing trial results took 3.732 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:47:11,967\tWARNING util.py:315 -- The `process_trial_result` operation took 3.734 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_3dc99959_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-46-52/wandb/run-20230810_224716-3dc99959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Syncing run FSR_Trainable_3dc99959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3dc99959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: \\ 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: | 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: / 0.005 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                mae_coord 4.40033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                mae_force 627.01823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               mape_coord 1.10618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               mape_force 8.156932322374835e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                   metric 0.64764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               rmse_coord 2.41338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               rmse_force 436.83736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:       time_since_restore 42.04718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:         time_this_iter_s 0.47911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:             time_total_s 42.04718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:                timestamp 1691675236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               tmae_coord 0.94592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:               tmae_force 0.24588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              tmape_coord 79254500073178.45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              tmape_force 351724684689200.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              trmse_coord 0.48464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:              trmse_force 0.16301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb:  View run FSR_Trainable_730fdf81 at: https://wandb.ai/seokjin/FSR-prediction/runs/730fdf81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147014)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224626-730fdf81/logs\n",
      "2023-08-10 22:47:26,870\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.268 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:26,874\tWARNING util.py:315 -- The `process_trial_result` operation took 2.274 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:26,876\tWARNING util.py:315 -- Processing trial results took 2.276 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:47:26,877\tWARNING util.py:315 -- The `process_trial_result` operation took 2.277 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_fbb4fbf5_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-47-07/wandb/run-20230810_224733-fbb4fbf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Syncing run FSR_Trainable_fbb4fbf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbb4fbf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                mae_coord 4.40522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                mae_force 626.53932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               mape_coord 1.10912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               mape_force 8.020489135207706e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                   metric 0.64878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               rmse_coord 2.41549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               rmse_force 438.50023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:       time_since_restore 44.94809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:         time_this_iter_s 0.41926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:             time_total_s 44.94809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:                timestamp 1691675251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               tmae_coord 0.94619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:               tmae_force 0.24516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              tmape_coord 78979105994553.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              tmape_force 343276181258240.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              trmse_coord 0.4847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:              trmse_force 0.16408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb:  View run FSR_Trainable_852e9620 at: https://wandb.ai/seokjin/FSR-prediction/runs/852e9620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147216)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224634-852e9620/logs\n",
      "2023-08-10 22:47:40,251\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.407 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:40,254\tWARNING util.py:315 -- The `process_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:40,257\tWARNING util.py:315 -- Processing trial results took 2.415 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:47:40,260\tWARNING util.py:315 -- The `process_trial_result` operation took 2.418 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_545d0224_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-47-23/wandb/run-20230810_224743-545d0224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Syncing run FSR_Trainable_545d0224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/545d0224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                mae_coord 4.97612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                mae_force 687.86118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               mape_coord 1.20811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               mape_force 1.1544852194623788e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                   metric 0.66871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               rmse_coord 2.46361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               rmse_force 429.65648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:       time_since_restore 8.63157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:         time_this_iter_s 0.34406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:             time_total_s 8.63157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:                timestamp 1691675255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               tmae_coord 1.06649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:               tmae_force 0.30553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              tmape_coord 81792074093322.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              tmape_force 630818026346504.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              trmse_coord 0.49396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:              trmse_force 0.17475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb:  View run FSR_Trainable_fbb4fbf5 at: https://wandb.ai/seokjin/FSR-prediction/runs/fbb4fbf5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148111)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224733-fbb4fbf5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb:              trmse_force \n",
      "2023-08-10 22:47:52,365\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.357 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:52,369\tWARNING util.py:315 -- The `process_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:47:52,371\tWARNING util.py:315 -- Processing trial results took 2.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:47:52,373\tWARNING util.py:315 -- The `process_trial_result` operation took 2.366 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_89d84316_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-47-37/wandb/run-20230810_224755-89d84316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Syncing run FSR_Trainable_89d84316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/89d84316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                mae_coord 5.01432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                mae_force 1185.91232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               mape_coord 1.31499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               mape_force 7.856340019840963e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                   metric 40.14303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               rmse_coord 2.66955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               rmse_force 991.40892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:       time_since_restore 0.50149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:         time_this_iter_s 0.50149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:             time_total_s 0.50149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:                timestamp 1691675270\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               tmae_coord 29.95131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:               tmae_force 6.12996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              tmape_coord 8811755666445031.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              tmape_force 2713074106541656.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              trmse_coord 33.58879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:              trmse_force 6.55424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb:  View run FSR_Trainable_89d84316 at: https://wandb.ai/seokjin/FSR-prediction/runs/89d84316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224755-89d84316/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148582)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224716-3dc99959/logs\n",
      "2023-08-10 22:48:02,867\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:02,872\tWARNING util.py:315 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:02,875\tWARNING util.py:315 -- Processing trial results took 2.224 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:48:02,877\tWARNING util.py:315 -- The `process_trial_result` operation took 2.225 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_549e2a1c_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-47-49/wandb/run-20230810_224805-549e2a1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Syncing run FSR_Trainable_549e2a1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/549e2a1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147870)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                mae_coord 16.15334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                mae_force 1395.67985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               mape_coord 2.71449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               mape_force 3.1249550761722906e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                   metric 1.53748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               rmse_coord 6.09244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               rmse_force 854.64649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:       time_since_restore 0.7781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:         time_this_iter_s 0.7781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:             time_total_s 0.7781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:                timestamp 1691675280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               tmae_coord 3.43236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:               tmae_force 0.51926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              tmape_coord 70381522799258.97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              tmape_force 1245289509787725.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              trmse_coord 1.26349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:              trmse_force 0.27399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb:  View run FSR_Trainable_549e2a1c at: https://wandb.ai/seokjin/FSR-prediction/runs/549e2a1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224805-549e2a1c/logs\n",
      "2023-08-10 22:48:11,514\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.202 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:11,518\tWARNING util.py:315 -- The `process_trial_result` operation took 2.207 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:11,520\tWARNING util.py:315 -- Processing trial results took 2.210 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:48:11,522\tWARNING util.py:315 -- The `process_trial_result` operation took 2.211 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148810)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_223f2432_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-48-00/wandb/run-20230810_224814-223f2432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Syncing run FSR_Trainable_223f2432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/223f2432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:48:18,295\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.441 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:18,300\tWARNING util.py:315 -- The `process_trial_result` operation took 2.446 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:18,302\tWARNING util.py:315 -- Processing trial results took 2.449 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:48:18,303\tWARNING util.py:315 -- The `process_trial_result` operation took 2.450 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                mae_coord 4.74409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                mae_force 1419.7268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               mape_coord 1.22264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               mape_force 2.8188369653521633e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                   metric 0.80787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               rmse_coord 2.43657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               rmse_force 940.76651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:       time_since_restore 1.17405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:         time_this_iter_s 0.4047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:             time_total_s 1.17405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:                timestamp 1691675291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               tmae_coord 1.0431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:               tmae_force 0.52041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              tmape_coord 82433043273374.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              tmape_force 1123201257222990.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              trmse_coord 0.50361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:              trmse_force 0.30426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb:  View run FSR_Trainable_223f2432 at: https://wandb.ai/seokjin/FSR-prediction/runs/223f2432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224814-223f2432/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149033)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_00cc9211_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-48-08/wandb/run-20230810_224821-00cc9211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Syncing run FSR_Trainable_00cc9211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/00cc9211\n",
      "2023-08-10 22:48:26,096\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.693 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:26,098\tWARNING util.py:315 -- The `process_trial_result` operation took 2.696 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:26,101\tWARNING util.py:315 -- Processing trial results took 2.699 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:48:26,104\tWARNING util.py:315 -- The `process_trial_result` operation took 2.702 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_622e1df8_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-48-15/wandb/run-20230810_224829-622e1df8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Syncing run FSR_Trainable_622e1df8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/622e1df8\n",
      "2023-08-10 22:48:34,614\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:34,616\tWARNING util.py:315 -- The `process_trial_result` operation took 2.326 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:34,618\tWARNING util.py:315 -- Processing trial results took 2.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:48:34,620\tWARNING util.py:315 -- The `process_trial_result` operation took 2.330 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_116c5420_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-48-23/wandb/run-20230810_224838-116c5420\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Syncing run FSR_Trainable_116c5420\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/116c5420\n",
      "2023-08-10 22:48:47,602\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.273 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:47,606\tWARNING util.py:315 -- The `process_trial_result` operation took 2.277 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:48:47,607\tWARNING util.py:315 -- Processing trial results took 2.278 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:48:47,608\tWARNING util.py:315 -- The `process_trial_result` operation took 2.280 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_bf2ad8fb_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-48-31/wandb/run-20230810_224851-bf2ad8fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Syncing run FSR_Trainable_bf2ad8fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bf2ad8fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                mae_coord 4.46879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                mae_force 632.66197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               mape_coord 1.11428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               mape_force 8.320137568777628e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                   metric 0.65263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               rmse_coord 2.43054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               rmse_force 438.15719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:       time_since_restore 24.44842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:         time_this_iter_s 0.63191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:             time_total_s 24.44842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:                timestamp 1691675331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               tmae_coord 0.95916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:               tmae_force 0.25238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              tmape_coord 80139179372475.14\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              tmape_force 371882816025699.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              trmse_coord 0.48682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:              trmse_force 0.16581\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb:  View run FSR_Trainable_00cc9211 at: https://wandb.ai/seokjin/FSR-prediction/runs/00cc9211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149213)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224821-00cc9211/logs\n",
      "2023-08-10 22:49:07,741\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.348 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:49:07,745\tWARNING util.py:315 -- The `process_trial_result` operation took 2.353 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:49:07,747\tWARNING util.py:315 -- Processing trial results took 2.355 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:49:07,748\tWARNING util.py:315 -- The `process_trial_result` operation took 2.356 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_8b481b07_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-48-44/wandb/run-20230810_224911-8b481b07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Syncing run FSR_Trainable_8b481b07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8b481b07\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                mae_coord 4.44215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                mae_force 629.87292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               mape_coord 1.10093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               mape_force 8.274172352031611e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                   metric 0.6508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               rmse_coord 2.42421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               rmse_force 436.32568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:       time_since_restore 39.77374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:         time_this_iter_s 0.45645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:             time_total_s 39.77374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:                timestamp 1691675358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               tmae_coord 0.95472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:               tmae_force 0.25044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              tmape_coord 78967944752651.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              tmape_force 367420856768866.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              trmse_coord 0.48608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:              trmse_force 0.16473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb:  View run FSR_Trainable_622e1df8 at: https://wandb.ai/seokjin/FSR-prediction/runs/622e1df8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149397)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224829-622e1df8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                mae_coord 4.42188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                mae_force 627.98815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               mape_coord 1.10052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               mape_force 8.187038543629285e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                   metric 0.64956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               rmse_coord 2.41949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               rmse_force 436.4718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:       time_since_restore 41.3902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:         time_this_iter_s 0.4637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:             time_total_s 41.3902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:                timestamp 1691675365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               tmae_coord 0.95066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:               tmae_force 0.24773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              tmape_coord 79043582116410.58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              tmape_force 357310361569167.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              trmse_coord 0.48579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:              trmse_force 0.16376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb:  View run FSR_Trainable_116c5420 at: https://wandb.ai/seokjin/FSR-prediction/runs/116c5420\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149575)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224838-116c5420/logs\n",
      "2023-08-10 22:49:35,442\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.481 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:49:35,447\tWARNING util.py:315 -- The `process_trial_result` operation took 2.488 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:49:35,449\tWARNING util.py:315 -- Processing trial results took 2.490 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:49:35,451\tWARNING util.py:315 -- The `process_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_405740e0_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-49-04/wandb/run-20230810_224939-405740e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Syncing run FSR_Trainable_405740e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/405740e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                mae_coord 4.39906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                mae_force 632.19908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               mape_coord 1.09632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               mape_force 8.148620819624891e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                   metric 0.64885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               rmse_coord 2.41195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               rmse_force 440.746\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:       time_since_restore 41.80502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:         time_this_iter_s 0.3679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:             time_total_s 41.80502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:                timestamp 1691675379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               tmae_coord 0.94542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:               tmae_force 0.24795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              tmape_coord 78721814380336.86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              tmape_force 351056664689167.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              trmse_coord 0.48397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:              trmse_force 0.16488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb:  View run FSR_Trainable_bf2ad8fb at: https://wandb.ai/seokjin/FSR-prediction/runs/bf2ad8fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149797)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224851-bf2ad8fb/logs\n",
      "2023-08-10 22:49:48,707\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.345 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:49:48,713\tWARNING util.py:315 -- The `process_trial_result` operation took 2.351 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:49:48,714\tWARNING util.py:315 -- Processing trial results took 2.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:49:48,716\tWARNING util.py:315 -- The `process_trial_result` operation took 2.355 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_b6a00942_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-49-32/wandb/run-20230810_224952-b6a00942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Syncing run FSR_Trainable_b6a00942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6a00942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                mae_coord 16.93056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                mae_force 1673.09469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               mape_coord 2.83456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               mape_force 2.0948064774068506e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                   metric 1.63559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               rmse_coord 5.90696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               rmse_force 1337.78685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:       time_since_restore 0.47146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:         time_this_iter_s 0.47146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:             time_total_s 0.47146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:                timestamp 1691675386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               tmae_coord 3.7315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:               tmae_force 0.57986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              tmape_coord 37803435272202.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              tmape_force 965604752670789.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              trmse_coord 1.27143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:              trmse_force 0.36416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb:  View run FSR_Trainable_b6a00942 at: https://wandb.ai/seokjin/FSR-prediction/runs/b6a00942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150551)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224952-b6a00942/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224911-8b481b07/logs\n",
      "2023-08-10 22:50:02,323\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.449 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:02,327\tWARNING util.py:315 -- The `process_trial_result` operation took 2.453 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:02,329\tWARNING util.py:315 -- Processing trial results took 2.455 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:50:02,331\tWARNING util.py:315 -- The `process_trial_result` operation took 2.458 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150044)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_fe4d7d4d_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-49-45/wandb/run-20230810_225008-fe4d7d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Syncing run FSR_Trainable_fe4d7d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fe4d7d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:50:13,681\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.790 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:13,688\tWARNING util.py:315 -- The `process_trial_result` operation took 2.797 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:13,689\tWARNING util.py:315 -- Processing trial results took 2.799 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:50:13,690\tWARNING util.py:315 -- The `process_trial_result` operation took 2.800 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                mae_coord 15.80183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                mae_force 1278.90398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               mape_coord 2.6493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               mape_force 2.2419641257355612e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                   metric 1.51781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               rmse_coord 5.67161\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               rmse_force 828.54275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:       time_since_restore 0.57839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:         time_this_iter_s 0.57839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:             time_total_s 0.57839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:                timestamp 1691675399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               tmae_coord 3.53713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:               tmae_force 0.52099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              tmape_coord 62971699603031.02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              tmape_force 1140292371890383.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              trmse_coord 1.24006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:              trmse_force 0.27775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb:  View run FSR_Trainable_fe4d7d4d at: https://wandb.ai/seokjin/FSR-prediction/runs/fe4d7d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150790)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225008-fe4d7d4d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_72ac5990_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-49-59/wandb/run-20230810_225017-72ac5990\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Syncing run FSR_Trainable_72ac5990\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/72ac5990\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                mae_coord 21.00233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                mae_force 1830.98392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               mape_coord 3.36066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               mape_force 4.1585094273843676e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                   metric 1.92191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               rmse_coord 7.09398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               rmse_force 1022.61727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:       time_since_restore 0.57668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:         time_this_iter_s 0.57668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:             time_total_s 0.57668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:                timestamp 1691675410\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               tmae_coord 4.55974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:               tmae_force 0.77955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              tmape_coord 57007154622932.87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              tmape_force 1967113523253507.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              trmse_coord 1.51601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:              trmse_force 0.4059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb:  View run FSR_Trainable_72ac5990 at: https://wandb.ai/seokjin/FSR-prediction/runs/72ac5990\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151013)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225017-72ac5990/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:50:25,538\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.545 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:25,545\tWARNING util.py:315 -- The `process_trial_result` operation took 2.552 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:25,547\tWARNING util.py:315 -- Processing trial results took 2.555 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:50:25,549\tWARNING util.py:315 -- The `process_trial_result` operation took 2.556 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_224939-405740e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150318)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_292a4961_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-50-10/wandb/run-20230810_225028-292a4961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Syncing run FSR_Trainable_292a4961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/292a4961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                mae_coord 14.91112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                mae_force 1406.49596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               mape_coord 2.53813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               mape_force 3.009546453911953e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                   metric 1.47468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               rmse_coord 5.21231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               rmse_force 932.57882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:       time_since_restore 0.70236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:         time_this_iter_s 0.70236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:             time_total_s 0.70236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:                timestamp 1691675422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               tmae_coord 3.39262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:               tmae_force 0.51128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              tmape_coord 67396695721700.375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              tmape_force 1131024480268789.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              trmse_coord 1.1834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:              trmse_force 0.29128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb:  View run FSR_Trainable_292a4961 at: https://wandb.ai/seokjin/FSR-prediction/runs/292a4961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225028-292a4961/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151247)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-10 22:50:35,464\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.509 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:35,468\tWARNING util.py:315 -- The `process_trial_result` operation took 2.513 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:35,472\tWARNING util.py:315 -- Processing trial results took 2.518 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:50:35,474\tWARNING util.py:315 -- The `process_trial_result` operation took 2.519 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_2ae0cce2_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-50-22/wandb/run-20230810_225038-2ae0cce2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Syncing run FSR_Trainable_2ae0cce2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2ae0cce2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:50:43,137\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.600 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:43,142\tWARNING util.py:315 -- The `process_trial_result` operation took 2.606 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:43,144\tWARNING util.py:315 -- Processing trial results took 2.607 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:50:43,145\tWARNING util.py:315 -- The `process_trial_result` operation took 2.609 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                mae_coord 6.64063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                mae_force 881.87407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               mape_coord 1.38804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               mape_force 1.6490128618651863e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                   metric 0.7894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               rmse_coord 2.93014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               rmse_force 537.59463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:       time_since_restore 1.27937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:         time_this_iter_s 0.53884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:             time_total_s 1.27937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:                timestamp 1691675436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               tmae_coord 1.35995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:               tmae_force 0.37605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              tmape_coord 74819484514036.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              tmape_force 774329039981261.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              trmse_coord 0.57232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:              trmse_force 0.21708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb:  View run FSR_Trainable_2ae0cce2 at: https://wandb.ai/seokjin/FSR-prediction/runs/2ae0cce2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151474)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225038-2ae0cce2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_352e843d_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-50-32/wandb/run-20230810_225045-352e843d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Syncing run FSR_Trainable_352e843d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/352e843d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                mae_coord 6.6401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                mae_force 897.05393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               mape_coord 1.29754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               mape_force 1.549318173234666e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                   metric 0.80379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               rmse_coord 2.90689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               rmse_force 564.93526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:       time_since_restore 1.41834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:         time_this_iter_s 0.46719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:             time_total_s 1.41834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:                timestamp 1691675443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               tmae_coord 1.39896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:               tmae_force 0.39401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              tmape_coord 77772917166115.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              tmape_force 789838929600062.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              trmse_coord 0.57765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:              trmse_force 0.22614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb:  View run FSR_Trainable_352e843d at: https://wandb.ai/seokjin/FSR-prediction/runs/352e843d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151656)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225045-352e843d/logs\n",
      "2023-08-10 22:50:52,614\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.574 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:52,619\tWARNING util.py:315 -- The `process_trial_result` operation took 2.580 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:50:52,621\tWARNING util.py:315 -- Processing trial results took 2.582 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:50:52,623\tWARNING util.py:315 -- The `process_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_cb45896d_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-50-39/wandb/run-20230810_225055-cb45896d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Syncing run FSR_Trainable_cb45896d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cb45896d\n",
      "2023-08-10 22:51:00,442\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.233 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:00,448\tWARNING util.py:315 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:00,450\tWARNING util.py:315 -- Processing trial results took 2.241 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:51:00,451\tWARNING util.py:315 -- The `process_trial_result` operation took 2.243 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_6f44ace7_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-50-49/wandb/run-20230810_225106-6f44ace7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Syncing run FSR_Trainable_6f44ace7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6f44ace7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:51:11,853\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.742 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:11,856\tWARNING util.py:315 -- The `process_trial_result` operation took 2.746 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:11,860\tWARNING util.py:315 -- Processing trial results took 2.749 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:51:11,862\tWARNING util.py:315 -- The `process_trial_result` operation took 2.751 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                mae_coord 5.07079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                mae_force 672.20783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               mape_coord 1.22629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               mape_force 1.0578660573624498e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                   metric 0.68017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               rmse_coord 2.49162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               rmse_force 433.00358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:       time_since_restore 3.26757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:         time_this_iter_s 0.37256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:             time_total_s 3.26757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:                timestamp 1691675463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               tmae_coord 1.0738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:               tmae_force 0.30767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              tmape_coord 78657070890092.34\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              tmape_force 614238292916405.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              trmse_coord 0.49708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:              trmse_force 0.18309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb:  View run FSR_Trainable_6f44ace7 at: https://wandb.ai/seokjin/FSR-prediction/runs/6f44ace7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152061)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225106-6f44ace7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_b898cfb7_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-50-57/wandb/run-20230810_225115-b898cfb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Syncing run FSR_Trainable_b898cfb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b898cfb7\n",
      "2023-08-10 22:51:25,215\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.575 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:25,220\tWARNING util.py:315 -- The `process_trial_result` operation took 2.580 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:25,224\tWARNING util.py:315 -- Processing trial results took 2.585 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:51:25,226\tWARNING util.py:315 -- The `process_trial_result` operation took 2.587 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_7a48eded_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-51-08/wandb/run-20230810_225128-7a48eded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Syncing run FSR_Trainable_7a48eded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7a48eded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                mae_coord 5.1202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                mae_force 1000.28722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               mape_coord 1.32434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               mape_force 1.8399034052274903e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                   metric 0.74419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               rmse_coord 2.49798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               rmse_force 711.84997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:       time_since_restore 1.31403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:         time_this_iter_s 0.63623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:             time_total_s 1.31403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:                timestamp 1691675485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               tmae_coord 1.12801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:               tmae_force 0.38907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              tmape_coord 90825249879606.39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              tmape_force 808107523661858.6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              trmse_coord 0.51187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:              trmse_force 0.23231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb:  View run FSR_Trainable_7a48eded at: https://wandb.ai/seokjin/FSR-prediction/runs/7a48eded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152509)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225128-7a48eded/logs\n",
      "2023-08-10 22:51:39,232\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.596 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:39,237\tWARNING util.py:315 -- The `process_trial_result` operation took 2.602 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:39,239\tWARNING util.py:315 -- Processing trial results took 2.604 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:51:39,241\tWARNING util.py:315 -- The `process_trial_result` operation took 2.606 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_9da4ea97_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-51-22/wandb/run-20230810_225143-9da4ea97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Syncing run FSR_Trainable_9da4ea97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9da4ea97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                mae_coord 4.41471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                mae_force 630.64591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               mape_coord 1.11806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               mape_force 8.110889358401578e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                   metric 0.64933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               rmse_coord 2.41886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               rmse_force 440.70071\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:       time_since_restore 36.91932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:         time_this_iter_s 0.34499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:             time_total_s 36.91932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:                timestamp 1691675503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               tmae_coord 0.94747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:               tmae_force 0.24725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              tmape_coord 79638980947242.38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              tmape_force 349845106492912.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              trmse_coord 0.48483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:              trmse_force 0.1645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb:  View run FSR_Trainable_cb45896d at: https://wandb.ai/seokjin/FSR-prediction/runs/cb45896d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151881)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225055-cb45896d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225143-9da4ea97/logs\n",
      "2023-08-10 22:51:52,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.787 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:52,239\tWARNING util.py:315 -- The `process_trial_result` operation took 2.792 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:51:52,241\tWARNING util.py:315 -- Processing trial results took 2.794 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:51:52,242\tWARNING util.py:315 -- The `process_trial_result` operation took 2.796 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_ee2a2b18_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-51-35/wandb/run-20230810_225155-ee2a2b18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Syncing run FSR_Trainable_ee2a2b18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ee2a2b18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:52:05,365\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.468 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:05,368\tWARNING util.py:315 -- The `process_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:05,370\tWARNING util.py:315 -- Processing trial results took 2.474 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:52:05,371\tWARNING util.py:315 -- The `process_trial_result` operation took 2.475 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                mae_coord 4.33443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                mae_force 620.67241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               mape_coord 1.09445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               mape_force 7.87412640930419e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                   metric 0.64136\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               rmse_coord 2.38934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               rmse_force 437.16629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:       time_since_restore 37.25045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:         time_this_iter_s 0.33829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:             time_total_s 37.25045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:                timestamp 1691675521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               tmae_coord 0.93283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:               tmae_force 0.23978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              tmape_coord 79166555058953.97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              tmape_force 330726919205780.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              trmse_coord 0.48042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:              trmse_force 0.16093\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb:  View run FSR_Trainable_b898cfb7 at: https://wandb.ai/seokjin/FSR-prediction/runs/b898cfb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152280)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225115-b898cfb7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_970091e8_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-51-48/wandb/run-20230810_225208-970091e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Syncing run FSR_Trainable_970091e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/970091e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                mae_coord 4.25008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                mae_force 1338.76322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               mape_coord 1.32694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               mape_force 702675450.97737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                   metric 6.5132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               rmse_coord 2.42622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               rmse_force 938.195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:       time_since_restore 0.97736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:         time_this_iter_s 0.97736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:             time_total_s 0.97736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:                timestamp 1691675522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               tmae_coord 7.99132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:               tmae_force 3.09082\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              tmape_coord 1685089813134807.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              tmape_force 59.82069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              trmse_coord 4.5299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:              trmse_force 1.9833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb:  View run FSR_Trainable_970091e8 at: https://wandb.ai/seokjin/FSR-prediction/runs/970091e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153205)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225208-970091e8/logs\n",
      "2023-08-10 22:52:17,312\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:17,316\tWARNING util.py:315 -- The `process_trial_result` operation took 2.501 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:17,318\tWARNING util.py:315 -- Processing trial results took 2.503 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:52:17,320\tWARNING util.py:315 -- The `process_trial_result` operation took 2.505 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_63e20809_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-52-02/wandb/run-20230810_225220-63e20809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Syncing run FSR_Trainable_63e20809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/63e20809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                mae_coord 4.51279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                mae_force 1158.26439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               mape_coord 1.28684\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               mape_force 7.529967106321478e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                   metric 39.33757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               rmse_coord 2.54964\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               rmse_force 1016.28861\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:       time_since_restore 0.48695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:         time_this_iter_s 0.48695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:             time_total_s 0.48695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:                timestamp 1691675534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               tmae_coord 29.30287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:               tmae_force 5.37073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              tmape_coord 4378632358796586.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              tmape_force 3046944542517447.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              trmse_coord 33.65592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:              trmse_force 5.68165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb:  View run FSR_Trainable_63e20809 at: https://wandb.ai/seokjin/FSR-prediction/runs/63e20809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153443)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225220-63e20809/logs\n",
      "2023-08-10 22:52:28,313\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:28,318\tWARNING util.py:315 -- The `process_trial_result` operation took 2.590 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:28,320\tWARNING util.py:315 -- Processing trial results took 2.592 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:52:28,322\tWARNING util.py:315 -- The `process_trial_result` operation took 2.594 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_d194dbfc_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-52-14/wandb/run-20230810_225231-d194dbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Syncing run FSR_Trainable_d194dbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d194dbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                mae_coord 5.38326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                mae_force 1515.69475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               mape_coord 1.37738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               mape_force 1.1573334117162763e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                   metric 55.18201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               rmse_coord 2.73269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               rmse_force 1554.13047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:       time_since_restore 0.4751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:         time_this_iter_s 0.4751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:             time_total_s 0.4751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:                timestamp 1691675545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               tmae_coord 46.04368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:               tmae_force 6.77678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              tmape_coord 8599093702139593.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              tmape_force 4960144020887699.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              trmse_coord 48.10943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:              trmse_force 7.07258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb:  View run FSR_Trainable_d194dbfc at: https://wandb.ai/seokjin/FSR-prediction/runs/d194dbfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153664)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225231-d194dbfc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb:              trmse_force \n",
      "2023-08-10 22:52:39,697\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.330 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:39,699\tWARNING util.py:315 -- The `process_trial_result` operation took 2.334 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:39,702\tWARNING util.py:315 -- Processing trial results took 2.336 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:52:39,704\tWARNING util.py:315 -- The `process_trial_result` operation took 2.339 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152984)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_ef2e7036_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-52-25/wandb/run-20230810_225245-ef2e7036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Syncing run FSR_Trainable_ef2e7036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ef2e7036\n",
      "2023-08-10 22:52:49,859\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.686 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:49,862\tWARNING util.py:315 -- The `process_trial_result` operation took 2.690 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:52:49,866\tWARNING util.py:315 -- Processing trial results took 2.694 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:52:49,867\tWARNING util.py:315 -- The `process_trial_result` operation took 2.695 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_7aceb6ac_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-52-36/wandb/run-20230810_225253-7aceb6ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: Syncing run FSR_Trainable_7aceb6ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7aceb6ac\n",
      "2023-08-10 22:53:01,386\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.121 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:01,388\tWARNING util.py:315 -- The `process_trial_result` operation took 2.125 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:01,389\tWARNING util.py:315 -- Processing trial results took 2.126 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:53:01,391\tWARNING util.py:315 -- The `process_trial_result` operation took 2.127 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_7745b095_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-52-46/wandb/run-20230810_225305-7745b095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Syncing run FSR_Trainable_7745b095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7745b095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: - 0.002 MB of 0.005 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                mae_coord 5.32362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                mae_force 965.9729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               mape_coord 1.28006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               mape_force 1.5724481926288704e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                   metric 0.748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               rmse_coord 2.50137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               rmse_force 711.2025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:       time_since_restore 1.2232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:         time_this_iter_s 0.53693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:             time_total_s 1.2232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:                timestamp 1691675581\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               tmae_coord 1.17064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:               tmae_force 0.35528\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              tmape_coord 87690430812436.97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              tmape_force 617303222649385.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              trmse_coord 0.51694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:              trmse_force 0.23106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb:  View run FSR_Trainable_7745b095 at: https://wandb.ai/seokjin/FSR-prediction/runs/7745b095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154341)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225305-7745b095/logs\n",
      "2023-08-10 22:53:14,308\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.580 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:14,310\tWARNING util.py:315 -- The `process_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:14,311\tWARNING util.py:315 -- Processing trial results took 2.585 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:53:14,312\tWARNING util.py:315 -- The `process_trial_result` operation took 2.586 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_2b727f4f_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-52-58/wandb/run-20230810_225321-2b727f4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Syncing run FSR_Trainable_2b727f4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2b727f4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                mae_coord 5.56732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                mae_force 945.73203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               mape_coord 1.35103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               mape_force 1.7157524668762002e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                   metric 0.73825\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               rmse_coord 2.62621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               rmse_force 639.18057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:       time_since_restore 1.20457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:         time_this_iter_s 0.64827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:             time_total_s 1.20457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:                timestamp 1691675594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               tmae_coord 1.2004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:               tmae_force 0.36123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              tmape_coord 83378787324198.23\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              tmape_force 708431533640673.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              trmse_coord 0.52631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:              trmse_force 0.21194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb:  View run FSR_Trainable_2b727f4f at: https://wandb.ai/seokjin/FSR-prediction/runs/2b727f4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154574)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225321-2b727f4f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:53:28,595\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.702 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:28,603\tWARNING util.py:315 -- The `process_trial_result` operation took 2.710 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:28,604\tWARNING util.py:315 -- Processing trial results took 2.712 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:53:28,607\tWARNING util.py:315 -- The `process_trial_result` operation took 2.715 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153897)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_ffa11e7b_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-53-11/wandb/run-20230810_225332-ffa11e7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Syncing run FSR_Trainable_ffa11e7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ffa11e7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-08-10 22:53:41,434\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.875 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:41,437\tWARNING util.py:315 -- The `process_trial_result` operation took 2.880 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:41,439\tWARNING util.py:315 -- Processing trial results took 2.882 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:53:41,442\tWARNING util.py:315 -- The `process_trial_result` operation took 2.885 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                mae_coord 19.11769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                mae_force 1487.49704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               mape_coord 3.15246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               mape_force 3.358483081247376e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                   metric 1.81723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               rmse_coord 6.48324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               rmse_force 895.2152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:       time_since_restore 0.57513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:         time_this_iter_s 0.57513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:             time_total_s 0.57513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:                timestamp 1691675605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               tmae_coord 4.44931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:               tmae_force 0.60971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              tmape_coord 51162621729638.57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              tmape_force 1496623276372953.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              trmse_coord 1.49749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:              trmse_force 0.31975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb:  View run FSR_Trainable_ffa11e7b at: https://wandb.ai/seokjin/FSR-prediction/runs/ffa11e7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154800)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225332-ffa11e7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_17ad2c46_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-10_22-53-25/wandb/run-20230810_225344-17ad2c46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Syncing run FSR_Trainable_17ad2c46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/17ad2c46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154123)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                mae_coord 22.97918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                mae_force 2687.16958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               mape_coord 3.99761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               mape_force 3.5071399821193984e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                   metric 2.4335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               rmse_coord 8.11512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               rmse_force 1751.45373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:       time_since_restore 0.4531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:         time_this_iter_s 0.4531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:             time_total_s 0.4531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:                timestamp 1691675618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               tmae_coord 5.25306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:               tmae_force 0.97343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              tmape_coord 46988882441305.875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              tmape_force 1544230470350960.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              trmse_coord 1.86678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:              trmse_force 0.56672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb:  View run FSR_Trainable_17ad2c46 at: https://wandb.ai/seokjin/FSR-prediction/runs/17ad2c46\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225344-17ad2c46/logs\n",
      "2023-08-10 22:53:50,911\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:50,916\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-08-10 22:53:50,918\tWARNING util.py:315 -- Processing trial results took 2.069 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-10 22:53:50,919\tWARNING util.py:315 -- The `process_trial_result` operation took 2.071 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155039)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-10_22-35-32/FSR_Trainable_64ab2554_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-08-10_22-53-38/wandb/run-20230810_225353-64ab2554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Syncing run FSR_Trainable_64ab2554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/64ab2554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                mae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                mae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               mape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               mape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                   metric \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               rmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               rmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               tmae_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               tmae_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              tmape_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              tmape_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              trmse_coord \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              trmse_force \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                mae_coord 4.33015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                mae_force 771.19451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               mape_coord 1.1446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               mape_force 1.2355604608221425e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                   metric 0.66055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               rmse_coord 2.36006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               rmse_force 557.09914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:       time_since_restore 4.88313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:         time_this_iter_s 0.2064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:             time_total_s 4.88313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:                timestamp 1691675635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               tmae_coord 0.93411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:               tmae_force 0.2896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              tmape_coord 79593033095712.67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              tmape_force 494046076326562.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              trmse_coord 0.47409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:              trmse_force 0.18645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb:  View run FSR_Trainable_64ab2554 at: https://wandb.ai/seokjin/FSR-prediction/runs/64ab2554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: Find logs at: ./wandb/run-20230810_225353-64ab2554/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155273)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-10 22:53:59,087\tINFO tune.py:1111 -- Total run time: 1101.98 seconds (1098.35 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
