{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_06-27-50/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_06-27-50\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "105.363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 06:27:50,361] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 06:27:53,537\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2023-07-19 06:27:55,238\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 06:41:53</td></tr>\n",
       "<tr><td>Running for: </td><td>00:13:58.46        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.2/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -110.56244565005441 | Iter 32.000: -106.164258512608 | Iter 16.000: -107.680138226812 | Iter 8.000: -108.37182915059289 | Iter 4.000: -107.02965304614874 | Iter 2.000: -116.16095285831297 | Iter 1.000: -170.13898262752855<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model        </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_3657c235</td><td>TERMINATED</td><td>172.26.215.93:449879</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_f040</td><td>[&#x27;force&#x27;, &#x27;x_co_f0c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00304487 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       14.49    </td><td style=\"text-align: right;\">179.692</td><td style=\"text-align: right;\"> 65.9713</td><td style=\"text-align: right;\">1.19726e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d1db0969</td><td>TERMINATED</td><td>172.26.215.93:449952</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_8240</td><td>[&#x27;force&#x27;, &#x27;x_co_7580</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00483327 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       29.9616  </td><td style=\"text-align: right;\">119.489</td><td style=\"text-align: right;\"> 40.7078</td><td style=\"text-align: right;\">6.30634e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_81e3fc24</td><td>TERMINATED</td><td>172.26.215.93:450130</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_7200</td><td>[&#x27;force&#x27;, &#x27;x_co_55c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00875011 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.881525</td><td style=\"text-align: right;\">194.399</td><td style=\"text-align: right;\"> 69.9469</td><td style=\"text-align: right;\">1.26829e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_02b0913f</td><td>TERMINATED</td><td>172.26.215.93:450311</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7780</td><td>[&#x27;force&#x27;, &#x27;x_co_e380</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0364778  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        7.88485 </td><td style=\"text-align: right;\">153.006</td><td style=\"text-align: right;\"> 51.0264</td><td style=\"text-align: right;\">3.64131e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_26ca7713</td><td>TERMINATED</td><td>172.26.215.93:450643</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_ec00</td><td>[&#x27;force&#x27;, &#x27;x_co_db00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000131124</td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.32192 </td><td style=\"text-align: right;\">244.557</td><td style=\"text-align: right;\"> 76.8407</td><td style=\"text-align: right;\">8.35638e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_e1831fa4</td><td>TERMINATED</td><td>172.26.215.93:450886</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_e800</td><td>[&#x27;force&#x27;, &#x27;x_co_0640</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.56336e-05</td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.393864</td><td style=\"text-align: right;\">256.772</td><td style=\"text-align: right;\"> 69.4363</td><td style=\"text-align: right;\">3.3769e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_a8d2b7da</td><td>TERMINATED</td><td>172.26.215.93:451122</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_f300</td><td>[&#x27;force&#x27;, &#x27;x_co_b280</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.39284e-05</td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.728214</td><td style=\"text-align: right;\">221.404</td><td style=\"text-align: right;\"> 63.8986</td><td style=\"text-align: right;\">1.50851e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_ba3a6850</td><td>TERMINATED</td><td>172.26.215.93:451364</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_47c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0218836  </td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.655068</td><td style=\"text-align: right;\">253.055</td><td style=\"text-align: right;\"> 68.5802</td><td style=\"text-align: right;\">6.06597e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_8271662f</td><td>TERMINATED</td><td>172.26.215.93:451452</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_c280</td><td>[&#x27;force&#x27;, &#x27;x_co_4c80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0153624  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.43384 </td><td style=\"text-align: right;\">183.945</td><td style=\"text-align: right;\"> 65.4606</td><td style=\"text-align: right;\">4.31077e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3dfe355c</td><td>TERMINATED</td><td>172.26.215.93:451622</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_4a80</td><td>[&#x27;force&#x27;, &#x27;x_co_4f00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.16576e-05</td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.651438</td><td style=\"text-align: right;\">230.445</td><td style=\"text-align: right;\"> 64.7147</td><td style=\"text-align: right;\">3.3357e+15 </td></tr>\n",
       "<tr><td>FSR_Trainable_9b6f7a70</td><td>TERMINATED</td><td>172.26.215.93:451825</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_2b40</td><td>[&#x27;force&#x27;, &#x27;x_co_1140</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00429067 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       16.5356  </td><td style=\"text-align: right;\">117.211</td><td style=\"text-align: right;\"> 40.9935</td><td style=\"text-align: right;\">6.55006e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cd9cf579</td><td>TERMINATED</td><td>172.26.215.93:452145</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_10c0</td><td>[&#x27;force&#x27;, &#x27;x_co_a0c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000853106</td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.332765</td><td style=\"text-align: right;\">283.119</td><td style=\"text-align: right;\"> 93.5895</td><td style=\"text-align: right;\">1.00657e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_a2fad63a</td><td>TERMINATED</td><td>172.26.215.93:452372</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_ef00</td><td>[&#x27;force&#x27;, &#x27;x_co_ecc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.099046   </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.663334</td><td style=\"text-align: right;\">203.782</td><td style=\"text-align: right;\"> 74.1572</td><td style=\"text-align: right;\">1.39407e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_85951629</td><td>TERMINATED</td><td>172.26.215.93:452607</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_73c0</td><td>[&#x27;force&#x27;, &#x27;x_co_5d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0876477  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.30478 </td><td style=\"text-align: right;\">146.83 </td><td style=\"text-align: right;\"> 47.9605</td><td style=\"text-align: right;\">3.64354e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7ca7828d</td><td>TERMINATED</td><td>172.26.215.93:452851</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_c740</td><td>[&#x27;force&#x27;, &#x27;x_co_eb00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00185453 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.896481</td><td style=\"text-align: right;\">181.649</td><td style=\"text-align: right;\"> 62.9871</td><td style=\"text-align: right;\">1.06663e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d45936b5</td><td>TERMINATED</td><td>172.26.215.93:452938</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_fb00</td><td>[&#x27;force&#x27;, &#x27;x_co_4500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00351587 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       31.458   </td><td style=\"text-align: right;\">106.558</td><td style=\"text-align: right;\"> 38.3648</td><td style=\"text-align: right;\">6.22208e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_8cf05336</td><td>TERMINATED</td><td>172.26.215.93:453126</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7580</td><td>[&#x27;force&#x27;, &#x27;x_co_f5c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00516628 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        2.98969 </td><td style=\"text-align: right;\">112.273</td><td style=\"text-align: right;\"> 43.735 </td><td style=\"text-align: right;\">7.93991e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_460ff324</td><td>TERMINATED</td><td>172.26.215.93:453312</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_5300</td><td>[&#x27;force&#x27;, &#x27;x_co_4ac0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00481961 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        5.90353 </td><td style=\"text-align: right;\">114.096</td><td style=\"text-align: right;\"> 43.9099</td><td style=\"text-align: right;\">7.62252e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_6fa528d8</td><td>TERMINATED</td><td>172.26.215.93:453631</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_e600</td><td>[&#x27;force&#x27;, &#x27;x_co_ad40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000650563</td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.484017</td><td style=\"text-align: right;\">295.066</td><td style=\"text-align: right;\"> 88.9189</td><td style=\"text-align: right;\">9.63354e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_323e9721</td><td>TERMINATED</td><td>172.26.215.93:453880</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7dc0</td><td>[&#x27;force&#x27;, &#x27;x_co_7f00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000775791</td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.493366</td><td style=\"text-align: right;\">245.265</td><td style=\"text-align: right;\"> 93.9522</td><td style=\"text-align: right;\">1.6561e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_4dc78491</td><td>TERMINATED</td><td>172.26.215.93:454109</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_c400</td><td>[&#x27;force&#x27;, &#x27;x_co_c100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00973529 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.91073 </td><td style=\"text-align: right;\">120.432</td><td style=\"text-align: right;\"> 43.5714</td><td style=\"text-align: right;\">7.61856e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_d192c08f</td><td>TERMINATED</td><td>172.26.215.93:454340</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_db40</td><td>[&#x27;force&#x27;, &#x27;x_co_7980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000303131</td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.06704 </td><td style=\"text-align: right;\">157.744</td><td style=\"text-align: right;\"> 48.6325</td><td style=\"text-align: right;\">1.46412e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2eb64ca4</td><td>TERMINATED</td><td>172.26.215.93:454449</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_c200</td><td>[&#x27;force&#x27;, &#x27;x_co_4140</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00250415 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.74184 </td><td style=\"text-align: right;\">105.363</td><td style=\"text-align: right;\"> 41.076 </td><td style=\"text-align: right;\">7.2155e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_f26eec80</td><td>TERMINATED</td><td>172.26.215.93:454766</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_71c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6680</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00204438 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.75674 </td><td style=\"text-align: right;\">109.147</td><td style=\"text-align: right;\"> 42.6605</td><td style=\"text-align: right;\">7.85742e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ebf2d36a</td><td>TERMINATED</td><td>172.26.215.93:454862</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_a6c0</td><td>[&#x27;force&#x27;, &#x27;x_co_e740</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0028862  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.80399 </td><td style=\"text-align: right;\">105.791</td><td style=\"text-align: right;\"> 41.5997</td><td style=\"text-align: right;\">7.38711e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4c312fa1</td><td>TERMINATED</td><td>172.26.215.93:455180</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_4c80</td><td>[&#x27;force&#x27;, &#x27;x_co_8840</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00143896 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.26627 </td><td style=\"text-align: right;\">131.192</td><td style=\"text-align: right;\"> 48.6538</td><td style=\"text-align: right;\">8.36188e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_5379e322</td><td>TERMINATED</td><td>172.26.215.93:455276</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_6180</td><td>[&#x27;force&#x27;, &#x27;x_co_7540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00200303 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.18638 </td><td style=\"text-align: right;\">134.822</td><td style=\"text-align: right;\"> 53.5954</td><td style=\"text-align: right;\">1.02873e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_09d33090</td><td>TERMINATED</td><td>172.26.215.93:455591</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_6580</td><td>[&#x27;force&#x27;, &#x27;x_co_1f40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00210075 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.19361 </td><td style=\"text-align: right;\">145.042</td><td style=\"text-align: right;\"> 54.5245</td><td style=\"text-align: right;\">9.11645e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_e2113b0a</td><td>TERMINATED</td><td>172.26.215.93:455690</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_c9c0</td><td>[&#x27;force&#x27;, &#x27;x_co_d100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000465387</td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.811226</td><td style=\"text-align: right;\">310.174</td><td style=\"text-align: right;\">117.804 </td><td style=\"text-align: right;\">1.77532e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_65ca294c</td><td>TERMINATED</td><td>172.26.215.93:456008</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_61c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000479   </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.652703</td><td style=\"text-align: right;\">250.684</td><td style=\"text-align: right;\"> 85.9479</td><td style=\"text-align: right;\">4.98122e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5bccc8eb</td><td>TERMINATED</td><td>172.26.215.93:456103</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_cb80</td><td>[&#x27;force&#x27;, &#x27;x_co_ce80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00313013 </td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.26221 </td><td style=\"text-align: right;\">136.787</td><td style=\"text-align: right;\"> 44.2198</td><td style=\"text-align: right;\">1.68113e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4e4c2fa2</td><td>TERMINATED</td><td>172.26.215.93:456418</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_e6c0</td><td>[&#x27;force&#x27;, &#x27;x_co_f4c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00353962 </td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.59686 </td><td style=\"text-align: right;\">116.717</td><td style=\"text-align: right;\"> 39.2163</td><td style=\"text-align: right;\">1.58593e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_7e7f666f</td><td>TERMINATED</td><td>172.26.215.93:456515</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7c00</td><td>[&#x27;force&#x27;, &#x27;x_co_6a00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00151407 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.2243  </td><td style=\"text-align: right;\">143.362</td><td style=\"text-align: right;\"> 49.9337</td><td style=\"text-align: right;\">8.50799e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fd3f4683</td><td>TERMINATED</td><td>172.26.215.93:456834</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_8300</td><td>[&#x27;force&#x27;, &#x27;x_co_6700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00137996 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.15099 </td><td style=\"text-align: right;\">146.403</td><td style=\"text-align: right;\"> 49.9291</td><td style=\"text-align: right;\">7.96993e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_db705091</td><td>TERMINATED</td><td>172.26.215.93:456929</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_a300</td><td>[&#x27;force&#x27;, &#x27;x_co_81c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00683217 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.1103  </td><td style=\"text-align: right;\">115.658</td><td style=\"text-align: right;\"> 41.2944</td><td style=\"text-align: right;\">6.90405e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c6df9388</td><td>TERMINATED</td><td>172.26.215.93:457248</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_ae40</td><td>[&#x27;force&#x27;, &#x27;x_co_ae80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00633749 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.97575 </td><td style=\"text-align: right;\">110.647</td><td style=\"text-align: right;\"> 41.125 </td><td style=\"text-align: right;\">7.10222e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_e5a6e2a8</td><td>TERMINATED</td><td>172.26.215.93:457346</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_bb00</td><td>[&#x27;force&#x27;, &#x27;x_co_82c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00279317 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.578103</td><td style=\"text-align: right;\">214.393</td><td style=\"text-align: right;\"> 69.4252</td><td style=\"text-align: right;\">1.43072e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2864a2be</td><td>TERMINATED</td><td>172.26.215.93:457663</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_6140</td><td>[&#x27;force&#x27;, &#x27;x_co_0ac0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00281155 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.678979</td><td style=\"text-align: right;\">330.863</td><td style=\"text-align: right;\">110.572 </td><td style=\"text-align: right;\">1.46128e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_eb2d32e6</td><td>TERMINATED</td><td>172.26.215.93:457760</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_28c0</td><td>[&#x27;force&#x27;, &#x27;x_co_2c40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0105935  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.902375</td><td style=\"text-align: right;\">190.203</td><td style=\"text-align: right;\"> 67.856 </td><td style=\"text-align: right;\">1.12195e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2449d287</td><td>TERMINATED</td><td>172.26.215.93:458085</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_aec0</td><td>[&#x27;force&#x27;, &#x27;x_co_af00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000947436</td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.774552</td><td style=\"text-align: right;\">290.644</td><td style=\"text-align: right;\"> 96.447 </td><td style=\"text-align: right;\">2.00459e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_b8ae8cd2</td><td>TERMINATED</td><td>172.26.215.93:458174</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_30c0</td><td>[&#x27;force&#x27;, &#x27;x_co_0940</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00104693 </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.04593 </td><td style=\"text-align: right;\">225.068</td><td style=\"text-align: right;\"> 78.9226</td><td style=\"text-align: right;\">3.58286e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ce487b52</td><td>TERMINATED</td><td>172.26.215.93:458494</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_0180</td><td>[&#x27;force&#x27;, &#x27;x_co_1f00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0141166  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.26541 </td><td style=\"text-align: right;\">213.663</td><td style=\"text-align: right;\"> 68.9368</td><td style=\"text-align: right;\">5.2771e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_d66a9693</td><td>TERMINATED</td><td>172.26.215.93:458585</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_2e00</td><td>[&#x27;force&#x27;, &#x27;x_co_3c80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00721968 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.88156 </td><td style=\"text-align: right;\">116.078</td><td style=\"text-align: right;\"> 41.269 </td><td style=\"text-align: right;\">6.97748e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_842abfbc</td><td>TERMINATED</td><td>172.26.215.93:458902</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_3640</td><td>[&#x27;force&#x27;, &#x27;x_co_1dc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00702617 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.29409 </td><td style=\"text-align: right;\">120.865</td><td style=\"text-align: right;\"> 43.2839</td><td style=\"text-align: right;\">7.14687e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_ba38fa3e</td><td>TERMINATED</td><td>172.26.215.93:459001</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_6200</td><td>[&#x27;force&#x27;, &#x27;x_co_6cc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00591292 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        2.94611 </td><td style=\"text-align: right;\">118.81 </td><td style=\"text-align: right;\"> 46.1974</td><td style=\"text-align: right;\">8.40094e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_71119e56</td><td>TERMINATED</td><td>172.26.215.93:459318</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_87c0</td><td>[&#x27;force&#x27;, &#x27;x_co_9340</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00427833 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        7.12217 </td><td style=\"text-align: right;\">119.764</td><td style=\"text-align: right;\"> 46.7858</td><td style=\"text-align: right;\">8.46832e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_04c07675</td><td>TERMINATED</td><td>172.26.215.93:459419</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_2f40</td><td>[&#x27;force&#x27;, &#x27;x_co_3fc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00403908 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        7.40221 </td><td style=\"text-align: right;\">117.419</td><td style=\"text-align: right;\"> 45.5473</td><td style=\"text-align: right;\">8.29521e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_12e0355e</td><td>TERMINATED</td><td>172.26.215.93:459601</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_5c80</td><td>[&#x27;force&#x27;, &#x27;x_co_4a40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00330315 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.89832 </td><td style=\"text-align: right;\">113.546</td><td style=\"text-align: right;\"> 45.3714</td><td style=\"text-align: right;\">8.44398e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_b7b45e41</td><td>TERMINATED</td><td>172.26.215.93:459926</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_e3c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b1c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0025352  </td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.77182 </td><td style=\"text-align: right;\">176.675</td><td style=\"text-align: right;\"> 56.0561</td><td style=\"text-align: right;\">1.70362e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_efdc8a31</td><td>TERMINATED</td><td>172.26.215.93:460162</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_a580</td><td>[&#x27;force&#x27;, &#x27;x_co_93c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0192667  </td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.559785</td><td style=\"text-align: right;\">216.857</td><td style=\"text-align: right;\"> 58.3147</td><td style=\"text-align: right;\">1.69722e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_5a8948ac</td><td>TERMINATED</td><td>172.26.215.93:460253</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_3500</td><td>[&#x27;force&#x27;, &#x27;x_co_2040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0247909  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.6488  </td><td style=\"text-align: right;\">124.505</td><td style=\"text-align: right;\"> 46.3112</td><td style=\"text-align: right;\">8.20331e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_b0892721</td><td>TERMINATED</td><td>172.26.215.93:460439</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_bc40</td><td>[&#x27;force&#x27;, &#x27;x_co_9080</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0121902  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.336   </td><td style=\"text-align: right;\">256.469</td><td style=\"text-align: right;\"> 97.9066</td><td style=\"text-align: right;\">2.09864e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_cc0a970b</td><td>TERMINATED</td><td>172.26.215.93:460626</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_1a40</td><td>[&#x27;force&#x27;, &#x27;x_co_3c80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00508771 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.09951 </td><td style=\"text-align: right;\">110.079</td><td style=\"text-align: right;\"> 43.3805</td><td style=\"text-align: right;\">7.99018e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_7ef58eae</td><td>TERMINATED</td><td>172.26.215.93:460946</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_5ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_5140</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00620447 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        2.89146 </td><td style=\"text-align: right;\">113.874</td><td style=\"text-align: right;\"> 43.9788</td><td style=\"text-align: right;\">7.90131e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c58472f9</td><td>TERMINATED</td><td>172.26.215.93:461180</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7e00</td><td>[&#x27;force&#x27;, &#x27;x_co_6d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00742362 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.75944 </td><td style=\"text-align: right;\">108.856</td><td style=\"text-align: right;\"> 42.5482</td><td style=\"text-align: right;\">7.6634e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_c1862fb9</td><td>TERMINATED</td><td>172.26.215.93:461272</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_b640</td><td>[&#x27;force&#x27;, &#x27;x_co_9d40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00221375 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.06189 </td><td style=\"text-align: right;\">120.25 </td><td style=\"text-align: right;\"> 44.2522</td><td style=\"text-align: right;\">7.64594e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c92621f2</td><td>TERMINATED</td><td>172.26.215.93:461446</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_6140</td><td>[&#x27;force&#x27;, &#x27;x_co_4600</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0089144  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.696492</td><td style=\"text-align: right;\">320.023</td><td style=\"text-align: right;\">102.839 </td><td style=\"text-align: right;\">1.11436e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_aac9dd0d</td><td>TERMINATED</td><td>172.26.215.93:461647</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_ffc0</td><td>[&#x27;force&#x27;, &#x27;x_co_9b80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00903588 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.667914</td><td style=\"text-align: right;\">246.357</td><td style=\"text-align: right;\"> 92.1204</td><td style=\"text-align: right;\">1.88997e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d17c72c7</td><td>TERMINATED</td><td>172.26.215.93:461965</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_32c0</td><td>[&#x27;force&#x27;, &#x27;x_co_37c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00172049 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.646471</td><td style=\"text-align: right;\">212.179</td><td style=\"text-align: right;\"> 78.6717</td><td style=\"text-align: right;\">1.6352e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_4773fdc2</td><td>TERMINATED</td><td>172.26.215.93:462062</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_3200</td><td>[&#x27;force&#x27;, &#x27;x_co_3600</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00398146 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.910386</td><td style=\"text-align: right;\">223.422</td><td style=\"text-align: right;\"> 77.7447</td><td style=\"text-align: right;\">1.41254e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_76752b90</td><td>TERMINATED</td><td>172.26.215.93:462384</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_0a40</td><td>[&#x27;force&#x27;, &#x27;x_co_dd80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00475346 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.766449</td><td style=\"text-align: right;\">194.41 </td><td style=\"text-align: right;\"> 68.7589</td><td style=\"text-align: right;\">1.14571e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_5e128218</td><td>TERMINATED</td><td>172.26.215.93:462479</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_1d00</td><td>[&#x27;force&#x27;, &#x27;x_co_1540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00444359 </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.977633</td><td style=\"text-align: right;\">228.064</td><td style=\"text-align: right;\"> 71.13  </td><td style=\"text-align: right;\">6.00657e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c5b02e9a</td><td>TERMINATED</td><td>172.26.215.93:462796</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_0440</td><td>[&#x27;force&#x27;, &#x27;x_co_0a40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00555951 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.03669 </td><td style=\"text-align: right;\">224.602</td><td style=\"text-align: right;\"> 70.7466</td><td style=\"text-align: right;\">9.82617e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fda56acc</td><td>TERMINATED</td><td>172.26.215.93:462895</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_8cc0</td><td>[&#x27;force&#x27;, &#x27;x_co_0ac0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00232254 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.57482 </td><td style=\"text-align: right;\">111.489</td><td style=\"text-align: right;\"> 42.2299</td><td style=\"text-align: right;\">7.39759e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_205b45bb</td><td>TERMINATED</td><td>172.26.215.93:463212</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_ab00</td><td>[&#x27;force&#x27;, &#x27;x_co_2900</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00246904 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.37296 </td><td style=\"text-align: right;\">131.426</td><td style=\"text-align: right;\"> 49.9417</td><td style=\"text-align: right;\">9.53632e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_d8067052</td><td>TERMINATED</td><td>172.26.215.93:463314</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_9e40</td><td>[&#x27;force&#x27;, &#x27;x_co_2f00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00308487 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.50942 </td><td style=\"text-align: right;\">111.696</td><td style=\"text-align: right;\"> 42.6782</td><td style=\"text-align: right;\">7.62759e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_dd9338d0</td><td>TERMINATED</td><td>172.26.215.93:463495</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_1300</td><td>[&#x27;force&#x27;, &#x27;x_co_0bc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0011682  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.18508 </td><td style=\"text-align: right;\">149.484</td><td style=\"text-align: right;\"> 53.4434</td><td style=\"text-align: right;\">9.52217e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f337be85</td><td>TERMINATED</td><td>172.26.215.93:463687</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_c080</td><td>[&#x27;force&#x27;, &#x27;x_co_3fc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00117498 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.08162 </td><td style=\"text-align: right;\">188.042</td><td style=\"text-align: right;\"> 59.0832</td><td style=\"text-align: right;\">9.82473e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_0b93e2f8</td><td>TERMINATED</td><td>172.26.215.93:463874</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_d9c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3c40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00162407 </td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.08463 </td><td style=\"text-align: right;\">210.265</td><td style=\"text-align: right;\"> 60.3052</td><td style=\"text-align: right;\">6.65746e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_7034511a</td><td>TERMINATED</td><td>172.26.215.93:464199</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_9400</td><td>[&#x27;force&#x27;, &#x27;x_co_b240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0017711  </td><td>sklearn.preproc_08d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.773845</td><td style=\"text-align: right;\">217.35 </td><td style=\"text-align: right;\"> 64.226 </td><td style=\"text-align: right;\">1.09816e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_deb042fa</td><td>TERMINATED</td><td>172.26.215.93:464294</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_3fc0</td><td>[&#x27;force&#x27;, &#x27;x_co_0e00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00787178 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       18.0662  </td><td style=\"text-align: right;\">110.716</td><td style=\"text-align: right;\"> 40.197 </td><td style=\"text-align: right;\">6.63481e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4c87d99c</td><td>TERMINATED</td><td>172.26.215.93:464611</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_0100</td><td>[&#x27;force&#x27;, &#x27;x_co_bcc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00795114 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.43676 </td><td style=\"text-align: right;\">109.675</td><td style=\"text-align: right;\"> 43.0198</td><td style=\"text-align: right;\">7.87505e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fdf72de3</td><td>TERMINATED</td><td>172.26.215.93:464707</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_71c0</td><td>[&#x27;force&#x27;, &#x27;x_co_70c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0100083  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       10.1929  </td><td style=\"text-align: right;\">108.558</td><td style=\"text-align: right;\"> 40.9284</td><td style=\"text-align: right;\">6.80617e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_4900ef1d</td><td>TERMINATED</td><td>172.26.215.93:465032</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_4a40</td><td>[&#x27;force&#x27;, &#x27;x_co_7480</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0110586  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        8.02    </td><td style=\"text-align: right;\">115.393</td><td style=\"text-align: right;\"> 42.9687</td><td style=\"text-align: right;\">7.49102e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_a66ee56f</td><td>TERMINATED</td><td>172.26.215.93:465272</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_d200</td><td>[&#x27;force&#x27;, &#x27;x_co_f000</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0108779  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.41269 </td><td style=\"text-align: right;\">107.84 </td><td style=\"text-align: right;\"> 42.005 </td><td style=\"text-align: right;\">7.48647e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_475870e9</td><td>TERMINATED</td><td>172.26.215.93:465508</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7080</td><td>[&#x27;force&#x27;, &#x27;x_co_7800</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.011442   </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.976404</td><td style=\"text-align: right;\">118.649</td><td style=\"text-align: right;\"> 43.1949</td><td style=\"text-align: right;\">7.30461e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_61048410</td><td>TERMINATED</td><td>172.26.215.93:465749</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_3680</td><td>[&#x27;force&#x27;, &#x27;x_co_7500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0160044  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       23.0041  </td><td style=\"text-align: right;\">109.707</td><td style=\"text-align: right;\"> 39.1812</td><td style=\"text-align: right;\">6.01421e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f47854d8</td><td>TERMINATED</td><td>172.26.215.93:465840</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7040</td><td>[&#x27;force&#x27;, &#x27;x_co_55c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00340676 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.376785</td><td style=\"text-align: right;\">373.93 </td><td style=\"text-align: right;\">129.18  </td><td style=\"text-align: right;\">1.85561e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2b5adf1f</td><td>TERMINATED</td><td>172.26.215.93:466024</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_24c0</td><td>[&#x27;force&#x27;, &#x27;x_co_2d80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00869007 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.45932 </td><td style=\"text-align: right;\">214.508</td><td style=\"text-align: right;\"> 88.8551</td><td style=\"text-align: right;\">1.97701e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_22c2b8aa</td><td>TERMINATED</td><td>172.26.215.93:466344</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_a3c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b1c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0162203  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.29105 </td><td style=\"text-align: right;\">111.524</td><td style=\"text-align: right;\"> 39.8159</td><td style=\"text-align: right;\">2.50963e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c1c9a5cc</td><td>TERMINATED</td><td>172.26.215.93:466586</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_8100</td><td>[&#x27;force&#x27;, &#x27;x_co_99c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00755247 </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.30363 </td><td style=\"text-align: right;\">107.784</td><td style=\"text-align: right;\"> 36.7986</td><td style=\"text-align: right;\">1.98746e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_48bc3541</td><td>TERMINATED</td><td>172.26.215.93:466830</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_6a40</td><td>[&#x27;force&#x27;, &#x27;x_co_45c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0233397  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.59492 </td><td style=\"text-align: right;\">109.214</td><td style=\"text-align: right;\"> 40.4398</td><td style=\"text-align: right;\">7.26238e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_cdd4bc1d</td><td>TERMINATED</td><td>172.26.215.93:466890</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_4b00</td><td>[&#x27;force&#x27;, &#x27;x_co_ce80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00571537 </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.944959</td><td style=\"text-align: right;\">121.259</td><td style=\"text-align: right;\"> 41.2918</td><td style=\"text-align: right;\">2.14107e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f0493a55</td><td>TERMINATED</td><td>172.26.215.93:467081</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_d040</td><td>[&#x27;force&#x27;, &#x27;x_co_f000</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0312299  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.944005</td><td style=\"text-align: right;\">116.996</td><td style=\"text-align: right;\"> 41.7302</td><td style=\"text-align: right;\">2.97871e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_913dcdb5</td><td>TERMINATED</td><td>172.26.215.93:467436</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_68c0</td><td>[&#x27;force&#x27;, &#x27;x_co_01c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0359659  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.716424</td><td style=\"text-align: right;\">208.31 </td><td style=\"text-align: right;\"> 57.9984</td><td style=\"text-align: right;\">2.99037e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_4f5cc13f</td><td>TERMINATED</td><td>172.26.215.93:467667</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7300</td><td>[&#x27;force&#x27;, &#x27;x_co_fb40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0135405  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.898407</td><td style=\"text-align: right;\">151.718</td><td style=\"text-align: right;\"> 50.7885</td><td style=\"text-align: right;\">3.68321e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_355f65ec</td><td>TERMINATED</td><td>172.26.215.93:467758</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_c5c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c600</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0217286  </td><td>sklearn.preproc_06f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.743823</td><td style=\"text-align: right;\">178.934</td><td style=\"text-align: right;\"> 58.0646</td><td style=\"text-align: right;\">4.12335e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_74c841d3</td><td>TERMINATED</td><td>172.26.215.93:467945</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_ccc0</td><td>[&#x27;force&#x27;, &#x27;x_co_cdc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0187283  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.35578 </td><td style=\"text-align: right;\">117.736</td><td style=\"text-align: right;\"> 42.2295</td><td style=\"text-align: right;\">7.06888e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_98162884</td><td>TERMINATED</td><td>172.26.215.93:468114</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_74c0</td><td>[&#x27;force&#x27;, &#x27;x_co_fdc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0036241  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.707895</td><td style=\"text-align: right;\">265.58 </td><td style=\"text-align: right;\">109.158 </td><td style=\"text-align: right;\">2.64102e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_9d2bbc70</td><td>TERMINATED</td><td>172.26.215.93:468458</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_8500</td><td>[&#x27;force&#x27;, &#x27;x_co_9ac0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00379128 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.61594 </td><td style=\"text-align: right;\">420.245</td><td style=\"text-align: right;\">132.526 </td><td style=\"text-align: right;\">1.42099e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_5267dba2</td><td>TERMINATED</td><td>172.26.215.93:468553</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_1280</td><td>[&#x27;force&#x27;, &#x27;x_co_3040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0109549  </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.627952</td><td style=\"text-align: right;\">180.822</td><td style=\"text-align: right;\"> 60.5426</td><td style=\"text-align: right;\">1.0279e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_31674976</td><td>TERMINATED</td><td>172.26.215.93:468872</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_4c00</td><td>[&#x27;force&#x27;, &#x27;x_co_74c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.010343   </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.919721</td><td style=\"text-align: right;\">134.35 </td><td style=\"text-align: right;\"> 46.0734</td><td style=\"text-align: right;\">8.17173e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_d7905aa3</td><td>TERMINATED</td><td>172.26.215.93:468965</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_bc00</td><td>[&#x27;force&#x27;, &#x27;x_co_9f40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00701425 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       19.9999  </td><td style=\"text-align: right;\">117.523</td><td style=\"text-align: right;\"> 41.7128</td><td style=\"text-align: right;\">6.78285e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_5741b0ff</td><td>TERMINATED</td><td>172.26.215.93:469284</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_4800</td><td>[&#x27;force&#x27;, &#x27;x_co_4700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00780507 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       32.9634  </td><td style=\"text-align: right;\">115.097</td><td style=\"text-align: right;\"> 41.0377</td><td style=\"text-align: right;\">6.69138e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_9ccc3e96</td><td>TERMINATED</td><td>172.26.215.93:469382</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_8940</td><td>[&#x27;force&#x27;, &#x27;x_co_9080</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00744483 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       11.4026  </td><td style=\"text-align: right;\">106.554</td><td style=\"text-align: right;\"> 39.1065</td><td style=\"text-align: right;\">6.27476e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_2ec2154f</td><td>TERMINATED</td><td>172.26.215.93:469696</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_a240</td><td>[&#x27;force&#x27;, &#x27;x_co_4c80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00666633 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       20.7322  </td><td style=\"text-align: right;\">112.709</td><td style=\"text-align: right;\"> 39.8964</td><td style=\"text-align: right;\">6.2622e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_946af611</td><td>TERMINATED</td><td>172.26.215.93:469955</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_7b80</td><td>[&#x27;force&#x27;, &#x27;x_co_4980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00776605 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.83558 </td><td style=\"text-align: right;\">107.13 </td><td style=\"text-align: right;\"> 41.7469</td><td style=\"text-align: right;\">7.59082e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_21e65c71</td><td>TERMINATED</td><td>172.26.215.93:470192</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_5200</td><td>[&#x27;force&#x27;, &#x27;x_co_4300</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00527078 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.88472 </td><td style=\"text-align: right;\">110.727</td><td style=\"text-align: right;\"> 43.9608</td><td style=\"text-align: right;\">8.14965e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_853fdefa</td><td>TERMINATED</td><td>172.26.215.93:470444</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>median</td><td>[&#x27;FSR_for_force_5a40</td><td>[&#x27;force&#x27;, &#x27;x_co_5900</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00489876 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.714971</td><td style=\"text-align: right;\">213.955</td><td style=\"text-align: right;\"> 66.2023</td><td style=\"text-align: right;\">1.13109e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_5105c7d3</td><td>TERMINATED</td><td>172.26.215.93:470686</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0930</td><td>sklearn.impute._7af0</td><td>mean  </td><td>[&#x27;FSR_for_force_d040</td><td>[&#x27;force&#x27;, &#x27;x_co_fcc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00203735 </td><td>sklearn.preproc_0810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.12511 </td><td style=\"text-align: right;\">143.866</td><td style=\"text-align: right;\"> 53.2806</td><td style=\"text-align: right;\">8.98039e+16</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 06:27:55,323\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_02b0913f</td><td>2023-07-19_06-28-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 51.0264</td><td style=\"text-align: right;\">3.64131e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">450311</td><td style=\"text-align: right;\">153.006</td><td style=\"text-align: right;\">            7.88485 </td><td style=\"text-align: right;\">          0.235955</td><td style=\"text-align: right;\">      7.88485 </td><td style=\"text-align: right;\"> 1689715716</td><td style=\"text-align: right;\">                  32</td><td>02b0913f  </td></tr>\n",
       "<tr><td>FSR_Trainable_04c07675</td><td>2023-07-19_06-34-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 45.5473</td><td style=\"text-align: right;\">8.29521e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">459419</td><td style=\"text-align: right;\">117.419</td><td style=\"text-align: right;\">            7.40221 </td><td style=\"text-align: right;\">          0.529573</td><td style=\"text-align: right;\">      7.40221 </td><td style=\"text-align: right;\"> 1689716067</td><td style=\"text-align: right;\">                  16</td><td>04c07675  </td></tr>\n",
       "<tr><td>FSR_Trainable_09d33090</td><td>2023-07-19_06-31-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 54.5245</td><td style=\"text-align: right;\">9.11645e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">455591</td><td style=\"text-align: right;\">145.042</td><td style=\"text-align: right;\">            1.19361 </td><td style=\"text-align: right;\">          0.412155</td><td style=\"text-align: right;\">      1.19361 </td><td style=\"text-align: right;\"> 1689715911</td><td style=\"text-align: right;\">                   2</td><td>09d33090  </td></tr>\n",
       "<tr><td>FSR_Trainable_0b93e2f8</td><td>2023-07-19_06-37-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 60.3052</td><td style=\"text-align: right;\">6.65746e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">463874</td><td style=\"text-align: right;\">210.265</td><td style=\"text-align: right;\">            1.08463 </td><td style=\"text-align: right;\">          1.08463 </td><td style=\"text-align: right;\">      1.08463 </td><td style=\"text-align: right;\"> 1689716231</td><td style=\"text-align: right;\">                   1</td><td>0b93e2f8  </td></tr>\n",
       "<tr><td>FSR_Trainable_12e0355e</td><td>2023-07-19_06-34-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 45.3714</td><td style=\"text-align: right;\">8.44398e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">459601</td><td style=\"text-align: right;\">113.546</td><td style=\"text-align: right;\">            3.89832 </td><td style=\"text-align: right;\">          0.41147 </td><td style=\"text-align: right;\">      3.89832 </td><td style=\"text-align: right;\"> 1689716069</td><td style=\"text-align: right;\">                   8</td><td>12e0355e  </td></tr>\n",
       "<tr><td>FSR_Trainable_205b45bb</td><td>2023-07-19_06-36-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 49.9417</td><td style=\"text-align: right;\">9.53632e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">463212</td><td style=\"text-align: right;\">131.426</td><td style=\"text-align: right;\">            1.37296 </td><td style=\"text-align: right;\">          0.515424</td><td style=\"text-align: right;\">      1.37296 </td><td style=\"text-align: right;\"> 1689716202</td><td style=\"text-align: right;\">                   2</td><td>205b45bb  </td></tr>\n",
       "<tr><td>FSR_Trainable_21e65c71</td><td>2023-07-19_06-41-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 43.9608</td><td style=\"text-align: right;\">8.14965e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">470192</td><td style=\"text-align: right;\">110.727</td><td style=\"text-align: right;\">            1.88472 </td><td style=\"text-align: right;\">          0.445139</td><td style=\"text-align: right;\">      1.88472 </td><td style=\"text-align: right;\"> 1689716495</td><td style=\"text-align: right;\">                   4</td><td>21e65c71  </td></tr>\n",
       "<tr><td>FSR_Trainable_22c2b8aa</td><td>2023-07-19_06-38-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 39.8159</td><td style=\"text-align: right;\">2.50963e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">466344</td><td style=\"text-align: right;\">111.524</td><td style=\"text-align: right;\">            1.29105 </td><td style=\"text-align: right;\">          0.242728</td><td style=\"text-align: right;\">      1.29105 </td><td style=\"text-align: right;\"> 1689716334</td><td style=\"text-align: right;\">                   4</td><td>22c2b8aa  </td></tr>\n",
       "<tr><td>FSR_Trainable_2449d287</td><td>2023-07-19_06-33-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 96.447 </td><td style=\"text-align: right;\">2.00459e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">458085</td><td style=\"text-align: right;\">290.644</td><td style=\"text-align: right;\">            0.774552</td><td style=\"text-align: right;\">          0.774552</td><td style=\"text-align: right;\">      0.774552</td><td style=\"text-align: right;\"> 1689716000</td><td style=\"text-align: right;\">                   1</td><td>2449d287  </td></tr>\n",
       "<tr><td>FSR_Trainable_26ca7713</td><td>2023-07-19_06-28-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 76.8407</td><td style=\"text-align: right;\">8.35638e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">450643</td><td style=\"text-align: right;\">244.557</td><td style=\"text-align: right;\">            1.32192 </td><td style=\"text-align: right;\">          1.32192 </td><td style=\"text-align: right;\">      1.32192 </td><td style=\"text-align: right;\"> 1689715718</td><td style=\"text-align: right;\">                   1</td><td>26ca7713  </td></tr>\n",
       "<tr><td>FSR_Trainable_2864a2be</td><td>2023-07-19_06-33-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">110.572 </td><td style=\"text-align: right;\">1.46128e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">457663</td><td style=\"text-align: right;\">330.863</td><td style=\"text-align: right;\">            0.678979</td><td style=\"text-align: right;\">          0.678979</td><td style=\"text-align: right;\">      0.678979</td><td style=\"text-align: right;\"> 1689715985</td><td style=\"text-align: right;\">                   1</td><td>2864a2be  </td></tr>\n",
       "<tr><td>FSR_Trainable_2b5adf1f</td><td>2023-07-19_06-38-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 88.8551</td><td style=\"text-align: right;\">1.97701e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">466024</td><td style=\"text-align: right;\">214.508</td><td style=\"text-align: right;\">            0.45932 </td><td style=\"text-align: right;\">          0.45932 </td><td style=\"text-align: right;\">      0.45932 </td><td style=\"text-align: right;\"> 1689716320</td><td style=\"text-align: right;\">                   1</td><td>2b5adf1f  </td></tr>\n",
       "<tr><td>FSR_Trainable_2eb64ca4</td><td>2023-07-19_06-31-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 41.076 </td><td style=\"text-align: right;\">7.2155e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">454449</td><td style=\"text-align: right;\">105.363</td><td style=\"text-align: right;\">            1.74184 </td><td style=\"text-align: right;\">          0.264661</td><td style=\"text-align: right;\">      1.74184 </td><td style=\"text-align: right;\"> 1689715872</td><td style=\"text-align: right;\">                   4</td><td>2eb64ca4  </td></tr>\n",
       "<tr><td>FSR_Trainable_2ec2154f</td><td>2023-07-19_06-41-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 39.8964</td><td style=\"text-align: right;\">6.2622e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">469696</td><td style=\"text-align: right;\">112.709</td><td style=\"text-align: right;\">           20.7322  </td><td style=\"text-align: right;\">          0.312453</td><td style=\"text-align: right;\">     20.7322  </td><td style=\"text-align: right;\"> 1689716497</td><td style=\"text-align: right;\">                  64</td><td>2ec2154f  </td></tr>\n",
       "<tr><td>FSR_Trainable_31674976</td><td>2023-07-19_06-40-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 46.0734</td><td style=\"text-align: right;\">8.17173e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">468872</td><td style=\"text-align: right;\">134.35 </td><td style=\"text-align: right;\">            0.919721</td><td style=\"text-align: right;\">          0.320515</td><td style=\"text-align: right;\">      0.919721</td><td style=\"text-align: right;\"> 1689716429</td><td style=\"text-align: right;\">                   2</td><td>31674976  </td></tr>\n",
       "<tr><td>FSR_Trainable_323e9721</td><td>2023-07-19_06-30-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 93.9522</td><td style=\"text-align: right;\">1.6561e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">453880</td><td style=\"text-align: right;\">245.265</td><td style=\"text-align: right;\">            0.493366</td><td style=\"text-align: right;\">          0.493366</td><td style=\"text-align: right;\">      0.493366</td><td style=\"text-align: right;\"> 1689715844</td><td style=\"text-align: right;\">                   1</td><td>323e9721  </td></tr>\n",
       "<tr><td>FSR_Trainable_355f65ec</td><td>2023-07-19_06-39-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 58.0646</td><td style=\"text-align: right;\">4.12335e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">467758</td><td style=\"text-align: right;\">178.934</td><td style=\"text-align: right;\">            0.743823</td><td style=\"text-align: right;\">          0.743823</td><td style=\"text-align: right;\">      0.743823</td><td style=\"text-align: right;\"> 1689716388</td><td style=\"text-align: right;\">                   1</td><td>355f65ec  </td></tr>\n",
       "<tr><td>FSR_Trainable_3657c235</td><td>2023-07-19_06-28-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 65.9713</td><td style=\"text-align: right;\">1.19726e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">449879</td><td style=\"text-align: right;\">179.692</td><td style=\"text-align: right;\">           14.49    </td><td style=\"text-align: right;\">          0.861954</td><td style=\"text-align: right;\">     14.49    </td><td style=\"text-align: right;\"> 1689715701</td><td style=\"text-align: right;\">                  16</td><td>3657c235  </td></tr>\n",
       "<tr><td>FSR_Trainable_3dfe355c</td><td>2023-07-19_06-29-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 64.7147</td><td style=\"text-align: right;\">3.3357e+15 </td><td>172.26.215.93</td><td style=\"text-align: right;\">451622</td><td style=\"text-align: right;\">230.445</td><td style=\"text-align: right;\">            0.651438</td><td style=\"text-align: right;\">          0.651438</td><td style=\"text-align: right;\">      0.651438</td><td style=\"text-align: right;\"> 1689715757</td><td style=\"text-align: right;\">                   1</td><td>3dfe355c  </td></tr>\n",
       "<tr><td>FSR_Trainable_460ff324</td><td>2023-07-19_06-30-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 43.9099</td><td style=\"text-align: right;\">7.62252e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">453312</td><td style=\"text-align: right;\">114.096</td><td style=\"text-align: right;\">            5.90353 </td><td style=\"text-align: right;\">          0.339075</td><td style=\"text-align: right;\">      5.90353 </td><td style=\"text-align: right;\"> 1689715831</td><td style=\"text-align: right;\">                  16</td><td>460ff324  </td></tr>\n",
       "<tr><td>FSR_Trainable_475870e9</td><td>2023-07-19_06-38-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 43.1949</td><td style=\"text-align: right;\">7.30461e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">465508</td><td style=\"text-align: right;\">118.649</td><td style=\"text-align: right;\">            0.976404</td><td style=\"text-align: right;\">          0.323603</td><td style=\"text-align: right;\">      0.976404</td><td style=\"text-align: right;\"> 1689716299</td><td style=\"text-align: right;\">                   2</td><td>475870e9  </td></tr>\n",
       "<tr><td>FSR_Trainable_4773fdc2</td><td>2023-07-19_06-35-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 77.7447</td><td style=\"text-align: right;\">1.41254e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">462062</td><td style=\"text-align: right;\">223.422</td><td style=\"text-align: right;\">            0.910386</td><td style=\"text-align: right;\">          0.910386</td><td style=\"text-align: right;\">      0.910386</td><td style=\"text-align: right;\"> 1689716157</td><td style=\"text-align: right;\">                   1</td><td>4773fdc2  </td></tr>\n",
       "<tr><td>FSR_Trainable_48bc3541</td><td>2023-07-19_06-39-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 40.4398</td><td style=\"text-align: right;\">7.26238e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">466830</td><td style=\"text-align: right;\">109.214</td><td style=\"text-align: right;\">            1.59492 </td><td style=\"text-align: right;\">          0.32854 </td><td style=\"text-align: right;\">      1.59492 </td><td style=\"text-align: right;\"> 1689716353</td><td style=\"text-align: right;\">                   4</td><td>48bc3541  </td></tr>\n",
       "<tr><td>FSR_Trainable_4900ef1d</td><td>2023-07-19_06-38-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 42.9687</td><td style=\"text-align: right;\">7.49102e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">465032</td><td style=\"text-align: right;\">115.393</td><td style=\"text-align: right;\">            8.02    </td><td style=\"text-align: right;\">          0.294035</td><td style=\"text-align: right;\">      8.02    </td><td style=\"text-align: right;\"> 1689716290</td><td style=\"text-align: right;\">                  32</td><td>4900ef1d  </td></tr>\n",
       "<tr><td>FSR_Trainable_4c312fa1</td><td>2023-07-19_06-31-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 48.6538</td><td style=\"text-align: right;\">8.36188e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">455180</td><td style=\"text-align: right;\">131.192</td><td style=\"text-align: right;\">            1.26627 </td><td style=\"text-align: right;\">          0.38716 </td><td style=\"text-align: right;\">      1.26627 </td><td style=\"text-align: right;\"> 1689715896</td><td style=\"text-align: right;\">                   2</td><td>4c312fa1  </td></tr>\n",
       "<tr><td>FSR_Trainable_4c87d99c</td><td>2023-07-19_06-37-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 43.0198</td><td style=\"text-align: right;\">7.87505e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">464611</td><td style=\"text-align: right;\">109.675</td><td style=\"text-align: right;\">            1.43676 </td><td style=\"text-align: right;\">          0.290358</td><td style=\"text-align: right;\">      1.43676 </td><td style=\"text-align: right;\"> 1689716260</td><td style=\"text-align: right;\">                   4</td><td>4c87d99c  </td></tr>\n",
       "<tr><td>FSR_Trainable_4dc78491</td><td>2023-07-19_06-30-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 43.5714</td><td style=\"text-align: right;\">7.61856e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">454109</td><td style=\"text-align: right;\">120.432</td><td style=\"text-align: right;\">            1.91073 </td><td style=\"text-align: right;\">          0.370588</td><td style=\"text-align: right;\">      1.91073 </td><td style=\"text-align: right;\"> 1689715857</td><td style=\"text-align: right;\">                   4</td><td>4dc78491  </td></tr>\n",
       "<tr><td>FSR_Trainable_4e4c2fa2</td><td>2023-07-19_06-32-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 39.2163</td><td style=\"text-align: right;\">1.58593e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">456418</td><td style=\"text-align: right;\">116.717</td><td style=\"text-align: right;\">            1.59686 </td><td style=\"text-align: right;\">          0.322511</td><td style=\"text-align: right;\">      1.59686 </td><td style=\"text-align: right;\"> 1689715942</td><td style=\"text-align: right;\">                   4</td><td>4e4c2fa2  </td></tr>\n",
       "<tr><td>FSR_Trainable_4f5cc13f</td><td>2023-07-19_06-39-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 50.7885</td><td style=\"text-align: right;\">3.68321e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">467667</td><td style=\"text-align: right;\">151.718</td><td style=\"text-align: right;\">            0.898407</td><td style=\"text-align: right;\">          0.248595</td><td style=\"text-align: right;\">      0.898407</td><td style=\"text-align: right;\"> 1689716384</td><td style=\"text-align: right;\">                   2</td><td>4f5cc13f  </td></tr>\n",
       "<tr><td>FSR_Trainable_5105c7d3</td><td>2023-07-19_06-41-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 53.2806</td><td style=\"text-align: right;\">8.98039e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">470686</td><td style=\"text-align: right;\">143.866</td><td style=\"text-align: right;\">            1.12511 </td><td style=\"text-align: right;\">          0.296148</td><td style=\"text-align: right;\">      1.12511 </td><td style=\"text-align: right;\"> 1689716513</td><td style=\"text-align: right;\">                   2</td><td>5105c7d3  </td></tr>\n",
       "<tr><td>FSR_Trainable_5267dba2</td><td>2023-07-19_06-40-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 60.5426</td><td style=\"text-align: right;\">1.0279e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">468553</td><td style=\"text-align: right;\">180.822</td><td style=\"text-align: right;\">            0.627952</td><td style=\"text-align: right;\">          0.627952</td><td style=\"text-align: right;\">      0.627952</td><td style=\"text-align: right;\"> 1689716418</td><td style=\"text-align: right;\">                   1</td><td>5267dba2  </td></tr>\n",
       "<tr><td>FSR_Trainable_5379e322</td><td>2023-07-19_06-31-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 53.5954</td><td style=\"text-align: right;\">1.02873e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">455276</td><td style=\"text-align: right;\">134.822</td><td style=\"text-align: right;\">            1.18638 </td><td style=\"text-align: right;\">          0.405025</td><td style=\"text-align: right;\">      1.18638 </td><td style=\"text-align: right;\"> 1689715903</td><td style=\"text-align: right;\">                   2</td><td>5379e322  </td></tr>\n",
       "<tr><td>FSR_Trainable_5741b0ff</td><td>2023-07-19_06-41-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 41.0377</td><td style=\"text-align: right;\">6.69138e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">469284</td><td style=\"text-align: right;\">115.097</td><td style=\"text-align: right;\">           32.9634  </td><td style=\"text-align: right;\">          0.301525</td><td style=\"text-align: right;\">     32.9634  </td><td style=\"text-align: right;\"> 1689716489</td><td style=\"text-align: right;\">                 100</td><td>5741b0ff  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a8948ac</td><td>2023-07-19_06-34-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 46.3112</td><td style=\"text-align: right;\">8.20331e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">460253</td><td style=\"text-align: right;\">124.505</td><td style=\"text-align: right;\">            1.6488  </td><td style=\"text-align: right;\">          0.24542 </td><td style=\"text-align: right;\">      1.6488  </td><td style=\"text-align: right;\"> 1689716091</td><td style=\"text-align: right;\">                   4</td><td>5a8948ac  </td></tr>\n",
       "<tr><td>FSR_Trainable_5bccc8eb</td><td>2023-07-19_06-32-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 44.2198</td><td style=\"text-align: right;\">1.68113e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">456103</td><td style=\"text-align: right;\">136.787</td><td style=\"text-align: right;\">            1.26221 </td><td style=\"text-align: right;\">          0.432404</td><td style=\"text-align: right;\">      1.26221 </td><td style=\"text-align: right;\"> 1689715933</td><td style=\"text-align: right;\">                   2</td><td>5bccc8eb  </td></tr>\n",
       "<tr><td>FSR_Trainable_5e128218</td><td>2023-07-19_06-36-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 71.13  </td><td style=\"text-align: right;\">6.00657e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">462479</td><td style=\"text-align: right;\">228.064</td><td style=\"text-align: right;\">            0.977633</td><td style=\"text-align: right;\">          0.977633</td><td style=\"text-align: right;\">      0.977633</td><td style=\"text-align: right;\"> 1689716173</td><td style=\"text-align: right;\">                   1</td><td>5e128218  </td></tr>\n",
       "<tr><td>FSR_Trainable_61048410</td><td>2023-07-19_06-38-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 39.1812</td><td style=\"text-align: right;\">6.01421e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">465749</td><td style=\"text-align: right;\">109.707</td><td style=\"text-align: right;\">           23.0041  </td><td style=\"text-align: right;\">          0.196984</td><td style=\"text-align: right;\">     23.0041  </td><td style=\"text-align: right;\"> 1689716338</td><td style=\"text-align: right;\">                 100</td><td>61048410  </td></tr>\n",
       "<tr><td>FSR_Trainable_65ca294c</td><td>2023-07-19_06-32-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 85.9479</td><td style=\"text-align: right;\">4.98122e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">456008</td><td style=\"text-align: right;\">250.684</td><td style=\"text-align: right;\">            0.652703</td><td style=\"text-align: right;\">          0.652703</td><td style=\"text-align: right;\">      0.652703</td><td style=\"text-align: right;\"> 1689715923</td><td style=\"text-align: right;\">                   1</td><td>65ca294c  </td></tr>\n",
       "<tr><td>FSR_Trainable_6fa528d8</td><td>2023-07-19_06-30-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 88.9189</td><td style=\"text-align: right;\">9.63354e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">453631</td><td style=\"text-align: right;\">295.066</td><td style=\"text-align: right;\">            0.484017</td><td style=\"text-align: right;\">          0.484017</td><td style=\"text-align: right;\">      0.484017</td><td style=\"text-align: right;\"> 1689715834</td><td style=\"text-align: right;\">                   1</td><td>6fa528d8  </td></tr>\n",
       "<tr><td>FSR_Trainable_7034511a</td><td>2023-07-19_06-37-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 64.226 </td><td style=\"text-align: right;\">1.09816e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">464199</td><td style=\"text-align: right;\">217.35 </td><td style=\"text-align: right;\">            0.773845</td><td style=\"text-align: right;\">          0.773845</td><td style=\"text-align: right;\">      0.773845</td><td style=\"text-align: right;\"> 1689716239</td><td style=\"text-align: right;\">                   1</td><td>7034511a  </td></tr>\n",
       "<tr><td>FSR_Trainable_71119e56</td><td>2023-07-19_06-34-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 46.7858</td><td style=\"text-align: right;\">8.46832e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">459318</td><td style=\"text-align: right;\">119.764</td><td style=\"text-align: right;\">            7.12217 </td><td style=\"text-align: right;\">          0.455454</td><td style=\"text-align: right;\">      7.12217 </td><td style=\"text-align: right;\"> 1689716059</td><td style=\"text-align: right;\">                  16</td><td>71119e56  </td></tr>\n",
       "<tr><td>FSR_Trainable_74c841d3</td><td>2023-07-19_06-39-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 42.2295</td><td style=\"text-align: right;\">7.06888e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">467945</td><td style=\"text-align: right;\">117.736</td><td style=\"text-align: right;\">            1.35578 </td><td style=\"text-align: right;\">          0.343489</td><td style=\"text-align: right;\">      1.35578 </td><td style=\"text-align: right;\"> 1689716399</td><td style=\"text-align: right;\">                   2</td><td>74c841d3  </td></tr>\n",
       "<tr><td>FSR_Trainable_76752b90</td><td>2023-07-19_06-36-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 68.7589</td><td style=\"text-align: right;\">1.14571e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">462384</td><td style=\"text-align: right;\">194.41 </td><td style=\"text-align: right;\">            0.766449</td><td style=\"text-align: right;\">          0.766449</td><td style=\"text-align: right;\">      0.766449</td><td style=\"text-align: right;\"> 1689716166</td><td style=\"text-align: right;\">                   1</td><td>76752b90  </td></tr>\n",
       "<tr><td>FSR_Trainable_7ca7828d</td><td>2023-07-19_06-30-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 62.9871</td><td style=\"text-align: right;\">1.06663e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">452851</td><td style=\"text-align: right;\">181.649</td><td style=\"text-align: right;\">            0.896481</td><td style=\"text-align: right;\">          0.234491</td><td style=\"text-align: right;\">      0.896481</td><td style=\"text-align: right;\"> 1689715804</td><td style=\"text-align: right;\">                   2</td><td>7ca7828d  </td></tr>\n",
       "<tr><td>FSR_Trainable_7e7f666f</td><td>2023-07-19_06-32-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 49.9337</td><td style=\"text-align: right;\">8.50799e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">456515</td><td style=\"text-align: right;\">143.362</td><td style=\"text-align: right;\">            1.2243  </td><td style=\"text-align: right;\">          0.469634</td><td style=\"text-align: right;\">      1.2243  </td><td style=\"text-align: right;\"> 1689715948</td><td style=\"text-align: right;\">                   2</td><td>7e7f666f  </td></tr>\n",
       "<tr><td>FSR_Trainable_7ef58eae</td><td>2023-07-19_06-35-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 43.9788</td><td style=\"text-align: right;\">7.90131e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">460946</td><td style=\"text-align: right;\">113.874</td><td style=\"text-align: right;\">            2.89146 </td><td style=\"text-align: right;\">          0.282143</td><td style=\"text-align: right;\">      2.89146 </td><td style=\"text-align: right;\"> 1689716115</td><td style=\"text-align: right;\">                   8</td><td>7ef58eae  </td></tr>\n",
       "<tr><td>FSR_Trainable_81e3fc24</td><td>2023-07-19_06-28-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 69.9469</td><td style=\"text-align: right;\">1.26829e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">450130</td><td style=\"text-align: right;\">194.399</td><td style=\"text-align: right;\">            0.881525</td><td style=\"text-align: right;\">          0.881525</td><td style=\"text-align: right;\">      0.881525</td><td style=\"text-align: right;\"> 1689715698</td><td style=\"text-align: right;\">                   1</td><td>81e3fc24  </td></tr>\n",
       "<tr><td>FSR_Trainable_8271662f</td><td>2023-07-19_06-29-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 65.4606</td><td style=\"text-align: right;\">4.31077e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">451452</td><td style=\"text-align: right;\">183.945</td><td style=\"text-align: right;\">            1.43384 </td><td style=\"text-align: right;\">          0.4359  </td><td style=\"text-align: right;\">      1.43384 </td><td style=\"text-align: right;\"> 1689715753</td><td style=\"text-align: right;\">                   2</td><td>8271662f  </td></tr>\n",
       "<tr><td>FSR_Trainable_842abfbc</td><td>2023-07-19_06-33-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 43.2839</td><td style=\"text-align: right;\">7.14687e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">458902</td><td style=\"text-align: right;\">120.865</td><td style=\"text-align: right;\">            2.29409 </td><td style=\"text-align: right;\">          0.3636  </td><td style=\"text-align: right;\">      2.29409 </td><td style=\"text-align: right;\"> 1689716036</td><td style=\"text-align: right;\">                   4</td><td>842abfbc  </td></tr>\n",
       "<tr><td>FSR_Trainable_853fdefa</td><td>2023-07-19_06-41-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 66.2023</td><td style=\"text-align: right;\">1.13109e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">470444</td><td style=\"text-align: right;\">213.955</td><td style=\"text-align: right;\">            0.714971</td><td style=\"text-align: right;\">          0.714971</td><td style=\"text-align: right;\">      0.714971</td><td style=\"text-align: right;\"> 1689716502</td><td style=\"text-align: right;\">                   1</td><td>853fdefa  </td></tr>\n",
       "<tr><td>FSR_Trainable_85951629</td><td>2023-07-19_06-29-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 47.9605</td><td style=\"text-align: right;\">3.64354e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">452607</td><td style=\"text-align: right;\">146.83 </td><td style=\"text-align: right;\">            1.30478 </td><td style=\"text-align: right;\">          0.195453</td><td style=\"text-align: right;\">      1.30478 </td><td style=\"text-align: right;\"> 1689715795</td><td style=\"text-align: right;\">                   4</td><td>85951629  </td></tr>\n",
       "<tr><td>FSR_Trainable_8cf05336</td><td>2023-07-19_06-30-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 43.735 </td><td style=\"text-align: right;\">7.93991e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">453126</td><td style=\"text-align: right;\">112.273</td><td style=\"text-align: right;\">            2.98969 </td><td style=\"text-align: right;\">          0.30258 </td><td style=\"text-align: right;\">      2.98969 </td><td style=\"text-align: right;\"> 1689715820</td><td style=\"text-align: right;\">                   8</td><td>8cf05336  </td></tr>\n",
       "<tr><td>FSR_Trainable_913dcdb5</td><td>2023-07-19_06-39-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 57.9984</td><td style=\"text-align: right;\">2.99037e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">467436</td><td style=\"text-align: right;\">208.31 </td><td style=\"text-align: right;\">            0.716424</td><td style=\"text-align: right;\">          0.716424</td><td style=\"text-align: right;\">      0.716424</td><td style=\"text-align: right;\"> 1689716372</td><td style=\"text-align: right;\">                   1</td><td>913dcdb5  </td></tr>\n",
       "<tr><td>FSR_Trainable_946af611</td><td>2023-07-19_06-41-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 41.7469</td><td style=\"text-align: right;\">7.59082e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">469955</td><td style=\"text-align: right;\">107.13 </td><td style=\"text-align: right;\">            1.83558 </td><td style=\"text-align: right;\">          0.410171</td><td style=\"text-align: right;\">      1.83558 </td><td style=\"text-align: right;\"> 1689716483</td><td style=\"text-align: right;\">                   4</td><td>946af611  </td></tr>\n",
       "<tr><td>FSR_Trainable_98162884</td><td>2023-07-19_06-40-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">109.158 </td><td style=\"text-align: right;\">2.64102e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">468114</td><td style=\"text-align: right;\">265.58 </td><td style=\"text-align: right;\">            0.707895</td><td style=\"text-align: right;\">          0.707895</td><td style=\"text-align: right;\">      0.707895</td><td style=\"text-align: right;\"> 1689716403</td><td style=\"text-align: right;\">                   1</td><td>98162884  </td></tr>\n",
       "<tr><td>FSR_Trainable_9b6f7a70</td><td>2023-07-19_06-29-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 40.9935</td><td style=\"text-align: right;\">6.55006e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">451825</td><td style=\"text-align: right;\">117.211</td><td style=\"text-align: right;\">           16.5356  </td><td style=\"text-align: right;\">          0.268819</td><td style=\"text-align: right;\">     16.5356  </td><td style=\"text-align: right;\"> 1689715787</td><td style=\"text-align: right;\">                  64</td><td>9b6f7a70  </td></tr>\n",
       "<tr><td>FSR_Trainable_9ccc3e96</td><td>2023-07-19_06-41-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 39.1065</td><td style=\"text-align: right;\">6.27476e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">469382</td><td style=\"text-align: right;\">106.554</td><td style=\"text-align: right;\">           11.4026  </td><td style=\"text-align: right;\">          0.309203</td><td style=\"text-align: right;\">     11.4026  </td><td style=\"text-align: right;\"> 1689716466</td><td style=\"text-align: right;\">                  32</td><td>9ccc3e96  </td></tr>\n",
       "<tr><td>FSR_Trainable_9d2bbc70</td><td>2023-07-19_06-40-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">132.526 </td><td style=\"text-align: right;\">1.42099e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">468458</td><td style=\"text-align: right;\">420.245</td><td style=\"text-align: right;\">            0.61594 </td><td style=\"text-align: right;\">          0.61594 </td><td style=\"text-align: right;\">      0.61594 </td><td style=\"text-align: right;\"> 1689716411</td><td style=\"text-align: right;\">                   1</td><td>9d2bbc70  </td></tr>\n",
       "<tr><td>FSR_Trainable_a2fad63a</td><td>2023-07-19_06-29-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 74.1572</td><td style=\"text-align: right;\">1.39407e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">452372</td><td style=\"text-align: right;\">203.782</td><td style=\"text-align: right;\">            0.663334</td><td style=\"text-align: right;\">          0.258332</td><td style=\"text-align: right;\">      0.663334</td><td style=\"text-align: right;\"> 1689715786</td><td style=\"text-align: right;\">                   2</td><td>a2fad63a  </td></tr>\n",
       "<tr><td>FSR_Trainable_a66ee56f</td><td>2023-07-19_06-38-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 42.005 </td><td style=\"text-align: right;\">7.48647e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">465272</td><td style=\"text-align: right;\">107.84 </td><td style=\"text-align: right;\">            1.41269 </td><td style=\"text-align: right;\">          0.273711</td><td style=\"text-align: right;\">      1.41269 </td><td style=\"text-align: right;\"> 1689716291</td><td style=\"text-align: right;\">                   4</td><td>a66ee56f  </td></tr>\n",
       "<tr><td>FSR_Trainable_a8d2b7da</td><td>2023-07-19_06-28-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 63.8986</td><td style=\"text-align: right;\">1.50851e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">451122</td><td style=\"text-align: right;\">221.404</td><td style=\"text-align: right;\">            0.728214</td><td style=\"text-align: right;\">          0.728214</td><td style=\"text-align: right;\">      0.728214</td><td style=\"text-align: right;\"> 1689715736</td><td style=\"text-align: right;\">                   1</td><td>a8d2b7da  </td></tr>\n",
       "<tr><td>FSR_Trainable_aac9dd0d</td><td>2023-07-19_06-35-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 92.1204</td><td style=\"text-align: right;\">1.88997e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">461647</td><td style=\"text-align: right;\">246.357</td><td style=\"text-align: right;\">            0.667914</td><td style=\"text-align: right;\">          0.667914</td><td style=\"text-align: right;\">      0.667914</td><td style=\"text-align: right;\"> 1689716140</td><td style=\"text-align: right;\">                   1</td><td>aac9dd0d  </td></tr>\n",
       "<tr><td>FSR_Trainable_b0892721</td><td>2023-07-19_06-34-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 97.9066</td><td style=\"text-align: right;\">2.09864e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">460439</td><td style=\"text-align: right;\">256.469</td><td style=\"text-align: right;\">            1.336   </td><td style=\"text-align: right;\">          1.336   </td><td style=\"text-align: right;\">      1.336   </td><td style=\"text-align: right;\"> 1689716095</td><td style=\"text-align: right;\">                   1</td><td>b0892721  </td></tr>\n",
       "<tr><td>FSR_Trainable_b7b45e41</td><td>2023-07-19_06-34-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 56.0561</td><td style=\"text-align: right;\">1.70362e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">459926</td><td style=\"text-align: right;\">176.675</td><td style=\"text-align: right;\">            0.77182 </td><td style=\"text-align: right;\">          0.77182 </td><td style=\"text-align: right;\">      0.77182 </td><td style=\"text-align: right;\"> 1689716073</td><td style=\"text-align: right;\">                   1</td><td>b7b45e41  </td></tr>\n",
       "<tr><td>FSR_Trainable_b8ae8cd2</td><td>2023-07-19_06-33-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.9226</td><td style=\"text-align: right;\">3.58286e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">458174</td><td style=\"text-align: right;\">225.068</td><td style=\"text-align: right;\">            1.04593 </td><td style=\"text-align: right;\">          1.04593 </td><td style=\"text-align: right;\">      1.04593 </td><td style=\"text-align: right;\"> 1689716007</td><td style=\"text-align: right;\">                   1</td><td>b8ae8cd2  </td></tr>\n",
       "<tr><td>FSR_Trainable_ba38fa3e</td><td>2023-07-19_06-34-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 46.1974</td><td style=\"text-align: right;\">8.40094e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">459001</td><td style=\"text-align: right;\">118.81 </td><td style=\"text-align: right;\">            2.94611 </td><td style=\"text-align: right;\">          0.28583 </td><td style=\"text-align: right;\">      2.94611 </td><td style=\"text-align: right;\"> 1689716044</td><td style=\"text-align: right;\">                   8</td><td>ba38fa3e  </td></tr>\n",
       "<tr><td>FSR_Trainable_ba3a6850</td><td>2023-07-19_06-29-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 68.5802</td><td style=\"text-align: right;\">6.06597e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">451364</td><td style=\"text-align: right;\">253.055</td><td style=\"text-align: right;\">            0.655068</td><td style=\"text-align: right;\">          0.655068</td><td style=\"text-align: right;\">      0.655068</td><td style=\"text-align: right;\"> 1689715744</td><td style=\"text-align: right;\">                   1</td><td>ba3a6850  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1862fb9</td><td>2023-07-19_06-35-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 44.2522</td><td style=\"text-align: right;\">7.64594e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">461272</td><td style=\"text-align: right;\">120.25 </td><td style=\"text-align: right;\">            1.06189 </td><td style=\"text-align: right;\">          0.309478</td><td style=\"text-align: right;\">      1.06189 </td><td style=\"text-align: right;\"> 1689716130</td><td style=\"text-align: right;\">                   2</td><td>c1862fb9  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1c9a5cc</td><td>2023-07-19_06-39-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 36.7986</td><td style=\"text-align: right;\">1.98746e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">466586</td><td style=\"text-align: right;\">107.784</td><td style=\"text-align: right;\">            1.30363 </td><td style=\"text-align: right;\">          0.205507</td><td style=\"text-align: right;\">      1.30363 </td><td style=\"text-align: right;\"> 1689716344</td><td style=\"text-align: right;\">                   4</td><td>c1c9a5cc  </td></tr>\n",
       "<tr><td>FSR_Trainable_c58472f9</td><td>2023-07-19_06-35-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 42.5482</td><td style=\"text-align: right;\">7.6634e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">461180</td><td style=\"text-align: right;\">108.856</td><td style=\"text-align: right;\">            1.75944 </td><td style=\"text-align: right;\">          0.372207</td><td style=\"text-align: right;\">      1.75944 </td><td style=\"text-align: right;\"> 1689716123</td><td style=\"text-align: right;\">                   4</td><td>c58472f9  </td></tr>\n",
       "<tr><td>FSR_Trainable_c5b02e9a</td><td>2023-07-19_06-36-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 70.7466</td><td style=\"text-align: right;\">9.82617e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">462796</td><td style=\"text-align: right;\">224.602</td><td style=\"text-align: right;\">            1.03669 </td><td style=\"text-align: right;\">          1.03669 </td><td style=\"text-align: right;\">      1.03669 </td><td style=\"text-align: right;\"> 1689716183</td><td style=\"text-align: right;\">                   1</td><td>c5b02e9a  </td></tr>\n",
       "<tr><td>FSR_Trainable_c6df9388</td><td>2023-07-19_06-32-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 41.125 </td><td style=\"text-align: right;\">7.10222e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">457248</td><td style=\"text-align: right;\">110.647</td><td style=\"text-align: right;\">            1.97575 </td><td style=\"text-align: right;\">          0.3822  </td><td style=\"text-align: right;\">      1.97575 </td><td style=\"text-align: right;\"> 1689715974</td><td style=\"text-align: right;\">                   4</td><td>c6df9388  </td></tr>\n",
       "<tr><td>FSR_Trainable_c92621f2</td><td>2023-07-19_06-35-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">102.839 </td><td style=\"text-align: right;\">1.11436e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">461446</td><td style=\"text-align: right;\">320.023</td><td style=\"text-align: right;\">            0.696492</td><td style=\"text-align: right;\">          0.696492</td><td style=\"text-align: right;\">      0.696492</td><td style=\"text-align: right;\"> 1689716134</td><td style=\"text-align: right;\">                   1</td><td>c92621f2  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc0a970b</td><td>2023-07-19_06-35-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 43.3805</td><td style=\"text-align: right;\">7.99018e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">460626</td><td style=\"text-align: right;\">110.079</td><td style=\"text-align: right;\">            3.09951 </td><td style=\"text-align: right;\">          0.258915</td><td style=\"text-align: right;\">      3.09951 </td><td style=\"text-align: right;\"> 1689716107</td><td style=\"text-align: right;\">                   8</td><td>cc0a970b  </td></tr>\n",
       "<tr><td>FSR_Trainable_cd9cf579</td><td>2023-07-19_06-29-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 93.5895</td><td style=\"text-align: right;\">1.00657e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">452145</td><td style=\"text-align: right;\">283.119</td><td style=\"text-align: right;\">            0.332765</td><td style=\"text-align: right;\">          0.332765</td><td style=\"text-align: right;\">      0.332765</td><td style=\"text-align: right;\"> 1689715773</td><td style=\"text-align: right;\">                   1</td><td>cd9cf579  </td></tr>\n",
       "<tr><td>FSR_Trainable_cdd4bc1d</td><td>2023-07-19_06-39-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 41.2918</td><td style=\"text-align: right;\">2.14107e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">466890</td><td style=\"text-align: right;\">121.259</td><td style=\"text-align: right;\">            0.944959</td><td style=\"text-align: right;\">          0.262804</td><td style=\"text-align: right;\">      0.944959</td><td style=\"text-align: right;\"> 1689716359</td><td style=\"text-align: right;\">                   2</td><td>cdd4bc1d  </td></tr>\n",
       "<tr><td>FSR_Trainable_ce487b52</td><td>2023-07-19_06-33-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 68.9368</td><td style=\"text-align: right;\">5.2771e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">458494</td><td style=\"text-align: right;\">213.663</td><td style=\"text-align: right;\">            1.26541 </td><td style=\"text-align: right;\">          0.31013 </td><td style=\"text-align: right;\">      1.26541 </td><td style=\"text-align: right;\"> 1689716019</td><td style=\"text-align: right;\">                   2</td><td>ce487b52  </td></tr>\n",
       "<tr><td>FSR_Trainable_d17c72c7</td><td>2023-07-19_06-35-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.6717</td><td style=\"text-align: right;\">1.6352e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">461965</td><td style=\"text-align: right;\">212.179</td><td style=\"text-align: right;\">            0.646471</td><td style=\"text-align: right;\">          0.646471</td><td style=\"text-align: right;\">      0.646471</td><td style=\"text-align: right;\"> 1689716149</td><td style=\"text-align: right;\">                   1</td><td>d17c72c7  </td></tr>\n",
       "<tr><td>FSR_Trainable_d192c08f</td><td>2023-07-19_06-31-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 48.6325</td><td style=\"text-align: right;\">1.46412e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">454340</td><td style=\"text-align: right;\">157.744</td><td style=\"text-align: right;\">            1.06704 </td><td style=\"text-align: right;\">          0.321076</td><td style=\"text-align: right;\">      1.06704 </td><td style=\"text-align: right;\"> 1689715865</td><td style=\"text-align: right;\">                   2</td><td>d192c08f  </td></tr>\n",
       "<tr><td>FSR_Trainable_d1db0969</td><td>2023-07-19_06-28-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 40.7078</td><td style=\"text-align: right;\">6.30634e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">449952</td><td style=\"text-align: right;\">119.489</td><td style=\"text-align: right;\">           29.9616  </td><td style=\"text-align: right;\">          0.322063</td><td style=\"text-align: right;\">     29.9616  </td><td style=\"text-align: right;\"> 1689715727</td><td style=\"text-align: right;\">                 100</td><td>d1db0969  </td></tr>\n",
       "<tr><td>FSR_Trainable_d45936b5</td><td>2023-07-19_06-30-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.3648</td><td style=\"text-align: right;\">6.22208e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">452938</td><td style=\"text-align: right;\">106.558</td><td style=\"text-align: right;\">           31.458   </td><td style=\"text-align: right;\">          0.216794</td><td style=\"text-align: right;\">     31.458   </td><td style=\"text-align: right;\"> 1689715852</td><td style=\"text-align: right;\">                 100</td><td>d45936b5  </td></tr>\n",
       "<tr><td>FSR_Trainable_d66a9693</td><td>2023-07-19_06-33-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 41.269 </td><td style=\"text-align: right;\">6.97748e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">458585</td><td style=\"text-align: right;\">116.078</td><td style=\"text-align: right;\">            1.88156 </td><td style=\"text-align: right;\">          0.394361</td><td style=\"text-align: right;\">      1.88156 </td><td style=\"text-align: right;\"> 1689716026</td><td style=\"text-align: right;\">                   4</td><td>d66a9693  </td></tr>\n",
       "<tr><td>FSR_Trainable_d7905aa3</td><td>2023-07-19_06-41-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 41.7128</td><td style=\"text-align: right;\">6.78285e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">468965</td><td style=\"text-align: right;\">117.523</td><td style=\"text-align: right;\">           19.9999  </td><td style=\"text-align: right;\">          0.334985</td><td style=\"text-align: right;\">     19.9999  </td><td style=\"text-align: right;\"> 1689716462</td><td style=\"text-align: right;\">                  64</td><td>d7905aa3  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8067052</td><td>2023-07-19_06-36-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 42.6782</td><td style=\"text-align: right;\">7.62759e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">463314</td><td style=\"text-align: right;\">111.696</td><td style=\"text-align: right;\">            3.50942 </td><td style=\"text-align: right;\">          0.292594</td><td style=\"text-align: right;\">      3.50942 </td><td style=\"text-align: right;\"> 1689716211</td><td style=\"text-align: right;\">                   8</td><td>d8067052  </td></tr>\n",
       "<tr><td>FSR_Trainable_db705091</td><td>2023-07-19_06-32-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 41.2944</td><td style=\"text-align: right;\">6.90405e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">456929</td><td style=\"text-align: right;\">115.658</td><td style=\"text-align: right;\">            2.1103  </td><td style=\"text-align: right;\">          0.36916 </td><td style=\"text-align: right;\">      2.1103  </td><td style=\"text-align: right;\"> 1689715964</td><td style=\"text-align: right;\">                   4</td><td>db705091  </td></tr>\n",
       "<tr><td>FSR_Trainable_dd9338d0</td><td>2023-07-19_06-36-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 53.4434</td><td style=\"text-align: right;\">9.52217e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">463495</td><td style=\"text-align: right;\">149.484</td><td style=\"text-align: right;\">            1.18508 </td><td style=\"text-align: right;\">          0.454991</td><td style=\"text-align: right;\">      1.18508 </td><td style=\"text-align: right;\"> 1689716217</td><td style=\"text-align: right;\">                   2</td><td>dd9338d0  </td></tr>\n",
       "<tr><td>FSR_Trainable_deb042fa</td><td>2023-07-19_06-37-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 40.197 </td><td style=\"text-align: right;\">6.63481e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">464294</td><td style=\"text-align: right;\">110.716</td><td style=\"text-align: right;\">           18.0662  </td><td style=\"text-align: right;\">          0.335849</td><td style=\"text-align: right;\">     18.0662  </td><td style=\"text-align: right;\"> 1689716272</td><td style=\"text-align: right;\">                  64</td><td>deb042fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_e1831fa4</td><td>2023-07-19_06-28-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 69.4363</td><td style=\"text-align: right;\">3.3769e+15 </td><td>172.26.215.93</td><td style=\"text-align: right;\">450886</td><td style=\"text-align: right;\">256.772</td><td style=\"text-align: right;\">            0.393864</td><td style=\"text-align: right;\">          0.393864</td><td style=\"text-align: right;\">      0.393864</td><td style=\"text-align: right;\"> 1689715727</td><td style=\"text-align: right;\">                   1</td><td>e1831fa4  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2113b0a</td><td>2023-07-19_06-31-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">117.804 </td><td style=\"text-align: right;\">1.77532e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">455690</td><td style=\"text-align: right;\">310.174</td><td style=\"text-align: right;\">            0.811226</td><td style=\"text-align: right;\">          0.811226</td><td style=\"text-align: right;\">      0.811226</td><td style=\"text-align: right;\"> 1689715915</td><td style=\"text-align: right;\">                   1</td><td>e2113b0a  </td></tr>\n",
       "<tr><td>FSR_Trainable_e5a6e2a8</td><td>2023-07-19_06-32-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 69.4252</td><td style=\"text-align: right;\">1.43072e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">457346</td><td style=\"text-align: right;\">214.393</td><td style=\"text-align: right;\">            0.578103</td><td style=\"text-align: right;\">          0.578103</td><td style=\"text-align: right;\">      0.578103</td><td style=\"text-align: right;\"> 1689715977</td><td style=\"text-align: right;\">                   1</td><td>e5a6e2a8  </td></tr>\n",
       "<tr><td>FSR_Trainable_eb2d32e6</td><td>2023-07-19_06-33-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 67.856 </td><td style=\"text-align: right;\">1.12195e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">457760</td><td style=\"text-align: right;\">190.203</td><td style=\"text-align: right;\">            0.902375</td><td style=\"text-align: right;\">          0.902375</td><td style=\"text-align: right;\">      0.902375</td><td style=\"text-align: right;\"> 1689715991</td><td style=\"text-align: right;\">                   1</td><td>eb2d32e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_ebf2d36a</td><td>2023-07-19_06-31-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 41.5997</td><td style=\"text-align: right;\">7.38711e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">454862</td><td style=\"text-align: right;\">105.791</td><td style=\"text-align: right;\">            1.80399 </td><td style=\"text-align: right;\">          0.314596</td><td style=\"text-align: right;\">      1.80399 </td><td style=\"text-align: right;\"> 1689715888</td><td style=\"text-align: right;\">                   4</td><td>ebf2d36a  </td></tr>\n",
       "<tr><td>FSR_Trainable_efdc8a31</td><td>2023-07-19_06-34-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 58.3147</td><td style=\"text-align: right;\">1.69722e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">460162</td><td style=\"text-align: right;\">216.857</td><td style=\"text-align: right;\">            0.559785</td><td style=\"text-align: right;\">          0.559785</td><td style=\"text-align: right;\">      0.559785</td><td style=\"text-align: right;\"> 1689716081</td><td style=\"text-align: right;\">                   1</td><td>efdc8a31  </td></tr>\n",
       "<tr><td>FSR_Trainable_f0493a55</td><td>2023-07-19_06-39-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 41.7302</td><td style=\"text-align: right;\">2.97871e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">467081</td><td style=\"text-align: right;\">116.996</td><td style=\"text-align: right;\">            0.944005</td><td style=\"text-align: right;\">          0.275492</td><td style=\"text-align: right;\">      0.944005</td><td style=\"text-align: right;\"> 1689716366</td><td style=\"text-align: right;\">                   2</td><td>f0493a55  </td></tr>\n",
       "<tr><td>FSR_Trainable_f26eec80</td><td>2023-07-19_06-31-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 42.6605</td><td style=\"text-align: right;\">7.85742e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">454766</td><td style=\"text-align: right;\">109.147</td><td style=\"text-align: right;\">            1.75674 </td><td style=\"text-align: right;\">          0.312333</td><td style=\"text-align: right;\">      1.75674 </td><td style=\"text-align: right;\"> 1689715881</td><td style=\"text-align: right;\">                   4</td><td>f26eec80  </td></tr>\n",
       "<tr><td>FSR_Trainable_f337be85</td><td>2023-07-19_06-37-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 59.0832</td><td style=\"text-align: right;\">9.82473e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">463687</td><td style=\"text-align: right;\">188.042</td><td style=\"text-align: right;\">            1.08162 </td><td style=\"text-align: right;\">          1.08162 </td><td style=\"text-align: right;\">      1.08162 </td><td style=\"text-align: right;\"> 1689716222</td><td style=\"text-align: right;\">                   1</td><td>f337be85  </td></tr>\n",
       "<tr><td>FSR_Trainable_f47854d8</td><td>2023-07-19_06-38-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">129.18  </td><td style=\"text-align: right;\">1.85561e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">465840</td><td style=\"text-align: right;\">373.93 </td><td style=\"text-align: right;\">            0.376785</td><td style=\"text-align: right;\">          0.376785</td><td style=\"text-align: right;\">      0.376785</td><td style=\"text-align: right;\"> 1689716312</td><td style=\"text-align: right;\">                   1</td><td>f47854d8  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd3f4683</td><td>2023-07-19_06-32-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 49.9291</td><td style=\"text-align: right;\">7.96993e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">456834</td><td style=\"text-align: right;\">146.403</td><td style=\"text-align: right;\">            1.15099 </td><td style=\"text-align: right;\">          0.380035</td><td style=\"text-align: right;\">      1.15099 </td><td style=\"text-align: right;\"> 1689715956</td><td style=\"text-align: right;\">                   2</td><td>fd3f4683  </td></tr>\n",
       "<tr><td>FSR_Trainable_fda56acc</td><td>2023-07-19_06-36-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 42.2299</td><td style=\"text-align: right;\">7.39759e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">462895</td><td style=\"text-align: right;\">111.489</td><td style=\"text-align: right;\">            3.57482 </td><td style=\"text-align: right;\">          0.392958</td><td style=\"text-align: right;\">      3.57482 </td><td style=\"text-align: right;\"> 1689716194</td><td style=\"text-align: right;\">                   8</td><td>fda56acc  </td></tr>\n",
       "<tr><td>FSR_Trainable_fdf72de3</td><td>2023-07-19_06-38-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 40.9284</td><td style=\"text-align: right;\">6.80617e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">464707</td><td style=\"text-align: right;\">108.558</td><td style=\"text-align: right;\">           10.1929  </td><td style=\"text-align: right;\">          0.325901</td><td style=\"text-align: right;\">     10.1929  </td><td style=\"text-align: right;\"> 1689716280</td><td style=\"text-align: right;\">                  32</td><td>fdf72de3  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_3657c235_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-27-55/wandb/run-20230719_062805-3657c235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Syncing run FSR_Trainable_3657c235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3657c235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_d1db0969_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-28-00/wandb/run-20230719_062815-d1db0969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Syncing run FSR_Trainable_d1db0969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d1db0969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_81e3fc24_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-28-07/wandb/run-20230719_062823-81e3fc24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Syncing run FSR_Trainable_81e3fc24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/81e3fc24\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                      mae ▇██▄▄▆▁▂▄▃▂▄▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                     mape ▅▆█▄▂█▁▂▅▃▂▄▅▄▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                     rmse ▅██▄▅▅▁▁▃▃▁▃▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▃▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:         time_this_iter_s ▆▂▂▂▁▁▁▂▆█▆▂▃▂▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▃▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                timestamp ▁▂▂▂▃▃▃▃▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                      mae 65.97132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                     mape 1.1972571508916152e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                     rmse 179.6916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:       time_since_restore 14.49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:         time_this_iter_s 0.86195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:             time_total_s 14.49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:                timestamp 1689715701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: 🚀 View run FSR_Trainable_3657c235 at: https://wandb.ai/seokjin/FSR-prediction/runs/3657c235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=449951)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062805-3657c235/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062823-81e3fc24/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450310)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_02b0913f_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-28-17/wandb/run-20230719_062831-02b0913f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Syncing run FSR_Trainable_02b0913f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/02b0913f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                      mae █▃▁▁▁▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                     mape ▂▁▁▂▃▅▅▆▆▇▇▇▇▇▇▇▇▇██████████████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                     rmse █▃▁▁▁▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:         time_this_iter_s ▇▃▄▇▆▄▄█▃▃▂▅▄▂▄▆▃▂▄▁▄▃▃▁▃▂▁▃▄▅▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                timestamp ▁▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                      mae 51.02642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                     mape 36413058.55425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                     rmse 153.00591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:       time_since_restore 7.88485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:         time_this_iter_s 0.23596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:             time_total_s 7.88485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:                timestamp 1689715716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: 🚀 View run FSR_Trainable_02b0913f at: https://wandb.ai/seokjin/FSR-prediction/runs/02b0913f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450500)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062831-02b0913f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_26ca7713_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-28-25/wandb/run-20230719_062842-26ca7713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Syncing run FSR_Trainable_26ca7713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/26ca7713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                      mae 76.84066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                     mape 8.356379150695645e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                     rmse 244.55671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:       time_since_restore 1.32192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:         time_this_iter_s 1.32192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:             time_total_s 1.32192\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:                timestamp 1689715718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: 🚀 View run FSR_Trainable_26ca7713 at: https://wandb.ai/seokjin/FSR-prediction/runs/26ca7713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450753)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062842-26ca7713/logs\n",
      "2023-07-19 06:28:49,863\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.281 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:28:49,879\tWARNING util.py:315 -- The `process_trial_result` operation took 2.298 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:28:49,881\tWARNING util.py:315 -- Processing trial results took 2.300 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:28:49,883\tWARNING util.py:315 -- The `process_trial_result` operation took 2.302 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_e1831fa4_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-28-36/wandb/run-20230719_062852-e1831fa4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Syncing run FSR_Trainable_e1831fa4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e1831fa4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                      mae █▄▃▂▂▂▂▁▁▁▁▂▂▁▂▂▂▂▂▁▃▂▁▁▁▂▂▁▂▂▂▂▃▃▃▃▃▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                     mape █▆▅▄▄▄▃▃▂▂▂▃▃▁▃▂▃▃▂▂▃▂▁▁▁▁▂▁▂▂▂▃▃▄▃▃▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                     rmse █▂▁▁▁▁▁▁▁▁▁▂▂▂▃▂▃▃▃▃▃▃▂▃▃▄▃▃▄▄▄▄▅▅▅▆▆▆▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:         time_this_iter_s ▄▆█▂▂▃▃▄▃▃▂▂▁▃▁▇▃▃▂▃▃▃▃▂▂▄▄▅▃▄▂▂▂▁▁▂▁▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                      mae 40.7078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                     mape 6.3063406660500024e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                     rmse 119.48858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:       time_since_restore 29.96156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:         time_this_iter_s 0.32206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:             time_total_s 29.96156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:                timestamp 1689715727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: 🚀 View run FSR_Trainable_d1db0969 at: https://wandb.ai/seokjin/FSR-prediction/runs/d1db0969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450128)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062815-d1db0969/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062852-e1831fa4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062852-e1831fa4/logs\n",
      "2023-07-19 06:28:58,219\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.084 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:28:58,224\tWARNING util.py:315 -- The `process_trial_result` operation took 2.089 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:28:58,227\tWARNING util.py:315 -- Processing trial results took 2.092 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:28:58,228\tWARNING util.py:315 -- The `process_trial_result` operation took 2.093 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=450991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_a8d2b7da_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-28-47/wandb/run-20230719_062900-a8d2b7da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Syncing run FSR_Trainable_a8d2b7da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a8d2b7da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                      mae 63.89859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                     mape 1508513325841368.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                     rmse 221.4038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:       time_since_restore 0.72821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:         time_this_iter_s 0.72821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:             time_total_s 0.72821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:                timestamp 1689715736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: 🚀 View run FSR_Trainable_a8d2b7da at: https://wandb.ai/seokjin/FSR-prediction/runs/a8d2b7da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062900-a8d2b7da/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451226)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 06:29:06,419\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:06,424\tWARNING util.py:315 -- The `process_trial_result` operation took 2.263 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:06,430\tWARNING util.py:315 -- Processing trial results took 2.269 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:29:06,437\tWARNING util.py:315 -- The `process_trial_result` operation took 2.276 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_ba3a6850_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-28-55/wandb/run-20230719_062908-ba3a6850\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Syncing run FSR_Trainable_ba3a6850\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ba3a6850\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:29:12,963\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:12,966\tWARNING util.py:315 -- The `process_trial_result` operation took 2.286 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:12,969\tWARNING util.py:315 -- Processing trial results took 2.289 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:29:12,971\tWARNING util.py:315 -- The `process_trial_result` operation took 2.291 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                      mae 68.58018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                     mape 6065974950587275.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                     rmse 253.05534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:       time_since_restore 0.65507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:         time_this_iter_s 0.65507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:             time_total_s 0.65507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:                timestamp 1689715744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: 🚀 View run FSR_Trainable_ba3a6850 at: https://wandb.ai/seokjin/FSR-prediction/runs/ba3a6850\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451451)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062908-ba3a6850/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_8271662f_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_06-29-03/wandb/run-20230719_062915-8271662f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Syncing run FSR_Trainable_8271662f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8271662f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:29:19,612\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.461 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:19,618\tWARNING util.py:315 -- The `process_trial_result` operation took 2.467 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:19,619\tWARNING util.py:315 -- Processing trial results took 2.469 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:29:19,623\tWARNING util.py:315 -- The `process_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                      mae 65.46056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                     mape 43107709.13286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                     rmse 183.94463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:       time_since_restore 1.43384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:         time_this_iter_s 0.4359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:             time_total_s 1.43384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:                timestamp 1689715753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: 🚀 View run FSR_Trainable_8271662f at: https://wandb.ai/seokjin/FSR-prediction/runs/8271662f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451621)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062915-8271662f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_3dfe355c_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-29-09/wandb/run-20230719_062921-3dfe355c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Syncing run FSR_Trainable_3dfe355c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3dfe355c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:29:25,983\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.157 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:25,987\tWARNING util.py:315 -- The `process_trial_result` operation took 2.162 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:25,989\tWARNING util.py:315 -- Processing trial results took 2.164 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:29:25,991\tWARNING util.py:315 -- The `process_trial_result` operation took 2.166 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                      mae 64.71475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                     mape 3335701497982881.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                     rmse 230.44501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:       time_since_restore 0.65144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:         time_this_iter_s 0.65144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:             time_total_s 0.65144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:                timestamp 1689715757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: 🚀 View run FSR_Trainable_3dfe355c at: https://wandb.ai/seokjin/FSR-prediction/runs/3dfe355c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=451824)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062921-3dfe355c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_9b6f7a70_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-29-16/wandb/run-20230719_062928-9b6f7a70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Syncing run FSR_Trainable_9b6f7a70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9b6f7a70\n",
      "2023-07-19 06:29:35,742\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:35,747\tWARNING util.py:315 -- The `process_trial_result` operation took 2.343 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:35,750\tWARNING util.py:315 -- Processing trial results took 2.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:29:35,752\tWARNING util.py:315 -- The `process_trial_result` operation took 2.349 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_cd9cf579_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-29-23/wandb/run-20230719_062938-cd9cf579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Syncing run FSR_Trainable_cd9cf579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cd9cf579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                      mae 93.58954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                     mape 1.0065709476477803e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                     rmse 283.11902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:       time_since_restore 0.33277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:         time_this_iter_s 0.33277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:             time_total_s 0.33277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:                timestamp 1689715773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: 🚀 View run FSR_Trainable_cd9cf579 at: https://wandb.ai/seokjin/FSR-prediction/runs/cd9cf579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452244)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062938-cd9cf579/logs\n",
      "2023-07-19 06:29:45,812\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.463 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:45,816\tWARNING util.py:315 -- The `process_trial_result` operation took 2.469 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:45,817\tWARNING util.py:315 -- Processing trial results took 2.470 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:29:45,819\tWARNING util.py:315 -- The `process_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_a2fad63a_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-29-33/wandb/run-20230719_062948-a2fad63a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Syncing run FSR_Trainable_a2fad63a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a2fad63a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                      mae █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▂▂▂▂▁▂▂▂▂▂▁▂▁▂▁▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                     mape █▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▁▁▂▁▃▃▃▃▂▁▂▂▂▁▂▁▂▁▂▁▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                     rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▂▂▂▂▃▃▂▃▃▃▃▃▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:         time_this_iter_s █▃▂▃▃▂▁▂▂▂▁▁▁▂▁▁▂▁▂▂▃▂▂▂▁▁▂▁▁▂▂▄▁▁▁▁▄▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                      mae 40.99348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                     mape 6.550061304927984e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                     rmse 117.21134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:       time_since_restore 16.53557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:         time_this_iter_s 0.26882\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:             time_total_s 16.53557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:                timestamp 1689715787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: 🚀 View run FSR_Trainable_9b6f7a70 at: https://wandb.ai/seokjin/FSR-prediction/runs/9b6f7a70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062928-9b6f7a70/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-07-19 06:29:55,055\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.324 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:55,059\tWARNING util.py:315 -- The `process_trial_result` operation took 2.329 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:29:55,061\tWARNING util.py:315 -- Processing trial results took 2.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:29:55,063\tWARNING util.py:315 -- The `process_trial_result` operation took 2.333 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_85951629_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-29-42/wandb/run-20230719_062957-85951629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Syncing run FSR_Trainable_85951629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/85951629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                      mae ▁▁▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                     mape ▁▆█▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                     rmse ▂▁▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                      mae 47.96053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                     mape 36435387.77158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                     rmse 146.83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:       time_since_restore 1.30478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:         time_this_iter_s 0.19545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:             time_total_s 1.30478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:                timestamp 1689715795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: 🚀 View run FSR_Trainable_85951629 at: https://wandb.ai/seokjin/FSR-prediction/runs/85951629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_062957-85951629/logs\n",
      "2023-07-19 06:30:03,859\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.481 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:03,862\tWARNING util.py:315 -- The `process_trial_result` operation took 2.486 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:03,867\tWARNING util.py:315 -- Processing trial results took 2.491 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:30:03,869\tWARNING util.py:315 -- The `process_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452708)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_7ca7828d_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-29-52/wandb/run-20230719_063006-7ca7828d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Syncing run FSR_Trainable_7ca7828d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7ca7828d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:30:10,524\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.423 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:10,528\tWARNING util.py:315 -- The `process_trial_result` operation took 2.428 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:10,531\tWARNING util.py:315 -- Processing trial results took 2.431 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:30:10,533\tWARNING util.py:315 -- The `process_trial_result` operation took 2.433 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                      mae 62.98711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                     mape 1.066634315247044e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                     rmse 181.64857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:       time_since_restore 0.89648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:         time_this_iter_s 0.23449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:             time_total_s 0.89648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:                timestamp 1689715804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: 🚀 View run FSR_Trainable_7ca7828d at: https://wandb.ai/seokjin/FSR-prediction/runs/7ca7828d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063006-7ca7828d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=452937)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_d45936b5_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-30-00/wandb/run-20230719_063013-d45936b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: Syncing run FSR_Trainable_d45936b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d45936b5\n",
      "2023-07-19 06:30:18,022\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.640 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:18,026\tWARNING util.py:315 -- The `process_trial_result` operation took 2.645 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:18,029\tWARNING util.py:315 -- Processing trial results took 2.648 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:30:18,032\tWARNING util.py:315 -- The `process_trial_result` operation took 2.652 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_8cf05336_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-30-07/wandb/run-20230719_063021-8cf05336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Syncing run FSR_Trainable_8cf05336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8cf05336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:30:26,004\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.342 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:26,006\tWARNING util.py:315 -- The `process_trial_result` operation took 2.346 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:26,008\tWARNING util.py:315 -- Processing trial results took 2.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:30:26,011\tWARNING util.py:315 -- The `process_trial_result` operation took 2.351 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                      mae ▅▂▁▂▄▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                     mape ▃▂▁▃▄▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                     rmse █▃▁▂▃▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:         time_this_iter_s ▇▇▄▂▂▄█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                timestamp ▁▅▅▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                      mae 43.73505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                     mape 7.939911252141744e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                     rmse 112.27254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:       time_since_restore 2.98969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:         time_this_iter_s 0.30258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:             time_total_s 2.98969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:                timestamp 1689715820\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: 🚀 View run FSR_Trainable_8cf05336 at: https://wandb.ai/seokjin/FSR-prediction/runs/8cf05336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453311)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063021-8cf05336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_460ff324_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-30-14/wandb/run-20230719_063028-460ff324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Syncing run FSR_Trainable_460ff324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/460ff324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                      mae ▆▂▁▁▂▃▄▅▆▆▆▆▇█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                     mape ▃▂▁▂▃▄▅▆▆▆▇▆▇█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                     rmse █▂▁▁▂▂▃▄▅▅▅▅▆▆▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:         time_this_iter_s █▄▄▅▆▃▅▄▄▁▄▄▁▃▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                timestamp ▁▄▄▅▅▅▅▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                      mae 43.90989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                     mape 7.622518967768658e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                     rmse 114.09589\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:       time_since_restore 5.90353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:         time_this_iter_s 0.33908\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:             time_total_s 5.90353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:                timestamp 1689715831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: 🚀 View run FSR_Trainable_460ff324 at: https://wandb.ai/seokjin/FSR-prediction/runs/460ff324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453497)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063028-460ff324/logs\n",
      "2023-07-19 06:30:36,769\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.347 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:36,774\tWARNING util.py:315 -- The `process_trial_result` operation took 2.353 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:36,776\tWARNING util.py:315 -- Processing trial results took 2.355 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:30:36,778\tWARNING util.py:315 -- The `process_trial_result` operation took 2.357 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_6fa528d8_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-30-23/wandb/run-20230719_063039-6fa528d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Syncing run FSR_Trainable_6fa528d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6fa528d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                      mae 88.91894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                     mape 9.633538444443347e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                     rmse 295.06619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:       time_since_restore 0.48402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:         time_this_iter_s 0.48402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:             time_total_s 0.48402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:                timestamp 1689715834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: 🚀 View run FSR_Trainable_6fa528d8 at: https://wandb.ai/seokjin/FSR-prediction/runs/6fa528d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063039-6fa528d8/logs\n",
      "2023-07-19 06:30:47,037\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:47,042\tWARNING util.py:315 -- The `process_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:47,049\tWARNING util.py:315 -- Processing trial results took 2.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:30:47,052\tWARNING util.py:315 -- The `process_trial_result` operation took 2.348 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_323e9721_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-30-34/wandb/run-20230719_063049-323e9721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Syncing run FSR_Trainable_323e9721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/323e9721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:30:56,525\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.795 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:56,530\tWARNING util.py:315 -- The `process_trial_result` operation took 1.801 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:30:56,532\tWARNING util.py:315 -- Processing trial results took 1.803 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:30:56,535\tWARNING util.py:315 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                      mae 93.95223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                     mape 1.6561033714823414e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                     rmse 245.26485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:       time_since_restore 0.49337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:         time_this_iter_s 0.49337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:             time_total_s 0.49337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:                timestamp 1689715844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: 🚀 View run FSR_Trainable_323e9721 at: https://wandb.ai/seokjin/FSR-prediction/runs/323e9721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063049-323e9721/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453971)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:                      mae █▂▃▄▄▄▄▄▄▄▄▃▂▂▃▃▂▂▂▂▃▃▁▂▂▁▂▂▂▂▂▁▁▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:                     mape █▄▅▅▅▅▅▅▅▅▅▄▃▃▃▃▃▃▃▂▄▃▂▂▂▂▂▂▂▃▂▁▁▂▂▂▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:                     rmse █▁▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▂▁▂▂▁▂▂▂▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▂▂▃▃▃▂▁▃▄▄▃▂▂▃▁▁▂▄▂▂▂▃▁▁▃▂▁▂▃▃▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_4dc78491_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-30-44/wandb/run-20230719_063059-4dc78491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Syncing run FSR_Trainable_4dc78491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4dc78491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4dc78491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=453125)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                      mae █▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                     mape ▇▆▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: iterations_since_restore 4\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                      mae 43.57139\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                     mape 7.618559534837334e+16\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                     rmse 120.43239\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:       time_since_restore 1.91073\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:         time_this_iter_s 0.37059\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:             time_total_s 1.91073\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:                timestamp 1689715857\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb:       training_iteration 4\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: 🚀 View run FSR_Trainable_4dc78491 at: https://wandb.ai/seokjin/FSR-prediction/runs/4dc78491\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063059-4dc78491/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454208)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063059-4dc78491/logs\n",
      "2023-07-19 06:31:05,396\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:05,400\tWARNING util.py:315 -- The `process_trial_result` operation took 2.256 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:05,404\tWARNING util.py:315 -- Processing trial results took 2.259 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:05,405\tWARNING util.py:315 -- The `process_trial_result` operation took 2.261 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_d192c08f_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-30-53/wandb/run-20230719_063107-d192c08f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Syncing run FSR_Trainable_d192c08f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d192c08f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:31:11,956\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.140 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:11,964\tWARNING util.py:315 -- The `process_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:11,966\tWARNING util.py:315 -- Processing trial results took 2.150 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:11,968\tWARNING util.py:315 -- The `process_trial_result` operation took 2.153 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                      mae 48.63248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                     mape 14641232.86355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                     rmse 157.74379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:       time_since_restore 1.06704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:         time_this_iter_s 0.32108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:             time_total_s 1.06704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:                timestamp 1689715865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: 🚀 View run FSR_Trainable_d192c08f at: https://wandb.ai/seokjin/FSR-prediction/runs/d192c08f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454448)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063107-d192c08f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_2eb64ca4_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-31-02/wandb/run-20230719_063114-2eb64ca4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Syncing run FSR_Trainable_2eb64ca4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2eb64ca4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:                      mae █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:                     mape █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:         time_this_iter_s █▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454631)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063114-2eb64ca4/logs\n",
      "2023-07-19 06:31:20,831\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.493 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:20,835\tWARNING util.py:315 -- The `process_trial_result` operation took 2.497 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:20,837\tWARNING util.py:315 -- Processing trial results took 2.499 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:20,840\tWARNING util.py:315 -- The `process_trial_result` operation took 2.502 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_f26eec80_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-31-09/wandb/run-20230719_063123-f26eec80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Syncing run FSR_Trainable_f26eec80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f26eec80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:31:27,431\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:27,436\tWARNING util.py:315 -- The `process_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:27,438\tWARNING util.py:315 -- Processing trial results took 2.031 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:27,440\tWARNING util.py:315 -- The `process_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                      mae █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                     mape █▃▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                     rmse █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                      mae 42.66045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                     mape 7.857422594699486e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                     rmse 109.14728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:       time_since_restore 1.75674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:         time_this_iter_s 0.31233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:             time_total_s 1.75674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:                timestamp 1689715881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: 🚀 View run FSR_Trainable_f26eec80 at: https://wandb.ai/seokjin/FSR-prediction/runs/f26eec80\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=454861)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063123-f26eec80/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_ebf2d36a_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-31-17/wandb/run-20230719_063129-ebf2d36a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Syncing run FSR_Trainable_ebf2d36a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ebf2d36a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                      mae █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                     mape █▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                      mae 41.59974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                     mape 7.387106435956814e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                     rmse 105.79069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:       time_since_restore 1.80399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:         time_this_iter_s 0.3146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:             time_total_s 1.80399\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:                timestamp 1689715888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: 🚀 View run FSR_Trainable_ebf2d36a at: https://wandb.ai/seokjin/FSR-prediction/runs/ebf2d36a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455045)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063129-ebf2d36a/logs\n",
      "2023-07-19 06:31:36,307\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.274 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:36,311\tWARNING util.py:315 -- The `process_trial_result` operation took 2.280 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:36,313\tWARNING util.py:315 -- Processing trial results took 2.281 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:36,315\tWARNING util.py:315 -- The `process_trial_result` operation took 2.283 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "wandb: / Waiting for wandb.init()...275)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_4c312fa1_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-31-24/wandb/run-20230719_063138-4c312fa1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Syncing run FSR_Trainable_4c312fa1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4c312fa1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 06:31:42,592\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:42,596\tWARNING util.py:315 -- The `process_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:42,597\tWARNING util.py:315 -- Processing trial results took 2.067 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:42,599\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                      mae 48.65376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                     mape 8.361875341434618e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                     rmse 131.19245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:       time_since_restore 1.26627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:         time_this_iter_s 0.38716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:             time_total_s 1.26627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:                timestamp 1689715896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: 🚀 View run FSR_Trainable_4c312fa1 at: https://wandb.ai/seokjin/FSR-prediction/runs/4c312fa1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455275)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063138-4c312fa1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: \n",
      "2023-07-19 06:31:51,309\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.434 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:51,313\tWARNING util.py:315 -- The `process_trial_result` operation took 2.440 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:51,316\tWARNING util.py:315 -- Processing trial results took 2.442 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:51,318\tWARNING util.py:315 -- The `process_trial_result` operation took 2.445 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455440)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_09d33090_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-31-39/wandb/run-20230719_063153-09d33090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Syncing run FSR_Trainable_09d33090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/09d33090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:31:57,781\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.168 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:57,785\tWARNING util.py:315 -- The `process_trial_result` operation took 2.173 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:31:57,787\tWARNING util.py:315 -- Processing trial results took 2.174 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:31:57,788\tWARNING util.py:315 -- The `process_trial_result` operation took 2.176 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                      mae 54.52452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                     mape 9.116449371270758e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                     rmse 145.04151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:       time_since_restore 1.19361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:         time_this_iter_s 0.41215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:             time_total_s 1.19361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:                timestamp 1689715911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: 🚀 View run FSR_Trainable_09d33090 at: https://wandb.ai/seokjin/FSR-prediction/runs/09d33090\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063153-09d33090/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455689)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_e2113b0a_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-31-48/wandb/run-20230719_063200-e2113b0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Syncing run FSR_Trainable_e2113b0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2113b0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063200-e2113b0a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=455873)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 06:32:06,162\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:06,164\tWARNING util.py:315 -- The `process_trial_result` operation took 2.415 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:06,170\tWARNING util.py:315 -- Processing trial results took 2.420 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:06,172\tWARNING util.py:315 -- The `process_trial_result` operation took 2.422 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_65ca294c_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-31-54/wandb/run-20230719_063208-65ca294c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Syncing run FSR_Trainable_65ca294c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/65ca294c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:32:12,567\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:12,574\tWARNING util.py:315 -- The `process_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:12,575\tWARNING util.py:315 -- Processing trial results took 1.984 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:12,579\tWARNING util.py:315 -- The `process_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                      mae 85.94786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                     mape 49812167.7323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                     rmse 250.68439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:       time_since_restore 0.6527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:         time_this_iter_s 0.6527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:             time_total_s 0.6527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:                timestamp 1689715923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: 🚀 View run FSR_Trainable_65ca294c at: https://wandb.ai/seokjin/FSR-prediction/runs/65ca294c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456102)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063208-65ca294c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_5bccc8eb_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-03/wandb/run-20230719_063215-5bccc8eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Syncing run FSR_Trainable_5bccc8eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5bccc8eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456284)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063215-5bccc8eb/logs\n",
      "2023-07-19 06:32:21,294\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.446 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:21,296\tWARNING util.py:315 -- The `process_trial_result` operation took 2.450 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:21,298\tWARNING util.py:315 -- Processing trial results took 2.451 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:21,301\tWARNING util.py:315 -- The `process_trial_result` operation took 2.454 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_4e4c2fa2_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-09/wandb/run-20230719_063223-4e4c2fa2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Syncing run FSR_Trainable_4e4c2fa2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4e4c2fa2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:32:27,787\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:27,789\tWARNING util.py:315 -- The `process_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:27,791\tWARNING util.py:315 -- Processing trial results took 2.038 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:27,792\tWARNING util.py:315 -- The `process_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                      mae █▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                     mape ▁▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                     rmse █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:         time_this_iter_s █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                      mae 39.21631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                     mape 1.5859312364876138e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                     rmse 116.71677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:       time_since_restore 1.59686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:         time_this_iter_s 0.32251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:             time_total_s 1.59686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:                timestamp 1689715942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: 🚀 View run FSR_Trainable_4e4c2fa2 at: https://wandb.ai/seokjin/FSR-prediction/runs/4e4c2fa2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456514)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063223-4e4c2fa2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_7e7f666f_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-18/wandb/run-20230719_063230-7e7f666f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Syncing run FSR_Trainable_7e7f666f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7e7f666f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456697)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063230-7e7f666f/logs\n",
      "2023-07-19 06:32:36,500\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.364 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:36,504\tWARNING util.py:315 -- The `process_trial_result` operation took 2.369 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:36,506\tWARNING util.py:315 -- Processing trial results took 2.371 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:36,509\tWARNING util.py:315 -- The `process_trial_result` operation took 2.374 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_fd3f4683_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-25/wandb/run-20230719_063239-fd3f4683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Syncing run FSR_Trainable_fd3f4683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd3f4683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:32:43,091\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.927 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:43,095\tWARNING util.py:315 -- The `process_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:43,096\tWARNING util.py:315 -- Processing trial results took 1.932 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:43,098\tWARNING util.py:315 -- The `process_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                      mae 49.92905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                     mape 7.969933436906666e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                     rmse 146.40329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:       time_since_restore 1.15099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:         time_this_iter_s 0.38004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:             time_total_s 1.15099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:                timestamp 1689715956\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: 🚀 View run FSR_Trainable_fd3f4683 at: https://wandb.ai/seokjin/FSR-prediction/runs/fd3f4683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=456928)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063239-fd3f4683/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_db705091_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-33/wandb/run-20230719_063245-db705091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Syncing run FSR_Trainable_db705091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/db705091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                      mae █▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                     mape ▆▁█▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                     rmse █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:         time_this_iter_s █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                      mae 41.29439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                     mape 6.904050461243967e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                     rmse 115.6576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:       time_since_restore 2.1103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:         time_this_iter_s 0.36916\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:             time_total_s 2.1103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:                timestamp 1689715964\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: 🚀 View run FSR_Trainable_db705091 at: https://wandb.ai/seokjin/FSR-prediction/runs/db705091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457112)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063245-db705091/logs\n",
      "2023-07-19 06:32:52,910\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.263 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:52,919\tWARNING util.py:315 -- The `process_trial_result` operation took 2.272 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:52,921\tWARNING util.py:315 -- Processing trial results took 2.274 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:52,928\tWARNING util.py:315 -- The `process_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_c6df9388_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-40/wandb/run-20230719_063255-c6df9388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Syncing run FSR_Trainable_c6df9388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c6df9388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:32:59,325\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:59,329\tWARNING util.py:315 -- The `process_trial_result` operation took 2.090 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:32:59,332\tWARNING util.py:315 -- Processing trial results took 2.093 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:32:59,333\tWARNING util.py:315 -- The `process_trial_result` operation took 2.094 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                      mae █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                     mape █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                      mae 41.12502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                     mape 7.1022230554359896e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                     rmse 110.64695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:       time_since_restore 1.97575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:         time_this_iter_s 0.3822\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:             time_total_s 1.97575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:                timestamp 1689715974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: 🚀 View run FSR_Trainable_c6df9388 at: https://wandb.ai/seokjin/FSR-prediction/runs/c6df9388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457344)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063255-c6df9388/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_e5a6e2a8_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-49/wandb/run-20230719_063301-e5a6e2a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Syncing run FSR_Trainable_e5a6e2a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e5a6e2a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:33:07,686\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.358 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:07,691\tWARNING util.py:315 -- The `process_trial_result` operation took 2.364 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:07,693\tWARNING util.py:315 -- Processing trial results took 2.366 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:33:07,695\tWARNING util.py:315 -- The `process_trial_result` operation took 2.368 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                      mae 69.42518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                     mape 1.4307241902257854e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                     rmse 214.39268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:       time_since_restore 0.5781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:         time_this_iter_s 0.5781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:             time_total_s 0.5781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:                timestamp 1689715977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: 🚀 View run FSR_Trainable_e5a6e2a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/e5a6e2a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457530)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063301-e5a6e2a8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_2864a2be_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-32-56/wandb/run-20230719_063309-2864a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Syncing run FSR_Trainable_2864a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2864a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:33:14,329\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.341 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:14,334\tWARNING util.py:315 -- The `process_trial_result` operation took 2.347 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:14,336\tWARNING util.py:315 -- Processing trial results took 2.349 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:33:14,338\tWARNING util.py:315 -- The `process_trial_result` operation took 2.350 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                      mae 110.57244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                     mape 1.461277616210846e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                     rmse 330.86335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:       time_since_restore 0.67898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:         time_this_iter_s 0.67898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:             time_total_s 0.67898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:                timestamp 1689715985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: 🚀 View run FSR_Trainable_2864a2be at: https://wandb.ai/seokjin/FSR-prediction/runs/2864a2be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457759)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063309-2864a2be/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_eb2d32e6_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-04/wandb/run-20230719_063316-eb2d32e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Syncing run FSR_Trainable_eb2d32e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/eb2d32e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=457938)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063316-eb2d32e6/logs\n",
      "2023-07-19 06:33:23,036\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.273 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:23,040\tWARNING util.py:315 -- The `process_trial_result` operation took 2.278 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:23,042\tWARNING util.py:315 -- Processing trial results took 2.279 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:33:23,045\tWARNING util.py:315 -- The `process_trial_result` operation took 2.282 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_2449d287_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-11/wandb/run-20230719_063325-2449d287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Syncing run FSR_Trainable_2449d287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2449d287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:33:30,084\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.190 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:30,089\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:30,092\tWARNING util.py:315 -- Processing trial results took 2.199 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:33:30,095\tWARNING util.py:315 -- The `process_trial_result` operation took 2.202 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                      mae 96.44698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                     mape 2.0045929532072275e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                     rmse 290.64395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:       time_since_restore 0.77455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:         time_this_iter_s 0.77455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:             time_total_s 0.77455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:                timestamp 1689716000\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: 🚀 View run FSR_Trainable_2449d287 at: https://wandb.ai/seokjin/FSR-prediction/runs/2449d287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458173)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063325-2449d287/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_b8ae8cd2_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-20/wandb/run-20230719_063332-b8ae8cd2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Syncing run FSR_Trainable_b8ae8cd2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b8ae8cd2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                      mae 78.92258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                     mape 35828643.20668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                     rmse 225.06797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:       time_since_restore 1.04593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:         time_this_iter_s 1.04593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:             time_total_s 1.04593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:                timestamp 1689716007\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: 🚀 View run FSR_Trainable_b8ae8cd2 at: https://wandb.ai/seokjin/FSR-prediction/runs/b8ae8cd2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458356)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063332-b8ae8cd2/logs\n",
      "2023-07-19 06:33:39,023\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.423 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:39,027\tWARNING util.py:315 -- The `process_trial_result` operation took 2.428 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:39,029\tWARNING util.py:315 -- Processing trial results took 2.430 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:33:39,031\tWARNING util.py:315 -- The `process_trial_result` operation took 2.432 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_ce487b52_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-26/wandb/run-20230719_063341-ce487b52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Syncing run FSR_Trainable_ce487b52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ce487b52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:33:45,497\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.969 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:45,499\tWARNING util.py:315 -- The `process_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:45,503\tWARNING util.py:315 -- Processing trial results took 1.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:33:45,506\tWARNING util.py:315 -- The `process_trial_result` operation took 1.978 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                      mae 68.93675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                     mape 52771039.88191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                     rmse 213.66298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:       time_since_restore 1.26541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:         time_this_iter_s 0.31013\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:             time_total_s 1.26541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:                timestamp 1689716019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: 🚀 View run FSR_Trainable_ce487b52 at: https://wandb.ai/seokjin/FSR-prediction/runs/ce487b52\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458584)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063341-ce487b52/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_d66a9693_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-35/wandb/run-20230719_063348-d66a9693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Syncing run FSR_Trainable_d66a9693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d66a9693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                      mae █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                     mape ▇▁▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:         time_this_iter_s █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                      mae 41.26899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                     mape 6.977478034263881e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                     rmse 116.07789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:       time_since_restore 1.88156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:         time_this_iter_s 0.39436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:             time_total_s 1.88156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:                timestamp 1689716026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: 🚀 View run FSR_Trainable_d66a9693 at: https://wandb.ai/seokjin/FSR-prediction/runs/d66a9693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=458771)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063348-d66a9693/logs\n",
      "2023-07-19 06:33:54,823\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.327 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:54,826\tWARNING util.py:315 -- The `process_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:33:54,829\tWARNING util.py:315 -- Processing trial results took 2.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:33:54,831\tWARNING util.py:315 -- The `process_trial_result` operation took 2.337 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_842abfbc_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-42/wandb/run-20230719_063357-842abfbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Syncing run FSR_Trainable_842abfbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/842abfbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:34:01,873\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.119 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:01,877\tWARNING util.py:315 -- The `process_trial_result` operation took 2.123 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:01,880\tWARNING util.py:315 -- Processing trial results took 2.126 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:01,881\tWARNING util.py:315 -- The `process_trial_result` operation took 2.128 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                      mae █▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                     mape █▁▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                     rmse █▃▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:         time_this_iter_s █▄▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                      mae 43.28395\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                     mape 7.1468697278054264e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                     rmse 120.8651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:       time_since_restore 2.29409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:         time_this_iter_s 0.3636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:             time_total_s 2.29409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:                timestamp 1689716036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: 🚀 View run FSR_Trainable_842abfbc at: https://wandb.ai/seokjin/FSR-prediction/runs/842abfbc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459000)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063357-842abfbc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_ba38fa3e_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-51/wandb/run-20230719_063404-ba38fa3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Syncing run FSR_Trainable_ba38fa3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ba38fa3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                      mae ▄▁▁▂▄▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                     mape ▃▁▂▃▅▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                     rmse ▇▃▁▁▃▄▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:         time_this_iter_s █▂▁▂▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                timestamp ▁▅▅▅▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                      mae 46.19741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                     mape 8.400944420750802e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                     rmse 118.80958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:       time_since_restore 2.94611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:         time_this_iter_s 0.28583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:             time_total_s 2.94611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:                timestamp 1689716044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: 🚀 View run FSR_Trainable_ba38fa3e at: https://wandb.ai/seokjin/FSR-prediction/runs/ba38fa3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459185)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063404-ba38fa3e/logs\n",
      "2023-07-19 06:34:11,033\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.275 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:11,036\tWARNING util.py:315 -- The `process_trial_result` operation took 2.279 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:11,039\tWARNING util.py:315 -- Processing trial results took 2.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:11,041\tWARNING util.py:315 -- The `process_trial_result` operation took 2.284 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_71119e56_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-33-59/wandb/run-20230719_063413-71119e56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Syncing run FSR_Trainable_71119e56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/71119e56\n",
      "2023-07-19 06:34:18,680\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.531 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:18,683\tWARNING util.py:315 -- The `process_trial_result` operation took 2.536 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:18,685\tWARNING util.py:315 -- Processing trial results took 2.537 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:18,687\tWARNING util.py:315 -- The `process_trial_result` operation took 2.539 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_04c07675_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-34-07/wandb/run-20230719_063421-04c07675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Syncing run FSR_Trainable_04c07675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/04c07675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                      mae ▂▁▁▂▁▁▁▂▃▅▆▆████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                     mape ▁▁▁▂▂▃▃▄▅▆▆▇███▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                     rmse ▅▃▃▃▁▁▁▂▃▅▆▆████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▁▂▁▁▂▁▁▃▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                timestamp ▁▃▃▄▄▄▄▄▅▅▅▅▅▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                      mae 46.7858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                     mape 8.468323001996608e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                     rmse 119.76442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:       time_since_restore 7.12217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:         time_this_iter_s 0.45545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:             time_total_s 7.12217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:                timestamp 1689716059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: 🚀 View run FSR_Trainable_71119e56 at: https://wandb.ai/seokjin/FSR-prediction/runs/71119e56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063413-71119e56/logs\n",
      "2023-07-19 06:34:26,381\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.128 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:26,385\tWARNING util.py:315 -- The `process_trial_result` operation took 2.132 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:26,386\tWARNING util.py:315 -- Processing trial results took 2.133 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:26,387\tWARNING util.py:315 -- The `process_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_12e0355e_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-34-15/wandb/run-20230719_063429-12e0355e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Syncing run FSR_Trainable_12e0355e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/12e0355e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                      mae ▆▃▁▁▂▂▃▄▅▅▆▇▇█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                     mape ▅▃▁▂▃▄▅▅▆▆▆▇▇█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                     rmse ▆▃▂▁▁▁▂▃▄▄▅▇▇█▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:         time_this_iter_s █▆▆▆▄▃▃▃▃▄▁▁▆▇▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                timestamp ▁▃▃▄▄▄▄▄▅▅▅▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                      mae 45.54731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                     mape 8.295205386545381e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                     rmse 117.4186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:       time_since_restore 7.40221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:         time_this_iter_s 0.52957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:             time_total_s 7.40221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:                timestamp 1689716067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: 🚀 View run FSR_Trainable_04c07675 at: https://wandb.ai/seokjin/FSR-prediction/runs/04c07675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459600)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063421-04c07675/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:                      mae ▃▂▁▂▃▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:                     mape ▂▁▁▂▄▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:                     rmse ▇▄▁▁▂▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:         time_this_iter_s █▃▅▅▃▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:                timestamp ▁▄▅▅▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "2023-07-19 06:34:35,916\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:35,920\tWARNING util.py:315 -- The `process_trial_result` operation took 1.980 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:35,922\tWARNING util.py:315 -- Processing trial results took 1.981 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:35,923\tWARNING util.py:315 -- The `process_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=459789)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_b7b45e41_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-34-23/wandb/run-20230719_063438-b7b45e41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Syncing run FSR_Trainable_b7b45e41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b7b45e41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                      mae 56.05606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                     mape 1.703618661263487e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                     rmse 176.67492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:       time_since_restore 0.77182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:         time_this_iter_s 0.77182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:             time_total_s 0.77182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:                timestamp 1689716073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: 🚀 View run FSR_Trainable_b7b45e41 at: https://wandb.ai/seokjin/FSR-prediction/runs/b7b45e41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063438-b7b45e41/logs\n",
      "2023-07-19 06:34:44,338\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.489 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:44,342\tWARNING util.py:315 -- The `process_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:44,344\tWARNING util.py:315 -- Processing trial results took 2.497 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:44,346\tWARNING util.py:315 -- The `process_trial_result` operation took 2.498 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460030)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_efdc8a31_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-34-33/wandb/run-20230719_063446-efdc8a31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Syncing run FSR_Trainable_efdc8a31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/efdc8a31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:34:50,796\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.372 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:50,801\tWARNING util.py:315 -- The `process_trial_result` operation took 2.378 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:50,803\tWARNING util.py:315 -- Processing trial results took 2.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:50,804\tWARNING util.py:315 -- The `process_trial_result` operation took 2.382 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                      mae 58.31467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                     mape 1.6972202991599268e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                     rmse 216.85743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:       time_since_restore 0.55979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:         time_this_iter_s 0.55979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:             time_total_s 0.55979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:                timestamp 1689716081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: 🚀 View run FSR_Trainable_efdc8a31 at: https://wandb.ai/seokjin/FSR-prediction/runs/efdc8a31\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063446-efdc8a31/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460251)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_5a8948ac_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-34-41/wandb/run-20230719_063453-5a8948ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Syncing run FSR_Trainable_5a8948ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a8948ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:34:58,293\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.307 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:58,298\tWARNING util.py:315 -- The `process_trial_result` operation took 2.313 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:34:58,299\tWARNING util.py:315 -- Processing trial results took 2.314 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:34:58,302\tWARNING util.py:315 -- The `process_trial_result` operation took 2.317 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                      mae █▁▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                     mape █▁▁▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                     rmse █▁▁▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:         time_this_iter_s █▄▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                      mae 46.31118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                     mape 8.203312537648224e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                     rmse 124.50458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:       time_since_restore 1.6488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:         time_this_iter_s 0.24542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:             time_total_s 1.6488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:                timestamp 1689716091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: 🚀 View run FSR_Trainable_5a8948ac at: https://wandb.ai/seokjin/FSR-prediction/runs/5a8948ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460437)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063453-5a8948ac/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_b0892721_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-34-47/wandb/run-20230719_063500-b0892721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Syncing run FSR_Trainable_b0892721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b0892721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:35:04,677\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:04,680\tWARNING util.py:315 -- The `process_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:04,681\tWARNING util.py:315 -- Processing trial results took 1.976 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:04,682\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                      mae 97.90664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                     mape 2.0986423510677987e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                     rmse 256.46872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:       time_since_restore 1.336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:         time_this_iter_s 1.336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:             time_total_s 1.336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:                timestamp 1689716095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: 🚀 View run FSR_Trainable_b0892721 at: https://wandb.ai/seokjin/FSR-prediction/runs/b0892721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460625)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063500-b0892721/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_cc0a970b_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-34-54/wandb/run-20230719_063507-cc0a970b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Syncing run FSR_Trainable_cc0a970b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc0a970b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                      mae █▃▂▁▁▃▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                     mape ▅▁▃▂▃▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                     rmse █▄▂▁▁▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:         time_this_iter_s █▂▂▂▃▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                timestamp ▁▄▅▅▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                      mae 43.38055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                     mape 7.990182710482808e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                     rmse 110.07919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:       time_since_restore 3.09951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:         time_this_iter_s 0.25892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:             time_total_s 3.09951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:                timestamp 1689716107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: 🚀 View run FSR_Trainable_cc0a970b at: https://wandb.ai/seokjin/FSR-prediction/runs/cc0a970b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=460812)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063507-cc0a970b/logs\n",
      "2023-07-19 06:35:13,646\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:13,650\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:13,651\tWARNING util.py:315 -- Processing trial results took 2.069 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:13,653\tWARNING util.py:315 -- The `process_trial_result` operation took 2.071 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_7ef58eae_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-35-01/wandb/run-20230719_063516-7ef58eae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Syncing run FSR_Trainable_7ef58eae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7ef58eae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                      mae ▄▁▁▃▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                     mape ▁▁▁▄▆███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                     rmse █▂▁▁▂▄▆▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:         time_this_iter_s █▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                timestamp ▁▅▆▆▆███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                      mae 43.97885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                     mape 7.901307926603829e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                     rmse 113.874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:       time_since_restore 2.89146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:         time_this_iter_s 0.28214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:             time_total_s 2.89146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:                timestamp 1689716115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: 🚀 View run FSR_Trainable_7ef58eae at: https://wandb.ai/seokjin/FSR-prediction/runs/7ef58eae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063516-7ef58eae/logs\n",
      "2023-07-19 06:35:22,679\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.357 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:22,683\tWARNING util.py:315 -- The `process_trial_result` operation took 2.361 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:22,684\tWARNING util.py:315 -- Processing trial results took 2.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:22,685\tWARNING util.py:315 -- The `process_trial_result` operation took 2.364 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_c58472f9_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-35-10/wandb/run-20230719_063525-c58472f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Syncing run FSR_Trainable_c58472f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c58472f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:35:29,716\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:29,719\tWARNING util.py:315 -- The `process_trial_result` operation took 2.499 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:29,721\tWARNING util.py:315 -- Processing trial results took 2.502 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:29,723\tWARNING util.py:315 -- The `process_trial_result` operation took 2.503 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                      mae █▁▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                     mape █▁▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                     rmse █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:         time_this_iter_s █▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                timestamp ▁███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                      mae 42.54824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                     mape 7.663396458143086e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                     rmse 108.85563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:       time_since_restore 1.75944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:         time_this_iter_s 0.37221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:             time_total_s 1.75944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:                timestamp 1689716123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: 🚀 View run FSR_Trainable_c58472f9 at: https://wandb.ai/seokjin/FSR-prediction/runs/c58472f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461271)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063525-c58472f9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_c1862fb9_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-35-19/wandb/run-20230719_063532-c1862fb9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Syncing run FSR_Trainable_c1862fb9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1862fb9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:35:36,526\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.488 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:36,531\tWARNING util.py:315 -- The `process_trial_result` operation took 2.495 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:36,535\tWARNING util.py:315 -- Processing trial results took 2.499 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:36,539\tWARNING util.py:315 -- The `process_trial_result` operation took 2.502 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461445)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063532-c1862fb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_c92621f2_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-35-26/wandb/run-20230719_063538-c92621f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Syncing run FSR_Trainable_c92621f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c92621f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:35:43,004\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:43,008\tWARNING util.py:315 -- The `process_trial_result` operation took 2.203 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:43,009\tWARNING util.py:315 -- Processing trial results took 2.204 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:43,013\tWARNING util.py:315 -- The `process_trial_result` operation took 2.208 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                      mae 102.83918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                     mape 1.1143643587788963e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                     rmse 320.02323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:       time_since_restore 0.69649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:         time_this_iter_s 0.69649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:             time_total_s 0.69649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:                timestamp 1689716134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: 🚀 View run FSR_Trainable_c92621f2 at: https://wandb.ai/seokjin/FSR-prediction/runs/c92621f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461646)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063538-c92621f2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_aac9dd0d_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-35-33/wandb/run-20230719_063545-aac9dd0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Syncing run FSR_Trainable_aac9dd0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/aac9dd0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=461831)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063545-aac9dd0d/logs\n",
      "2023-07-19 06:35:51,552\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.480 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:51,560\tWARNING util.py:315 -- The `process_trial_result` operation took 2.489 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:51,561\tWARNING util.py:315 -- Processing trial results took 2.490 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:51,563\tWARNING util.py:315 -- The `process_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_d17c72c7_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-35-40/wandb/run-20230719_063554-d17c72c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Syncing run FSR_Trainable_d17c72c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d17c72c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:35:59,436\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.356 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:59,440\tWARNING util.py:315 -- The `process_trial_result` operation took 2.360 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:35:59,442\tWARNING util.py:315 -- Processing trial results took 2.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:35:59,444\tWARNING util.py:315 -- The `process_trial_result` operation took 2.364 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                      mae 78.67174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                     mape 1.6352048532829738e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                     rmse 212.1795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:       time_since_restore 0.64647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:         time_this_iter_s 0.64647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:             time_total_s 0.64647\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:                timestamp 1689716149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: 🚀 View run FSR_Trainable_d17c72c7 at: https://wandb.ai/seokjin/FSR-prediction/runs/d17c72c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462061)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063554-d17c72c7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4773fdc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462228)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063601-4773fdc2/logs\n",
      "2023-07-19 06:36:08,608\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.547 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:08,613\tWARNING util.py:315 -- The `process_trial_result` operation took 2.553 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:08,616\tWARNING util.py:315 -- Processing trial results took 2.555 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:36:08,624\tWARNING util.py:315 -- The `process_trial_result` operation took 2.563 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_76752b90_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-35-56/wandb/run-20230719_063611-76752b90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Syncing run FSR_Trainable_76752b90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/76752b90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:36:15,883\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:15,886\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:15,888\tWARNING util.py:315 -- Processing trial results took 1.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:36:15,889\tWARNING util.py:315 -- The `process_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                      mae 68.75891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                     mape 1.1457051716256349e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                     rmse 194.41027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:       time_since_restore 0.76645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:         time_this_iter_s 0.76645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:             time_total_s 0.76645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:                timestamp 1689716166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: 🚀 View run FSR_Trainable_76752b90 at: https://wandb.ai/seokjin/FSR-prediction/runs/76752b90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462478)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063611-76752b90/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_5e128218_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-36-05/wandb/run-20230719_063618-5e128218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Syncing run FSR_Trainable_5e128218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5e128218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462665)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063618-5e128218/logs\n",
      "2023-07-19 06:36:25,332\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.240 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:25,337\tWARNING util.py:315 -- The `process_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:25,338\tWARNING util.py:315 -- Processing trial results took 2.247 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:36:25,339\tWARNING util.py:315 -- The `process_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_c5b02e9a_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-36-13/wandb/run-20230719_063627-c5b02e9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Syncing run FSR_Trainable_c5b02e9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c5b02e9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:36:32,124\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.917 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:32,128\tWARNING util.py:315 -- The `process_trial_result` operation took 1.921 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:32,130\tWARNING util.py:315 -- Processing trial results took 1.924 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:36:32,132\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                      mae 70.74656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                     mape 9.826169691051613e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                     rmse 224.60222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:       time_since_restore 1.03669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:         time_this_iter_s 1.03669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:             time_total_s 1.03669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:                timestamp 1689716183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: 🚀 View run FSR_Trainable_c5b02e9a at: https://wandb.ai/seokjin/FSR-prediction/runs/c5b02e9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=462894)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063627-c5b02e9a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_fda56acc_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-36-22/wandb/run-20230719_063634-fda56acc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Syncing run FSR_Trainable_fda56acc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fda56acc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                      mae █▃▂▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                     mape █▄▂▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                     rmse █▂▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:         time_this_iter_s █▁▁▁▂▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                timestamp ▁▅▅▆▆███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                      mae 42.22987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                     mape 7.397587790136685e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                     rmse 111.48928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:       time_since_restore 3.57482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:         time_this_iter_s 0.39296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:             time_total_s 3.57482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:                timestamp 1689716194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: 🚀 View run FSR_Trainable_fda56acc at: https://wandb.ai/seokjin/FSR-prediction/runs/fda56acc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463079)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063634-fda56acc/logs\n",
      "2023-07-19 06:36:41,934\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.270 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:41,936\tWARNING util.py:315 -- The `process_trial_result` operation took 2.273 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:41,939\tWARNING util.py:315 -- Processing trial results took 2.276 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:36:41,940\tWARNING util.py:315 -- The `process_trial_result` operation took 2.277 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_205b45bb_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-36-29/wandb/run-20230719_063644-205b45bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Syncing run FSR_Trainable_205b45bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/205b45bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:36:49,068\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.220 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:49,071\tWARNING util.py:315 -- The `process_trial_result` operation took 2.224 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:49,073\tWARNING util.py:315 -- Processing trial results took 2.226 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:36:49,074\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                      mae 49.94172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                     mape 9.536315292576859e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                     rmse 131.4265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:       time_since_restore 1.37296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:         time_this_iter_s 0.51542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:             time_total_s 1.37296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:                timestamp 1689716202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: 🚀 View run FSR_Trainable_205b45bb at: https://wandb.ai/seokjin/FSR-prediction/runs/205b45bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463313)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063644-205b45bb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_d8067052_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-36-38/wandb/run-20230719_063651-d8067052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Syncing run FSR_Trainable_d8067052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8067052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:36:56,969\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.888 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:56,971\tWARNING util.py:315 -- The `process_trial_result` operation took 2.892 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:36:56,973\tWARNING util.py:315 -- Processing trial results took 2.894 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:36:56,975\tWARNING util.py:315 -- The `process_trial_result` operation took 2.895 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                      mae █▂▁▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                     mape █▂▂▁▁▃▅▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                     rmse █▂▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                timestamp ▁▅▅▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                      mae 42.67819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                     mape 7.627588815062842e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                     rmse 111.69563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:       time_since_restore 3.50942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:         time_this_iter_s 0.29259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:             time_total_s 3.50942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:                timestamp 1689716211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: 🚀 View run FSR_Trainable_d8067052 at: https://wandb.ai/seokjin/FSR-prediction/runs/d8067052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463494)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063651-d8067052/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_dd9338d0_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-36-45/wandb/run-20230719_063659-dd9338d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Syncing run FSR_Trainable_dd9338d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dd9338d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:37:05,403\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.259 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:05,409\tWARNING util.py:315 -- The `process_trial_result` operation took 3.265 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:05,411\tWARNING util.py:315 -- Processing trial results took 3.268 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:37:05,413\tWARNING util.py:315 -- The `process_trial_result` operation took 3.269 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                      mae 53.4434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                     mape 9.5221672018567e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                     rmse 149.48379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:       time_since_restore 1.18508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:         time_this_iter_s 0.45499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:             time_total_s 1.18508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:                timestamp 1689716217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: 🚀 View run FSR_Trainable_dd9338d0 at: https://wandb.ai/seokjin/FSR-prediction/runs/dd9338d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463686)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063659-dd9338d0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_f337be85_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-36-53/wandb/run-20230719_063707-f337be85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Syncing run FSR_Trainable_f337be85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f337be85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:37:13,388\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:13,392\tWARNING util.py:315 -- The `process_trial_result` operation took 2.342 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:13,394\tWARNING util.py:315 -- Processing trial results took 2.344 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:37:13,396\tWARNING util.py:315 -- The `process_trial_result` operation took 2.347 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                      mae 59.08323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                     mape 9.824732431228526e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                     rmse 188.04197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:       time_since_restore 1.08162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:         time_this_iter_s 1.08162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:             time_total_s 1.08162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:                timestamp 1689716222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: 🚀 View run FSR_Trainable_f337be85 at: https://wandb.ai/seokjin/FSR-prediction/runs/f337be85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=463873)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063707-f337be85/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_0b93e2f8_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-37-01/wandb/run-20230719_063715-0b93e2f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Syncing run FSR_Trainable_0b93e2f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0b93e2f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                      mae 60.30516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                     mape 6657459585499593.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                     rmse 210.26512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:       time_since_restore 1.08463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:         time_this_iter_s 1.08463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:             time_total_s 1.08463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:                timestamp 1689716231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: 🚀 View run FSR_Trainable_0b93e2f8 at: https://wandb.ai/seokjin/FSR-prediction/runs/0b93e2f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464062)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063715-0b93e2f8/logs\n",
      "2023-07-19 06:37:22,303\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.378 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:22,307\tWARNING util.py:315 -- The `process_trial_result` operation took 2.383 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:22,308\tWARNING util.py:315 -- Processing trial results took 2.384 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:37:22,310\tWARNING util.py:315 -- The `process_trial_result` operation took 2.386 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_7034511a_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-37-10/wandb/run-20230719_063724-7034511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Syncing run FSR_Trainable_7034511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7034511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:37:28,673\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.287 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:28,678\tWARNING util.py:315 -- The `process_trial_result` operation took 2.293 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:28,679\tWARNING util.py:315 -- Processing trial results took 2.294 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:37:28,681\tWARNING util.py:315 -- The `process_trial_result` operation took 2.296 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                      mae 64.22602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                     mape 1.0981580821851852e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                     rmse 217.34977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:       time_since_restore 0.77385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:         time_this_iter_s 0.77385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:             time_total_s 0.77385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:                timestamp 1689716239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: 🚀 View run FSR_Trainable_7034511a at: https://wandb.ai/seokjin/FSR-prediction/runs/7034511a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464293)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063724-7034511a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_deb042fa_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-37-19/wandb/run-20230719_063731-deb042fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Syncing run FSR_Trainable_deb042fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/deb042fa\n",
      "2023-07-19 06:37:38,914\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.665 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:38,919\tWARNING util.py:315 -- The `process_trial_result` operation took 2.671 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:38,922\tWARNING util.py:315 -- Processing trial results took 2.674 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:37:38,924\tWARNING util.py:315 -- The `process_trial_result` operation took 2.676 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_4c87d99c_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-37-25/wandb/run-20230719_063742-4c87d99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Syncing run FSR_Trainable_4c87d99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4c87d99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                      mae █▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                     mape █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:         time_this_iter_s ▅█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                      mae 43.01985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                     mape 7.875050595784816e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                     rmse 109.67541\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:       time_since_restore 1.43676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:         time_this_iter_s 0.29036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:             time_total_s 1.43676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:                timestamp 1689716260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: 🚀 View run FSR_Trainable_4c87d99c at: https://wandb.ai/seokjin/FSR-prediction/runs/4c87d99c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464706)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063742-4c87d99c/logs\n",
      "2023-07-19 06:37:47,709\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.740 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:47,711\tWARNING util.py:315 -- The `process_trial_result` operation took 2.743 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:47,714\tWARNING util.py:315 -- Processing trial results took 2.746 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:37:47,716\tWARNING util.py:315 -- The `process_trial_result` operation took 2.748 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_fdf72de3_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-37-35/wandb/run-20230719_063751-fdf72de3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Syncing run FSR_Trainable_fdf72de3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fdf72de3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                      mae █▅▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▂▁▂▂▂▂▁▃▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                     mape █▇▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▂▃▂▁▂▁▃▂▂▂▂▂▁▃▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                     rmse █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▂▁▁▂▂▁▁▁▁▂▂▂▂▂▃▃▃▄▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:         time_this_iter_s █▂▁▂▂▂▁▁▁▁▂▁▁▁▂▂▁▂▁▂▃▃▄▄▂▁▂▂▃▂▂▄▄▃▄▄▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                      mae 40.19704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                     mape 6.6348142038713064e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                     rmse 110.71629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:       time_since_restore 18.06624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:         time_this_iter_s 0.33585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:             time_total_s 18.06624\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:                timestamp 1689716272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: 🚀 View run FSR_Trainable_deb042fa at: https://wandb.ai/seokjin/FSR-prediction/runs/deb042fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464476)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063731-deb042fa/logs\n",
      "2023-07-19 06:37:59,693\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.313 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:59,697\tWARNING util.py:315 -- The `process_trial_result` operation took 2.317 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:37:59,698\tWARNING util.py:315 -- Processing trial results took 2.319 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:37:59,699\tWARNING util.py:315 -- The `process_trial_result` operation took 2.320 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_4900ef1d_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-37-44/wandb/run-20230719_063802-4900ef1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Syncing run FSR_Trainable_4900ef1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4900ef1d\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                      mae █▅▃▂▂▂▃▃▄▄▄▃▃▃▃▃▂▂▃▂▃▃▁▁▃▁▁▂▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                     mape █▆▄▃▃▃▃▄▄▄▄▄▃▃▃▃▃▃▃▂▃▃▁▂▃▁▂▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                     rmse █▄▂▁▁▁▂▂▃▃▃▃▂▂▂▃▂▂▃▂▂▃▁▁▃▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:         time_this_iter_s ▆▅▇▄▅▆▄▃▃▃▄▃▃▃▁▂▁▇▂▃▂▄█▄▃▁▁▂▄▃▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                timestamp ▁▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                      mae 40.92841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                     mape 6.806168123471947e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                     rmse 108.5577\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:       time_since_restore 10.19286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:         time_this_iter_s 0.3259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:             time_total_s 10.19286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:                timestamp 1689716280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: 🚀 View run FSR_Trainable_fdf72de3 at: https://wandb.ai/seokjin/FSR-prediction/runs/fdf72de3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=464893)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063751-fdf72de3/logs\n",
      "2023-07-19 06:38:09,966\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.281 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:09,967\tWARNING util.py:315 -- The `process_trial_result` operation took 2.284 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:09,969\tWARNING util.py:315 -- Processing trial results took 2.286 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:38:09,971\tWARNING util.py:315 -- The `process_trial_result` operation took 2.288 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_a66ee56f_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-37-57/wandb/run-20230719_063812-a66ee56f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Syncing run FSR_Trainable_a66ee56f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a66ee56f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                      mae █▃▂▁▁▂▃▄▄▃▄▃▃▅▄▄▃▄▄▃▃▄▂▂▃▂▂▂▃▂▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                     mape █▇▆▄▄▅▇█▇▅▇▆▆▇▅▆▄▅▆▅▅▅▃▃▃▃▂▁▃▁▁▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                     rmse █▂▁▁▁▁▂▂▃▂▃▃▂▄▃▃▃▄▄▃▄▄▄▄▄▄▄▄▅▄▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:         time_this_iter_s ▇▄▄█▅▄▃▂▃▂▃▄▂▁▂▄▃▅▂▂▂▂▄▁▂▂▁▂▁▃▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                timestamp ▁▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                      mae 42.96868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                     mape 7.491023113979248e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                     rmse 115.3925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:       time_since_restore 8.02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:         time_this_iter_s 0.29404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:             time_total_s 8.02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:                timestamp 1689716290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: 🚀 View run FSR_Trainable_4900ef1d at: https://wandb.ai/seokjin/FSR-prediction/runs/4900ef1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465136)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063802-4900ef1d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:                      mae █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:                     mape ▁█▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:                     rmse █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:         time_this_iter_s █▅▇▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063812-a66ee56f/logs\n",
      "2023-07-19 06:38:18,715\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:18,718\tWARNING util.py:315 -- The `process_trial_result` operation took 2.026 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:18,720\tWARNING util.py:315 -- Processing trial results took 2.028 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:38:18,721\tWARNING util.py:315 -- The `process_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465376)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_475870e9_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-38-07/wandb/run-20230719_063821-475870e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Syncing run FSR_Trainable_475870e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/475870e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                      mae 43.19488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                     mape 7.304605750316346e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                     rmse 118.64868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:       time_since_restore 0.9764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:         time_this_iter_s 0.3236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:             time_total_s 0.9764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:                timestamp 1689716299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: 🚀 View run FSR_Trainable_475870e9 at: https://wandb.ai/seokjin/FSR-prediction/runs/475870e9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063821-475870e9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465612)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 06:38:27,386\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.307 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:27,389\tWARNING util.py:315 -- The `process_trial_result` operation took 2.312 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:27,390\tWARNING util.py:315 -- Processing trial results took 2.313 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:38:27,392\tWARNING util.py:315 -- The `process_trial_result` operation took 2.315 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_61048410_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-38-16/wandb/run-20230719_063830-61048410\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: Syncing run FSR_Trainable_61048410\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/61048410\n",
      "2023-07-19 06:38:35,222\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.687 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:35,228\tWARNING util.py:315 -- The `process_trial_result` operation took 2.693 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:35,229\tWARNING util.py:315 -- Processing trial results took 2.694 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:38:35,231\tWARNING util.py:315 -- The `process_trial_result` operation took 2.695 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_f47854d8_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-38-24/wandb/run-20230719_063837-f47854d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Syncing run FSR_Trainable_f47854d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f47854d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:38:42,791\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.323 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:42,795\tWARNING util.py:315 -- The `process_trial_result` operation took 2.328 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:42,796\tWARNING util.py:315 -- Processing trial results took 2.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:38:42,799\tWARNING util.py:315 -- The `process_trial_result` operation took 2.332 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                      mae 129.18011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                     mape 1.855608452102976e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                     rmse 373.92967\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:       time_since_restore 0.37679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:         time_this_iter_s 0.37679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:             time_total_s 0.37679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:                timestamp 1689716312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: 🚀 View run FSR_Trainable_f47854d8 at: https://wandb.ai/seokjin/FSR-prediction/runs/f47854d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466023)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063837-f47854d8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_2b5adf1f_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-38-32/wandb/run-20230719_063845-2b5adf1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Syncing run FSR_Trainable_2b5adf1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2b5adf1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                      mae 88.85512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                     mape 1.977011176874776e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                     rmse 214.50829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:       time_since_restore 0.45932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:         time_this_iter_s 0.45932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:             time_total_s 0.45932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:                timestamp 1689716320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: 🚀 View run FSR_Trainable_2b5adf1f at: https://wandb.ai/seokjin/FSR-prediction/runs/2b5adf1f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466211)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063845-2b5adf1f/logs\n",
      "2023-07-19 06:38:53,043\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.428 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:53,048\tWARNING util.py:315 -- The `process_trial_result` operation took 2.434 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:38:53,050\tWARNING util.py:315 -- Processing trial results took 2.435 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:38:53,051\tWARNING util.py:315 -- The `process_trial_result` operation took 2.437 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_22c2b8aa_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-38-39/wandb/run-20230719_063856-22c2b8aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Syncing run FSR_Trainable_22c2b8aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/22c2b8aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                      mae █▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                     mape ▃▁▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                     rmse █▁▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:         time_this_iter_s ██▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                      mae 39.8159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                     mape 25096273.57236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                     rmse 111.5241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:       time_since_restore 1.29105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:         time_this_iter_s 0.24273\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:             time_total_s 1.29105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:                timestamp 1689716334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: 🚀 View run FSR_Trainable_22c2b8aa at: https://wandb.ai/seokjin/FSR-prediction/runs/22c2b8aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466447)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063856-22c2b8aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:                      mae █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:                     mape █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:                     rmse █▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:         time_this_iter_s █▂▂▂▂▁▁▂▂▃▂▂▂▁▂▂▂▄▃▂▂▂▂▁▃▁▂▁▂▁▂▃▃▂█▂▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063830-61048410/logs\n",
      "2023-07-19 06:39:03,606\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.206 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:03,609\tWARNING util.py:315 -- The `process_trial_result` operation took 2.209 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:03,610\tWARNING util.py:315 -- Processing trial results took 2.211 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:03,611\tWARNING util.py:315 -- The `process_trial_result` operation took 2.212 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=465839)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "2023-07-19 06:39:12,119\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.383 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:12,122\tWARNING util.py:315 -- The `process_trial_result` operation took 2.388 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:12,124\tWARNING util.py:315 -- Processing trial results took 2.390 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:12,126\tWARNING util.py:315 -- The `process_trial_result` operation took 2.392 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_c1c9a5cc_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-38-50/wandb/run-20230719_063910-c1c9a5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Syncing run FSR_Trainable_c1c9a5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1c9a5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: - 0.000 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: \\ 0.000 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.000 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:39:19,384\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.638 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:19,388\tWARNING util.py:315 -- The `process_trial_result` operation took 2.644 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:19,391\tWARNING util.py:315 -- Processing trial results took 2.647 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:19,393\tWARNING util.py:315 -- The `process_trial_result` operation took 2.648 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                      mae █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                     mape █▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                     rmse █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                      mae 36.79864\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                     mape 19874646.30856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                     rmse 107.78361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:       time_since_restore 1.30363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:         time_this_iter_s 0.20551\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:             time_total_s 1.30363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:                timestamp 1689716344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: 🚀 View run FSR_Trainable_c1c9a5cc at: https://wandb.ai/seokjin/FSR-prediction/runs/c1c9a5cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466684)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063910-c1c9a5cc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb:                      mae ██▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb:                     mape ▂▄▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb:                     rmse █▇▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb:         time_this_iter_s █▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063915-48bc3541/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_cdd4bc1d_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-39-09/wandb/run-20230719_063921-cdd4bc1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Syncing run FSR_Trainable_cdd4bc1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cdd4bc1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=466889)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:39:25,967\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.102 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:25,970\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:25,971\tWARNING util.py:315 -- Processing trial results took 2.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:25,972\tWARNING util.py:315 -- The `process_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                      mae 41.29181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                     mape 21410714.31523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                     rmse 121.25878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:       time_since_restore 0.94496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:         time_this_iter_s 0.2628\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:             time_total_s 0.94496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:                timestamp 1689716359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: 🚀 View run FSR_Trainable_cdd4bc1d at: https://wandb.ai/seokjin/FSR-prediction/runs/cdd4bc1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063921-cdd4bc1d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_f0493a55_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-39-16/wandb/run-20230719_063928-f0493a55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Syncing run FSR_Trainable_f0493a55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0493a55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467080)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063928-f0493a55/logs\n",
      "2023-07-19 06:39:34,931\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.510 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:34,935\tWARNING util.py:315 -- The `process_trial_result` operation took 2.514 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:34,936\tWARNING util.py:315 -- Processing trial results took 2.516 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:34,938\tWARNING util.py:315 -- The `process_trial_result` operation took 2.517 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467302)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_913dcdb5_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-39-23/wandb/run-20230719_063937-913dcdb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Syncing run FSR_Trainable_913dcdb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/913dcdb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                      mae 57.99843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                     mape 29903691.53713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                     rmse 208.30983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:       time_since_restore 0.71642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:         time_this_iter_s 0.71642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:             time_total_s 0.71642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:                timestamp 1689716372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: 🚀 View run FSR_Trainable_913dcdb5 at: https://wandb.ai/seokjin/FSR-prediction/runs/913dcdb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063937-913dcdb5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467529)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 06:39:44,549\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.416 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:44,552\tWARNING util.py:315 -- The `process_trial_result` operation took 2.420 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:44,555\tWARNING util.py:315 -- Processing trial results took 2.423 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:44,557\tWARNING util.py:315 -- The `process_trial_result` operation took 2.425 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_4f5cc13f_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-39-31/wandb/run-20230719_063946-4f5cc13f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Syncing run FSR_Trainable_4f5cc13f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f5cc13f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:39:51,569\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.630 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:51,575\tWARNING util.py:315 -- The `process_trial_result` operation took 2.636 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:51,577\tWARNING util.py:315 -- Processing trial results took 2.638 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:51,579\tWARNING util.py:315 -- The `process_trial_result` operation took 2.641 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                      mae 50.78847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                     mape 36832068.10299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                     rmse 151.71827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:       time_since_restore 0.89841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:         time_this_iter_s 0.24859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:             time_total_s 0.89841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:                timestamp 1689716384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: 🚀 View run FSR_Trainable_4f5cc13f at: https://wandb.ai/seokjin/FSR-prediction/runs/4f5cc13f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467757)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063946-4f5cc13f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_355f65ec_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-39-41/wandb/run-20230719_063953-355f65ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Syncing run FSR_Trainable_355f65ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/355f65ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 06:39:58,815\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.447 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:58,821\tWARNING util.py:315 -- The `process_trial_result` operation took 2.454 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:39:58,822\tWARNING util.py:315 -- Processing trial results took 2.455 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:39:58,826\tWARNING util.py:315 -- The `process_trial_result` operation took 2.459 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                      mae 58.06465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                     mape 41233473.98079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                     rmse 178.93425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:       time_since_restore 0.74382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:         time_this_iter_s 0.74382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:             time_total_s 0.74382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:                timestamp 1689716388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: 🚀 View run FSR_Trainable_355f65ec at: https://wandb.ai/seokjin/FSR-prediction/runs/355f65ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=467943)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_063953-355f65ec/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/74c841d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:40:05,572\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.103 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:05,577\tWARNING util.py:315 -- The `process_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:05,579\tWARNING util.py:315 -- Processing trial results took 2.110 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:40:05,581\tWARNING util.py:315 -- The `process_trial_result` operation took 2.112 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468113)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_98162884_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-39-55/wandb/run-20230719_064007-98162884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Syncing run FSR_Trainable_98162884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/98162884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                      mae 109.15754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                     mape 2.641018310525011e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                     rmse 265.58002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:       time_since_restore 0.7079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:         time_this_iter_s 0.7079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:             time_total_s 0.7079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:                timestamp 1689716403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: 🚀 View run FSR_Trainable_98162884 at: https://wandb.ai/seokjin/FSR-prediction/runs/98162884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064007-98162884/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468326)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 06:40:14,274\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.449 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:14,278\tWARNING util.py:315 -- The `process_trial_result` operation took 2.453 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:14,280\tWARNING util.py:315 -- Processing trial results took 2.455 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:40:14,282\tWARNING util.py:315 -- The `process_trial_result` operation took 2.457 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_9d2bbc70_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-40-02/wandb/run-20230719_064016-9d2bbc70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Syncing run FSR_Trainable_9d2bbc70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d2bbc70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:40:20,723\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.218 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:20,725\tWARNING util.py:315 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:20,729\tWARNING util.py:315 -- Processing trial results took 2.225 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:40:20,731\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                      mae 132.52631\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                     mape 1.4209868607197757e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                     rmse 420.24474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:       time_since_restore 0.61594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:         time_this_iter_s 0.61594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:             time_total_s 0.61594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:                timestamp 1689716411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: 🚀 View run FSR_Trainable_9d2bbc70 at: https://wandb.ai/seokjin/FSR-prediction/runs/9d2bbc70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468552)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064016-9d2bbc70/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_5267dba2_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-40-11/wandb/run-20230719_064023-5267dba2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Syncing run FSR_Trainable_5267dba2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5267dba2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468737)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064023-5267dba2/logs\n",
      "2023-07-19 06:40:29,599\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.507 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:29,602\tWARNING util.py:315 -- The `process_trial_result` operation took 2.511 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:29,604\tWARNING util.py:315 -- Processing trial results took 2.513 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:40:29,606\tWARNING util.py:315 -- The `process_trial_result` operation took 2.515 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_31674976_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-40-17/wandb/run-20230719_064031-31674976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Syncing run FSR_Trainable_31674976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/31674976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:40:36,153\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:36,156\tWARNING util.py:315 -- The `process_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:36,157\tWARNING util.py:315 -- Processing trial results took 2.022 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:40:36,160\tWARNING util.py:315 -- The `process_trial_result` operation took 2.024 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                      mae 46.07337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                     mape 8.17172942895627e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                     rmse 134.34975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:       time_since_restore 0.91972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:         time_this_iter_s 0.32052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:             time_total_s 0.91972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:                timestamp 1689716429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: 🚀 View run FSR_Trainable_31674976 at: https://wandb.ai/seokjin/FSR-prediction/runs/31674976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=468963)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064031-31674976/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_d7905aa3_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-40-26/wandb/run-20230719_064038-d7905aa3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Syncing run FSR_Trainable_d7905aa3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d7905aa3\n",
      "2023-07-19 06:40:46,428\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.609 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:46,431\tWARNING util.py:315 -- The `process_trial_result` operation took 2.613 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:46,433\tWARNING util.py:315 -- Processing trial results took 2.614 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:40:46,434\tWARNING util.py:315 -- The `process_trial_result` operation took 2.615 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_5741b0ff_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-40-33/wandb/run-20230719_064049-5741b0ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Syncing run FSR_Trainable_5741b0ff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5741b0ff\n",
      "2023-07-19 06:40:55,064\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.499 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:55,072\tWARNING util.py:315 -- The `process_trial_result` operation took 2.508 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:40:55,073\tWARNING util.py:315 -- Processing trial results took 2.509 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:40:55,076\tWARNING util.py:315 -- The `process_trial_result` operation took 2.512 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_9ccc3e96_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-40-43/wandb/run-20230719_064059-9ccc3e96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Syncing run FSR_Trainable_9ccc3e96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9ccc3e96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                      mae █▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▂▁▁▂▁▂▂▂▂▂▃▂▃▂▃▃▃▃▃▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                     mape █▅▄▄▄▃▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▂▂▂▂▃▂▃▃▃▄▃▄▃▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                     rmse █▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:         time_this_iter_s █▃▃▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▃▃▂▃▃▂▂▃▃▂▂▂▄▃▅▄▄▄▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▅▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                      mae 41.71278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                     mape 6.782853560190198e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                     rmse 117.52325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:       time_since_restore 19.99986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:         time_this_iter_s 0.33499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:             time_total_s 19.99986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:                timestamp 1689716462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: 🚀 View run FSR_Trainable_d7905aa3 at: https://wandb.ai/seokjin/FSR-prediction/runs/d7905aa3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469152)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064038-d7905aa3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 06:41:10,118\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.397 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:10,121\tWARNING util.py:315 -- The `process_trial_result` operation took 2.401 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:10,123\tWARNING util.py:315 -- Processing trial results took 2.404 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:41:10,125\tWARNING util.py:315 -- The `process_trial_result` operation took 2.405 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:                      mae █▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:                     mape █▅▅▅▅▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:                     rmse █▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▁▁▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:       time_since_restore ▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:         time_this_iter_s ▄█▅▄▃▃▃▃▅▃▂▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:             time_total_s ▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:                timestamp ▁▃▃▃▃▃▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064059-9ccc3e96/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_2ec2154f_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-40-52/wandb/run-20230719_064113-2ec2154f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Syncing run FSR_Trainable_2ec2154f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2ec2154f\n",
      "2023-07-19 06:41:21,849\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.249 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:21,853\tWARNING util.py:315 -- The `process_trial_result` operation took 2.254 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:21,855\tWARNING util.py:315 -- Processing trial results took 2.256 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:41:21,856\tWARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_946af611_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-41-07/wandb/run-20230719_064125-946af611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Syncing run FSR_Trainable_946af611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/946af611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                      mae █▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                     mape ▅▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:         time_this_iter_s ▇█▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                timestamp ▁▆▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                      mae 41.74686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                     mape 7.590823843429002e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                     rmse 107.1296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:       time_since_restore 1.83558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:         time_this_iter_s 0.41017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:             time_total_s 1.83558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:                timestamp 1689716483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: 🚀 View run FSR_Trainable_946af611 at: https://wandb.ai/seokjin/FSR-prediction/runs/946af611\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470057)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064125-946af611/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:                      mae █▃▂▂▂▂▂▁▁▁▁▁▁▁▂▁▂▂▂▂▂▂▂▃▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:                     mape █▇▆▄▃▃▂▂▂▂▂▁▁▂▃▂▃▃▃▃▂▃▄▄▃▃▄▄▄▃▄▃▅▄▄▄▆▅▆▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:                     rmse █▁▁▁▂▂▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▂▃▃▄▃▄▄▃▄▄▄▃▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:         time_this_iter_s ▆▃▃▂▃▂▃▂▇▅█▄▄▃▃▂▃▄▁▃▅▅▄▃▃▃▃▂▁▂▃▅▃▄▃▁▄▃▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-07-19 06:41:34,497\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.931 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:34,501\tWARNING util.py:315 -- The `process_trial_result` operation took 2.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:34,504\tWARNING util.py:315 -- Processing trial results took 2.939 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:41:34,505\tWARNING util.py:315 -- The `process_trial_result` operation took 2.941 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_21e65c71_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-41-19/wandb/run-20230719_064138-21e65c71\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Syncing run FSR_Trainable_21e65c71\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/21e65c71\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                      mae █▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▃▃▁▁▂▄▁▄▄▆▄▄▅▄▃▅▃▅▄▄▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                     mape ▁▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▄▄▂▂▄▆▂▆▅█▅▅▅▅▄▅▃▅▄▄▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                     rmse █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁▁▂▁▂▂▃▂▃▃▃▃▄▃▄▄▄▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:         time_this_iter_s █▃▃▄▃▂▃▃▂▂▂▂▂▁▁▂▂▂▃▅▃▃▂▂▂▁▃▃▂▂▃▂▃▁▁▂▄▄▇▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                      mae 39.89642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                     mape 6.262199785190814e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                     rmse 112.7088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:       time_since_restore 20.73219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:         time_this_iter_s 0.31245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:             time_total_s 20.73219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:                timestamp 1689716497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: 🚀 View run FSR_Trainable_2ec2154f at: https://wandb.ai/seokjin/FSR-prediction/runs/2ec2154f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=469812)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064113-2ec2154f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:                      mae █▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:                     mape █▁▇▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:                     rmse █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:         time_this_iter_s ▇▁█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064138-21e65c71/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2023-07-19 06:41:45,099\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.256 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:45,101\tWARNING util.py:315 -- The `process_trial_result` operation took 2.259 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:45,104\tWARNING util.py:315 -- Processing trial results took 2.261 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:41:45,106\tWARNING util.py:315 -- The `process_trial_result` operation took 2.264 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_853fdefa_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_06-41-31/wandb/run-20230719_064147-853fdefa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Syncing run FSR_Trainable_853fdefa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/853fdefa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                      mae 66.20229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                     mape 1.131091061921648e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                     rmse 213.95532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:       time_since_restore 0.71497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:         time_this_iter_s 0.71497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:             time_total_s 0.71497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:                timestamp 1689716502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: 🚀 View run FSR_Trainable_853fdefa at: https://wandb.ai/seokjin/FSR-prediction/runs/853fdefa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470548)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064147-853fdefa/logs\n",
      "2023-07-19 06:41:53,324\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:53,327\tWARNING util.py:315 -- The `process_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "2023-07-19 06:41:53,330\tWARNING util.py:315 -- Processing trial results took 1.850 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 06:41:53,331\tWARNING util.py:315 -- The `process_trial_result` operation took 1.851 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_06-27-50/FSR_Trainable_5105c7d3_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-07-19_06-41-42/wandb/run-20230719_064155-5105c7d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Syncing run FSR_Trainable_5105c7d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5105c7d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                      mae 53.28055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                     mape 8.980390258030266e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                     rmse 143.8665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:       time_since_restore 1.12511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:         time_this_iter_s 0.29615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:             time_total_s 1.12511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:                timestamp 1689716513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: 🚀 View run FSR_Trainable_5105c7d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/5105c7d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=470770)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_064155-5105c7d3/logs\n",
      "2023-07-19 06:42:00,806\tINFO tune.py:1111 -- Total run time: 845.55 seconds (838.43 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
