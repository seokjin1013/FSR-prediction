{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task3\n",
    "\n",
    "Index_X = FSR_for_coord\n",
    "\n",
    "Index_y = x_coord, y_coord\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_04-38-35/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_04-38-35\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "0.5864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_coord',\n",
    "        'index_y': ['x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 04:38:35,093] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 04:38:37,272\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2023-07-19 04:38:38,883\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 05:48:58</td></tr>\n",
       "<tr><td>Running for: </td><td>01:10:19.33        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.2/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.6189408931519235 | Iter 32.000: -0.6304493143447627 | Iter 16.000: -0.6519697338788316 | Iter 8.000: -0.6671639447074706 | Iter 4.000: -0.6753359663731685 | Iter 2.000: -0.6857283211418639 | Iter 1.000: -0.6982074170645542<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X      </th><th>index_y             </th><th>model         </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_99ae87a3</td><td>TERMINATED</td><td>172.26.215.93:422259</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0996678  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        40.5624 </td><td style=\"text-align: right;\">0.712342</td><td style=\"text-align: right;\">0.392865</td><td style=\"text-align: right;\">0.109277 </td></tr>\n",
       "<tr><td>FSR_Trainable_50eaad01</td><td>TERMINATED</td><td>172.26.215.93:422330</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0026709  </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.97311</td><td style=\"text-align: right;\">2.66453 </td><td style=\"text-align: right;\">2.17743 </td><td style=\"text-align: right;\">0.396915 </td></tr>\n",
       "<tr><td>FSR_Trainable_d4d76e25</td><td>TERMINATED</td><td>172.26.215.93:422505</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.12705e-05</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.74461</td><td style=\"text-align: right;\">2.79348 </td><td style=\"text-align: right;\">2.51432 </td><td style=\"text-align: right;\">0.398377 </td></tr>\n",
       "<tr><td>FSR_Trainable_61ab7eb6</td><td>TERMINATED</td><td>172.26.215.93:422684</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cfc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000446   </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       102.437  </td><td style=\"text-align: right;\">0.701203</td><td style=\"text-align: right;\">0.35673 </td><td style=\"text-align: right;\">0.102973 </td></tr>\n",
       "<tr><td>FSR_Trainable_b8c4a990</td><td>TERMINATED</td><td>172.26.215.93:423004</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        4.13763e-05</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        91.5706 </td><td style=\"text-align: right;\">0.663669</td><td style=\"text-align: right;\">0.346678</td><td style=\"text-align: right;\">0.0992888</td></tr>\n",
       "<tr><td>FSR_Trainable_4f0e8c70</td><td>TERMINATED</td><td>172.26.215.93:423225</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__26c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0950699  </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.62438</td><td style=\"text-align: right;\">0.765546</td><td style=\"text-align: right;\">0.476027</td><td style=\"text-align: right;\">0.125486 </td></tr>\n",
       "<tr><td>FSR_Trainable_dd9a73a3</td><td>TERMINATED</td><td>172.26.215.93:423462</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bdc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.001937   </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       174.11   </td><td style=\"text-align: right;\">0.645463</td><td style=\"text-align: right;\">0.348929</td><td style=\"text-align: right;\">0.101239 </td></tr>\n",
       "<tr><td>FSR_Trainable_1e0b8bff</td><td>TERMINATED</td><td>172.26.215.93:423677</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1a40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000329301</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         5.73248</td><td style=\"text-align: right;\">0.711497</td><td style=\"text-align: right;\">0.393996</td><td style=\"text-align: right;\">0.108056 </td></tr>\n",
       "<tr><td>FSR_Trainable_9b7539f6</td><td>TERMINATED</td><td>172.26.215.93:423957</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0080</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00787518 </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.42431</td><td style=\"text-align: right;\">0.958985</td><td style=\"text-align: right;\">0.673007</td><td style=\"text-align: right;\">0.126299 </td></tr>\n",
       "<tr><td>FSR_Trainable_68dc015f</td><td>TERMINATED</td><td>172.26.215.93:424174</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fe00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.26374e-05</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.44965</td><td style=\"text-align: right;\">0.715395</td><td style=\"text-align: right;\">0.370902</td><td style=\"text-align: right;\">0.0939557</td></tr>\n",
       "<tr><td>FSR_Trainable_97fe2006</td><td>TERMINATED</td><td>172.26.215.93:424413</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__aec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0018484  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       148.673  </td><td style=\"text-align: right;\">0.631969</td><td style=\"text-align: right;\">0.331042</td><td style=\"text-align: right;\">0.0954702</td></tr>\n",
       "<tr><td>FSR_Trainable_790ae3b3</td><td>TERMINATED</td><td>172.26.215.93:424680</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000146394</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.36752</td><td style=\"text-align: right;\">2.71721 </td><td style=\"text-align: right;\">2.3711  </td><td style=\"text-align: right;\">0.377225 </td></tr>\n",
       "<tr><td>FSR_Trainable_d67889bd</td><td>TERMINATED</td><td>172.26.215.93:424869</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000136294</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.0813 </td><td style=\"text-align: right;\">0.87126 </td><td style=\"text-align: right;\">0.544949</td><td style=\"text-align: right;\">0.121854 </td></tr>\n",
       "<tr><td>FSR_Trainable_db7b2524</td><td>TERMINATED</td><td>172.26.215.93:425091</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4d00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        5.30231e-05</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      1857.83   </td><td style=\"text-align: right;\">0.641626</td><td style=\"text-align: right;\">0.336399</td><td style=\"text-align: right;\">0.0996622</td></tr>\n",
       "<tr><td>FSR_Trainable_67d1b926</td><td>TERMINATED</td><td>172.26.215.93:425354</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__aa00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.7192e-05 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       705.869  </td><td style=\"text-align: right;\">0.65356 </td><td style=\"text-align: right;\">0.349049</td><td style=\"text-align: right;\">0.102456 </td></tr>\n",
       "<tr><td>FSR_Trainable_c9d152bb</td><td>TERMINATED</td><td>172.26.215.93:425661</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5b00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.83352e-05</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.52855</td><td style=\"text-align: right;\">0.700885</td><td style=\"text-align: right;\">0.366524</td><td style=\"text-align: right;\">0.107753 </td></tr>\n",
       "<tr><td>FSR_Trainable_2e45e1fe</td><td>TERMINATED</td><td>172.26.215.93:425900</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.25095e-05</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         5.9422 </td><td style=\"text-align: right;\">0.702265</td><td style=\"text-align: right;\">0.358801</td><td style=\"text-align: right;\">0.104101 </td></tr>\n",
       "<tr><td>FSR_Trainable_c0373b8f</td><td>TERMINATED</td><td>172.26.215.93:426118</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6bc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00139105 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        13.4791 </td><td style=\"text-align: right;\">0.69091 </td><td style=\"text-align: right;\">0.357381</td><td style=\"text-align: right;\">0.102785 </td></tr>\n",
       "<tr><td>FSR_Trainable_55b20ff5</td><td>TERMINATED</td><td>172.26.215.93:426321</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0200</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00150985 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       922.931  </td><td style=\"text-align: right;\">0.639061</td><td style=\"text-align: right;\">0.341584</td><td style=\"text-align: right;\">0.0964358</td></tr>\n",
       "<tr><td>FSR_Trainable_15c95f2b</td><td>TERMINATED</td><td>172.26.215.93:426556</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c9c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00123404 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       914.873  </td><td style=\"text-align: right;\">0.64387 </td><td style=\"text-align: right;\">0.34988 </td><td style=\"text-align: right;\">0.0975154</td></tr>\n",
       "<tr><td>FSR_Trainable_d77e91ba</td><td>TERMINATED</td><td>172.26.215.93:427248</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3ac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00486747 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        38.3897 </td><td style=\"text-align: right;\">0.698052</td><td style=\"text-align: right;\">0.346369</td><td style=\"text-align: right;\">0.101779 </td></tr>\n",
       "<tr><td>FSR_Trainable_bfb68440</td><td>TERMINATED</td><td>172.26.215.93:427504</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00449417 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.8818 </td><td style=\"text-align: right;\">0.73876 </td><td style=\"text-align: right;\">0.462089</td><td style=\"text-align: right;\">0.117985 </td></tr>\n",
       "<tr><td>FSR_Trainable_7dd581fc</td><td>TERMINATED</td><td>172.26.215.93:427739</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000773292</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.7725 </td><td style=\"text-align: right;\">0.701164</td><td style=\"text-align: right;\">0.37358 </td><td style=\"text-align: right;\">0.110133 </td></tr>\n",
       "<tr><td>FSR_Trainable_c581bc54</td><td>TERMINATED</td><td>172.26.215.93:427952</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__02c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0126985  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       168.189  </td><td style=\"text-align: right;\">0.644737</td><td style=\"text-align: right;\">0.350362</td><td style=\"text-align: right;\">0.101088 </td></tr>\n",
       "<tr><td>FSR_Trainable_da2ca992</td><td>TERMINATED</td><td>172.26.215.93:428612</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0127382  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        13.6007 </td><td style=\"text-align: right;\">0.693958</td><td style=\"text-align: right;\">0.34131 </td><td style=\"text-align: right;\">0.100742 </td></tr>\n",
       "<tr><td>FSR_Trainable_93bca7f9</td><td>TERMINATED</td><td>172.26.215.93:428860</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.012524   </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        41.6836 </td><td style=\"text-align: right;\">0.663117</td><td style=\"text-align: right;\">0.356184</td><td style=\"text-align: right;\">0.102658 </td></tr>\n",
       "<tr><td>FSR_Trainable_370f612d</td><td>TERMINATED</td><td>172.26.215.93:429066</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4dc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0163651  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        25.7671 </td><td style=\"text-align: right;\">0.671124</td><td style=\"text-align: right;\">0.356206</td><td style=\"text-align: right;\">0.10137  </td></tr>\n",
       "<tr><td>FSR_Trainable_83d5cc84</td><td>TERMINATED</td><td>172.26.215.93:429281</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7f00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00096632 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       197.942  </td><td style=\"text-align: right;\">0.671861</td><td style=\"text-align: right;\">0.351064</td><td style=\"text-align: right;\">0.10186  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec4e1bdd</td><td>TERMINATED</td><td>172.26.215.93:429574</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f800</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000967624</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">      1138.65   </td><td style=\"text-align: right;\">0.645236</td><td style=\"text-align: right;\">0.346976</td><td style=\"text-align: right;\">0.101881 </td></tr>\n",
       "<tr><td>FSR_Trainable_a32f1ec5</td><td>TERMINATED</td><td>172.26.215.93:429753</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__07c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00088651 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.2248 </td><td style=\"text-align: right;\">0.710227</td><td style=\"text-align: right;\">0.364631</td><td style=\"text-align: right;\">0.0978781</td></tr>\n",
       "<tr><td>FSR_Trainable_e99194b2</td><td>TERMINATED</td><td>172.26.215.93:430038</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00121041 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.9097 </td><td style=\"text-align: right;\">0.710794</td><td style=\"text-align: right;\">0.363575</td><td style=\"text-align: right;\">0.095998 </td></tr>\n",
       "<tr><td>FSR_Trainable_0028facd</td><td>TERMINATED</td><td>172.26.215.93:430276</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00107446 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        27.9579 </td><td style=\"text-align: right;\">0.701186</td><td style=\"text-align: right;\">0.348666</td><td style=\"text-align: right;\">0.10033  </td></tr>\n",
       "<tr><td>FSR_Trainable_7cee5b7f</td><td>TERMINATED</td><td>172.26.215.93:430528</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000492613</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        21.9323 </td><td style=\"text-align: right;\">0.70166 </td><td style=\"text-align: right;\">0.390716</td><td style=\"text-align: right;\">0.109843 </td></tr>\n",
       "<tr><td>FSR_Trainable_037f32d8</td><td>TERMINATED</td><td>172.26.215.93:430777</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7c00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000454227</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        83.2215 </td><td style=\"text-align: right;\">0.687046</td><td style=\"text-align: right;\">0.348308</td><td style=\"text-align: right;\">0.10295  </td></tr>\n",
       "<tr><td>FSR_Trainable_6c487573</td><td>TERMINATED</td><td>172.26.215.93:431016</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0ec0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00210253 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       890.01   </td><td style=\"text-align: right;\">0.63138 </td><td style=\"text-align: right;\">0.333457</td><td style=\"text-align: right;\">0.0959044</td></tr>\n",
       "<tr><td>FSR_Trainable_5278ad2f</td><td>TERMINATED</td><td>172.26.215.93:431285</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00251729 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.5942 </td><td style=\"text-align: right;\">0.704941</td><td style=\"text-align: right;\">0.375168</td><td style=\"text-align: right;\">0.106853 </td></tr>\n",
       "<tr><td>FSR_Trainable_862fc8b3</td><td>TERMINATED</td><td>172.26.215.93:431490</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3ac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00231974 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.90601</td><td style=\"text-align: right;\">0.721748</td><td style=\"text-align: right;\">0.399651</td><td style=\"text-align: right;\">0.103969 </td></tr>\n",
       "<tr><td>FSR_Trainable_e16e66cb</td><td>TERMINATED</td><td>172.26.215.93:431719</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__57c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00310151 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.35457</td><td style=\"text-align: right;\">0.706834</td><td style=\"text-align: right;\">0.377134</td><td style=\"text-align: right;\">0.104198 </td></tr>\n",
       "<tr><td>FSR_Trainable_fb678dc2</td><td>TERMINATED</td><td>172.26.215.93:431953</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__27c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00468872 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.90359</td><td style=\"text-align: right;\">0.703351</td><td style=\"text-align: right;\">0.363499</td><td style=\"text-align: right;\">0.101257 </td></tr>\n",
       "<tr><td>FSR_Trainable_66707558</td><td>TERMINATED</td><td>172.26.215.93:432186</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b700</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00445943 </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.6858 </td><td style=\"text-align: right;\">2.54514 </td><td style=\"text-align: right;\">2.14496 </td><td style=\"text-align: right;\">0.347708 </td></tr>\n",
       "<tr><td>FSR_Trainable_e5680a88</td><td>TERMINATED</td><td>172.26.215.93:432413</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b4c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00184768 </td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.51698</td><td style=\"text-align: right;\">2.89686 </td><td style=\"text-align: right;\">2.47681 </td><td style=\"text-align: right;\">0.35259  </td></tr>\n",
       "<tr><td>FSR_Trainable_c7594953</td><td>TERMINATED</td><td>172.26.215.93:432647</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00192434 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.4694 </td><td style=\"text-align: right;\">0.703641</td><td style=\"text-align: right;\">0.382127</td><td style=\"text-align: right;\">0.104462 </td></tr>\n",
       "<tr><td>FSR_Trainable_2f3ebc88</td><td>TERMINATED</td><td>172.26.215.93:432919</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2f80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0277868  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.32323</td><td style=\"text-align: right;\">0.703157</td><td style=\"text-align: right;\">0.377073</td><td style=\"text-align: right;\">0.103398 </td></tr>\n",
       "<tr><td>FSR_Trainable_f1c1a58b</td><td>TERMINATED</td><td>172.26.215.93:433118</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2000</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0308763  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.8694 </td><td style=\"text-align: right;\">0.698916</td><td style=\"text-align: right;\">0.389901</td><td style=\"text-align: right;\">0.110095 </td></tr>\n",
       "<tr><td>FSR_Trainable_7b3c7daa</td><td>TERMINATED</td><td>172.26.215.93:433376</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000663476</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       373.995  </td><td style=\"text-align: right;\">0.586407</td><td style=\"text-align: right;\">0.300007</td><td style=\"text-align: right;\">0.0784201</td></tr>\n",
       "<tr><td>FSR_Trainable_7fc34d8d</td><td>TERMINATED</td><td>172.26.215.93:433924</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bf00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00144638 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         8.26124</td><td style=\"text-align: right;\">0.68997 </td><td style=\"text-align: right;\">0.349244</td><td style=\"text-align: right;\">0.105582 </td></tr>\n",
       "<tr><td>FSR_Trainable_c7c45904</td><td>TERMINATED</td><td>172.26.215.93:434162</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8640</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000263463</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        36.6912 </td><td style=\"text-align: right;\">0.718455</td><td style=\"text-align: right;\">0.396434</td><td style=\"text-align: right;\">0.107831 </td></tr>\n",
       "<tr><td>FSR_Trainable_627be693</td><td>TERMINATED</td><td>172.26.215.93:434418</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__10c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000274505</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        37.7215 </td><td style=\"text-align: right;\">0.716095</td><td style=\"text-align: right;\">0.401408</td><td style=\"text-align: right;\">0.10668  </td></tr>\n",
       "<tr><td>FSR_Trainable_97db8431</td><td>TERMINATED</td><td>172.26.215.93:434602</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0b00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000555406</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.09465</td><td style=\"text-align: right;\">0.714362</td><td style=\"text-align: right;\">0.38675 </td><td style=\"text-align: right;\">0.102646 </td></tr>\n",
       "<tr><td>FSR_Trainable_ba5a2d7b</td><td>TERMINATED</td><td>172.26.215.93:434878</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.16007e-05</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         7.4323 </td><td style=\"text-align: right;\">0.710208</td><td style=\"text-align: right;\">0.366426</td><td style=\"text-align: right;\">0.0993657</td></tr>\n",
       "<tr><td>FSR_Trainable_3d5d4cc7</td><td>TERMINATED</td><td>172.26.215.93:435067</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0c40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000298812</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        50.0285 </td><td style=\"text-align: right;\">0.676543</td><td style=\"text-align: right;\">0.355624</td><td style=\"text-align: right;\">0.102463 </td></tr>\n",
       "<tr><td>FSR_Trainable_d85b6355</td><td>TERMINATED</td><td>172.26.215.93:435292</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2400</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000666052</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.29664</td><td style=\"text-align: right;\">0.801825</td><td style=\"text-align: right;\">0.505923</td><td style=\"text-align: right;\">0.120233 </td></tr>\n",
       "<tr><td>FSR_Trainable_25130e68</td><td>TERMINATED</td><td>172.26.215.93:435565</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__73c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000195021</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.24242</td><td style=\"text-align: right;\">0.887049</td><td style=\"text-align: right;\">0.588165</td><td style=\"text-align: right;\">0.124658 </td></tr>\n",
       "<tr><td>FSR_Trainable_2fd5054c</td><td>TERMINATED</td><td>172.26.215.93:435798</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8500</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000180948</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       375.032  </td><td style=\"text-align: right;\">0.622106</td><td style=\"text-align: right;\">0.327033</td><td style=\"text-align: right;\">0.0944572</td></tr>\n",
       "<tr><td>FSR_Trainable_103e2280</td><td>TERMINATED</td><td>172.26.215.93:436003</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000121848</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       879.705  </td><td style=\"text-align: right;\">0.600161</td><td style=\"text-align: right;\">0.311801</td><td style=\"text-align: right;\">0.0937261</td></tr>\n",
       "<tr><td>FSR_Trainable_ca34a79a</td><td>TERMINATED</td><td>172.26.215.93:436370</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f940</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        8.798e-05  </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        19.9718 </td><td style=\"text-align: right;\">0.699728</td><td style=\"text-align: right;\">0.350808</td><td style=\"text-align: right;\">0.101144 </td></tr>\n",
       "<tr><td>FSR_Trainable_8ed6e88f</td><td>TERMINATED</td><td>172.26.215.93:436625</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4780</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000734174</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       883.873  </td><td style=\"text-align: right;\">0.589899</td><td style=\"text-align: right;\">0.304317</td><td style=\"text-align: right;\">0.0794331</td></tr>\n",
       "<tr><td>FSR_Trainable_3b723117</td><td>TERMINATED</td><td>172.26.215.93:436864</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f4c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000632357</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       286.756  </td><td style=\"text-align: right;\">0.662251</td><td style=\"text-align: right;\">0.360422</td><td style=\"text-align: right;\">0.0988305</td></tr>\n",
       "<tr><td>FSR_Trainable_150c9986</td><td>TERMINATED</td><td>172.26.215.93:437198</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__50c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00142311 </td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        38.1842 </td><td style=\"text-align: right;\">0.698904</td><td style=\"text-align: right;\">0.351306</td><td style=\"text-align: right;\">0.102773 </td></tr>\n",
       "<tr><td>FSR_Trainable_2be437fa</td><td>TERMINATED</td><td>172.26.215.93:437459</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0680</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000123174</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        30.1152 </td><td style=\"text-align: right;\">0.679513</td><td style=\"text-align: right;\">0.364082</td><td style=\"text-align: right;\">0.105424 </td></tr>\n",
       "<tr><td>FSR_Trainable_724f13a8</td><td>TERMINATED</td><td>172.26.215.93:437712</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ab80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000392306</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       298.472  </td><td style=\"text-align: right;\">0.614532</td><td style=\"text-align: right;\">0.32214 </td><td style=\"text-align: right;\">0.0949654</td></tr>\n",
       "<tr><td>FSR_Trainable_2a16827f</td><td>TERMINATED</td><td>172.26.215.93:437966</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2a00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000399624</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       293.552  </td><td style=\"text-align: right;\">0.591582</td><td style=\"text-align: right;\">0.304691</td><td style=\"text-align: right;\">0.0847175</td></tr>\n",
       "<tr><td>FSR_Trainable_b681b3a2</td><td>TERMINATED</td><td>172.26.215.93:438413</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1d80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000406334</td><td>sklearn.preproc_9b90</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       287.483  </td><td style=\"text-align: right;\">0.626605</td><td style=\"text-align: right;\">0.322406</td><td style=\"text-align: right;\">0.0853338</td></tr>\n",
       "<tr><td>FSR_Trainable_93d2f812</td><td>TERMINATED</td><td>172.26.215.93:438662</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000401052</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.23094</td><td style=\"text-align: right;\">0.71022 </td><td style=\"text-align: right;\">0.342227</td><td style=\"text-align: right;\">0.0906096</td></tr>\n",
       "<tr><td>FSR_Trainable_c371adcf</td><td>TERMINATED</td><td>172.26.215.93:438894</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000191531</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.59614</td><td style=\"text-align: right;\">0.720231</td><td style=\"text-align: right;\">0.349015</td><td style=\"text-align: right;\">0.0900087</td></tr>\n",
       "<tr><td>FSR_Trainable_63721144</td><td>TERMINATED</td><td>172.26.215.93:439108</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000370948</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        46.4892 </td><td style=\"text-align: right;\">0.663593</td><td style=\"text-align: right;\">0.361132</td><td style=\"text-align: right;\">0.10286  </td></tr>\n",
       "<tr><td>FSR_Trainable_b339785d</td><td>TERMINATED</td><td>172.26.215.93:439376</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000643821</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       206.481  </td><td style=\"text-align: right;\">0.619144</td><td style=\"text-align: right;\">0.325563</td><td style=\"text-align: right;\">0.0947538</td></tr>\n",
       "<tr><td>FSR_Trainable_2e5c9233</td><td>TERMINATED</td><td>172.26.215.93:439622</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000371842</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       206.05   </td><td style=\"text-align: right;\">0.619231</td><td style=\"text-align: right;\">0.326982</td><td style=\"text-align: right;\">0.0954666</td></tr>\n",
       "<tr><td>FSR_Trainable_bf496b10</td><td>TERMINATED</td><td>172.26.215.93:439986</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__17c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000845669</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       203.875  </td><td style=\"text-align: right;\">0.605562</td><td style=\"text-align: right;\">0.315949</td><td style=\"text-align: right;\">0.0875605</td></tr>\n",
       "<tr><td>FSR_Trainable_59ca80f0</td><td>TERMINATED</td><td>172.26.215.93:440173</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9340</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000650719</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       206.208  </td><td style=\"text-align: right;\">0.612074</td><td style=\"text-align: right;\">0.323553</td><td style=\"text-align: right;\">0.090509 </td></tr>\n",
       "<tr><td>FSR_Trainable_9b684290</td><td>TERMINATED</td><td>172.26.215.93:440450</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1280</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000634745</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        33.2778 </td><td style=\"text-align: right;\">0.664227</td><td style=\"text-align: right;\">0.370438</td><td style=\"text-align: right;\">0.103788 </td></tr>\n",
       "<tr><td>FSR_Trainable_b227b79a</td><td>TERMINATED</td><td>172.26.215.93:440704</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2d80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000732735</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       201.886  </td><td style=\"text-align: right;\">0.599061</td><td style=\"text-align: right;\">0.314907</td><td style=\"text-align: right;\">0.0908252</td></tr>\n",
       "<tr><td>FSR_Trainable_03820ab4</td><td>TERMINATED</td><td>172.26.215.93:440907</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1fc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000763333</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       203.87   </td><td style=\"text-align: right;\">0.601156</td><td style=\"text-align: right;\">0.313437</td><td style=\"text-align: right;\">0.088133 </td></tr>\n",
       "<tr><td>FSR_Trainable_01df0bfb</td><td>TERMINATED</td><td>172.26.215.93:441302</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0d40</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000494805</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        84.3835 </td><td style=\"text-align: right;\">0.624855</td><td style=\"text-align: right;\">0.331538</td><td style=\"text-align: right;\">0.0972941</td></tr>\n",
       "<tr><td>FSR_Trainable_14b20cd1</td><td>TERMINATED</td><td>172.26.215.93:441482</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3880</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000729935</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       128.815  </td><td style=\"text-align: right;\">0.607324</td><td style=\"text-align: right;\">0.319448</td><td style=\"text-align: right;\">0.0899343</td></tr>\n",
       "<tr><td>FSR_Trainable_7df97178</td><td>TERMINATED</td><td>172.26.215.93:441804</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__08c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000832833</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       126.707  </td><td style=\"text-align: right;\">0.61092 </td><td style=\"text-align: right;\">0.321327</td><td style=\"text-align: right;\">0.093086 </td></tr>\n",
       "<tr><td>FSR_Trainable_a6f25235</td><td>TERMINATED</td><td>172.26.215.93:442011</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bb80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00085528 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       124.879  </td><td style=\"text-align: right;\">0.605353</td><td style=\"text-align: right;\">0.314786</td><td style=\"text-align: right;\">0.0913061</td></tr>\n",
       "<tr><td>FSR_Trainable_e0d167a2</td><td>TERMINATED</td><td>172.26.215.93:442288</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000903607</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       129.704  </td><td style=\"text-align: right;\">0.621592</td><td style=\"text-align: right;\">0.333757</td><td style=\"text-align: right;\">0.0949958</td></tr>\n",
       "<tr><td>FSR_Trainable_1e57ef8f</td><td>TERMINATED</td><td>172.26.215.93:442573</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4bc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00081577 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        83.389  </td><td style=\"text-align: right;\">0.625021</td><td style=\"text-align: right;\">0.335926</td><td style=\"text-align: right;\">0.0961791</td></tr>\n",
       "<tr><td>FSR_Trainable_70da697c</td><td>TERMINATED</td><td>172.26.215.93:442852</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bfc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000813964</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       132.843  </td><td style=\"text-align: right;\">0.613109</td><td style=\"text-align: right;\">0.326881</td><td style=\"text-align: right;\">0.0939789</td></tr>\n",
       "<tr><td>FSR_Trainable_a07ed3a2</td><td>TERMINATED</td><td>172.26.215.93:443036</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5d00</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000984525</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        34.8522 </td><td style=\"text-align: right;\">0.633714</td><td style=\"text-align: right;\">0.341147</td><td style=\"text-align: right;\">0.0949036</td></tr>\n",
       "<tr><td>FSR_Trainable_f27fd036</td><td>TERMINATED</td><td>172.26.215.93:443339</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__89c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00105285 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       131.987  </td><td style=\"text-align: right;\">0.62043 </td><td style=\"text-align: right;\">0.329277</td><td style=\"text-align: right;\">0.0902519</td></tr>\n",
       "<tr><td>FSR_Trainable_61f740fc</td><td>TERMINATED</td><td>172.26.215.93:443532</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2600</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00119816 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.44051</td><td style=\"text-align: right;\">0.679904</td><td style=\"text-align: right;\">0.366395</td><td style=\"text-align: right;\">0.105577 </td></tr>\n",
       "<tr><td>FSR_Trainable_062fe3c1</td><td>TERMINATED</td><td>172.26.215.93:443758</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__53c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00111262 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        41.7076 </td><td style=\"text-align: right;\">0.653524</td><td style=\"text-align: right;\">0.361568</td><td style=\"text-align: right;\">0.102922 </td></tr>\n",
       "<tr><td>FSR_Trainable_792c1063</td><td>TERMINATED</td><td>172.26.215.93:443983</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4580</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000493112</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        41.7288 </td><td style=\"text-align: right;\">0.636646</td><td style=\"text-align: right;\">0.34119 </td><td style=\"text-align: right;\">0.0972099</td></tr>\n",
       "<tr><td>FSR_Trainable_8b389096</td><td>TERMINATED</td><td>172.26.215.93:444308</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2300</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000529103</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       134.331  </td><td style=\"text-align: right;\">0.620201</td><td style=\"text-align: right;\">0.330978</td><td style=\"text-align: right;\">0.0938102</td></tr>\n",
       "<tr><td>FSR_Trainable_35537741</td><td>TERMINATED</td><td>172.26.215.93:444488</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b540</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000759642</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         9.36181</td><td style=\"text-align: right;\">0.676614</td><td style=\"text-align: right;\">0.365929</td><td style=\"text-align: right;\">0.10439  </td></tr>\n",
       "<tr><td>FSR_Trainable_2895b6e6</td><td>TERMINATED</td><td>172.26.215.93:444780</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f0c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000756034</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       207.856  </td><td style=\"text-align: right;\">0.602338</td><td style=\"text-align: right;\">0.317366</td><td style=\"text-align: right;\">0.0913183</td></tr>\n",
       "<tr><td>FSR_Trainable_a05e09bf</td><td>TERMINATED</td><td>172.26.215.93:444968</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00150316 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       137.503  </td><td style=\"text-align: right;\">0.603241</td><td style=\"text-align: right;\">0.319046</td><td style=\"text-align: right;\">0.0901059</td></tr>\n",
       "<tr><td>FSR_Trainable_731370c8</td><td>TERMINATED</td><td>172.26.215.93:445270</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9740</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00160519 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.52372</td><td style=\"text-align: right;\">0.686079</td><td style=\"text-align: right;\">0.345068</td><td style=\"text-align: right;\">0.100009 </td></tr>\n",
       "<tr><td>FSR_Trainable_0500b8ec</td><td>TERMINATED</td><td>172.26.215.93:445486</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00157928 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        17.7862 </td><td style=\"text-align: right;\">0.668075</td><td style=\"text-align: right;\">0.368832</td><td style=\"text-align: right;\">0.103501 </td></tr>\n",
       "<tr><td>FSR_Trainable_60823b81</td><td>TERMINATED</td><td>172.26.215.93:445760</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5ac0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000307957</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.47973</td><td style=\"text-align: right;\">0.702649</td><td style=\"text-align: right;\">0.36958 </td><td style=\"text-align: right;\">0.104586 </td></tr>\n",
       "<tr><td>FSR_Trainable_5901cdec</td><td>TERMINATED</td><td>172.26.215.93:445966</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5240</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000572413</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.64569</td><td style=\"text-align: right;\">2.64162 </td><td style=\"text-align: right;\">2.3866  </td><td style=\"text-align: right;\">0.387071 </td></tr>\n",
       "<tr><td>FSR_Trainable_2a4d8c27</td><td>TERMINATED</td><td>172.26.215.93:446180</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2a80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000236647</td><td>sklearn.preproc_86f0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.89696</td><td style=\"text-align: right;\">1.91139 </td><td style=\"text-align: right;\">1.66601 </td><td style=\"text-align: right;\">0.27593  </td></tr>\n",
       "<tr><td>FSR_Trainable_76c223ee</td><td>TERMINATED</td><td>172.26.215.93:446407</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c4c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000809863</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       120.675  </td><td style=\"text-align: right;\">0.606673</td><td style=\"text-align: right;\">0.314952</td><td style=\"text-align: right;\">0.0910887</td></tr>\n",
       "<tr><td>FSR_Trainable_2596e03a</td><td>TERMINATED</td><td>172.26.215.93:446635</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1380</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000815629</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        42.5037 </td><td style=\"text-align: right;\">0.634987</td><td style=\"text-align: right;\">0.345554</td><td style=\"text-align: right;\">0.0989352</td></tr>\n",
       "<tr><td>FSR_Trainable_b47a652f</td><td>TERMINATED</td><td>172.26.215.93:446928</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1440</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000772603</td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       102.98   </td><td style=\"text-align: right;\">0.619042</td><td style=\"text-align: right;\">0.325762</td><td style=\"text-align: right;\">0.092586 </td></tr>\n",
       "<tr><td>FSR_Trainable_2d78adff</td><td>TERMINATED</td><td>172.26.215.93:447190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5cc0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0011177  </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.745  </td><td style=\"text-align: right;\">0.698665</td><td style=\"text-align: right;\">0.360697</td><td style=\"text-align: right;\">0.103284 </td></tr>\n",
       "<tr><td>FSR_Trainable_588d072c</td><td>TERMINATED</td><td>172.26.215.93:447392</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fa80</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00120596 </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.14621</td><td style=\"text-align: right;\">0.692241</td><td style=\"text-align: right;\">0.34594 </td><td style=\"text-align: right;\">0.102433 </td></tr>\n",
       "<tr><td>FSR_Trainable_5dcf6429</td><td>TERMINATED</td><td>172.26.215.93:447634</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8930</td><td>sklearn.impute._35f0</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__b0c0</td><td>fsr_model.LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0013121  </td><td>sklearn.preproc_88d0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.75695</td><td style=\"text-align: right;\">0.711717</td><td style=\"text-align: right;\">0.356121</td><td style=\"text-align: right;\">0.0967863</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 04:38:38,941\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_0028facd</td><td>2023-07-19_05-01-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.348666</td><td style=\"text-align: right;\">0.10033  </td><td>172.26.215.93</td><td style=\"text-align: right;\">430276</td><td style=\"text-align: right;\">0.701186</td><td style=\"text-align: right;\">            27.9579 </td><td style=\"text-align: right;\">         12.8793  </td><td style=\"text-align: right;\">      27.9579 </td><td style=\"text-align: right;\"> 1689710517</td><td style=\"text-align: right;\">                   2</td><td>0028facd  </td></tr>\n",
       "<tr><td>FSR_Trainable_01df0bfb</td><td>2023-07-19_05-39-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.331538</td><td style=\"text-align: right;\">0.0972941</td><td>172.26.215.93</td><td style=\"text-align: right;\">441302</td><td style=\"text-align: right;\">0.624855</td><td style=\"text-align: right;\">            84.3835 </td><td style=\"text-align: right;\">          1.30111 </td><td style=\"text-align: right;\">      84.3835 </td><td style=\"text-align: right;\"> 1689712776</td><td style=\"text-align: right;\">                  64</td><td>01df0bfb  </td></tr>\n",
       "<tr><td>FSR_Trainable_037f32d8</td><td>2023-07-19_05-04-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.348308</td><td style=\"text-align: right;\">0.10295  </td><td>172.26.215.93</td><td style=\"text-align: right;\">430777</td><td style=\"text-align: right;\">0.687046</td><td style=\"text-align: right;\">            83.2215 </td><td style=\"text-align: right;\">         10.1992  </td><td style=\"text-align: right;\">      83.2215 </td><td style=\"text-align: right;\"> 1689710655</td><td style=\"text-align: right;\">                   8</td><td>037f32d8  </td></tr>\n",
       "<tr><td>FSR_Trainable_03820ab4</td><td>2023-07-19_05-39-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.313437</td><td style=\"text-align: right;\">0.088133 </td><td>172.26.215.93</td><td style=\"text-align: right;\">440907</td><td style=\"text-align: right;\">0.601156</td><td style=\"text-align: right;\">           203.87   </td><td style=\"text-align: right;\">          2.30116 </td><td style=\"text-align: right;\">     203.87   </td><td style=\"text-align: right;\"> 1689712747</td><td style=\"text-align: right;\">                 100</td><td>03820ab4  </td></tr>\n",
       "<tr><td>FSR_Trainable_0500b8ec</td><td>2023-07-19_05-45-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.368832</td><td style=\"text-align: right;\">0.103501 </td><td>172.26.215.93</td><td style=\"text-align: right;\">445486</td><td style=\"text-align: right;\">0.668075</td><td style=\"text-align: right;\">            17.7862 </td><td style=\"text-align: right;\">          2.08755 </td><td style=\"text-align: right;\">      17.7862 </td><td style=\"text-align: right;\"> 1689713138</td><td style=\"text-align: right;\">                   8</td><td>0500b8ec  </td></tr>\n",
       "<tr><td>FSR_Trainable_062fe3c1</td><td>2023-07-19_05-43-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.361568</td><td style=\"text-align: right;\">0.102922 </td><td>172.26.215.93</td><td style=\"text-align: right;\">443758</td><td style=\"text-align: right;\">0.653524</td><td style=\"text-align: right;\">            41.7076 </td><td style=\"text-align: right;\">          1.13045 </td><td style=\"text-align: right;\">      41.7076 </td><td style=\"text-align: right;\"> 1689713005</td><td style=\"text-align: right;\">                  32</td><td>062fe3c1  </td></tr>\n",
       "<tr><td>FSR_Trainable_103e2280</td><td>2023-07-19_05-30-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.311801</td><td style=\"text-align: right;\">0.0937261</td><td>172.26.215.93</td><td style=\"text-align: right;\">436003</td><td style=\"text-align: right;\">0.600161</td><td style=\"text-align: right;\">           879.705  </td><td style=\"text-align: right;\">          8.22896 </td><td style=\"text-align: right;\">     879.705  </td><td style=\"text-align: right;\"> 1689712248</td><td style=\"text-align: right;\">                 100</td><td>103e2280  </td></tr>\n",
       "<tr><td>FSR_Trainable_14b20cd1</td><td>2023-07-19_05-40-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.319448</td><td style=\"text-align: right;\">0.0899343</td><td>172.26.215.93</td><td style=\"text-align: right;\">441482</td><td style=\"text-align: right;\">0.607324</td><td style=\"text-align: right;\">           128.815  </td><td style=\"text-align: right;\">          1.22133 </td><td style=\"text-align: right;\">     128.815  </td><td style=\"text-align: right;\"> 1689712833</td><td style=\"text-align: right;\">                 100</td><td>14b20cd1  </td></tr>\n",
       "<tr><td>FSR_Trainable_150c9986</td><td>2023-07-19_05-22-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.351306</td><td style=\"text-align: right;\">0.102773 </td><td>172.26.215.93</td><td style=\"text-align: right;\">437198</td><td style=\"text-align: right;\">0.698904</td><td style=\"text-align: right;\">            38.1842 </td><td style=\"text-align: right;\">          9.66907 </td><td style=\"text-align: right;\">      38.1842 </td><td style=\"text-align: right;\"> 1689711777</td><td style=\"text-align: right;\">                   4</td><td>150c9986  </td></tr>\n",
       "<tr><td>FSR_Trainable_15c95f2b</td><td>2023-07-19_04-59-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.34988 </td><td style=\"text-align: right;\">0.0975154</td><td>172.26.215.93</td><td style=\"text-align: right;\">426556</td><td style=\"text-align: right;\">0.64387 </td><td style=\"text-align: right;\">           914.873  </td><td style=\"text-align: right;\">          9.51794 </td><td style=\"text-align: right;\">     914.873  </td><td style=\"text-align: right;\"> 1689710368</td><td style=\"text-align: right;\">                 100</td><td>15c95f2b  </td></tr>\n",
       "<tr><td>FSR_Trainable_1e0b8bff</td><td>2023-07-19_04-40-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.393996</td><td style=\"text-align: right;\">0.108056 </td><td>172.26.215.93</td><td style=\"text-align: right;\">423677</td><td style=\"text-align: right;\">0.711497</td><td style=\"text-align: right;\">             5.73248</td><td style=\"text-align: right;\">          2.62532 </td><td style=\"text-align: right;\">       5.73248</td><td style=\"text-align: right;\"> 1689709200</td><td style=\"text-align: right;\">                   2</td><td>1e0b8bff  </td></tr>\n",
       "<tr><td>FSR_Trainable_1e57ef8f</td><td>2023-07-19_05-42-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.335926</td><td style=\"text-align: right;\">0.0961791</td><td>172.26.215.93</td><td style=\"text-align: right;\">442573</td><td style=\"text-align: right;\">0.625021</td><td style=\"text-align: right;\">            83.389  </td><td style=\"text-align: right;\">          1.24642 </td><td style=\"text-align: right;\">      83.389  </td><td style=\"text-align: right;\"> 1689712934</td><td style=\"text-align: right;\">                  64</td><td>1e57ef8f  </td></tr>\n",
       "<tr><td>FSR_Trainable_25130e68</td><td>2023-07-19_05-15-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.588165</td><td style=\"text-align: right;\">0.124658 </td><td>172.26.215.93</td><td style=\"text-align: right;\">435565</td><td style=\"text-align: right;\">0.887049</td><td style=\"text-align: right;\">             5.24242</td><td style=\"text-align: right;\">          5.24242 </td><td style=\"text-align: right;\">       5.24242</td><td style=\"text-align: right;\"> 1689711328</td><td style=\"text-align: right;\">                   1</td><td>25130e68  </td></tr>\n",
       "<tr><td>FSR_Trainable_2596e03a</td><td>2023-07-19_05-47-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.345554</td><td style=\"text-align: right;\">0.0989352</td><td>172.26.215.93</td><td style=\"text-align: right;\">446635</td><td style=\"text-align: right;\">0.634987</td><td style=\"text-align: right;\">            42.5037 </td><td style=\"text-align: right;\">          1.28089 </td><td style=\"text-align: right;\">      42.5037 </td><td style=\"text-align: right;\"> 1689713245</td><td style=\"text-align: right;\">                  32</td><td>2596e03a  </td></tr>\n",
       "<tr><td>FSR_Trainable_2895b6e6</td><td>2023-07-19_05-47-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.317366</td><td style=\"text-align: right;\">0.0913183</td><td>172.26.215.93</td><td style=\"text-align: right;\">444780</td><td style=\"text-align: right;\">0.602338</td><td style=\"text-align: right;\">           207.856  </td><td style=\"text-align: right;\">          2.21852 </td><td style=\"text-align: right;\">     207.856  </td><td style=\"text-align: right;\"> 1689713276</td><td style=\"text-align: right;\">                 100</td><td>2895b6e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_2a16827f</td><td>2023-07-19_05-29-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.304691</td><td style=\"text-align: right;\">0.0847175</td><td>172.26.215.93</td><td style=\"text-align: right;\">437966</td><td style=\"text-align: right;\">0.591582</td><td style=\"text-align: right;\">           293.552  </td><td style=\"text-align: right;\">          2.91073 </td><td style=\"text-align: right;\">     293.552  </td><td style=\"text-align: right;\"> 1689712183</td><td style=\"text-align: right;\">                 100</td><td>2a16827f  </td></tr>\n",
       "<tr><td>FSR_Trainable_2a4d8c27</td><td>2023-07-19_05-46-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.66601 </td><td style=\"text-align: right;\">0.27593  </td><td>172.26.215.93</td><td style=\"text-align: right;\">446180</td><td style=\"text-align: right;\">1.91139 </td><td style=\"text-align: right;\">             1.89696</td><td style=\"text-align: right;\">          1.89696 </td><td style=\"text-align: right;\">       1.89696</td><td style=\"text-align: right;\"> 1689713178</td><td style=\"text-align: right;\">                   1</td><td>2a4d8c27  </td></tr>\n",
       "<tr><td>FSR_Trainable_2be437fa</td><td>2023-07-19_05-23-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.364082</td><td style=\"text-align: right;\">0.105424 </td><td>172.26.215.93</td><td style=\"text-align: right;\">437459</td><td style=\"text-align: right;\">0.679513</td><td style=\"text-align: right;\">            30.1152 </td><td style=\"text-align: right;\">          3.62268 </td><td style=\"text-align: right;\">      30.1152 </td><td style=\"text-align: right;\"> 1689711822</td><td style=\"text-align: right;\">                   8</td><td>2be437fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_2d78adff</td><td>2023-07-19_05-47-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.360697</td><td style=\"text-align: right;\">0.103284 </td><td>172.26.215.93</td><td style=\"text-align: right;\">447190</td><td style=\"text-align: right;\">0.698665</td><td style=\"text-align: right;\">             1.745  </td><td style=\"text-align: right;\">          1.745   </td><td style=\"text-align: right;\">       1.745  </td><td style=\"text-align: right;\"> 1689713258</td><td style=\"text-align: right;\">                   1</td><td>2d78adff  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e45e1fe</td><td>2023-07-19_04-43-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.358801</td><td style=\"text-align: right;\">0.104101 </td><td>172.26.215.93</td><td style=\"text-align: right;\">425900</td><td style=\"text-align: right;\">0.702265</td><td style=\"text-align: right;\">             5.9422 </td><td style=\"text-align: right;\">          1.38839 </td><td style=\"text-align: right;\">       5.9422 </td><td style=\"text-align: right;\"> 1689709408</td><td style=\"text-align: right;\">                   4</td><td>2e45e1fe  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e5c9233</td><td>2023-07-19_05-35-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.326982</td><td style=\"text-align: right;\">0.0954666</td><td>172.26.215.93</td><td style=\"text-align: right;\">439622</td><td style=\"text-align: right;\">0.619231</td><td style=\"text-align: right;\">           206.05   </td><td style=\"text-align: right;\">          2.06451 </td><td style=\"text-align: right;\">     206.05   </td><td style=\"text-align: right;\"> 1689712511</td><td style=\"text-align: right;\">                 100</td><td>2e5c9233  </td></tr>\n",
       "<tr><td>FSR_Trainable_2f3ebc88</td><td>2023-07-19_05-06-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.377073</td><td style=\"text-align: right;\">0.103398 </td><td>172.26.215.93</td><td style=\"text-align: right;\">432919</td><td style=\"text-align: right;\">0.703157</td><td style=\"text-align: right;\">             3.32323</td><td style=\"text-align: right;\">          3.32323 </td><td style=\"text-align: right;\">       3.32323</td><td style=\"text-align: right;\"> 1689710814</td><td style=\"text-align: right;\">                   1</td><td>2f3ebc88  </td></tr>\n",
       "<tr><td>FSR_Trainable_2fd5054c</td><td>2023-07-19_05-22-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.327033</td><td style=\"text-align: right;\">0.0944572</td><td>172.26.215.93</td><td style=\"text-align: right;\">435798</td><td style=\"text-align: right;\">0.622106</td><td style=\"text-align: right;\">           375.032  </td><td style=\"text-align: right;\">          3.43783 </td><td style=\"text-align: right;\">     375.032  </td><td style=\"text-align: right;\"> 1689711724</td><td style=\"text-align: right;\">                 100</td><td>2fd5054c  </td></tr>\n",
       "<tr><td>FSR_Trainable_35537741</td><td>2023-07-19_05-43-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.365929</td><td style=\"text-align: right;\">0.10439  </td><td>172.26.215.93</td><td style=\"text-align: right;\">444488</td><td style=\"text-align: right;\">0.676614</td><td style=\"text-align: right;\">             9.36181</td><td style=\"text-align: right;\">          1.98719 </td><td style=\"text-align: right;\">       9.36181</td><td style=\"text-align: right;\"> 1689713039</td><td style=\"text-align: right;\">                   4</td><td>35537741  </td></tr>\n",
       "<tr><td>FSR_Trainable_370f612d</td><td>2023-07-19_05-00-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.356206</td><td style=\"text-align: right;\">0.10137  </td><td>172.26.215.93</td><td style=\"text-align: right;\">429066</td><td style=\"text-align: right;\">0.671124</td><td style=\"text-align: right;\">            25.7671 </td><td style=\"text-align: right;\">          1.502   </td><td style=\"text-align: right;\">      25.7671 </td><td style=\"text-align: right;\"> 1689710400</td><td style=\"text-align: right;\">                  16</td><td>370f612d  </td></tr>\n",
       "<tr><td>FSR_Trainable_3b723117</td><td>2023-07-19_05-24-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.360422</td><td style=\"text-align: right;\">0.0988305</td><td>172.26.215.93</td><td style=\"text-align: right;\">436864</td><td style=\"text-align: right;\">0.662251</td><td style=\"text-align: right;\">           286.756  </td><td style=\"text-align: right;\">          8.67302 </td><td style=\"text-align: right;\">     286.756  </td><td style=\"text-align: right;\"> 1689711870</td><td style=\"text-align: right;\">                  32</td><td>3b723117  </td></tr>\n",
       "<tr><td>FSR_Trainable_3d5d4cc7</td><td>2023-07-19_05-15-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.355624</td><td style=\"text-align: right;\">0.102463 </td><td>172.26.215.93</td><td style=\"text-align: right;\">435067</td><td style=\"text-align: right;\">0.676543</td><td style=\"text-align: right;\">            50.0285 </td><td style=\"text-align: right;\">          6.13378 </td><td style=\"text-align: right;\">      50.0285 </td><td style=\"text-align: right;\"> 1689711345</td><td style=\"text-align: right;\">                   8</td><td>3d5d4cc7  </td></tr>\n",
       "<tr><td>FSR_Trainable_4f0e8c70</td><td>2023-07-19_04-39-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.476027</td><td style=\"text-align: right;\">0.125486 </td><td>172.26.215.93</td><td style=\"text-align: right;\">423225</td><td style=\"text-align: right;\">0.765546</td><td style=\"text-align: right;\">             1.62438</td><td style=\"text-align: right;\">          1.62438 </td><td style=\"text-align: right;\">       1.62438</td><td style=\"text-align: right;\"> 1689709170</td><td style=\"text-align: right;\">                   1</td><td>4f0e8c70  </td></tr>\n",
       "<tr><td>FSR_Trainable_50eaad01</td><td>2023-07-19_04-38-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.17743 </td><td style=\"text-align: right;\">0.396915 </td><td>172.26.215.93</td><td style=\"text-align: right;\">422330</td><td style=\"text-align: right;\">2.66453 </td><td style=\"text-align: right;\">             1.97311</td><td style=\"text-align: right;\">          1.97311 </td><td style=\"text-align: right;\">       1.97311</td><td style=\"text-align: right;\"> 1689709132</td><td style=\"text-align: right;\">                   1</td><td>50eaad01  </td></tr>\n",
       "<tr><td>FSR_Trainable_5278ad2f</td><td>2023-07-19_05-04-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.375168</td><td style=\"text-align: right;\">0.106853 </td><td>172.26.215.93</td><td style=\"text-align: right;\">431285</td><td style=\"text-align: right;\">0.704941</td><td style=\"text-align: right;\">             2.5942 </td><td style=\"text-align: right;\">          2.5942  </td><td style=\"text-align: right;\">       2.5942 </td><td style=\"text-align: right;\"> 1689710671</td><td style=\"text-align: right;\">                   1</td><td>5278ad2f  </td></tr>\n",
       "<tr><td>FSR_Trainable_55b20ff5</td><td>2023-07-19_04-59-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.341584</td><td style=\"text-align: right;\">0.0964358</td><td>172.26.215.93</td><td style=\"text-align: right;\">426321</td><td style=\"text-align: right;\">0.639061</td><td style=\"text-align: right;\">           922.931  </td><td style=\"text-align: right;\">         11.1802  </td><td style=\"text-align: right;\">     922.931  </td><td style=\"text-align: right;\"> 1689710359</td><td style=\"text-align: right;\">                 100</td><td>55b20ff5  </td></tr>\n",
       "<tr><td>FSR_Trainable_588d072c</td><td>2023-07-19_05-47-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.34594 </td><td style=\"text-align: right;\">0.102433 </td><td>172.26.215.93</td><td style=\"text-align: right;\">447392</td><td style=\"text-align: right;\">0.692241</td><td style=\"text-align: right;\">             3.14621</td><td style=\"text-align: right;\">          1.39688 </td><td style=\"text-align: right;\">       3.14621</td><td style=\"text-align: right;\"> 1689713278</td><td style=\"text-align: right;\">                   2</td><td>588d072c  </td></tr>\n",
       "<tr><td>FSR_Trainable_5901cdec</td><td>2023-07-19_05-46-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.3866  </td><td style=\"text-align: right;\">0.387071 </td><td>172.26.215.93</td><td style=\"text-align: right;\">445966</td><td style=\"text-align: right;\">2.64162 </td><td style=\"text-align: right;\">             2.64569</td><td style=\"text-align: right;\">          2.64569 </td><td style=\"text-align: right;\">       2.64569</td><td style=\"text-align: right;\"> 1689713169</td><td style=\"text-align: right;\">                   1</td><td>5901cdec  </td></tr>\n",
       "<tr><td>FSR_Trainable_59ca80f0</td><td>2023-07-19_05-38-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.323553</td><td style=\"text-align: right;\">0.090509 </td><td>172.26.215.93</td><td style=\"text-align: right;\">440173</td><td style=\"text-align: right;\">0.612074</td><td style=\"text-align: right;\">           206.208  </td><td style=\"text-align: right;\">          1.85322 </td><td style=\"text-align: right;\">     206.208  </td><td style=\"text-align: right;\"> 1689712682</td><td style=\"text-align: right;\">                 100</td><td>59ca80f0  </td></tr>\n",
       "<tr><td>FSR_Trainable_5dcf6429</td><td>2023-07-19_05-48-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.356121</td><td style=\"text-align: right;\">0.0967863</td><td>172.26.215.93</td><td style=\"text-align: right;\">447634</td><td style=\"text-align: right;\">0.711717</td><td style=\"text-align: right;\">             2.75695</td><td style=\"text-align: right;\">          2.75695 </td><td style=\"text-align: right;\">       2.75695</td><td style=\"text-align: right;\"> 1689713289</td><td style=\"text-align: right;\">                   1</td><td>5dcf6429  </td></tr>\n",
       "<tr><td>FSR_Trainable_60823b81</td><td>2023-07-19_05-45-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.36958 </td><td style=\"text-align: right;\">0.104586 </td><td>172.26.215.93</td><td style=\"text-align: right;\">445760</td><td style=\"text-align: right;\">0.702649</td><td style=\"text-align: right;\">             2.47973</td><td style=\"text-align: right;\">          2.47973 </td><td style=\"text-align: right;\">       2.47973</td><td style=\"text-align: right;\"> 1689713152</td><td style=\"text-align: right;\">                   1</td><td>60823b81  </td></tr>\n",
       "<tr><td>FSR_Trainable_61ab7eb6</td><td>2023-07-19_04-41-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.35673 </td><td style=\"text-align: right;\">0.102973 </td><td>172.26.215.93</td><td style=\"text-align: right;\">422684</td><td style=\"text-align: right;\">0.701203</td><td style=\"text-align: right;\">           102.437  </td><td style=\"text-align: right;\">          0.979285</td><td style=\"text-align: right;\">     102.437  </td><td style=\"text-align: right;\"> 1689709261</td><td style=\"text-align: right;\">                 100</td><td>61ab7eb6  </td></tr>\n",
       "<tr><td>FSR_Trainable_61f740fc</td><td>2023-07-19_05-42-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.366395</td><td style=\"text-align: right;\">0.105577 </td><td>172.26.215.93</td><td style=\"text-align: right;\">443532</td><td style=\"text-align: right;\">0.679904</td><td style=\"text-align: right;\">             6.44051</td><td style=\"text-align: right;\">          1.45461 </td><td style=\"text-align: right;\">       6.44051</td><td style=\"text-align: right;\"> 1689712957</td><td style=\"text-align: right;\">                   4</td><td>61f740fc  </td></tr>\n",
       "<tr><td>FSR_Trainable_627be693</td><td>2023-07-19_05-14-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.401408</td><td style=\"text-align: right;\">0.10668  </td><td>172.26.215.93</td><td style=\"text-align: right;\">434418</td><td style=\"text-align: right;\">0.716095</td><td style=\"text-align: right;\">            37.7215 </td><td style=\"text-align: right;\">         18.2424  </td><td style=\"text-align: right;\">      37.7215 </td><td style=\"text-align: right;\"> 1689711281</td><td style=\"text-align: right;\">                   2</td><td>627be693  </td></tr>\n",
       "<tr><td>FSR_Trainable_63721144</td><td>2023-07-19_05-31-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.361132</td><td style=\"text-align: right;\">0.10286  </td><td>172.26.215.93</td><td style=\"text-align: right;\">439108</td><td style=\"text-align: right;\">0.663593</td><td style=\"text-align: right;\">            46.4892 </td><td style=\"text-align: right;\">          2.61604 </td><td style=\"text-align: right;\">      46.4892 </td><td style=\"text-align: right;\"> 1689712283</td><td style=\"text-align: right;\">                  16</td><td>63721144  </td></tr>\n",
       "<tr><td>FSR_Trainable_66707558</td><td>2023-07-19_05-05-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.14496 </td><td style=\"text-align: right;\">0.347708 </td><td>172.26.215.93</td><td style=\"text-align: right;\">432186</td><td style=\"text-align: right;\">2.54514 </td><td style=\"text-align: right;\">             1.6858 </td><td style=\"text-align: right;\">          1.6858  </td><td style=\"text-align: right;\">       1.6858 </td><td style=\"text-align: right;\"> 1689710750</td><td style=\"text-align: right;\">                   1</td><td>66707558  </td></tr>\n",
       "<tr><td>FSR_Trainable_67d1b926</td><td>2023-07-19_04-53-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.349049</td><td style=\"text-align: right;\">0.102456 </td><td>172.26.215.93</td><td style=\"text-align: right;\">425354</td><td style=\"text-align: right;\">0.65356 </td><td style=\"text-align: right;\">           705.869  </td><td style=\"text-align: right;\">          7.15294 </td><td style=\"text-align: right;\">     705.869  </td><td style=\"text-align: right;\"> 1689710023</td><td style=\"text-align: right;\">                 100</td><td>67d1b926  </td></tr>\n",
       "<tr><td>FSR_Trainable_68dc015f</td><td>2023-07-19_04-40-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.370902</td><td style=\"text-align: right;\">0.0939557</td><td>172.26.215.93</td><td style=\"text-align: right;\">424174</td><td style=\"text-align: right;\">0.715395</td><td style=\"text-align: right;\">             2.44965</td><td style=\"text-align: right;\">          1.02675 </td><td style=\"text-align: right;\">       2.44965</td><td style=\"text-align: right;\"> 1689709232</td><td style=\"text-align: right;\">                   2</td><td>68dc015f  </td></tr>\n",
       "<tr><td>FSR_Trainable_6c487573</td><td>2023-07-19_05-18-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.333457</td><td style=\"text-align: right;\">0.0959044</td><td>172.26.215.93</td><td style=\"text-align: right;\">431016</td><td style=\"text-align: right;\">0.63138 </td><td style=\"text-align: right;\">           890.01   </td><td style=\"text-align: right;\">          8.50118 </td><td style=\"text-align: right;\">     890.01   </td><td style=\"text-align: right;\"> 1689711503</td><td style=\"text-align: right;\">                 100</td><td>6c487573  </td></tr>\n",
       "<tr><td>FSR_Trainable_70da697c</td><td>2023-07-19_05-44-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.326881</td><td style=\"text-align: right;\">0.0939789</td><td>172.26.215.93</td><td style=\"text-align: right;\">442852</td><td style=\"text-align: right;\">0.613109</td><td style=\"text-align: right;\">           132.843  </td><td style=\"text-align: right;\">          1.23055 </td><td style=\"text-align: right;\">     132.843  </td><td style=\"text-align: right;\"> 1689713044</td><td style=\"text-align: right;\">                 100</td><td>70da697c  </td></tr>\n",
       "<tr><td>FSR_Trainable_724f13a8</td><td>2023-07-19_05-29-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.32214 </td><td style=\"text-align: right;\">0.0949654</td><td>172.26.215.93</td><td style=\"text-align: right;\">437712</td><td style=\"text-align: right;\">0.614532</td><td style=\"text-align: right;\">           298.472  </td><td style=\"text-align: right;\">          2.80469 </td><td style=\"text-align: right;\">     298.472  </td><td style=\"text-align: right;\"> 1689712142</td><td style=\"text-align: right;\">                 100</td><td>724f13a8  </td></tr>\n",
       "<tr><td>FSR_Trainable_731370c8</td><td>2023-07-19_05-45-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.345068</td><td style=\"text-align: right;\">0.100009 </td><td>172.26.215.93</td><td style=\"text-align: right;\">445270</td><td style=\"text-align: right;\">0.686079</td><td style=\"text-align: right;\">             3.52372</td><td style=\"text-align: right;\">          1.71728 </td><td style=\"text-align: right;\">       3.52372</td><td style=\"text-align: right;\"> 1689713105</td><td style=\"text-align: right;\">                   2</td><td>731370c8  </td></tr>\n",
       "<tr><td>FSR_Trainable_76c223ee</td><td>2023-07-19_05-48-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.314952</td><td style=\"text-align: right;\">0.0910887</td><td>172.26.215.93</td><td style=\"text-align: right;\">446407</td><td style=\"text-align: right;\">0.606673</td><td style=\"text-align: right;\">           120.675  </td><td style=\"text-align: right;\">          0.893151</td><td style=\"text-align: right;\">     120.675  </td><td style=\"text-align: right;\"> 1689713321</td><td style=\"text-align: right;\">                 100</td><td>76c223ee  </td></tr>\n",
       "<tr><td>FSR_Trainable_790ae3b3</td><td>2023-07-19_04-41-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.3711  </td><td style=\"text-align: right;\">0.377225 </td><td>172.26.215.93</td><td style=\"text-align: right;\">424680</td><td style=\"text-align: right;\">2.71721 </td><td style=\"text-align: right;\">             3.36752</td><td style=\"text-align: right;\">          3.36752 </td><td style=\"text-align: right;\">       3.36752</td><td style=\"text-align: right;\"> 1689709272</td><td style=\"text-align: right;\">                   1</td><td>790ae3b3  </td></tr>\n",
       "<tr><td>FSR_Trainable_792c1063</td><td>2023-07-19_05-43-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.34119 </td><td style=\"text-align: right;\">0.0972099</td><td>172.26.215.93</td><td style=\"text-align: right;\">443983</td><td style=\"text-align: right;\">0.636646</td><td style=\"text-align: right;\">            41.7288 </td><td style=\"text-align: right;\">          1.22814 </td><td style=\"text-align: right;\">      41.7288 </td><td style=\"text-align: right;\"> 1689713015</td><td style=\"text-align: right;\">                  32</td><td>792c1063  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b3c7daa</td><td>2023-07-19_05-13-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.300007</td><td style=\"text-align: right;\">0.0784201</td><td>172.26.215.93</td><td style=\"text-align: right;\">433376</td><td style=\"text-align: right;\">0.586407</td><td style=\"text-align: right;\">           373.995  </td><td style=\"text-align: right;\">          3.86804 </td><td style=\"text-align: right;\">     373.995  </td><td style=\"text-align: right;\"> 1689711229</td><td style=\"text-align: right;\">                 100</td><td>7b3c7daa  </td></tr>\n",
       "<tr><td>FSR_Trainable_7cee5b7f</td><td>2023-07-19_05-02-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.390716</td><td style=\"text-align: right;\">0.109843 </td><td>172.26.215.93</td><td style=\"text-align: right;\">430528</td><td style=\"text-align: right;\">0.70166 </td><td style=\"text-align: right;\">            21.9323 </td><td style=\"text-align: right;\">         10.0873  </td><td style=\"text-align: right;\">      21.9323 </td><td style=\"text-align: right;\"> 1689710555</td><td style=\"text-align: right;\">                   2</td><td>7cee5b7f  </td></tr>\n",
       "<tr><td>FSR_Trainable_7dd581fc</td><td>2023-07-19_04-55-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.37358 </td><td style=\"text-align: right;\">0.110133 </td><td>172.26.215.93</td><td style=\"text-align: right;\">427739</td><td style=\"text-align: right;\">0.701164</td><td style=\"text-align: right;\">             3.7725 </td><td style=\"text-align: right;\">          1.7083  </td><td style=\"text-align: right;\">       3.7725 </td><td style=\"text-align: right;\"> 1689710121</td><td style=\"text-align: right;\">                   2</td><td>7dd581fc  </td></tr>\n",
       "<tr><td>FSR_Trainable_7df97178</td><td>2023-07-19_05-41-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.321327</td><td style=\"text-align: right;\">0.093086 </td><td>172.26.215.93</td><td style=\"text-align: right;\">441804</td><td style=\"text-align: right;\">0.61092 </td><td style=\"text-align: right;\">           126.707  </td><td style=\"text-align: right;\">          1.15336 </td><td style=\"text-align: right;\">     126.707  </td><td style=\"text-align: right;\"> 1689712883</td><td style=\"text-align: right;\">                 100</td><td>7df97178  </td></tr>\n",
       "<tr><td>FSR_Trainable_7fc34d8d</td><td>2023-07-19_05-13-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.349244</td><td style=\"text-align: right;\">0.105582 </td><td>172.26.215.93</td><td style=\"text-align: right;\">433924</td><td style=\"text-align: right;\">0.68997 </td><td style=\"text-align: right;\">             8.26124</td><td style=\"text-align: right;\">          1.84165 </td><td style=\"text-align: right;\">       8.26124</td><td style=\"text-align: right;\"> 1689711181</td><td style=\"text-align: right;\">                   4</td><td>7fc34d8d  </td></tr>\n",
       "<tr><td>FSR_Trainable_83d5cc84</td><td>2023-07-19_05-03-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.351064</td><td style=\"text-align: right;\">0.10186  </td><td>172.26.215.93</td><td style=\"text-align: right;\">429281</td><td style=\"text-align: right;\">0.671861</td><td style=\"text-align: right;\">           197.942  </td><td style=\"text-align: right;\">         13.1966  </td><td style=\"text-align: right;\">     197.942  </td><td style=\"text-align: right;\"> 1689710584</td><td style=\"text-align: right;\">                  16</td><td>83d5cc84  </td></tr>\n",
       "<tr><td>FSR_Trainable_862fc8b3</td><td>2023-07-19_05-04-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.399651</td><td style=\"text-align: right;\">0.103969 </td><td>172.26.215.93</td><td style=\"text-align: right;\">431490</td><td style=\"text-align: right;\">0.721748</td><td style=\"text-align: right;\">             3.90601</td><td style=\"text-align: right;\">          3.90601 </td><td style=\"text-align: right;\">       3.90601</td><td style=\"text-align: right;\"> 1689710693</td><td style=\"text-align: right;\">                   1</td><td>862fc8b3  </td></tr>\n",
       "<tr><td>FSR_Trainable_8b389096</td><td>2023-07-19_05-46-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.330978</td><td style=\"text-align: right;\">0.0938102</td><td>172.26.215.93</td><td style=\"text-align: right;\">444308</td><td style=\"text-align: right;\">0.620201</td><td style=\"text-align: right;\">           134.331  </td><td style=\"text-align: right;\">          1.34579 </td><td style=\"text-align: right;\">     134.331  </td><td style=\"text-align: right;\"> 1689713166</td><td style=\"text-align: right;\">                 100</td><td>8b389096  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ed6e88f</td><td>2023-07-19_05-34-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.304317</td><td style=\"text-align: right;\">0.0794331</td><td>172.26.215.93</td><td style=\"text-align: right;\">436625</td><td style=\"text-align: right;\">0.589899</td><td style=\"text-align: right;\">           883.873  </td><td style=\"text-align: right;\">          8.30003 </td><td style=\"text-align: right;\">     883.873  </td><td style=\"text-align: right;\"> 1689712445</td><td style=\"text-align: right;\">                 100</td><td>8ed6e88f  </td></tr>\n",
       "<tr><td>FSR_Trainable_93bca7f9</td><td>2023-07-19_05-00-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.356184</td><td style=\"text-align: right;\">0.102658 </td><td>172.26.215.93</td><td style=\"text-align: right;\">428860</td><td style=\"text-align: right;\">0.663117</td><td style=\"text-align: right;\">            41.6836 </td><td style=\"text-align: right;\">          1.24949 </td><td style=\"text-align: right;\">      41.6836 </td><td style=\"text-align: right;\"> 1689710401</td><td style=\"text-align: right;\">                  32</td><td>93bca7f9  </td></tr>\n",
       "<tr><td>FSR_Trainable_93d2f812</td><td>2023-07-19_05-29-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.342227</td><td style=\"text-align: right;\">0.0906096</td><td>172.26.215.93</td><td style=\"text-align: right;\">438662</td><td style=\"text-align: right;\">0.71022 </td><td style=\"text-align: right;\">             4.23094</td><td style=\"text-align: right;\">          4.23094 </td><td style=\"text-align: right;\">       4.23094</td><td style=\"text-align: right;\"> 1689712199</td><td style=\"text-align: right;\">                   1</td><td>93d2f812  </td></tr>\n",
       "<tr><td>FSR_Trainable_97db8431</td><td>2023-07-19_05-14-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.38675 </td><td style=\"text-align: right;\">0.102646 </td><td>172.26.215.93</td><td style=\"text-align: right;\">434602</td><td style=\"text-align: right;\">0.714362</td><td style=\"text-align: right;\">             7.09465</td><td style=\"text-align: right;\">          7.09465 </td><td style=\"text-align: right;\">       7.09465</td><td style=\"text-align: right;\"> 1689711260</td><td style=\"text-align: right;\">                   1</td><td>97db8431  </td></tr>\n",
       "<tr><td>FSR_Trainable_97fe2006</td><td>2023-07-19_04-43-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.331042</td><td style=\"text-align: right;\">0.0954702</td><td>172.26.215.93</td><td style=\"text-align: right;\">424413</td><td style=\"text-align: right;\">0.631969</td><td style=\"text-align: right;\">           148.673  </td><td style=\"text-align: right;\">          1.75059 </td><td style=\"text-align: right;\">     148.673  </td><td style=\"text-align: right;\"> 1689709406</td><td style=\"text-align: right;\">                 100</td><td>97fe2006  </td></tr>\n",
       "<tr><td>FSR_Trainable_99ae87a3</td><td>2023-07-19_04-39-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.392865</td><td style=\"text-align: right;\">0.109277 </td><td>172.26.215.93</td><td style=\"text-align: right;\">422259</td><td style=\"text-align: right;\">0.712342</td><td style=\"text-align: right;\">            40.5624 </td><td style=\"text-align: right;\">          2.91747 </td><td style=\"text-align: right;\">      40.5624 </td><td style=\"text-align: right;\"> 1689709171</td><td style=\"text-align: right;\">                  16</td><td>99ae87a3  </td></tr>\n",
       "<tr><td>FSR_Trainable_9b684290</td><td>2023-07-19_05-35-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.370438</td><td style=\"text-align: right;\">0.103788 </td><td>172.26.215.93</td><td style=\"text-align: right;\">440450</td><td style=\"text-align: right;\">0.664227</td><td style=\"text-align: right;\">            33.2778 </td><td style=\"text-align: right;\">          2.04212 </td><td style=\"text-align: right;\">      33.2778 </td><td style=\"text-align: right;\"> 1689712523</td><td style=\"text-align: right;\">                  16</td><td>9b684290  </td></tr>\n",
       "<tr><td>FSR_Trainable_9b7539f6</td><td>2023-07-19_04-40-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.673007</td><td style=\"text-align: right;\">0.126299 </td><td>172.26.215.93</td><td style=\"text-align: right;\">423957</td><td style=\"text-align: right;\">0.958985</td><td style=\"text-align: right;\">             2.42431</td><td style=\"text-align: right;\">          2.42431 </td><td style=\"text-align: right;\">       2.42431</td><td style=\"text-align: right;\"> 1689709214</td><td style=\"text-align: right;\">                   1</td><td>9b7539f6  </td></tr>\n",
       "<tr><td>FSR_Trainable_a05e09bf</td><td>2023-07-19_05-46-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.319046</td><td style=\"text-align: right;\">0.0901059</td><td>172.26.215.93</td><td style=\"text-align: right;\">444968</td><td style=\"text-align: right;\">0.603241</td><td style=\"text-align: right;\">           137.503  </td><td style=\"text-align: right;\">          1.29258 </td><td style=\"text-align: right;\">     137.503  </td><td style=\"text-align: right;\"> 1689713214</td><td style=\"text-align: right;\">                 100</td><td>a05e09bf  </td></tr>\n",
       "<tr><td>FSR_Trainable_a07ed3a2</td><td>2023-07-19_05-42-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.341147</td><td style=\"text-align: right;\">0.0949036</td><td>172.26.215.93</td><td style=\"text-align: right;\">443036</td><td style=\"text-align: right;\">0.633714</td><td style=\"text-align: right;\">            34.8522 </td><td style=\"text-align: right;\">          1.05008 </td><td style=\"text-align: right;\">      34.8522 </td><td style=\"text-align: right;\"> 1689712945</td><td style=\"text-align: right;\">                  32</td><td>a07ed3a2  </td></tr>\n",
       "<tr><td>FSR_Trainable_a32f1ec5</td><td>2023-07-19_05-00-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.364631</td><td style=\"text-align: right;\">0.0978781</td><td>172.26.215.93</td><td style=\"text-align: right;\">429753</td><td style=\"text-align: right;\">0.710227</td><td style=\"text-align: right;\">            13.2248 </td><td style=\"text-align: right;\">         13.2248  </td><td style=\"text-align: right;\">      13.2248 </td><td style=\"text-align: right;\"> 1689710440</td><td style=\"text-align: right;\">                   1</td><td>a32f1ec5  </td></tr>\n",
       "<tr><td>FSR_Trainable_a6f25235</td><td>2023-07-19_05-41-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.314786</td><td style=\"text-align: right;\">0.0913061</td><td>172.26.215.93</td><td style=\"text-align: right;\">442011</td><td style=\"text-align: right;\">0.605353</td><td style=\"text-align: right;\">           124.879  </td><td style=\"text-align: right;\">          1.39407 </td><td style=\"text-align: right;\">     124.879  </td><td style=\"text-align: right;\"> 1689712893</td><td style=\"text-align: right;\">                 100</td><td>a6f25235  </td></tr>\n",
       "<tr><td>FSR_Trainable_b227b79a</td><td>2023-07-19_05-38-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.314907</td><td style=\"text-align: right;\">0.0908252</td><td>172.26.215.93</td><td style=\"text-align: right;\">440704</td><td style=\"text-align: right;\">0.599061</td><td style=\"text-align: right;\">           201.886  </td><td style=\"text-align: right;\">          1.92998 </td><td style=\"text-align: right;\">     201.886  </td><td style=\"text-align: right;\"> 1689712733</td><td style=\"text-align: right;\">                 100</td><td>b227b79a  </td></tr>\n",
       "<tr><td>FSR_Trainable_b339785d</td><td>2023-07-19_05-34-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.325563</td><td style=\"text-align: right;\">0.0947538</td><td>172.26.215.93</td><td style=\"text-align: right;\">439376</td><td style=\"text-align: right;\">0.619144</td><td style=\"text-align: right;\">           206.481  </td><td style=\"text-align: right;\">          2.09634 </td><td style=\"text-align: right;\">     206.481  </td><td style=\"text-align: right;\"> 1689712476</td><td style=\"text-align: right;\">                 100</td><td>b339785d  </td></tr>\n",
       "<tr><td>FSR_Trainable_b47a652f</td><td>2023-07-19_05-48-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.325762</td><td style=\"text-align: right;\">0.092586 </td><td>172.26.215.93</td><td style=\"text-align: right;\">446928</td><td style=\"text-align: right;\">0.619042</td><td style=\"text-align: right;\">           102.98   </td><td style=\"text-align: right;\">          0.632871</td><td style=\"text-align: right;\">     102.98   </td><td style=\"text-align: right;\"> 1689713338</td><td style=\"text-align: right;\">                 100</td><td>b47a652f  </td></tr>\n",
       "<tr><td>FSR_Trainable_b681b3a2</td><td>2023-07-19_05-34-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.322406</td><td style=\"text-align: right;\">0.0853338</td><td>172.26.215.93</td><td style=\"text-align: right;\">438413</td><td style=\"text-align: right;\">0.626605</td><td style=\"text-align: right;\">           287.483  </td><td style=\"text-align: right;\">          2.68855 </td><td style=\"text-align: right;\">     287.483  </td><td style=\"text-align: right;\"> 1689712451</td><td style=\"text-align: right;\">                 100</td><td>b681b3a2  </td></tr>\n",
       "<tr><td>FSR_Trainable_b8c4a990</td><td>2023-07-19_04-41-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.346678</td><td style=\"text-align: right;\">0.0992888</td><td>172.26.215.93</td><td style=\"text-align: right;\">423004</td><td style=\"text-align: right;\">0.663669</td><td style=\"text-align: right;\">            91.5706 </td><td style=\"text-align: right;\">          0.810489</td><td style=\"text-align: right;\">      91.5706 </td><td style=\"text-align: right;\"> 1689709260</td><td style=\"text-align: right;\">                 100</td><td>b8c4a990  </td></tr>\n",
       "<tr><td>FSR_Trainable_ba5a2d7b</td><td>2023-07-19_05-14-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.366426</td><td style=\"text-align: right;\">0.0993657</td><td>172.26.215.93</td><td style=\"text-align: right;\">434878</td><td style=\"text-align: right;\">0.710208</td><td style=\"text-align: right;\">             7.4323 </td><td style=\"text-align: right;\">          7.4323  </td><td style=\"text-align: right;\">       7.4323 </td><td style=\"text-align: right;\"> 1689711284</td><td style=\"text-align: right;\">                   1</td><td>ba5a2d7b  </td></tr>\n",
       "<tr><td>FSR_Trainable_bf496b10</td><td>2023-07-19_05-37-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.315949</td><td style=\"text-align: right;\">0.0875605</td><td>172.26.215.93</td><td style=\"text-align: right;\">439986</td><td style=\"text-align: right;\">0.605562</td><td style=\"text-align: right;\">           203.875  </td><td style=\"text-align: right;\">          1.96565 </td><td style=\"text-align: right;\">     203.875  </td><td style=\"text-align: right;\"> 1689712672</td><td style=\"text-align: right;\">                 100</td><td>bf496b10  </td></tr>\n",
       "<tr><td>FSR_Trainable_bfb68440</td><td>2023-07-19_04-54-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.462089</td><td style=\"text-align: right;\">0.117985 </td><td>172.26.215.93</td><td style=\"text-align: right;\">427504</td><td style=\"text-align: right;\">0.73876 </td><td style=\"text-align: right;\">            10.8818 </td><td style=\"text-align: right;\">         10.8818  </td><td style=\"text-align: right;\">      10.8818 </td><td style=\"text-align: right;\"> 1689710099</td><td style=\"text-align: right;\">                   1</td><td>bfb68440  </td></tr>\n",
       "<tr><td>FSR_Trainable_c0373b8f</td><td>2023-07-19_04-43-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.357381</td><td style=\"text-align: right;\">0.102785 </td><td>172.26.215.93</td><td style=\"text-align: right;\">426118</td><td style=\"text-align: right;\">0.69091 </td><td style=\"text-align: right;\">            13.4791 </td><td style=\"text-align: right;\">          1.62863 </td><td style=\"text-align: right;\">      13.4791 </td><td style=\"text-align: right;\"> 1689709433</td><td style=\"text-align: right;\">                   8</td><td>c0373b8f  </td></tr>\n",
       "<tr><td>FSR_Trainable_c371adcf</td><td>2023-07-19_05-30-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.349015</td><td style=\"text-align: right;\">0.0900087</td><td>172.26.215.93</td><td style=\"text-align: right;\">438894</td><td style=\"text-align: right;\">0.720231</td><td style=\"text-align: right;\">             3.59614</td><td style=\"text-align: right;\">          3.59614 </td><td style=\"text-align: right;\">       3.59614</td><td style=\"text-align: right;\"> 1689712218</td><td style=\"text-align: right;\">                   1</td><td>c371adcf  </td></tr>\n",
       "<tr><td>FSR_Trainable_c581bc54</td><td>2023-07-19_04-58-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.350362</td><td style=\"text-align: right;\">0.101088 </td><td>172.26.215.93</td><td style=\"text-align: right;\">427952</td><td style=\"text-align: right;\">0.644737</td><td style=\"text-align: right;\">           168.189  </td><td style=\"text-align: right;\">          1.53939 </td><td style=\"text-align: right;\">     168.189  </td><td style=\"text-align: right;\"> 1689710307</td><td style=\"text-align: right;\">                 100</td><td>c581bc54  </td></tr>\n",
       "<tr><td>FSR_Trainable_c7594953</td><td>2023-07-19_05-06-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.382127</td><td style=\"text-align: right;\">0.104462 </td><td>172.26.215.93</td><td style=\"text-align: right;\">432647</td><td style=\"text-align: right;\">0.703641</td><td style=\"text-align: right;\">            14.4694 </td><td style=\"text-align: right;\">         14.4694  </td><td style=\"text-align: right;\">      14.4694 </td><td style=\"text-align: right;\"> 1689710796</td><td style=\"text-align: right;\">                   1</td><td>c7594953  </td></tr>\n",
       "<tr><td>FSR_Trainable_c7c45904</td><td>2023-07-19_05-13-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.396434</td><td style=\"text-align: right;\">0.107831 </td><td>172.26.215.93</td><td style=\"text-align: right;\">434162</td><td style=\"text-align: right;\">0.718455</td><td style=\"text-align: right;\">            36.6912 </td><td style=\"text-align: right;\">         17.7449  </td><td style=\"text-align: right;\">      36.6912 </td><td style=\"text-align: right;\"> 1689711232</td><td style=\"text-align: right;\">                   2</td><td>c7c45904  </td></tr>\n",
       "<tr><td>FSR_Trainable_c9d152bb</td><td>2023-07-19_04-43-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.366524</td><td style=\"text-align: right;\">0.107753 </td><td>172.26.215.93</td><td style=\"text-align: right;\">425661</td><td style=\"text-align: right;\">0.700885</td><td style=\"text-align: right;\">             5.52855</td><td style=\"text-align: right;\">          1.19648 </td><td style=\"text-align: right;\">       5.52855</td><td style=\"text-align: right;\"> 1689709388</td><td style=\"text-align: right;\">                   4</td><td>c9d152bb  </td></tr>\n",
       "<tr><td>FSR_Trainable_ca34a79a</td><td>2023-07-19_05-18-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.350808</td><td style=\"text-align: right;\">0.101144 </td><td>172.26.215.93</td><td style=\"text-align: right;\">436370</td><td style=\"text-align: right;\">0.699728</td><td style=\"text-align: right;\">            19.9718 </td><td style=\"text-align: right;\">          9.42893 </td><td style=\"text-align: right;\">      19.9718 </td><td style=\"text-align: right;\"> 1689711538</td><td style=\"text-align: right;\">                   2</td><td>ca34a79a  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4d76e25</td><td>2023-07-19_04-39-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.51432 </td><td style=\"text-align: right;\">0.398377 </td><td>172.26.215.93</td><td style=\"text-align: right;\">422505</td><td style=\"text-align: right;\">2.79348 </td><td style=\"text-align: right;\">             2.74461</td><td style=\"text-align: right;\">          2.74461 </td><td style=\"text-align: right;\">       2.74461</td><td style=\"text-align: right;\"> 1689709140</td><td style=\"text-align: right;\">                   1</td><td>d4d76e25  </td></tr>\n",
       "<tr><td>FSR_Trainable_d67889bd</td><td>2023-07-19_04-41-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.544949</td><td style=\"text-align: right;\">0.121854 </td><td>172.26.215.93</td><td style=\"text-align: right;\">424869</td><td style=\"text-align: right;\">0.87126 </td><td style=\"text-align: right;\">            16.0813 </td><td style=\"text-align: right;\">         16.0813  </td><td style=\"text-align: right;\">      16.0813 </td><td style=\"text-align: right;\"> 1689709295</td><td style=\"text-align: right;\">                   1</td><td>d67889bd  </td></tr>\n",
       "<tr><td>FSR_Trainable_d77e91ba</td><td>2023-07-19_04-54-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.346369</td><td style=\"text-align: right;\">0.101779 </td><td>172.26.215.93</td><td style=\"text-align: right;\">427248</td><td style=\"text-align: right;\">0.698052</td><td style=\"text-align: right;\">            38.3897 </td><td style=\"text-align: right;\">          9.24988 </td><td style=\"text-align: right;\">      38.3897 </td><td style=\"text-align: right;\"> 1689710076</td><td style=\"text-align: right;\">                   4</td><td>d77e91ba  </td></tr>\n",
       "<tr><td>FSR_Trainable_d85b6355</td><td>2023-07-19_05-15-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.505923</td><td style=\"text-align: right;\">0.120233 </td><td>172.26.215.93</td><td style=\"text-align: right;\">435292</td><td style=\"text-align: right;\">0.801825</td><td style=\"text-align: right;\">             5.29664</td><td style=\"text-align: right;\">          5.29664 </td><td style=\"text-align: right;\">       5.29664</td><td style=\"text-align: right;\"> 1689711309</td><td style=\"text-align: right;\">                   1</td><td>d85b6355  </td></tr>\n",
       "<tr><td>FSR_Trainable_da2ca992</td><td>2023-07-19_04-58-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.34131 </td><td style=\"text-align: right;\">0.100742 </td><td>172.26.215.93</td><td style=\"text-align: right;\">428612</td><td style=\"text-align: right;\">0.693958</td><td style=\"text-align: right;\">            13.6007 </td><td style=\"text-align: right;\">          1.42203 </td><td style=\"text-align: right;\">      13.6007 </td><td style=\"text-align: right;\"> 1689710339</td><td style=\"text-align: right;\">                   8</td><td>da2ca992  </td></tr>\n",
       "<tr><td>FSR_Trainable_db7b2524</td><td>2023-07-19_05-12-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.336399</td><td style=\"text-align: right;\">0.0996622</td><td>172.26.215.93</td><td style=\"text-align: right;\">425091</td><td style=\"text-align: right;\">0.641626</td><td style=\"text-align: right;\">          1857.83   </td><td style=\"text-align: right;\">         17.2193  </td><td style=\"text-align: right;\">    1857.83   </td><td style=\"text-align: right;\"> 1689711158</td><td style=\"text-align: right;\">                 100</td><td>db7b2524  </td></tr>\n",
       "<tr><td>FSR_Trainable_dd9a73a3</td><td>2023-07-19_04-42-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.348929</td><td style=\"text-align: right;\">0.101239 </td><td>172.26.215.93</td><td style=\"text-align: right;\">423462</td><td style=\"text-align: right;\">0.645463</td><td style=\"text-align: right;\">           174.11   </td><td style=\"text-align: right;\">          1.7582  </td><td style=\"text-align: right;\">     174.11   </td><td style=\"text-align: right;\"> 1689709369</td><td style=\"text-align: right;\">                 100</td><td>dd9a73a3  </td></tr>\n",
       "<tr><td>FSR_Trainable_e0d167a2</td><td>2023-07-19_05-42-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.333757</td><td style=\"text-align: right;\">0.0949958</td><td>172.26.215.93</td><td style=\"text-align: right;\">442288</td><td style=\"text-align: right;\">0.621592</td><td style=\"text-align: right;\">           129.704  </td><td style=\"text-align: right;\">          1.16772 </td><td style=\"text-align: right;\">     129.704  </td><td style=\"text-align: right;\"> 1689712926</td><td style=\"text-align: right;\">                 100</td><td>e0d167a2  </td></tr>\n",
       "<tr><td>FSR_Trainable_e16e66cb</td><td>2023-07-19_05-05-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.377134</td><td style=\"text-align: right;\">0.104198 </td><td>172.26.215.93</td><td style=\"text-align: right;\">431719</td><td style=\"text-align: right;\">0.706834</td><td style=\"text-align: right;\">             2.35457</td><td style=\"text-align: right;\">          2.35457 </td><td style=\"text-align: right;\">       2.35457</td><td style=\"text-align: right;\"> 1689710715</td><td style=\"text-align: right;\">                   1</td><td>e16e66cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_e5680a88</td><td>2023-07-19_05-06-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.47681 </td><td style=\"text-align: right;\">0.35259  </td><td>172.26.215.93</td><td style=\"text-align: right;\">432413</td><td style=\"text-align: right;\">2.89686 </td><td style=\"text-align: right;\">             1.51698</td><td style=\"text-align: right;\">          1.51698 </td><td style=\"text-align: right;\">       1.51698</td><td style=\"text-align: right;\"> 1689710766</td><td style=\"text-align: right;\">                   1</td><td>e5680a88  </td></tr>\n",
       "<tr><td>FSR_Trainable_e99194b2</td><td>2023-07-19_05-01-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.363575</td><td style=\"text-align: right;\">0.095998 </td><td>172.26.215.93</td><td style=\"text-align: right;\">430038</td><td style=\"text-align: right;\">0.710794</td><td style=\"text-align: right;\">            13.9097 </td><td style=\"text-align: right;\">         13.9097  </td><td style=\"text-align: right;\">      13.9097 </td><td style=\"text-align: right;\"> 1689710470</td><td style=\"text-align: right;\">                   1</td><td>e99194b2  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec4e1bdd</td><td>2023-07-19_05-19-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.346976</td><td style=\"text-align: right;\">0.101881 </td><td>172.26.215.93</td><td style=\"text-align: right;\">429574</td><td style=\"text-align: right;\">0.645236</td><td style=\"text-align: right;\">          1138.65   </td><td style=\"text-align: right;\">         10.7222  </td><td style=\"text-align: right;\">    1138.65   </td><td style=\"text-align: right;\"> 1689711567</td><td style=\"text-align: right;\">                 100</td><td>ec4e1bdd  </td></tr>\n",
       "<tr><td>FSR_Trainable_f1c1a58b</td><td>2023-07-19_05-07-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.389901</td><td style=\"text-align: right;\">0.110095 </td><td>172.26.215.93</td><td style=\"text-align: right;\">433118</td><td style=\"text-align: right;\">0.698916</td><td style=\"text-align: right;\">             4.8694 </td><td style=\"text-align: right;\">          2.19414 </td><td style=\"text-align: right;\">       4.8694 </td><td style=\"text-align: right;\"> 1689710836</td><td style=\"text-align: right;\">                   2</td><td>f1c1a58b  </td></tr>\n",
       "<tr><td>FSR_Trainable_f27fd036</td><td>2023-07-19_05-44-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.329277</td><td style=\"text-align: right;\">0.0902519</td><td>172.26.215.93</td><td style=\"text-align: right;\">443339</td><td style=\"text-align: right;\">0.62043 </td><td style=\"text-align: right;\">           131.987  </td><td style=\"text-align: right;\">          1.27965 </td><td style=\"text-align: right;\">     131.987  </td><td style=\"text-align: right;\"> 1689713087</td><td style=\"text-align: right;\">                 100</td><td>f27fd036  </td></tr>\n",
       "<tr><td>FSR_Trainable_fb678dc2</td><td>2023-07-19_05-05-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.363499</td><td style=\"text-align: right;\">0.101257 </td><td>172.26.215.93</td><td style=\"text-align: right;\">431953</td><td style=\"text-align: right;\">0.703351</td><td style=\"text-align: right;\">             1.90359</td><td style=\"text-align: right;\">          1.90359 </td><td style=\"text-align: right;\">       1.90359</td><td style=\"text-align: right;\"> 1689710734</td><td style=\"text-align: right;\">                   1</td><td>fb678dc2  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_99ae87a3_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-38-38/wandb/run-20230719_043849-99ae87a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Syncing run FSR_Trainable_99ae87a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/99ae87a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_50eaad01_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-38-43/wandb/run-20230719_043856-50eaad01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Syncing run FSR_Trainable_50eaad01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/50eaad01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                      mae 2.17743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                     mape 0.39692\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                     rmse 2.66453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:       time_since_restore 1.97311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:         time_this_iter_s 1.97311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:             time_total_s 1.97311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:                timestamp 1689709132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb:  View run FSR_Trainable_50eaad01 at: https://wandb.ai/seokjin/FSR-prediction/runs/50eaad01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422504)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043856-50eaad01/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_d4d76e25_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-38-50/wandb/run-20230719_043904-d4d76e25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Syncing run FSR_Trainable_d4d76e25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d4d76e25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                      mae 2.51432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                     mape 0.39838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                     rmse 2.79348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:       time_since_restore 2.74461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:         time_this_iter_s 2.74461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:             time_total_s 2.74461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:                timestamp 1689709140\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb:  View run FSR_Trainable_d4d76e25 at: https://wandb.ai/seokjin/FSR-prediction/runs/d4d76e25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422681)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043904-d4d76e25/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_61ab7eb6_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-38-58/wandb/run-20230719_043911-61ab7eb6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Syncing run FSR_Trainable_61ab7eb6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/61ab7eb6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_b8c4a990_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-39-05/wandb/run-20230719_043922-b8c4a990\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Syncing run FSR_Trainable_b8c4a990\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b8c4a990\n",
      "2023-07-19 04:39:32,485\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.758 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:39:32,488\tWARNING util.py:315 -- The `process_trial_result` operation took 1.762 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:39:32,489\tWARNING util.py:315 -- Processing trial results took 1.764 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:39:32,491\tWARNING util.py:315 -- The `process_trial_result` operation took 1.765 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                      mae 0.39287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                     mape 0.10928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                     rmse 0.71234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:       time_since_restore 40.56239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:         time_this_iter_s 2.91747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:             time_total_s 40.56239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:                timestamp 1689709171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb:  View run FSR_Trainable_99ae87a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/99ae87a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422329)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043849-99ae87a3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_4f0e8c70_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-39-16/wandb/run-20230719_043935-4f0e8c70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Syncing run FSR_Trainable_4f0e8c70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f0e8c70\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043935-4f0e8c70/logs\n",
      "2023-07-19 04:39:46,505\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:39:46,507\tWARNING util.py:315 -- The `process_trial_result` operation took 2.024 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:39:46,509\tWARNING util.py:315 -- Processing trial results took 2.026 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:39:46,511\tWARNING util.py:315 -- The `process_trial_result` operation took 2.028 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423313)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_dd9a73a3_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-39-29/wandb/run-20230719_043948-dd9a73a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Syncing run FSR_Trainable_dd9a73a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dd9a73a3\n",
      "2023-07-19 04:39:57,793\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.760 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:39:57,799\tWARNING util.py:315 -- The `process_trial_result` operation took 1.767 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:39:57,801\tWARNING util.py:315 -- Processing trial results took 1.768 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:39:57,804\tWARNING util.py:315 -- The `process_trial_result` operation took 1.772 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_1e0b8bff_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-39-42/wandb/run-20230719_043959-1e0b8bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Syncing run FSR_Trainable_1e0b8bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1e0b8bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                      mae 0.394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                     mape 0.10806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                     rmse 0.7115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:       time_since_restore 5.73248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:         time_this_iter_s 2.62532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:             time_total_s 5.73248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:                timestamp 1689709200\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb:  View run FSR_Trainable_1e0b8bff at: https://wandb.ai/seokjin/FSR-prediction/runs/1e0b8bff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423779)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043959-1e0b8bff/logs\n",
      "2023-07-19 04:40:16,131\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:40:16,136\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:40:16,137\tWARNING util.py:315 -- Processing trial results took 1.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:40:16,139\tWARNING util.py:315 -- The `process_trial_result` operation took 1.885 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_9b7539f6_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleIm_2023-07-19_04-39-52/wandb/run-20230719_044018-9b7539f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Syncing run FSR_Trainable_9b7539f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9b7539f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                      mae 0.67301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                     mape 0.1263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                     rmse 0.95898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:       time_since_restore 2.42431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:         time_this_iter_s 2.42431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:             time_total_s 2.42431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:                timestamp 1689709214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb:  View run FSR_Trainable_9b7539f6 at: https://wandb.ai/seokjin/FSR-prediction/runs/9b7539f6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424019)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044018-9b7539f6/logs\n",
      "2023-07-19 04:40:31,317\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.929 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:40:31,320\tWARNING util.py:315 -- The `process_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:40:31,323\tWARNING util.py:315 -- Processing trial results took 1.936 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:40:31,326\tWARNING util.py:315 -- The `process_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_68dc015f_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-40-11/wandb/run-20230719_044034-68dc015f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Syncing run FSR_Trainable_68dc015f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/68dc015f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                      mae 0.3709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                     mape 0.09396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                     rmse 0.7154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:       time_since_restore 2.44965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:         time_this_iter_s 1.02675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:             time_total_s 2.44965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:                timestamp 1689709232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb:  View run FSR_Trainable_68dc015f at: https://wandb.ai/seokjin/FSR-prediction/runs/68dc015f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424258)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044034-68dc015f/logs\n",
      "2023-07-19 04:40:47,394\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:40:47,397\tWARNING util.py:315 -- The `process_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:40:47,398\tWARNING util.py:315 -- Processing trial results took 1.920 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:40:47,410\tWARNING util.py:315 -- The `process_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_97fe2006_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-40-27/wandb/run-20230719_044050-97fe2006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Syncing run FSR_Trainable_97fe2006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/97fe2006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                      mae 0.35673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                     mape 0.10297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                     rmse 0.7012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:       time_since_restore 102.437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:         time_this_iter_s 0.97929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:             time_total_s 102.437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:                timestamp 1689709261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb:  View run FSR_Trainable_61ab7eb6 at: https://wandb.ai/seokjin/FSR-prediction/runs/61ab7eb6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043911-61ab7eb6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043911-61ab7eb6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb:                timestamp \n",
      "2023-07-19 04:41:14,878\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:41:14,883\tWARNING util.py:315 -- The `process_trial_result` operation took 2.235 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:41:14,890\tWARNING util.py:315 -- Processing trial results took 2.243 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:41:14,893\tWARNING util.py:315 -- The `process_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=422868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_790ae3b3_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-40-43/wandb/run-20230719_044116-790ae3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Syncing run FSR_Trainable_790ae3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/790ae3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                      mae 2.3711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                     mape 0.37722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                     rmse 2.71721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:       time_since_restore 3.36752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:         time_this_iter_s 3.36752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:             time_total_s 3.36752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:                timestamp 1689709272\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb:  View run FSR_Trainable_790ae3b3 at: https://wandb.ai/seokjin/FSR-prediction/runs/790ae3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044116-790ae3b3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424755)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_d67889bd_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-41-09/wandb/run-20230719_044127-d67889bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Syncing run FSR_Trainable_d67889bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d67889bd\n",
      "2023-07-19 04:41:38,004\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:41:38,007\tWARNING util.py:315 -- The `process_trial_result` operation took 2.105 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:41:38,010\tWARNING util.py:315 -- Processing trial results took 2.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:41:38,012\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_db7b2524_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-41-19/wandb/run-20230719_044139-db7b2524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Syncing run FSR_Trainable_db7b2524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/db7b2524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                      mae 0.54495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                     mape 0.12185\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                     rmse 0.87126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:       time_since_restore 16.08132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:         time_this_iter_s 16.08132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:             time_total_s 16.08132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:                timestamp 1689709295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb:  View run FSR_Trainable_d67889bd at: https://wandb.ai/seokjin/FSR-prediction/runs/d67889bd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424976)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044127-d67889bd/logs\n",
      "2023-07-19 04:41:50,632\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:41:50,635\tWARNING util.py:315 -- The `process_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:41:50,636\tWARNING util.py:315 -- Processing trial results took 1.773 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:41:50,637\tWARNING util.py:315 -- The `process_trial_result` operation took 1.774 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "2023-07-19 04:42:00,017\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:42:00,021\tWARNING util.py:315 -- The `process_trial_result` operation took 1.813 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:42:00,025\tWARNING util.py:315 -- Processing trial results took 1.817 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:42:00,027\tWARNING util.py:315 -- The `process_trial_result` operation took 1.819 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_67d1b926_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-41-31/wandb/run-20230719_044158-67d1b926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Syncing run FSR_Trainable_67d1b926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/67d1b926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                      mae 0.34893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                     mape 0.10124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                     rmse 0.64546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:       time_since_restore 174.11038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:         time_this_iter_s 1.7582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:             time_total_s 174.11038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:                timestamp 1689709369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb:  View run FSR_Trainable_dd9a73a3 at: https://wandb.ai/seokjin/FSR-prediction/runs/dd9a73a3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=423562)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_043948-dd9a73a3/logs\n",
      "2023-07-19 04:43:04,938\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.488 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:43:04,957\tWARNING util.py:315 -- The `process_trial_result` operation took 2.507 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:43:04,959\tWARNING util.py:315 -- Processing trial results took 2.509 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:43:04,962\tWARNING util.py:315 -- The `process_trial_result` operation took 2.512 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_c9d152bb_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-41-50/wandb/run-20230719_044308-c9d152bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Syncing run FSR_Trainable_c9d152bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c9d152bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                      mae 0.36652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                     mape 0.10775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                     rmse 0.70088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:       time_since_restore 5.52855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:         time_this_iter_s 1.19648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:             time_total_s 5.52855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:                timestamp 1689709388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb:  View run FSR_Trainable_c9d152bb at: https://wandb.ai/seokjin/FSR-prediction/runs/c9d152bb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425724)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044308-c9d152bb/logs\n",
      "2023-07-19 04:43:24,615\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.425 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:43:24,619\tWARNING util.py:315 -- The `process_trial_result` operation took 2.430 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:43:24,621\tWARNING util.py:315 -- Processing trial results took 2.432 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:43:24,623\tWARNING util.py:315 -- The `process_trial_result` operation took 2.434 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2e45e1fe_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-43-00/wandb/run-20230719_044328-2e45e1fe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Syncing run FSR_Trainable_2e45e1fe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e45e1fe\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                      mae 0.33104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                     mape 0.09547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                     rmse 0.63197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:       time_since_restore 148.67266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:         time_this_iter_s 1.75059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:             time_total_s 148.67266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:                timestamp 1689709406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb:  View run FSR_Trainable_97fe2006 at: https://wandb.ai/seokjin/FSR-prediction/runs/97fe2006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=424494)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044050-97fe2006/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044328-2e45e1fe/logs\n",
      "2023-07-19 04:43:41,577\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.431 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:43:41,581\tWARNING util.py:315 -- The `process_trial_result` operation took 2.436 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:43:41,584\tWARNING util.py:315 -- Processing trial results took 2.439 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:43:41,586\tWARNING util.py:315 -- The `process_trial_result` operation took 2.441 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_c0373b8f_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-43-20/wandb/run-20230719_044344-c0373b8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Syncing run FSR_Trainable_c0373b8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c0373b8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                      mae 0.35738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                     mape 0.10279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                     rmse 0.69091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:       time_since_restore 13.4791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:         time_this_iter_s 1.62863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:             time_total_s 13.4791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:                timestamp 1689709433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb:  View run FSR_Trainable_c0373b8f at: https://wandb.ai/seokjin/FSR-prediction/runs/c0373b8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426206)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044344-c0373b8f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_55b20ff5_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-43-37/wandb/run-20230719_044356-55b20ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Syncing run FSR_Trainable_55b20ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/55b20ff5\n",
      "2023-07-19 04:44:00,772\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.298 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:44:00,778\tWARNING util.py:315 -- The `process_trial_result` operation took 2.305 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:44:00,780\tWARNING util.py:315 -- Processing trial results took 2.306 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:44:00,784\tWARNING util.py:315 -- The `process_trial_result` operation took 2.311 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_15c95f2b_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-43-48/wandb/run-20230719_044413-15c95f2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Syncing run FSR_Trainable_15c95f2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/15c95f2b\n",
      "2023-07-19 04:44:17,279\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.391 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:44:17,281\tWARNING util.py:315 -- The `process_trial_result` operation took 2.394 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:44:17,283\tWARNING util.py:315 -- Processing trial results took 2.396 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:44:17,286\tWARNING util.py:315 -- The `process_trial_result` operation took 2.399 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                      mae 0.34905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                     mape 0.10246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                     rmse 0.65356\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:       time_since_restore 705.86899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:         time_this_iter_s 7.15294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:             time_total_s 705.86899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:                timestamp 1689710023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb:  View run FSR_Trainable_67d1b926 at: https://wandb.ai/seokjin/FSR-prediction/runs/67d1b926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425435)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044158-67d1b926/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_d77e91ba_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-44-04/wandb/run-20230719_045404-d77e91ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Syncing run FSR_Trainable_d77e91ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d77e91ba\n",
      "2023-07-19 04:54:09,098\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.195 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:54:09,100\tWARNING util.py:315 -- The `process_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:54:09,103\tWARNING util.py:315 -- Processing trial results took 2.201 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:54:09,106\tWARNING util.py:315 -- The `process_trial_result` operation took 2.203 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                      mae 0.34637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                     mape 0.10178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                     rmse 0.69805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:       time_since_restore 38.38966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:         time_this_iter_s 9.24988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:             time_total_s 38.38966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:                timestamp 1689710076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb:  View run FSR_Trainable_d77e91ba at: https://wandb.ai/seokjin/FSR-prediction/runs/d77e91ba\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427306)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_045404-d77e91ba/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_bfb68440_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-53-56/wandb/run-20230719_045457-bfb68440\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Syncing run FSR_Trainable_bfb68440\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bfb68440\n",
      "2023-07-19 04:55:02,084\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.329 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:55:02,087\tWARNING util.py:315 -- The `process_trial_result` operation took 2.334 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:55:02,089\tWARNING util.py:315 -- Processing trial results took 2.335 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:55:02,092\tWARNING util.py:315 -- The `process_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                      mae 0.46209\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                     mape 0.11798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                     rmse 0.73876\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:       time_since_restore 10.8818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:         time_this_iter_s 10.8818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:             time_total_s 10.8818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:                timestamp 1689710099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb:  View run FSR_Trainable_bfb68440 at: https://wandb.ai/seokjin/FSR-prediction/runs/bfb68440\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427560)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_045457-bfb68440/logs\n",
      "2023-07-19 04:55:19,408\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.906 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:55:19,417\tWARNING util.py:315 -- The `process_trial_result` operation took 2.915 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:55:19,419\tWARNING util.py:315 -- Processing trial results took 2.918 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:55:19,423\tWARNING util.py:315 -- The `process_trial_result` operation took 2.922 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_7dd581fc_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-54-48/wandb/run-20230719_045522-7dd581fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Syncing run FSR_Trainable_7dd581fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7dd581fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                      mae 0.37358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                     mape 0.11013\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                     rmse 0.70116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:       time_since_restore 3.7725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:         time_this_iter_s 1.7083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:             time_total_s 3.7725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:                timestamp 1689710121\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb:  View run FSR_Trainable_7dd581fc at: https://wandb.ai/seokjin/FSR-prediction/runs/7dd581fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=427798)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_045522-7dd581fc/logs\n",
      "2023-07-19 04:55:38,220\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.779 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:55:38,223\tWARNING util.py:315 -- The `process_trial_result` operation took 2.782 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:55:38,228\tWARNING util.py:315 -- Processing trial results took 2.788 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:55:38,234\tWARNING util.py:315 -- The `process_trial_result` operation took 2.793 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_c581bc54_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-55-14/wandb/run-20230719_045541-c581bc54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Syncing run FSR_Trainable_c581bc54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c581bc54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                      mae 0.35036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                     mape 0.10109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                     rmse 0.64474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:       time_since_restore 168.189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:         time_this_iter_s 1.53939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:             time_total_s 168.189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:                timestamp 1689710307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb:  View run FSR_Trainable_c581bc54 at: https://wandb.ai/seokjin/FSR-prediction/runs/c581bc54\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428030)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_045541-c581bc54/logs\n",
      "2023-07-19 04:58:48,263\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.627 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:58:48,269\tWARNING util.py:315 -- The `process_trial_result` operation took 3.634 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:58:48,271\tWARNING util.py:315 -- Processing trial results took 3.636 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:58:48,273\tWARNING util.py:315 -- The `process_trial_result` operation took 3.638 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_da2ca992_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-55-33/wandb/run-20230719_045851-da2ca992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Syncing run FSR_Trainable_da2ca992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/da2ca992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                      mae 0.34131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                     mape 0.10074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                     rmse 0.69396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:       time_since_restore 13.60067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:         time_this_iter_s 1.42203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:             time_total_s 13.60067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:                timestamp 1689710339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb:  View run FSR_Trainable_da2ca992 at: https://wandb.ai/seokjin/FSR-prediction/runs/da2ca992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428679)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_045851-da2ca992/logs\n",
      "2023-07-19 04:59:17,916\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.228 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:59:17,925\tWARNING util.py:315 -- The `process_trial_result` operation took 3.237 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:59:17,927\tWARNING util.py:315 -- Processing trial results took 3.240 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:59:17,932\tWARNING util.py:315 -- The `process_trial_result` operation took 3.245 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_93bca7f9_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-58-41/wandb/run-20230719_045922-93bca7f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Syncing run FSR_Trainable_93bca7f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/93bca7f9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                      mae 0.34158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                     mape 0.09644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                     rmse 0.63906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:       time_since_restore 922.93089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:         time_this_iter_s 11.18023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:             time_total_s 922.93089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:                timestamp 1689710359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb:  View run FSR_Trainable_55b20ff5 at: https://wandb.ai/seokjin/FSR-prediction/runs/55b20ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426423)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044356-55b20ff5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                      mae 0.34988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                     mape 0.09752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                     rmse 0.64387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:       time_since_restore 914.87318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:         time_this_iter_s 9.51794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:             time_total_s 914.87318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:                timestamp 1689710368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb:  View run FSR_Trainable_15c95f2b at: https://wandb.ai/seokjin/FSR-prediction/runs/15c95f2b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=426652)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044413-15c95f2b/logs\n",
      "2023-07-19 04:59:35,528\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:59:35,534\tWARNING util.py:315 -- The `process_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:59:35,535\tWARNING util.py:315 -- Processing trial results took 2.229 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:59:35,537\tWARNING util.py:315 -- The `process_trial_result` operation took 2.231 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_370f612d_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-59-12/wandb/run-20230719_045938-370f612d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Syncing run FSR_Trainable_370f612d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/370f612d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_83d5cc84_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-59-31/wandb/run-20230719_045952-83d5cc84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Syncing run FSR_Trainable_83d5cc84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/83d5cc84\n",
      "2023-07-19 04:59:57,847\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.194 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:59:57,849\tWARNING util.py:315 -- The `process_trial_result` operation took 2.197 s, which may be a performance bottleneck.\n",
      "2023-07-19 04:59:57,850\tWARNING util.py:315 -- Processing trial results took 2.198 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 04:59:57,852\tWARNING util.py:315 -- The `process_trial_result` operation took 2.200 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                      mae 0.35621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                     mape 0.10137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                     rmse 0.67112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:       time_since_restore 25.7671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:         time_this_iter_s 1.502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:             time_total_s 25.7671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:                timestamp 1689710400\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb:  View run FSR_Trainable_370f612d at: https://wandb.ai/seokjin/FSR-prediction/runs/370f612d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429162)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_045938-370f612d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=428918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_ec4e1bdd_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_04-59-43/wandb/run-20230719_050020-ec4e1bdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Syncing run FSR_Trainable_ec4e1bdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec4e1bdd\n",
      "2023-07-19 05:00:26,734\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:00:26,738\tWARNING util.py:315 -- The `process_trial_result` operation took 2.261 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:00:26,741\tWARNING util.py:315 -- Processing trial results took 2.265 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:00:26,743\tWARNING util.py:315 -- The `process_trial_result` operation took 2.267 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_a32f1ec5_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-00-12/wandb/run-20230719_050036-a32f1ec5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Syncing run FSR_Trainable_a32f1ec5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a32f1ec5\n",
      "2023-07-19 05:00:42,701\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.668 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:00:42,710\tWARNING util.py:315 -- The `process_trial_result` operation took 2.677 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:00:42,718\tWARNING util.py:315 -- Processing trial results took 2.685 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:00:42,720\tWARNING util.py:315 -- The `process_trial_result` operation took 2.687 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                      mae 0.36463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                     mape 0.09788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                     rmse 0.71023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:       time_since_restore 13.22477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:         time_this_iter_s 13.22477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:             time_total_s 13.22477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:                timestamp 1689710440\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb:  View run FSR_Trainable_a32f1ec5 at: https://wandb.ai/seokjin/FSR-prediction/runs/a32f1ec5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429856)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050036-a32f1ec5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_e99194b2_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-00-26/wandb/run-20230719_050105-e99194b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Syncing run FSR_Trainable_e99194b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e99194b2\n",
      "2023-07-19 05:01:12,992\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.834 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:01:13,003\tWARNING util.py:315 -- The `process_trial_result` operation took 2.846 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:01:13,005\tWARNING util.py:315 -- Processing trial results took 2.848 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:01:13,007\tWARNING util.py:315 -- The `process_trial_result` operation took 2.851 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: \\ 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: | 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: / 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: - 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: \\ 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                      mae 0.36357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                     mape 0.096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                     rmse 0.71079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:       time_since_restore 13.90974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:         time_this_iter_s 13.90974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:             time_total_s 13.90974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:                timestamp 1689710470\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb:  View run FSR_Trainable_e99194b2 at: https://wandb.ai/seokjin/FSR-prediction/runs/e99194b2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430095)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050105-e99194b2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_0028facd_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-00-56/wandb/run-20230719_050137-0028facd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Syncing run FSR_Trainable_0028facd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0028facd\n",
      "2023-07-19 05:01:44,346\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.526 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:01:44,348\tWARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:01:44,349\tWARNING util.py:315 -- Processing trial results took 2.530 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:01:44,351\tWARNING util.py:315 -- The `process_trial_result` operation took 2.532 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                      mae 0.34867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                     mape 0.10033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                     rmse 0.70119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:       time_since_restore 27.95788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:         time_this_iter_s 12.8793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:             time_total_s 27.95788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:                timestamp 1689710517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb:  View run FSR_Trainable_0028facd at: https://wandb.ai/seokjin/FSR-prediction/runs/0028facd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430335)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050137-0028facd/logs\n",
      "2023-07-19 05:02:25,189\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.364 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:02:25,191\tWARNING util.py:315 -- The `process_trial_result` operation took 2.368 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:02:25,194\tWARNING util.py:315 -- Processing trial results took 2.371 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:02:25,197\tWARNING util.py:315 -- The `process_trial_result` operation took 2.374 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_7cee5b7f_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-01-26/wandb/run-20230719_050225-7cee5b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Syncing run FSR_Trainable_7cee5b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7cee5b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                      mae 0.39072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                     mape 0.10984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                     rmse 0.70166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:       time_since_restore 21.93233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:         time_this_iter_s 10.08731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:             time_total_s 21.93233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:                timestamp 1689710555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb:  View run FSR_Trainable_7cee5b7f at: https://wandb.ai/seokjin/FSR-prediction/runs/7cee5b7f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430587)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050225-7cee5b7f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_037f32d8_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-02-11/wandb/run-20230719_050258-037f32d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Syncing run FSR_Trainable_037f32d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/037f32d8\n",
      "2023-07-19 05:03:03,486\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.974 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:03:03,489\tWARNING util.py:315 -- The `process_trial_result` operation took 2.982 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:03:03,492\tWARNING util.py:315 -- Processing trial results took 2.984 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:03:03,500\tWARNING util.py:315 -- The `process_trial_result` operation took 2.992 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                      mae 0.35106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                     mape 0.10186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                     rmse 0.67186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:       time_since_restore 197.94191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:         time_this_iter_s 13.19657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:             time_total_s 197.94191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:                timestamp 1689710584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb:  View run FSR_Trainable_83d5cc84 at: https://wandb.ai/seokjin/FSR-prediction/runs/83d5cc84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_045952-83d5cc84/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_6c487573_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-02-48/wandb/run-20230719_050328-6c487573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Syncing run FSR_Trainable_6c487573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6c487573\n",
      "2023-07-19 05:03:32,641\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:03:32,644\tWARNING util.py:315 -- The `process_trial_result` operation took 2.581 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:03:32,653\tWARNING util.py:315 -- Processing trial results took 2.590 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:03:32,656\tWARNING util.py:315 -- The `process_trial_result` operation took 2.593 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                      mae 0.34831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                     mape 0.10295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                     rmse 0.68705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:       time_since_restore 83.22154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:         time_this_iter_s 10.19918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:             time_total_s 83.22154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:                timestamp 1689710655\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb:  View run FSR_Trainable_037f32d8 at: https://wandb.ai/seokjin/FSR-prediction/runs/037f32d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=430834)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050258-037f32d8/logs\n",
      "2023-07-19 05:04:34,387\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.862 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:04:34,392\tWARNING util.py:315 -- The `process_trial_result` operation took 2.868 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:04:34,395\tWARNING util.py:315 -- Processing trial results took 2.871 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:04:34,410\tWARNING util.py:315 -- The `process_trial_result` operation took 2.886 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_5278ad2f_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-03-18/wandb/run-20230719_050437-5278ad2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Syncing run FSR_Trainable_5278ad2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5278ad2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                      mae 0.37517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                     mape 0.10685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                     rmse 0.70494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:       time_since_restore 2.5942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:         time_this_iter_s 2.5942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:             time_total_s 2.5942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:                timestamp 1689710671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb:  View run FSR_Trainable_5278ad2f at: https://wandb.ai/seokjin/FSR-prediction/runs/5278ad2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431344)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050437-5278ad2f/logs\n",
      "2023-07-19 05:04:57,019\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.681 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:04:57,031\tWARNING util.py:315 -- The `process_trial_result` operation took 3.694 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:04:57,037\tWARNING util.py:315 -- Processing trial results took 3.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:04:57,050\tWARNING util.py:315 -- The `process_trial_result` operation took 3.713 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_862fc8b3_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-04-29/wandb/run-20230719_050500-862fc8b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Syncing run FSR_Trainable_862fc8b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/862fc8b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                      mae 0.39965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                     mape 0.10397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                     rmse 0.72175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:       time_since_restore 3.90601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:         time_this_iter_s 3.90601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:             time_total_s 3.90601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:                timestamp 1689710693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb:  View run FSR_Trainable_862fc8b3 at: https://wandb.ai/seokjin/FSR-prediction/runs/862fc8b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431576)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050500-862fc8b3/logs\n",
      "2023-07-19 05:05:19,338\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.709 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:05:19,342\tWARNING util.py:315 -- The `process_trial_result` operation took 3.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:05:19,355\tWARNING util.py:315 -- Processing trial results took 3.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:05:19,357\tWARNING util.py:315 -- The `process_trial_result` operation took 3.729 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_e16e66cb_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-04-49/wandb/run-20230719_050523-e16e66cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Syncing run FSR_Trainable_e16e66cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e16e66cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                      mae 0.37713\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                     mape 0.1042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                     rmse 0.70683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:       time_since_restore 2.35457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:         time_this_iter_s 2.35457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:             time_total_s 2.35457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:                timestamp 1689710715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb:  View run FSR_Trainable_e16e66cb at: https://wandb.ai/seokjin/FSR-prediction/runs/e16e66cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431812)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050523-e16e66cb/logs\n",
      "2023-07-19 05:05:36,787\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.750 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:05:36,793\tWARNING util.py:315 -- The `process_trial_result` operation took 2.757 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:05:36,795\tWARNING util.py:315 -- Processing trial results took 2.759 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:05:36,802\tWARNING util.py:315 -- The `process_trial_result` operation took 2.766 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_fb678dc2_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-05-13/wandb/run-20230719_050540-fb678dc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Syncing run FSR_Trainable_fb678dc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb678dc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                      mae 0.3635\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                     mape 0.10126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                     rmse 0.70335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:       time_since_restore 1.90359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:         time_this_iter_s 1.90359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:             time_total_s 1.90359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:                timestamp 1689710734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb:  View run FSR_Trainable_fb678dc2 at: https://wandb.ai/seokjin/FSR-prediction/runs/fb678dc2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432048)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050540-fb678dc2/logs\n",
      "2023-07-19 05:05:53,330\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.848 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:05:53,334\tWARNING util.py:315 -- The `process_trial_result` operation took 2.853 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:05:53,337\tWARNING util.py:315 -- Processing trial results took 2.856 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:05:53,341\tWARNING util.py:315 -- The `process_trial_result` operation took 2.861 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...275)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_66707558_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-05-32/wandb/run-20230719_050556-66707558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Syncing run FSR_Trainable_66707558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/66707558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                      mae 2.14496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                     mape 0.34771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                     rmse 2.54514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:       time_since_restore 1.6858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:         time_this_iter_s 1.6858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:             time_total_s 1.6858\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:                timestamp 1689710750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb:  View run FSR_Trainable_66707558 at: https://wandb.ai/seokjin/FSR-prediction/runs/66707558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432275)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050556-66707558/logs\n",
      "2023-07-19 05:06:09,758\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.912 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:06:09,764\tWARNING util.py:315 -- The `process_trial_result` operation took 2.919 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:06:09,766\tWARNING util.py:315 -- Processing trial results took 2.921 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:06:09,768\tWARNING util.py:315 -- The `process_trial_result` operation took 2.924 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_e5680a88_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-05-48/wandb/run-20230719_050613-e5680a88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Syncing run FSR_Trainable_e5680a88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e5680a88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                      mae 2.47681\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                     mape 0.35259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                     rmse 2.89686\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:       time_since_restore 1.51698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:         time_this_iter_s 1.51698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:             time_total_s 1.51698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:                timestamp 1689710766\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb:  View run FSR_Trainable_e5680a88 at: https://wandb.ai/seokjin/FSR-prediction/runs/e5680a88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432505)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050613-e5680a88/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_c7594953_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-06-05/wandb/run-20230719_050630-c7594953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Syncing run FSR_Trainable_c7594953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7594953\n",
      "2023-07-19 05:06:38,811\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.074 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:06:38,814\tWARNING util.py:315 -- The `process_trial_result` operation took 2.079 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:06:38,817\tWARNING util.py:315 -- Processing trial results took 2.081 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:06:38,821\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                      mae 0.38213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                     mape 0.10446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                     rmse 0.70364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:       time_since_restore 14.46942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:         time_this_iter_s 14.46942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:             time_total_s 14.46942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:                timestamp 1689710796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb:  View run FSR_Trainable_c7594953 at: https://wandb.ai/seokjin/FSR-prediction/runs/c7594953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432736)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050630-c7594953/logs\n",
      "2023-07-19 05:06:56,981\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.679 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:06:56,987\tWARNING util.py:315 -- The `process_trial_result` operation took 2.686 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:06:56,989\tWARNING util.py:315 -- Processing trial results took 2.688 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:06:56,990\tWARNING util.py:315 -- The `process_trial_result` operation took 2.690 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2f3ebc88_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-06-22/wandb/run-20230719_050659-2f3ebc88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Syncing run FSR_Trainable_2f3ebc88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2f3ebc88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                      mae 0.37707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                     mape 0.1034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                     rmse 0.70316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:       time_since_restore 3.32323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:         time_this_iter_s 3.32323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:             time_total_s 3.32323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:                timestamp 1689710814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb:  View run FSR_Trainable_2f3ebc88 at: https://wandb.ai/seokjin/FSR-prediction/runs/2f3ebc88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=432974)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050659-2f3ebc88/logs\n",
      "2023-07-19 05:07:14,537\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.750 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:07:14,541\tWARNING util.py:315 -- The `process_trial_result` operation took 2.756 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:07:14,544\tWARNING util.py:315 -- Processing trial results took 2.758 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:07:14,546\tWARNING util.py:315 -- The `process_trial_result` operation took 2.760 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_f1c1a58b_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-06-51/wandb/run-20230719_050717-f1c1a58b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Syncing run FSR_Trainable_f1c1a58b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f1c1a58b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                      mae 0.3899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                     mape 0.11009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                     rmse 0.69892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:       time_since_restore 4.8694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:         time_this_iter_s 2.19414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:             time_total_s 4.8694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:                timestamp 1689710836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb:  View run FSR_Trainable_f1c1a58b at: https://wandb.ai/seokjin/FSR-prediction/runs/f1c1a58b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433204)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050717-f1c1a58b/logs\n",
      "2023-07-19 05:07:36,104\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.418 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:07:36,108\tWARNING util.py:315 -- The `process_trial_result` operation took 2.422 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:07:36,110\tWARNING util.py:315 -- Processing trial results took 2.425 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:07:36,111\tWARNING util.py:315 -- The `process_trial_result` operation took 2.426 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_7b3c7daa_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-07-09/wandb/run-20230719_050737-7b3c7daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Syncing run FSR_Trainable_7b3c7daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b3c7daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                      mae 0.3364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                     mape 0.09966\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                     rmse 0.64163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:       time_since_restore 1857.83413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:         time_this_iter_s 17.21928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:             time_total_s 1857.83413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:                timestamp 1689711158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb:  View run FSR_Trainable_db7b2524 at: https://wandb.ai/seokjin/FSR-prediction/runs/db7b2524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=425189)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_044139-db7b2524/logs\n",
      "2023-07-19 05:12:56,124\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.880 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:12:56,127\tWARNING util.py:315 -- The `process_trial_result` operation took 2.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:12:56,132\tWARNING util.py:315 -- Processing trial results took 2.889 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:12:56,137\tWARNING util.py:315 -- The `process_trial_result` operation took 2.894 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_7fc34d8d_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-07-29/wandb/run-20230719_051258-7fc34d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Syncing run FSR_Trainable_7fc34d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7fc34d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                      mae 0.34924\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                     mape 0.10558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                     rmse 0.68997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:       time_since_restore 8.26124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:         time_this_iter_s 1.84165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:             time_total_s 8.26124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:                timestamp 1689711181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb:  View run FSR_Trainable_7fc34d8d at: https://wandb.ai/seokjin/FSR-prediction/runs/7fc34d8d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433981)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051258-7fc34d8d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_c7c45904_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-12-50/wandb/run-20230719_051321-c7c45904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Syncing run FSR_Trainable_c7c45904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7c45904\n",
      "2023-07-19 05:13:34,540\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.091 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:13:34,543\tWARNING util.py:315 -- The `process_trial_result` operation took 2.095 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:13:34,545\tWARNING util.py:315 -- Processing trial results took 2.097 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:13:34,547\tWARNING util.py:315 -- The `process_trial_result` operation took 2.099 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                      mae 0.30001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                     mape 0.07842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                     rmse 0.58641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:       time_since_restore 373.99462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:         time_this_iter_s 3.86804\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:             time_total_s 373.99462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:                timestamp 1689711229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb:  View run FSR_Trainable_7b3c7daa at: https://wandb.ai/seokjin/FSR-prediction/runs/7b3c7daa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=433437)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050737-7b3c7daa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434219)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_627be693_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-13-13/wandb/run-20230719_051409-627be693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Syncing run FSR_Trainable_627be693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/627be693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_97db8431_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-14-00/wandb/run-20230719_051421-97db8431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Syncing run FSR_Trainable_97db8431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/97db8431\n",
      "2023-07-19 05:14:22,846\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:14:22,849\tWARNING util.py:315 -- The `process_trial_result` operation took 2.571 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:14:22,851\tWARNING util.py:315 -- Processing trial results took 2.573 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:14:22,853\tWARNING util.py:315 -- The `process_trial_result` operation took 2.575 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:14:25,630\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.704 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:14:25,643\tWARNING util.py:315 -- The `process_trial_result` operation took 2.718 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:14:25,645\tWARNING util.py:315 -- Processing trial results took 2.720 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:14:25,647\tWARNING util.py:315 -- The `process_trial_result` operation took 2.722 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                      mae 0.38675\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                     mape 0.10265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                     rmse 0.71436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:       time_since_restore 7.09465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:         time_this_iter_s 7.09465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:             time_total_s 7.09465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:                timestamp 1689711260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb:  View run FSR_Trainable_97db8431 at: https://wandb.ai/seokjin/FSR-prediction/runs/97db8431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434693)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051421-97db8431/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                      mae 0.40141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                     mape 0.10668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                     rmse 0.71609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:       time_since_restore 37.72153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:         time_this_iter_s 18.24237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:             time_total_s 37.72153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:                timestamp 1689711281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb:  View run FSR_Trainable_627be693 at: https://wandb.ai/seokjin/FSR-prediction/runs/627be693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434487)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051409-627be693/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_ba5a2d7b_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-14-13/wandb/run-20230719_051445-ba5a2d7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Syncing run FSR_Trainable_ba5a2d7b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ba5a2d7b\n",
      "2023-07-19 05:14:47,484\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:14:47,489\tWARNING util.py:315 -- The `process_trial_result` operation took 2.535 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:14:47,491\tWARNING util.py:315 -- Processing trial results took 2.537 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:14:47,493\tWARNING util.py:315 -- The `process_trial_result` operation took 2.539 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=434935)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051445-ba5a2d7b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_3d5d4cc7_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-14-37/wandb/run-20230719_051500-3d5d4cc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Syncing run FSR_Trainable_3d5d4cc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3d5d4cc7\n",
      "2023-07-19 05:15:02,008\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.370 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:02,011\tWARNING util.py:315 -- The `process_trial_result` operation took 2.374 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:02,014\tWARNING util.py:315 -- Processing trial results took 2.378 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:15:02,016\tWARNING util.py:315 -- The `process_trial_result` operation took 2.379 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:12,396\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.477 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:12,404\tWARNING util.py:315 -- The `process_trial_result` operation took 2.486 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:12,406\tWARNING util.py:315 -- Processing trial results took 2.487 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:15:12,408\tWARNING util.py:315 -- The `process_trial_result` operation took 2.490 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_d85b6355_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-14-52/wandb/run-20230719_051512-d85b6355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Syncing run FSR_Trainable_d85b6355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d85b6355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                      mae 0.50592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                     mape 0.12023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                     rmse 0.80183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:       time_since_restore 5.29664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:         time_this_iter_s 5.29664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:             time_total_s 5.29664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:                timestamp 1689711309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb:  View run FSR_Trainable_d85b6355 at: https://wandb.ai/seokjin/FSR-prediction/runs/d85b6355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435391)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051512-d85b6355/logs\n",
      "2023-07-19 05:15:31,233\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.281 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:31,237\tWARNING util.py:315 -- The `process_trial_result` operation took 2.286 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:31,239\tWARNING util.py:315 -- Processing trial results took 2.288 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:15:31,240\tWARNING util.py:315 -- The `process_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_25130e68_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-15-04/wandb/run-20230719_051531-25130e68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Syncing run FSR_Trainable_25130e68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/25130e68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                      mae 0.58816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                     mape 0.12466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                     rmse 0.88705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:       time_since_restore 5.24242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:         time_this_iter_s 5.24242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:             time_total_s 5.24242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:                timestamp 1689711328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb:  View run FSR_Trainable_25130e68 at: https://wandb.ai/seokjin/FSR-prediction/runs/25130e68\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435625)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051531-25130e68/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: \\ 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 05:15:48,989\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.039 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:48,992\tWARNING util.py:315 -- The `process_trial_result` operation took 2.043 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:15:48,995\tWARNING util.py:315 -- Processing trial results took 2.045 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:15:48,996\tWARNING util.py:315 -- The `process_trial_result` operation took 2.047 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: | 0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                      mae 0.35562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                     mape 0.10246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                     rmse 0.67654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:       time_since_restore 50.02851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:         time_this_iter_s 6.13378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:             time_total_s 50.02851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:                timestamp 1689711345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb:  View run FSR_Trainable_3d5d4cc7 at: https://wandb.ai/seokjin/FSR-prediction/runs/3d5d4cc7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435178)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051500-3d5d4cc7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2fd5054c_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-15-23/wandb/run-20230719_051549-2fd5054c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Syncing run FSR_Trainable_2fd5054c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2fd5054c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_103e2280_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-15-42/wandb/run-20230719_051606-103e2280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Syncing run FSR_Trainable_103e2280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/103e2280\n",
      "2023-07-19 05:16:11,096\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:16:11,098\tWARNING util.py:315 -- The `process_trial_result` operation took 2.324 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:16:11,099\tWARNING util.py:315 -- Processing trial results took 2.326 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:16:11,102\tWARNING util.py:315 -- The `process_trial_result` operation took 2.329 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                      mae 0.33346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                     mape 0.0959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                     rmse 0.63138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:       time_since_restore 890.01017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:         time_this_iter_s 8.50118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:             time_total_s 890.01017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:                timestamp 1689711503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb:  View run FSR_Trainable_6c487573 at: https://wandb.ai/seokjin/FSR-prediction/runs/6c487573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=431076)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050328-6c487573/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_ca34a79a_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-15-58/wandb/run-20230719_051844-ca34a79a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Syncing run FSR_Trainable_ca34a79a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ca34a79a\n",
      "2023-07-19 05:18:49,495\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.567 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:18:49,499\tWARNING util.py:315 -- The `process_trial_result` operation took 2.572 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:18:49,501\tWARNING util.py:315 -- Processing trial results took 2.574 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:18:49,503\tWARNING util.py:315 -- The `process_trial_result` operation took 2.576 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                      mae 0.35081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                     mape 0.10114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                     rmse 0.69973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:       time_since_restore 19.9718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:         time_this_iter_s 9.42893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:             time_total_s 19.9718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:                timestamp 1689711538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb:  View run FSR_Trainable_ca34a79a at: https://wandb.ai/seokjin/FSR-prediction/runs/ca34a79a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436436)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051844-ca34a79a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_8ed6e88f_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-18-36/wandb/run-20230719_051918-8ed6e88f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Syncing run FSR_Trainable_8ed6e88f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ed6e88f\n",
      "2023-07-19 05:19:23,422\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.489 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:19:23,425\tWARNING util.py:315 -- The `process_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:19:23,427\tWARNING util.py:315 -- Processing trial results took 2.495 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:19:23,430\tWARNING util.py:315 -- The `process_trial_result` operation took 2.497 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                      mae 0.34698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                     mape 0.10188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                     rmse 0.64524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:       time_since_restore 1138.64866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:         time_this_iter_s 10.72223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:             time_total_s 1138.64866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:                timestamp 1689711567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb:  View run FSR_Trainable_ec4e1bdd at: https://wandb.ai/seokjin/FSR-prediction/runs/ec4e1bdd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=429640)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_050020-ec4e1bdd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_3b723117_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-19-10/wandb/run-20230719_051947-3b723117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Syncing run FSR_Trainable_3b723117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3b723117\n",
      "2023-07-19 05:19:51,488\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.182 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:19:51,490\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:19:51,492\tWARNING util.py:315 -- Processing trial results took 2.186 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:19:51,494\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                      mae 0.32703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                     mape 0.09446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                     rmse 0.62211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:       time_since_restore 375.03233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:         time_this_iter_s 3.43783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:             time_total_s 375.03233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:                timestamp 1689711724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb:  View run FSR_Trainable_2fd5054c at: https://wandb.ai/seokjin/FSR-prediction/runs/2fd5054c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=435857)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051549-2fd5054c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_150c9986_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-19-39/wandb/run-20230719_052224-150c9986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Syncing run FSR_Trainable_150c9986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/150c9986\n",
      "2023-07-19 05:22:29,420\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.636 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:22:29,422\tWARNING util.py:315 -- The `process_trial_result` operation took 2.639 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:22:29,424\tWARNING util.py:315 -- Processing trial results took 2.642 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:22:29,428\tWARNING util.py:315 -- The `process_trial_result` operation took 2.645 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                      mae 0.35131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                     mape 0.10277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                     rmse 0.6989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:       time_since_restore 38.18424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:         time_this_iter_s 9.66907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:             time_total_s 38.18424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:                timestamp 1689711777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb:  View run FSR_Trainable_150c9986 at: https://wandb.ai/seokjin/FSR-prediction/runs/150c9986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437258)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052224-150c9986/logs\n",
      "2023-07-19 05:23:16,584\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.704 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:23:16,587\tWARNING util.py:315 -- The `process_trial_result` operation took 2.708 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:23:16,592\tWARNING util.py:315 -- Processing trial results took 2.713 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:23:16,594\tWARNING util.py:315 -- The `process_trial_result` operation took 2.715 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2be437fa_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-22-16/wandb/run-20230719_052317-2be437fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Syncing run FSR_Trainable_2be437fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2be437fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                      mae 0.36408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                     mape 0.10542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                     rmse 0.67951\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:       time_since_restore 30.11525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:         time_this_iter_s 3.62268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:             time_total_s 30.11525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:                timestamp 1689711822\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb:  View run FSR_Trainable_2be437fa at: https://wandb.ai/seokjin/FSR-prediction/runs/2be437fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437517)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052317-2be437fa/logs\n",
      "2023-07-19 05:24:00,859\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.749 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:24:00,864\tWARNING util.py:315 -- The `process_trial_result` operation took 2.755 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:24:00,867\tWARNING util.py:315 -- Processing trial results took 2.758 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:24:00,870\tWARNING util.py:315 -- The `process_trial_result` operation took 2.761 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_724f13a8_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-23-09/wandb/run-20230719_052402-724f13a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Syncing run FSR_Trainable_724f13a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/724f13a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                      mae 0.36042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                     mape 0.09883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                     rmse 0.66225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:       time_since_restore 286.75593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:         time_this_iter_s 8.67302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:             time_total_s 286.75593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:                timestamp 1689711870\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb:  View run FSR_Trainable_3b723117 at: https://wandb.ai/seokjin/FSR-prediction/runs/3b723117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436923)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051947-3b723117/logs\n",
      "2023-07-19 05:24:47,589\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.005 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:24:47,598\tWARNING util.py:315 -- The `process_trial_result` operation took 2.015 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:24:47,603\tWARNING util.py:315 -- Processing trial results took 2.019 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:24:47,605\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2a16827f_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-23-54/wandb/run-20230719_052449-2a16827f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Syncing run FSR_Trainable_2a16827f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2a16827f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                      mae 0.32214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                     mape 0.09497\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                     rmse 0.61453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:       time_since_restore 298.47167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:         time_this_iter_s 2.80469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:             time_total_s 298.47167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:                timestamp 1689712142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb:  View run FSR_Trainable_724f13a8 at: https://wandb.ai/seokjin/FSR-prediction/runs/724f13a8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=437769)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052402-724f13a8/logs\n",
      "2023-07-19 05:29:20,125\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.403 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:29:20,129\tWARNING util.py:315 -- The `process_trial_result` operation took 2.408 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:29:20,131\tWARNING util.py:315 -- Processing trial results took 2.410 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:29:20,132\tWARNING util.py:315 -- The `process_trial_result` operation took 2.411 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_b681b3a2_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-24-42/wandb/run-20230719_052921-b681b3a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Syncing run FSR_Trainable_b681b3a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b681b3a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                      mae 0.30469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                     mape 0.08472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                     rmse 0.59158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:       time_since_restore 293.55221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:         time_this_iter_s 2.91073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:             time_total_s 293.55221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:                timestamp 1689712183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb:  View run FSR_Trainable_2a16827f at: https://wandb.ai/seokjin/FSR-prediction/runs/2a16827f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438024)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052449-2a16827f/logs\n",
      "2023-07-19 05:30:01,765\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.698 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:30:01,768\tWARNING util.py:315 -- The `process_trial_result` operation took 2.701 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:30:01,776\tWARNING util.py:315 -- Processing trial results took 2.709 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:30:01,779\tWARNING util.py:315 -- The `process_trial_result` operation took 2.712 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_93d2f812_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-29-14/wandb/run-20230719_053003-93d2f812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Syncing run FSR_Trainable_93d2f812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/93d2f812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                      mae 0.34223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                     mape 0.09061\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                     rmse 0.71022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:       time_since_restore 4.23094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:         time_this_iter_s 4.23094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:             time_total_s 4.23094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:                timestamp 1689712199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb:  View run FSR_Trainable_93d2f812 at: https://wandb.ai/seokjin/FSR-prediction/runs/93d2f812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438719)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053003-93d2f812/logs\n",
      "2023-07-19 05:30:21,067\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:30:21,073\tWARNING util.py:315 -- The `process_trial_result` operation took 2.591 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:30:21,075\tWARNING util.py:315 -- Processing trial results took 2.593 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:30:21,085\tWARNING util.py:315 -- The `process_trial_result` operation took 2.603 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_c371adcf_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-29-54/wandb/run-20230719_053022-c371adcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Syncing run FSR_Trainable_c371adcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c371adcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                      mae 0.34901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                     mape 0.09001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                     rmse 0.72023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:       time_since_restore 3.59614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:         time_this_iter_s 3.59614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:             time_total_s 3.59614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:                timestamp 1689712218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb:  View run FSR_Trainable_c371adcf at: https://wandb.ai/seokjin/FSR-prediction/runs/c371adcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438955)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053022-c371adcf/logs\n",
      "2023-07-19 05:30:39,298\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.567 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:30:39,302\tWARNING util.py:315 -- The `process_trial_result` operation took 2.572 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:30:39,304\tWARNING util.py:315 -- Processing trial results took 2.575 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:30:39,306\tWARNING util.py:315 -- The `process_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_63721144_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-30-14/wandb/run-20230719_053040-63721144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Syncing run FSR_Trainable_63721144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/63721144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                      mae 0.3118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                     mape 0.09373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                     rmse 0.60016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:       time_since_restore 879.70511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:         time_this_iter_s 8.22896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:             time_total_s 879.70511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:                timestamp 1689712248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb:  View run FSR_Trainable_103e2280 at: https://wandb.ai/seokjin/FSR-prediction/runs/103e2280\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436090)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051606-103e2280/logs\n",
      "2023-07-19 05:31:06,322\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.567 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:31:06,327\tWARNING util.py:315 -- The `process_trial_result` operation took 2.573 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:31:06,331\tWARNING util.py:315 -- Processing trial results took 2.576 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:31:06,333\tWARNING util.py:315 -- The `process_trial_result` operation took 2.578 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_b339785d_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-30-33/wandb/run-20230719_053108-b339785d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Syncing run FSR_Trainable_b339785d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b339785d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                      mae 0.36113\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                     mape 0.10286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                     rmse 0.66359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:       time_since_restore 46.48917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:         time_this_iter_s 2.61604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:             time_total_s 46.48917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:                timestamp 1689712283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb:  View run FSR_Trainable_63721144 at: https://wandb.ai/seokjin/FSR-prediction/runs/63721144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439190)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053040-63721144/logs\n",
      "2023-07-19 05:31:39,673\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:31:39,676\tWARNING util.py:315 -- The `process_trial_result` operation took 2.255 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:31:39,678\tWARNING util.py:315 -- Processing trial results took 2.257 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:31:39,680\tWARNING util.py:315 -- The `process_trial_result` operation took 2.259 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2e5c9233_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-31-01/wandb/run-20230719_053142-2e5c9233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Syncing run FSR_Trainable_2e5c9233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e5c9233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                      mae 0.30432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                     mape 0.07943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                     rmse 0.5899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:       time_since_restore 883.87312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:         time_this_iter_s 8.30003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:             time_total_s 883.87312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:                timestamp 1689712445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb:  View run FSR_Trainable_8ed6e88f at: https://wandb.ai/seokjin/FSR-prediction/runs/8ed6e88f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=436683)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_051918-8ed6e88f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=438469)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_052921-b681b3a2/logs\n",
      "2023-07-19 05:34:20,944\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.138 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:34:20,950\tWARNING util.py:315 -- The `process_trial_result` operation took 2.144 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:34:20,952\tWARNING util.py:315 -- Processing trial results took 2.146 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:34:20,954\tWARNING util.py:315 -- The `process_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_bf496b10_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-31-34/wandb/run-20230719_053423-bf496b10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Syncing run FSR_Trainable_bf496b10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bf496b10\n",
      "2023-07-19 05:34:32,103\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.176 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:34:32,106\tWARNING util.py:315 -- The `process_trial_result` operation took 2.180 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:34:32,108\tWARNING util.py:315 -- Processing trial results took 2.181 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:34:32,109\tWARNING util.py:315 -- The `process_trial_result` operation took 2.183 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_59ca80f0_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-34-16/wandb/run-20230719_053434-59ca80f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Syncing run FSR_Trainable_59ca80f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/59ca80f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                      mae 0.32556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                     mape 0.09475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                     rmse 0.61914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:       time_since_restore 206.48144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:         time_this_iter_s 2.09634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:             time_total_s 206.48144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:                timestamp 1689712476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb:  View run FSR_Trainable_b339785d at: https://wandb.ai/seokjin/FSR-prediction/runs/b339785d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439433)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053108-b339785d/logs\n",
      "2023-07-19 05:34:52,133\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:34:52,136\tWARNING util.py:315 -- The `process_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:34:52,138\tWARNING util.py:315 -- Processing trial results took 1.905 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:34:52,139\tWARNING util.py:315 -- The `process_trial_result` operation took 1.906 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_9b684290_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-34-27/wandb/run-20230719_053454-9b684290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Syncing run FSR_Trainable_9b684290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9b684290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                      mae 0.32698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                     mape 0.09547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                     rmse 0.61923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:       time_since_restore 206.0503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:         time_this_iter_s 2.06451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:             time_total_s 206.0503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:                timestamp 1689712511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb:  View run FSR_Trainable_2e5c9233 at: https://wandb.ai/seokjin/FSR-prediction/runs/2e5c9233\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=439679)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053142-2e5c9233/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 05:35:27,240\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:35:27,243\tWARNING util.py:315 -- The `process_trial_result` operation took 1.858 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:35:27,244\tWARNING util.py:315 -- Processing trial results took 1.858 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:35:27,245\tWARNING util.py:315 -- The `process_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                      mae 0.37044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                     mape 0.10379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                     rmse 0.66423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:       time_since_restore 33.27779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:         time_this_iter_s 2.04212\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:             time_total_s 33.27779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:                timestamp 1689712523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb:  View run FSR_Trainable_9b684290 at: https://wandb.ai/seokjin/FSR-prediction/runs/9b684290\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440512)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053454-9b684290/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_b227b79a_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-34-47/wandb/run-20230719_053529-b227b79a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Syncing run FSR_Trainable_b227b79a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b227b79a\n",
      "2023-07-19 05:35:41,138\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:35:41,143\tWARNING util.py:315 -- The `process_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:35:41,144\tWARNING util.py:315 -- Processing trial results took 1.896 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:35:41,146\tWARNING util.py:315 -- The `process_trial_result` operation took 1.898 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_03820ab4_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-35-22/wandb/run-20230719_053543-03820ab4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Syncing run FSR_Trainable_03820ab4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/03820ab4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                      mae 0.31595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                     mape 0.08756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                     rmse 0.60556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:       time_since_restore 203.87491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:         time_this_iter_s 1.96565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:             time_total_s 203.87491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:                timestamp 1689712672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb:  View run FSR_Trainable_bf496b10 at: https://wandb.ai/seokjin/FSR-prediction/runs/bf496b10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440055)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053423-bf496b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                      mae 0.32355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                     mape 0.09051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                     rmse 0.61207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:       time_since_restore 206.20824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:         time_this_iter_s 1.85322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:             time_total_s 206.20824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:                timestamp 1689712682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb:  View run FSR_Trainable_59ca80f0 at: https://wandb.ai/seokjin/FSR-prediction/runs/59ca80f0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440274)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053434-59ca80f0/logs\n",
      "2023-07-19 05:38:07,809\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.284 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:38:07,814\tWARNING util.py:315 -- The `process_trial_result` operation took 2.289 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:38:07,816\tWARNING util.py:315 -- Processing trial results took 2.291 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:38:07,819\tWARNING util.py:315 -- The `process_trial_result` operation took 2.294 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_01df0bfb_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-35-36/wandb/run-20230719_053814-01df0bfb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Syncing run FSR_Trainable_01df0bfb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01df0bfb\n",
      "2023-07-19 05:38:19,076\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:38:19,079\tWARNING util.py:315 -- The `process_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:38:19,080\tWARNING util.py:315 -- Processing trial results took 2.267 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:38:19,081\tWARNING util.py:315 -- The `process_trial_result` operation took 2.268 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_14b20cd1_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-38-03/wandb/run-20230719_053822-14b20cd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Syncing run FSR_Trainable_14b20cd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/14b20cd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                      mae 0.31491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                     mape 0.09083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                     rmse 0.59906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:       time_since_restore 201.88634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:         time_this_iter_s 1.92998\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:             time_total_s 201.88634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:                timestamp 1689712733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb:  View run FSR_Trainable_b227b79a at: https://wandb.ai/seokjin/FSR-prediction/runs/b227b79a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440764)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053529-b227b79a/logs\n",
      "2023-07-19 05:39:09,561\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.301 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:39:09,565\tWARNING util.py:315 -- The `process_trial_result` operation took 2.305 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:39:09,566\tWARNING util.py:315 -- Processing trial results took 2.306 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:39:09,568\tWARNING util.py:315 -- The `process_trial_result` operation took 2.308 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                      mae 0.31344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                     mape 0.08813\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                     rmse 0.60116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:       time_since_restore 203.86962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:         time_this_iter_s 2.30116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:             time_total_s 203.86962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:                timestamp 1689712747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb:  View run FSR_Trainable_03820ab4 at: https://wandb.ai/seokjin/FSR-prediction/runs/03820ab4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=440997)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053543-03820ab4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_7df97178_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-38-14/wandb/run-20230719_053912-7df97178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Syncing run FSR_Trainable_7df97178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7df97178\n",
      "2023-07-19 05:39:24,669\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.750 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:39:24,671\tWARNING util.py:315 -- The `process_trial_result` operation took 1.754 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:39:24,673\tWARNING util.py:315 -- Processing trial results took 1.755 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:39:24,674\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_a6f25235_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-39-05/wandb/run-20230719_053927-a6f25235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Syncing run FSR_Trainable_a6f25235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a6f25235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                      mae 0.33154\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                     mape 0.09729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                     rmse 0.62485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:       time_since_restore 84.38352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:         time_this_iter_s 1.30111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:             time_total_s 84.38352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:                timestamp 1689712776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb:  View run FSR_Trainable_01df0bfb at: https://wandb.ai/seokjin/FSR-prediction/runs/01df0bfb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441364)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053814-01df0bfb/logs\n",
      "2023-07-19 05:39:50,942\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:39:50,947\tWARNING util.py:315 -- The `process_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:39:50,948\tWARNING util.py:315 -- Processing trial results took 1.667 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:39:50,950\tWARNING util.py:315 -- The `process_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_e0d167a2_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-39-21/wandb/run-20230719_053953-e0d167a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Syncing run FSR_Trainable_e0d167a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e0d167a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                      mae 0.31945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                     mape 0.08993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                     rmse 0.60732\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:       time_since_restore 128.81481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:         time_this_iter_s 1.22133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:             time_total_s 128.81481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:                timestamp 1689712833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb:  View run FSR_Trainable_14b20cd1 at: https://wandb.ai/seokjin/FSR-prediction/runs/14b20cd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441573)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053822-14b20cd1/logs\n",
      "2023-07-19 05:40:47,997\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.708 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:40:48,001\tWARNING util.py:315 -- The `process_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:40:48,002\tWARNING util.py:315 -- Processing trial results took 1.714 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:40:48,003\tWARNING util.py:315 -- The `process_trial_result` operation took 1.715 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_1e57ef8f_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-39-47/wandb/run-20230719_054051-1e57ef8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Syncing run FSR_Trainable_1e57ef8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1e57ef8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                      mae 0.32133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                     mape 0.09309\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                     rmse 0.61092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:       time_since_restore 126.70747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:         time_this_iter_s 1.15336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:             time_total_s 126.70747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:                timestamp 1689712883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb:  View run FSR_Trainable_7df97178 at: https://wandb.ai/seokjin/FSR-prediction/runs/7df97178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=441865)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053912-7df97178/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                      mae 0.31479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                     mape 0.09131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                     rmse 0.60535\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:       time_since_restore 124.87878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:         time_this_iter_s 1.39407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:             time_total_s 124.87878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:                timestamp 1689712893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb:  View run FSR_Trainable_a6f25235 at: https://wandb.ai/seokjin/FSR-prediction/runs/a6f25235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442101)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053927-a6f25235/logs\n",
      "2023-07-19 05:41:38,999\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.211 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:41:39,002\tWARNING util.py:315 -- The `process_trial_result` operation took 2.215 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:41:39,005\tWARNING util.py:315 -- Processing trial results took 2.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:41:39,007\tWARNING util.py:315 -- The `process_trial_result` operation took 2.220 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_70da697c_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-40-44/wandb/run-20230719_054142-70da697c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Syncing run FSR_Trainable_70da697c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/70da697c\n",
      "2023-07-19 05:41:49,771\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.208 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:41:49,775\tWARNING util.py:315 -- The `process_trial_result` operation took 2.213 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:41:49,777\tWARNING util.py:315 -- Processing trial results took 2.215 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:41:49,778\tWARNING util.py:315 -- The `process_trial_result` operation took 2.216 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_a07ed3a2_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-41-34/wandb/run-20230719_054153-a07ed3a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Syncing run FSR_Trainable_a07ed3a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a07ed3a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                      mae 0.33376\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                     mape 0.095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                     rmse 0.62159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:       time_since_restore 129.70401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:         time_this_iter_s 1.16772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:             time_total_s 129.70401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:                timestamp 1689712926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb:  View run FSR_Trainable_e0d167a2 at: https://wandb.ai/seokjin/FSR-prediction/runs/e0d167a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442349)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_053953-e0d167a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                      mae 0.33593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                     mape 0.09618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                     rmse 0.62502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:       time_since_restore 83.38899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:         time_this_iter_s 1.24642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:             time_total_s 83.38899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:                timestamp 1689712934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb:  View run FSR_Trainable_1e57ef8f at: https://wandb.ai/seokjin/FSR-prediction/runs/1e57ef8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442635)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054051-1e57ef8f/logs\n",
      "2023-07-19 05:42:21,248\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:21,253\tWARNING util.py:315 -- The `process_trial_result` operation took 1.790 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:21,254\tWARNING util.py:315 -- Processing trial results took 1.791 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:42:21,256\tWARNING util.py:315 -- The `process_trial_result` operation took 1.793 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_f27fd036_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-41-46/wandb/run-20230719_054224-f27fd036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Syncing run FSR_Trainable_f27fd036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f27fd036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                      mae 0.34115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                     mape 0.0949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                     rmse 0.63371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:       time_since_restore 34.85223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:         time_this_iter_s 1.05008\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:             time_total_s 34.85223\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:                timestamp 1689712945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb:  View run FSR_Trainable_a07ed3a2 at: https://wandb.ai/seokjin/FSR-prediction/runs/a07ed3a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443145)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054153-a07ed3a2/logs\n",
      "2023-07-19 05:42:32,505\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:32,510\tWARNING util.py:315 -- The `process_trial_result` operation took 1.921 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:32,511\tWARNING util.py:315 -- Processing trial results took 1.923 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:42:32,513\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_61f740fc_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-42-17/wandb/run-20230719_054235-61f740fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Syncing run FSR_Trainable_61f740fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/61f740fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                      mae 0.3664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                     mape 0.10558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                     rmse 0.6799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:       time_since_restore 6.44051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:         time_this_iter_s 1.45461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:             time_total_s 6.44051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:                timestamp 1689712957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb:  View run FSR_Trainable_61f740fc at: https://wandb.ai/seokjin/FSR-prediction/runs/61f740fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443640)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054235-61f740fc/logs\n",
      "2023-07-19 05:42:43,388\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.204 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:43,392\tWARNING util.py:315 -- The `process_trial_result` operation took 2.209 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:43,394\tWARNING util.py:315 -- Processing trial results took 2.210 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:42:43,398\tWARNING util.py:315 -- The `process_trial_result` operation took 2.214 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_062fe3c1_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-42-28/wandb/run-20230719_054246-062fe3c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Syncing run FSR_Trainable_062fe3c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/062fe3c1\n",
      "2023-07-19 05:42:54,293\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.031 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:54,297\tWARNING util.py:315 -- The `process_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:42:54,298\tWARNING util.py:315 -- Processing trial results took 2.037 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:42:54,300\tWARNING util.py:315 -- The `process_trial_result` operation took 2.038 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_792c1063_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-42-39/wandb/run-20230719_054258-792c1063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Syncing run FSR_Trainable_792c1063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/792c1063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                      mae 0.36157\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                     mape 0.10292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                     rmse 0.65352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:       time_since_restore 41.70759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:         time_this_iter_s 1.13045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:             time_total_s 41.70759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:                timestamp 1689713005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb:  View run FSR_Trainable_062fe3c1 at: https://wandb.ai/seokjin/FSR-prediction/runs/062fe3c1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443868)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054246-062fe3c1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                      mae 0.34119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                     mape 0.09721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                     rmse 0.63665\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:       time_since_restore 41.72884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:         time_this_iter_s 1.22814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:             time_total_s 41.72884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:                timestamp 1689713015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb:  View run FSR_Trainable_792c1063 at: https://wandb.ai/seokjin/FSR-prediction/runs/792c1063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444093)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054258-792c1063/logs\n",
      "2023-07-19 05:43:41,096\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:43:41,107\tWARNING util.py:315 -- The `process_trial_result` operation took 1.965 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:43:41,111\tWARNING util.py:315 -- Processing trial results took 1.969 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:43:41,116\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_8b389096_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-42-50/wandb/run-20230719_054344-8b389096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Syncing run FSR_Trainable_8b389096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8b389096\n",
      "2023-07-19 05:43:53,172\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.791 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:43:53,175\tWARNING util.py:315 -- The `process_trial_result` operation took 1.795 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:43:53,176\tWARNING util.py:315 -- Processing trial results took 1.796 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:43:53,177\tWARNING util.py:315 -- The `process_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_35537741_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-43-37/wandb/run-20230719_054355-35537741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Syncing run FSR_Trainable_35537741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/35537741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                      mae 0.36593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                     mape 0.10439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                     rmse 0.67661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:       time_since_restore 9.36181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:         time_this_iter_s 1.98719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:             time_total_s 9.36181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:                timestamp 1689713039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb:  View run FSR_Trainable_35537741 at: https://wandb.ai/seokjin/FSR-prediction/runs/35537741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444597)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054355-35537741/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054142-70da697c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054142-70da697c/logs\n",
      "2023-07-19 05:44:14,571\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:44:14,574\tWARNING util.py:315 -- The `process_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:44:14,576\tWARNING util.py:315 -- Processing trial results took 1.972 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:44:14,577\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=442918)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2895b6e6_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-43-48/wandb/run-20230719_054417-2895b6e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Syncing run FSR_Trainable_2895b6e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2895b6e6\n",
      "2023-07-19 05:44:24,732\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:44:24,735\tWARNING util.py:315 -- The `process_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:44:24,737\tWARNING util.py:315 -- Processing trial results took 1.810 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:44:24,739\tWARNING util.py:315 -- The `process_trial_result` operation took 1.812 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_a05e09bf_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-44-10/wandb/run-20230719_054428-a05e09bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Syncing run FSR_Trainable_a05e09bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a05e09bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                      mae 0.32928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                     mape 0.09025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                     rmse 0.62043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:       time_since_restore 131.98681\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:         time_this_iter_s 1.27965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:             time_total_s 131.98681\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:                timestamp 1689713087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb:  View run FSR_Trainable_f27fd036 at: https://wandb.ai/seokjin/FSR-prediction/runs/f27fd036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=443408)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054224-f27fd036/logs\n",
      "2023-07-19 05:45:03,640\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:45:03,643\tWARNING util.py:315 -- The `process_trial_result` operation took 1.952 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:45:03,644\tWARNING util.py:315 -- Processing trial results took 1.954 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:45:03,645\tWARNING util.py:315 -- The `process_trial_result` operation took 1.955 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_731370c8_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-44-21/wandb/run-20230719_054507-731370c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Syncing run FSR_Trainable_731370c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/731370c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                      mae 0.34507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                     mape 0.10001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                     rmse 0.68608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:       time_since_restore 3.52372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:         time_this_iter_s 1.71728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:             time_total_s 3.52372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:                timestamp 1689713105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb:  View run FSR_Trainable_731370c8 at: https://wandb.ai/seokjin/FSR-prediction/runs/731370c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445332)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054507-731370c8/logs\n",
      "2023-07-19 05:45:22,931\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:45:22,934\tWARNING util.py:315 -- The `process_trial_result` operation took 1.956 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:45:22,937\tWARNING util.py:315 -- Processing trial results took 1.959 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:45:22,938\tWARNING util.py:315 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_0500b8ec_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-44-59/wandb/run-20230719_054524-0500b8ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Syncing run FSR_Trainable_0500b8ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0500b8ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                      mae 0.36883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                     mape 0.1035\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                     rmse 0.66808\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:       time_since_restore 17.78617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:         time_this_iter_s 2.08755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:             time_total_s 17.78617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:                timestamp 1689713138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb:  View run FSR_Trainable_0500b8ec at: https://wandb.ai/seokjin/FSR-prediction/runs/0500b8ec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445569)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054524-0500b8ec/logs\n",
      "2023-07-19 05:45:54,331\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:45:54,336\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:45:54,337\tWARNING util.py:315 -- Processing trial results took 1.949 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:45:54,339\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_60823b81_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-45-18/wandb/run-20230719_054557-60823b81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Syncing run FSR_Trainable_60823b81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/60823b81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                      mae 0.36958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                     mape 0.10459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                     rmse 0.70265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:       time_since_restore 2.47973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:         time_this_iter_s 2.47973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:             time_total_s 2.47973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:                timestamp 1689713152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb:  View run FSR_Trainable_60823b81 at: https://wandb.ai/seokjin/FSR-prediction/runs/60823b81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445820)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054557-60823b81/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                      mae 0.33098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                     mape 0.09381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                     rmse 0.6202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:       time_since_restore 134.33128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:         time_this_iter_s 1.34579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:             time_total_s 134.33128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:                timestamp 1689713166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb:  View run FSR_Trainable_8b389096 at: https://wandb.ai/seokjin/FSR-prediction/runs/8b389096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444372)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054344-8b389096/logs\n",
      "2023-07-19 05:46:11,156\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.144 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:11,160\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:11,163\tWARNING util.py:315 -- Processing trial results took 2.152 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:46:11,165\tWARNING util.py:315 -- The `process_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_5901cdec_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-45-49/wandb/run-20230719_054613-5901cdec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Syncing run FSR_Trainable_5901cdec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5901cdec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                      mae 2.3866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                     mape 0.38707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                     rmse 2.64162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:       time_since_restore 2.64569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:         time_this_iter_s 2.64569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:             time_total_s 2.64569\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:                timestamp 1689713169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb:  View run FSR_Trainable_5901cdec at: https://wandb.ai/seokjin/FSR-prediction/runs/5901cdec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446056)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054613-5901cdec/logs\n",
      "2023-07-19 05:46:20,889\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:20,893\tWARNING util.py:315 -- The `process_trial_result` operation took 1.921 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:20,895\tWARNING util.py:315 -- Processing trial results took 1.922 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:46:20,896\tWARNING util.py:315 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2a4d8c27_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-46-06/wandb/run-20230719_054623-2a4d8c27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Syncing run FSR_Trainable_2a4d8c27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2a4d8c27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                      mae 1.66601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                     mape 0.27593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                     rmse 1.91139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:       time_since_restore 1.89696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:         time_this_iter_s 1.89696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:             time_total_s 1.89696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:                timestamp 1689713178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb:  View run FSR_Trainable_2a4d8c27 at: https://wandb.ai/seokjin/FSR-prediction/runs/2a4d8c27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446292)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054623-2a4d8c27/logs\n",
      "2023-07-19 05:46:30,975\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.057 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:30,980\tWARNING util.py:315 -- The `process_trial_result` operation took 2.062 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:30,981\tWARNING util.py:315 -- Processing trial results took 2.064 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:46:30,983\tWARNING util.py:315 -- The `process_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_76c223ee_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-46-17/wandb/run-20230719_054633-76c223ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Syncing run FSR_Trainable_76c223ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/76c223ee\n",
      "2023-07-19 05:46:42,173\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.144 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:42,177\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:46:42,178\tWARNING util.py:315 -- Processing trial results took 2.150 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:46:42,179\tWARNING util.py:315 -- The `process_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2596e03a_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-46-27/wandb/run-20230719_054645-2596e03a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Syncing run FSR_Trainable_2596e03a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2596e03a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                      mae 0.31905\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                     mape 0.09011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                     rmse 0.60324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:       time_since_restore 137.50317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:         time_this_iter_s 1.29258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:             time_total_s 137.50317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:                timestamp 1689713214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb:  View run FSR_Trainable_a05e09bf at: https://wandb.ai/seokjin/FSR-prediction/runs/a05e09bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=445068)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054428-a05e09bf/logs\n",
      "2023-07-19 05:47:09,288\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.639 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:47:09,304\tWARNING util.py:315 -- The `process_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:47:09,305\tWARNING util.py:315 -- Processing trial results took 1.660 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:47:09,306\tWARNING util.py:315 -- The `process_trial_result` operation took 1.662 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_b47a652f_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-46-38/wandb/run-20230719_054712-b47a652f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Syncing run FSR_Trainable_b47a652f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b47a652f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                      mae 0.34555\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                     mape 0.09894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                     rmse 0.63499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:       time_since_restore 42.50365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:         time_this_iter_s 1.28089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:             time_total_s 42.50365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:                timestamp 1689713245\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb:  View run FSR_Trainable_2596e03a at: https://wandb.ai/seokjin/FSR-prediction/runs/2596e03a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446743)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054645-2596e03a/logs\n",
      "2023-07-19 05:47:40,703\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.177 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:47:40,707\tWARNING util.py:315 -- The `process_trial_result` operation took 2.182 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:47:40,711\tWARNING util.py:315 -- Processing trial results took 2.187 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:47:40,713\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_2d78adff_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-47-06/wandb/run-20230719_054743-2d78adff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Syncing run FSR_Trainable_2d78adff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2d78adff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                      mae 0.3607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                     mape 0.10328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                     rmse 0.69867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:       time_since_restore 1.745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:         time_this_iter_s 1.745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:             time_total_s 1.745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:                timestamp 1689713258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb:  View run FSR_Trainable_2d78adff at: https://wandb.ai/seokjin/FSR-prediction/runs/2d78adff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447248)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054743-2d78adff/logs\n",
      "2023-07-19 05:47:56,774\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.469 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:47:56,777\tWARNING util.py:315 -- The `process_trial_result` operation took 2.473 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:47:56,780\tWARNING util.py:315 -- Processing trial results took 2.476 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:47:56,782\tWARNING util.py:315 -- The `process_trial_result` operation took 2.479 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_588d072c_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_SimpleI_2023-07-19_05-47-36/wandb/run-20230719_054759-588d072c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Syncing run FSR_Trainable_588d072c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/588d072c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                      mae 0.31737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                     mape 0.09132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                     rmse 0.60234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:       time_since_restore 207.8562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:         time_this_iter_s 2.21852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:             time_total_s 207.8562\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:                timestamp 1689713276\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb:  View run FSR_Trainable_2895b6e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/2895b6e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=444852)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054417-2895b6e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: / 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447486)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054759-588d072c/logs\n",
      "2023-07-19 05:48:12,286\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.368 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:48:12,292\tWARNING util.py:315 -- The `process_trial_result` operation took 2.375 s, which may be a performance bottleneck.\n",
      "2023-07-19 05:48:12,293\tWARNING util.py:315 -- Processing trial results took 2.377 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 05:48:12,296\tWARNING util.py:315 -- The `process_trial_result` operation took 2.379 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_04-38-35/FSR_Trainable_5dcf6429_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,imputer=sklearn_impute_Simple_2023-07-19_05-47-52/wandb/run-20230719_054814-5dcf6429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Syncing run FSR_Trainable_5dcf6429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5dcf6429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                      mae 0.35612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                     mape 0.09679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                     rmse 0.71172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:       time_since_restore 2.75695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:         time_this_iter_s 2.75695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:             time_total_s 2.75695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:                timestamp 1689713289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb:  View run FSR_Trainable_5dcf6429 at: https://wandb.ai/seokjin/FSR-prediction/runs/5dcf6429\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=447726)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054814-5dcf6429/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                      mae 0.31495\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                     mape 0.09109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                     rmse 0.60667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:       time_since_restore 120.67513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:         time_this_iter_s 0.89315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:             time_total_s 120.67513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:                timestamp 1689713321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb:  View run FSR_Trainable_76c223ee at: https://wandb.ai/seokjin/FSR-prediction/runs/76c223ee\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446521)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054633-76c223ee/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                      mae 0.32576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                     mape 0.09259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                     rmse 0.61904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:       time_since_restore 102.98027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:         time_this_iter_s 0.63287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:             time_total_s 102.98027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:                timestamp 1689713338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb:  View run FSR_Trainable_b47a652f at: https://wandb.ai/seokjin/FSR-prediction/runs/b47a652f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=446992)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_054712-b47a652f/logs\n",
      "2023-07-19 05:49:03,255\tINFO tune.py:1111 -- Total run time: 4224.37 seconds (4219.31 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
