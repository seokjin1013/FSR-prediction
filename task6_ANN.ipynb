{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task6\n",
    "\n",
    "Index_X = FSR_for_coord\n",
    "\n",
    "Index_y = x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-19_12-32-34/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-19_12-32-34\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "0.6716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': 'FSR_for_coord',\n",
    "        'index_y': ['x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-19 12:32:34,445] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 12:32:36,539\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8267 \u001b[39m\u001b[22m\n",
      "2023-07-19 12:32:37,963\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 12:52:53</td></tr>\n",
       "<tr><td>Running for: </td><td>00:20:15.69        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.2/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.6890788430878475 | Iter 32.000: -0.6860536150415938 | Iter 16.000: -0.6827910375283269 | Iter 8.000: -0.6830004902154033 | Iter 4.000: -0.6846830302052195 | Iter 2.000: -0.6889763933287286 | Iter 1.000: -0.6946412132401347<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X      </th><th>index_y             </th><th>model        </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_4c065307</td><td>TERMINATED</td><td>172.26.215.93:586175</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e9c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.98498e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       43.287   </td><td style=\"text-align: right;\">0.709447</td><td style=\"text-align: right;\">0.368948</td><td style=\"text-align: right;\">0.0952491</td></tr>\n",
       "<tr><td>FSR_Trainable_3f89db78</td><td>TERMINATED</td><td>172.26.215.93:586247</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c900</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000163876</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       24.661   </td><td style=\"text-align: right;\">0.738911</td><td style=\"text-align: right;\">0.431788</td><td style=\"text-align: right;\">0.110562 </td></tr>\n",
       "<tr><td>FSR_Trainable_cc674ded</td><td>TERMINATED</td><td>172.26.215.93:586426</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e740</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        5.31179e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.449091</td><td style=\"text-align: right;\">2.05699 </td><td style=\"text-align: right;\">1.72287 </td><td style=\"text-align: right;\">0.276691 </td></tr>\n",
       "<tr><td>FSR_Trainable_3daf2b4f</td><td>TERMINATED</td><td>172.26.215.93:586608</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1780</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0201231  </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        8.83764 </td><td style=\"text-align: right;\">0.748562</td><td style=\"text-align: right;\">0.410457</td><td style=\"text-align: right;\">0.0910491</td></tr>\n",
       "<tr><td>FSR_Trainable_d5cd8ecb</td><td>TERMINATED</td><td>172.26.215.93:586929</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0231207  </td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.947242</td><td style=\"text-align: right;\">0.858841</td><td style=\"text-align: right;\">0.579424</td><td style=\"text-align: right;\">0.135847 </td></tr>\n",
       "<tr><td>FSR_Trainable_975ec4d8</td><td>TERMINATED</td><td>172.26.215.93:587181</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000783791</td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       22.423   </td><td style=\"text-align: right;\">0.71998 </td><td style=\"text-align: right;\">0.384481</td><td style=\"text-align: right;\">0.0945442</td></tr>\n",
       "<tr><td>FSR_Trainable_73b9107b</td><td>TERMINATED</td><td>172.26.215.93:587434</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2b40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00011512 </td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.60202 </td><td style=\"text-align: right;\">0.766272</td><td style=\"text-align: right;\">0.367365</td><td style=\"text-align: right;\">0.108243 </td></tr>\n",
       "<tr><td>FSR_Trainable_94477dcb</td><td>TERMINATED</td><td>172.26.215.93:587666</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d640</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0490584  </td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.687717</td><td style=\"text-align: right;\">0.913409</td><td style=\"text-align: right;\">0.556933</td><td style=\"text-align: right;\">0.116071 </td></tr>\n",
       "<tr><td>FSR_Trainable_ba0d5db0</td><td>TERMINATED</td><td>172.26.215.93:587900</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000333684</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.547637</td><td style=\"text-align: right;\">2.99788 </td><td style=\"text-align: right;\">2.64216 </td><td style=\"text-align: right;\">0.450981 </td></tr>\n",
       "<tr><td>FSR_Trainable_4ac2d8e6</td><td>TERMINATED</td><td>172.26.215.93:588140</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bcc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00777067 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       16.0406  </td><td style=\"text-align: right;\">0.716007</td><td style=\"text-align: right;\">0.378713</td><td style=\"text-align: right;\">0.0907652</td></tr>\n",
       "<tr><td>FSR_Trainable_74dbef32</td><td>TERMINATED</td><td>172.26.215.93:588233</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cfc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000770853</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       29.4636  </td><td style=\"text-align: right;\">0.697817</td><td style=\"text-align: right;\">0.369542</td><td style=\"text-align: right;\">0.0939247</td></tr>\n",
       "<tr><td>FSR_Trainable_d9b9342e</td><td>TERMINATED</td><td>172.26.215.93:588552</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0027031  </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.720611</td><td style=\"text-align: right;\">0.771801</td><td style=\"text-align: right;\">0.450817</td><td style=\"text-align: right;\">0.108167 </td></tr>\n",
       "<tr><td>FSR_Trainable_3facbad1</td><td>TERMINATED</td><td>172.26.215.93:588774</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d180</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        7.46843e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.551296</td><td style=\"text-align: right;\">2.51697 </td><td style=\"text-align: right;\">2.24194 </td><td style=\"text-align: right;\">0.350986 </td></tr>\n",
       "<tr><td>FSR_Trainable_fb69d467</td><td>TERMINATED</td><td>172.26.215.93:589025</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.52669e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       49.5629  </td><td style=\"text-align: right;\">0.713594</td><td style=\"text-align: right;\">0.377818</td><td style=\"text-align: right;\">0.0937043</td></tr>\n",
       "<tr><td>FSR_Trainable_2800ec9e</td><td>TERMINATED</td><td>172.26.215.93:589258</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c880</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.10543e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       14.7173  </td><td style=\"text-align: right;\">0.721124</td><td style=\"text-align: right;\">0.378241</td><td style=\"text-align: right;\">0.0927852</td></tr>\n",
       "<tr><td>FSR_Trainable_6c791bc8</td><td>TERMINATED</td><td>172.26.215.93:589487</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__da40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.60031e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.610034</td><td style=\"text-align: right;\">2.49943 </td><td style=\"text-align: right;\">2.1848  </td><td style=\"text-align: right;\">0.353141 </td></tr>\n",
       "<tr><td>FSR_Trainable_93c5e213</td><td>TERMINATED</td><td>172.26.215.93:589712</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c600</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.2055e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       43.7829  </td><td style=\"text-align: right;\">0.681293</td><td style=\"text-align: right;\">0.353356</td><td style=\"text-align: right;\">0.0974838</td></tr>\n",
       "<tr><td>FSR_Trainable_5a119cb1</td><td>TERMINATED</td><td>172.26.215.93:589959</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3e00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000986316</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.742979</td><td style=\"text-align: right;\">2.27949 </td><td style=\"text-align: right;\">2.05894 </td><td style=\"text-align: right;\">0.346268 </td></tr>\n",
       "<tr><td>FSR_Trainable_dce86eaa</td><td>TERMINATED</td><td>172.26.215.93:590184</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6d40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00112544 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       50.1152  </td><td style=\"text-align: right;\">0.695836</td><td style=\"text-align: right;\">0.368148</td><td style=\"text-align: right;\">0.0936629</td></tr>\n",
       "<tr><td>FSR_Trainable_f7a9c5df</td><td>TERMINATED</td><td>172.26.215.93:590414</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6c40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.00155e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       36.9426  </td><td style=\"text-align: right;\">0.686824</td><td style=\"text-align: right;\">0.347616</td><td style=\"text-align: right;\">0.0980838</td></tr>\n",
       "<tr><td>FSR_Trainable_32a09485</td><td>TERMINATED</td><td>172.26.215.93:590657</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fdc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00314106 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       38.8931  </td><td style=\"text-align: right;\">0.695122</td><td style=\"text-align: right;\">0.36719 </td><td style=\"text-align: right;\">0.0939628</td></tr>\n",
       "<tr><td>FSR_Trainable_eec8f5aa</td><td>TERMINATED</td><td>172.26.215.93:590944</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fac0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00321337 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       35.0984  </td><td style=\"text-align: right;\">0.695118</td><td style=\"text-align: right;\">0.366887</td><td style=\"text-align: right;\">0.0933402</td></tr>\n",
       "<tr><td>FSR_Trainable_cffe4d2f</td><td>TERMINATED</td><td>172.26.215.93:591214</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7780</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000339577</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       38.1707  </td><td style=\"text-align: right;\">0.69357 </td><td style=\"text-align: right;\">0.366264</td><td style=\"text-align: right;\">0.0924884</td></tr>\n",
       "<tr><td>FSR_Trainable_f0e8e935</td><td>TERMINATED</td><td>172.26.215.93:591422</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0e80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.74473e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       36.196   </td><td style=\"text-align: right;\">0.68892 </td><td style=\"text-align: right;\">0.355519</td><td style=\"text-align: right;\">0.0973735</td></tr>\n",
       "<tr><td>FSR_Trainable_516c3546</td><td>TERMINATED</td><td>172.26.215.93:591659</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9180</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        4.72238e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.0268  </td><td style=\"text-align: right;\">0.700656</td><td style=\"text-align: right;\">0.360913</td><td style=\"text-align: right;\">0.0926143</td></tr>\n",
       "<tr><td>FSR_Trainable_ac021b55</td><td>TERMINATED</td><td>172.26.215.93:591896</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3f80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        3.39951e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       37.6297  </td><td style=\"text-align: right;\">0.677568</td><td style=\"text-align: right;\">0.349101</td><td style=\"text-align: right;\">0.096672 </td></tr>\n",
       "<tr><td>FSR_Trainable_6cbf283a</td><td>TERMINATED</td><td>172.26.215.93:592139</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        3.06356e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.530205</td><td style=\"text-align: right;\">0.741861</td><td style=\"text-align: right;\">0.420866</td><td style=\"text-align: right;\">0.113081 </td></tr>\n",
       "<tr><td>FSR_Trainable_9200f54b</td><td>TERMINATED</td><td>172.26.215.93:592379</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9200</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        3.76055e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       38.0979  </td><td style=\"text-align: right;\">0.697574</td><td style=\"text-align: right;\">0.364037</td><td style=\"text-align: right;\">0.093427 </td></tr>\n",
       "<tr><td>FSR_Trainable_9d1f181e</td><td>TERMINATED</td><td>172.26.215.93:592624</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.911e-05  </td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.00508 </td><td style=\"text-align: right;\">0.709229</td><td style=\"text-align: right;\">0.37488 </td><td style=\"text-align: right;\">0.104922 </td></tr>\n",
       "<tr><td>FSR_Trainable_13376944</td><td>TERMINATED</td><td>172.26.215.93:592850</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5bc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.96662e-05</td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.05478 </td><td style=\"text-align: right;\">0.71014 </td><td style=\"text-align: right;\">0.377046</td><td style=\"text-align: right;\">0.104644 </td></tr>\n",
       "<tr><td>FSR_Trainable_c1020d1d</td><td>TERMINATED</td><td>172.26.215.93:593089</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__db80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.0303e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       40.5935  </td><td style=\"text-align: right;\">0.675797</td><td style=\"text-align: right;\">0.332185</td><td style=\"text-align: right;\">0.0954011</td></tr>\n",
       "<tr><td>FSR_Trainable_c1baadff</td><td>TERMINATED</td><td>172.26.215.93:593330</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ef00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.34798e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       62.3849  </td><td style=\"text-align: right;\">0.689377</td><td style=\"text-align: right;\">0.362186</td><td style=\"text-align: right;\">0.0913388</td></tr>\n",
       "<tr><td>FSR_Trainable_7223ea8f</td><td>TERMINATED</td><td>172.26.215.93:593554</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0080</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.19717e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       63.6396  </td><td style=\"text-align: right;\">0.692058</td><td style=\"text-align: right;\">0.361229</td><td style=\"text-align: right;\">0.0943213</td></tr>\n",
       "<tr><td>FSR_Trainable_e4fcf99e</td><td>TERMINATED</td><td>172.26.215.93:593792</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.25357e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       63.1489  </td><td style=\"text-align: right;\">0.687492</td><td style=\"text-align: right;\">0.361628</td><td style=\"text-align: right;\">0.0942743</td></tr>\n",
       "<tr><td>FSR_Trainable_f1c23855</td><td>TERMINATED</td><td>172.26.215.93:594085</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d640</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.03753e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       57.3284  </td><td style=\"text-align: right;\">0.687581</td><td style=\"text-align: right;\">0.363373</td><td style=\"text-align: right;\">0.0958467</td></tr>\n",
       "<tr><td>FSR_Trainable_8c9518c7</td><td>TERMINATED</td><td>172.26.215.93:594359</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.19394e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.15526 </td><td style=\"text-align: right;\">0.690417</td><td style=\"text-align: right;\">0.332044</td><td style=\"text-align: right;\">0.0950253</td></tr>\n",
       "<tr><td>FSR_Trainable_93ffe5a7</td><td>TERMINATED</td><td>172.26.215.93:594562</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e1c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.09658e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.753346</td><td style=\"text-align: right;\">0.698185</td><td style=\"text-align: right;\">0.369852</td><td style=\"text-align: right;\">0.107496 </td></tr>\n",
       "<tr><td>FSR_Trainable_31faf724</td><td>TERMINATED</td><td>172.26.215.93:594811</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5dc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        2.22725e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.29734 </td><td style=\"text-align: right;\">0.693191</td><td style=\"text-align: right;\">0.357717</td><td style=\"text-align: right;\">0.102063 </td></tr>\n",
       "<tr><td>FSR_Trainable_c4f4b7da</td><td>TERMINATED</td><td>172.26.215.93:595046</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c840</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        6.73287e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.428261</td><td style=\"text-align: right;\">0.723395</td><td style=\"text-align: right;\">0.372315</td><td style=\"text-align: right;\">0.102087 </td></tr>\n",
       "<tr><td>FSR_Trainable_c9a9fa72</td><td>TERMINATED</td><td>172.26.215.93:595279</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        7.013e-05  </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        6.55527 </td><td style=\"text-align: right;\">0.695517</td><td style=\"text-align: right;\">0.367267</td><td style=\"text-align: right;\">0.0933371</td></tr>\n",
       "<tr><td>FSR_Trainable_600c6b6c</td><td>TERMINATED</td><td>172.26.215.93:595515</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cf00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        8.0151e-05 </td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       18.9437  </td><td style=\"text-align: right;\">0.698301</td><td style=\"text-align: right;\">0.367849</td><td style=\"text-align: right;\">0.0934042</td></tr>\n",
       "<tr><td>FSR_Trainable_dea91bda</td><td>TERMINATED</td><td>172.26.215.93:595744</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__de00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        9.72113e-05</td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        0.923195</td><td style=\"text-align: right;\">0.695942</td><td style=\"text-align: right;\">0.353322</td><td style=\"text-align: right;\">0.10491  </td></tr>\n",
       "<tr><td>FSR_Trainable_465577db</td><td>TERMINATED</td><td>172.26.215.93:595832</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__bd40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000111699</td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.16543 </td><td style=\"text-align: right;\">0.69402 </td><td style=\"text-align: right;\">0.33887 </td><td style=\"text-align: right;\">0.0974638</td></tr>\n",
       "<tr><td>FSR_Trainable_9f640d1b</td><td>TERMINATED</td><td>172.26.215.93:596151</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__cec0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.07196e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       25.9221  </td><td style=\"text-align: right;\">0.693141</td><td style=\"text-align: right;\">0.364206</td><td style=\"text-align: right;\">0.0956895</td></tr>\n",
       "<tr><td>FSR_Trainable_05183844</td><td>TERMINATED</td><td>172.26.215.93:596393</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ec40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.09005e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       34.863   </td><td style=\"text-align: right;\">0.687038</td><td style=\"text-align: right;\">0.347267</td><td style=\"text-align: right;\">0.09601  </td></tr>\n",
       "<tr><td>FSR_Trainable_926676f3</td><td>TERMINATED</td><td>172.26.215.93:596610</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ec00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.55886e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.04537 </td><td style=\"text-align: right;\">0.696644</td><td style=\"text-align: right;\">0.351266</td><td style=\"text-align: right;\">0.0972942</td></tr>\n",
       "<tr><td>FSR_Trainable_95726d4d</td><td>TERMINATED</td><td>172.26.215.93:596830</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3580</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.91555e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.544977</td><td style=\"text-align: right;\">0.712543</td><td style=\"text-align: right;\">0.364148</td><td style=\"text-align: right;\">0.0963233</td></tr>\n",
       "<tr><td>FSR_Trainable_43f6bf74</td><td>TERMINATED</td><td>172.26.215.93:597078</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f480</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.60445e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.364737</td><td style=\"text-align: right;\">0.738966</td><td style=\"text-align: right;\">0.402616</td><td style=\"text-align: right;\">0.11129  </td></tr>\n",
       "<tr><td>FSR_Trainable_e547a93a</td><td>TERMINATED</td><td>172.26.215.93:597307</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6ec0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        4.68517e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.300096</td><td style=\"text-align: right;\">2.82725 </td><td style=\"text-align: right;\">2.36352 </td><td style=\"text-align: right;\">0.40444  </td></tr>\n",
       "<tr><td>FSR_Trainable_7be5f46b</td><td>TERMINATED</td><td>172.26.215.93:597541</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1440</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        5.13224e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.23156 </td><td style=\"text-align: right;\">2.23853 </td><td style=\"text-align: right;\">1.98808 </td><td style=\"text-align: right;\">0.302478 </td></tr>\n",
       "<tr><td>FSR_Trainable_f89b1452</td><td>TERMINATED</td><td>172.26.215.93:597776</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4040</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000163749</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.74753 </td><td style=\"text-align: right;\">0.709189</td><td style=\"text-align: right;\">0.378499</td><td style=\"text-align: right;\">0.109805 </td></tr>\n",
       "<tr><td>FSR_Trainable_6fabf16c</td><td>TERMINATED</td><td>172.26.215.93:597865</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fdc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.7327e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.932467</td><td style=\"text-align: right;\">0.708778</td><td style=\"text-align: right;\">0.383967</td><td style=\"text-align: right;\">0.100731 </td></tr>\n",
       "<tr><td>FSR_Trainable_7ad4562c</td><td>TERMINATED</td><td>172.26.215.93:598050</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.02688e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.789543</td><td style=\"text-align: right;\">0.706983</td><td style=\"text-align: right;\">0.36389 </td><td style=\"text-align: right;\">0.0993147</td></tr>\n",
       "<tr><td>FSR_Trainable_22bf881b</td><td>TERMINATED</td><td>172.26.215.93:598233</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ac80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.5972e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.770888</td><td style=\"text-align: right;\">0.711317</td><td style=\"text-align: right;\">0.364555</td><td style=\"text-align: right;\">0.0965191</td></tr>\n",
       "<tr><td>FSR_Trainable_07d63ce3</td><td>TERMINATED</td><td>172.26.215.93:598554</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8b40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.75818e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        1.91434 </td><td style=\"text-align: right;\">0.692606</td><td style=\"text-align: right;\">0.334862</td><td style=\"text-align: right;\">0.0951913</td></tr>\n",
       "<tr><td>FSR_Trainable_26092a16</td><td>TERMINATED</td><td>172.26.215.93:598647</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d840</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.67812e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       32.5503  </td><td style=\"text-align: right;\">0.692268</td><td style=\"text-align: right;\">0.365088</td><td style=\"text-align: right;\">0.0946039</td></tr>\n",
       "<tr><td>FSR_Trainable_69f7d8fc</td><td>TERMINATED</td><td>172.26.215.93:598963</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f900</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.7684e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.419852</td><td style=\"text-align: right;\">0.704142</td><td style=\"text-align: right;\">0.364848</td><td style=\"text-align: right;\">0.10005  </td></tr>\n",
       "<tr><td>FSR_Trainable_43cad568</td><td>TERMINATED</td><td>172.26.215.93:599054</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__fe40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.7073e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       58.3205  </td><td style=\"text-align: right;\">0.689974</td><td style=\"text-align: right;\">0.362458</td><td style=\"text-align: right;\">0.0929477</td></tr>\n",
       "<tr><td>FSR_Trainable_99d79818</td><td>TERMINATED</td><td>172.26.215.93:599368</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__adc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.44237e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.54149 </td><td style=\"text-align: right;\">0.690215</td><td style=\"text-align: right;\">0.370529</td><td style=\"text-align: right;\">0.106992 </td></tr>\n",
       "<tr><td>FSR_Trainable_fffdff38</td><td>TERMINATED</td><td>172.26.215.93:599592</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.45031e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.779195</td><td style=\"text-align: right;\">0.705822</td><td style=\"text-align: right;\">0.376545</td><td style=\"text-align: right;\">0.0988597</td></tr>\n",
       "<tr><td>FSR_Trainable_dbeb54e6</td><td>TERMINATED</td><td>172.26.215.93:599831</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.01149e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.28711 </td><td style=\"text-align: right;\">0.688164</td><td style=\"text-align: right;\">0.353965</td><td style=\"text-align: right;\">0.0966764</td></tr>\n",
       "<tr><td>FSR_Trainable_8b3cb18b</td><td>TERMINATED</td><td>172.26.215.93:600069</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__af40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        4.00127e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.5456  </td><td style=\"text-align: right;\">2.59378 </td><td style=\"text-align: right;\">2.28835 </td><td style=\"text-align: right;\">0.37588  </td></tr>\n",
       "<tr><td>FSR_Trainable_7d2620f2</td><td>TERMINATED</td><td>172.26.215.93:600307</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__a280</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.67841e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.518257</td><td style=\"text-align: right;\">2.64992 </td><td style=\"text-align: right;\">2.46885 </td><td style=\"text-align: right;\">0.399757 </td></tr>\n",
       "<tr><td>FSR_Trainable_8be6c175</td><td>TERMINATED</td><td>172.26.215.93:600531</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5380</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.04389e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       39.4842  </td><td style=\"text-align: right;\">0.692058</td><td style=\"text-align: right;\">0.364049</td><td style=\"text-align: right;\">0.0908353</td></tr>\n",
       "<tr><td>FSR_Trainable_ad9d1ca1</td><td>TERMINATED</td><td>172.26.215.93:600758</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c880</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.98809e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.2802  </td><td style=\"text-align: right;\">0.68989 </td><td style=\"text-align: right;\">0.362244</td><td style=\"text-align: right;\">0.0925566</td></tr>\n",
       "<tr><td>FSR_Trainable_c0cac9a2</td><td>TERMINATED</td><td>172.26.215.93:600986</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d9c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.05765e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.0267  </td><td style=\"text-align: right;\">0.692614</td><td style=\"text-align: right;\">0.363592</td><td style=\"text-align: right;\">0.0920025</td></tr>\n",
       "<tr><td>FSR_Trainable_add6f9fc</td><td>TERMINATED</td><td>172.26.215.93:601213</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9b80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.11167e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       66.2366  </td><td style=\"text-align: right;\">0.690882</td><td style=\"text-align: right;\">0.360398</td><td style=\"text-align: right;\">0.0920104</td></tr>\n",
       "<tr><td>FSR_Trainable_95b55524</td><td>TERMINATED</td><td>172.26.215.93:601500</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2c40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.10114e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       62.7993  </td><td style=\"text-align: right;\">0.687858</td><td style=\"text-align: right;\">0.36098 </td><td style=\"text-align: right;\">0.0927179</td></tr>\n",
       "<tr><td>FSR_Trainable_d912899d</td><td>TERMINATED</td><td>172.26.215.93:601781</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8380</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.05238e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       21.0144  </td><td style=\"text-align: right;\">0.693966</td><td style=\"text-align: right;\">0.358589</td><td style=\"text-align: right;\">0.0933797</td></tr>\n",
       "<tr><td>FSR_Trainable_d7846332</td><td>TERMINATED</td><td>172.26.215.93:601987</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__c840</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.4493e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.672598</td><td style=\"text-align: right;\">0.707034</td><td style=\"text-align: right;\">0.362349</td><td style=\"text-align: right;\">0.0968753</td></tr>\n",
       "<tr><td>FSR_Trainable_18bf0dfa</td><td>TERMINATED</td><td>172.26.215.93:602234</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7d40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.50429e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.607731</td><td style=\"text-align: right;\">0.728544</td><td style=\"text-align: right;\">0.412381</td><td style=\"text-align: right;\">0.107606 </td></tr>\n",
       "<tr><td>FSR_Trainable_99a5a408</td><td>TERMINATED</td><td>172.26.215.93:602479</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>median</td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__ff80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        5.85705e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.542787</td><td style=\"text-align: right;\">0.70417 </td><td style=\"text-align: right;\">0.383632</td><td style=\"text-align: right;\">0.101932 </td></tr>\n",
       "<tr><td>FSR_Trainable_bb1c2a03</td><td>TERMINATED</td><td>172.26.215.93:602711</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7e80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.38902e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       59.923   </td><td style=\"text-align: right;\">0.688079</td><td style=\"text-align: right;\">0.359678</td><td style=\"text-align: right;\">0.0912273</td></tr>\n",
       "<tr><td>FSR_Trainable_d545cc45</td><td>TERMINATED</td><td>172.26.215.93:602806</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__97c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.51632e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       32.3323  </td><td style=\"text-align: right;\">0.694589</td><td style=\"text-align: right;\">0.367547</td><td style=\"text-align: right;\">0.0940652</td></tr>\n",
       "<tr><td>FSR_Trainable_bb057b2a</td><td>TERMINATED</td><td>172.26.215.93:603121</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__e500</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.2911e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.61701 </td><td style=\"text-align: right;\">0.686332</td><td style=\"text-align: right;\">0.323187</td><td style=\"text-align: right;\">0.0936087</td></tr>\n",
       "<tr><td>FSR_Trainable_7a508a51</td><td>TERMINATED</td><td>172.26.215.93:603346</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__adc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.36091e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.5096  </td><td style=\"text-align: right;\">0.692435</td><td style=\"text-align: right;\">0.322995</td><td style=\"text-align: right;\">0.0945223</td></tr>\n",
       "<tr><td>FSR_Trainable_f084baac</td><td>TERMINATED</td><td>172.26.215.93:603580</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2280</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        3.03739e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       42.4947  </td><td style=\"text-align: right;\">0.692194</td><td style=\"text-align: right;\">0.363642</td><td style=\"text-align: right;\">0.0934157</td></tr>\n",
       "<tr><td>FSR_Trainable_2c7ae2c4</td><td>TERMINATED</td><td>172.26.215.93:603816</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__38c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.11792e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       17.3043  </td><td style=\"text-align: right;\">0.691259</td><td style=\"text-align: right;\">0.363681</td><td style=\"text-align: right;\">0.0935615</td></tr>\n",
       "<tr><td>FSR_Trainable_08ebf9ae</td><td>TERMINATED</td><td>172.26.215.93:604051</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1a00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        3.32357e-05</td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       28.8554  </td><td style=\"text-align: right;\">0.697067</td><td style=\"text-align: right;\">0.364634</td><td style=\"text-align: right;\">0.0917729</td></tr>\n",
       "<tr><td>FSR_Trainable_3a0cddfa</td><td>TERMINATED</td><td>172.26.215.93:604298</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1880</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        2.31632e-05</td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.07842 </td><td style=\"text-align: right;\">0.690316</td><td style=\"text-align: right;\">0.331436</td><td style=\"text-align: right;\">0.0945542</td></tr>\n",
       "<tr><td>FSR_Trainable_52289b67</td><td>TERMINATED</td><td>172.26.215.93:604534</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__53c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.32683e-05</td><td>sklearn.preproc_8810</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.491789</td><td style=\"text-align: right;\">0.722536</td><td style=\"text-align: right;\">0.378678</td><td style=\"text-align: right;\">0.0960584</td></tr>\n",
       "<tr><td>FSR_Trainable_38fc685f</td><td>TERMINATED</td><td>172.26.215.93:604776</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__4100</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.29388e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.03449 </td><td style=\"text-align: right;\">0.694246</td><td style=\"text-align: right;\">0.357923</td><td style=\"text-align: right;\">0.0950116</td></tr>\n",
       "<tr><td>FSR_Trainable_49470abf</td><td>TERMINATED</td><td>172.26.215.93:605012</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__9140</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.93178e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.54175 </td><td style=\"text-align: right;\">0.689767</td><td style=\"text-align: right;\">0.326714</td><td style=\"text-align: right;\">0.0926423</td></tr>\n",
       "<tr><td>FSR_Trainable_14ce494c</td><td>TERMINATED</td><td>172.26.215.93:605104</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.33751e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.8377  </td><td style=\"text-align: right;\">0.690624</td><td style=\"text-align: right;\">0.34173 </td><td style=\"text-align: right;\">0.104097 </td></tr>\n",
       "<tr><td>FSR_Trainable_17171c9a</td><td>TERMINATED</td><td>172.26.215.93:605289</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1200</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.27546e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.8407  </td><td style=\"text-align: right;\">0.686689</td><td style=\"text-align: right;\">0.338669</td><td style=\"text-align: right;\">0.0957882</td></tr>\n",
       "<tr><td>FSR_Trainable_99fe5057</td><td>TERMINATED</td><td>172.26.215.93:605472</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__d000</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.76199e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.22919 </td><td style=\"text-align: right;\">0.69623 </td><td style=\"text-align: right;\">0.365063</td><td style=\"text-align: right;\">0.105731 </td></tr>\n",
       "<tr><td>FSR_Trainable_40f0ffa6</td><td>TERMINATED</td><td>172.26.215.93:605662</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__1440</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.74418e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.61476 </td><td style=\"text-align: right;\">0.691224</td><td style=\"text-align: right;\">0.369309</td><td style=\"text-align: right;\">0.0983989</td></tr>\n",
       "<tr><td>FSR_Trainable_44ce35cb</td><td>TERMINATED</td><td>172.26.215.93:605849</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f980</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        4.36693e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.88661 </td><td style=\"text-align: right;\">0.7159  </td><td style=\"text-align: right;\">0.387725</td><td style=\"text-align: right;\">0.104342 </td></tr>\n",
       "<tr><td>FSR_Trainable_306e4778</td><td>TERMINATED</td><td>172.26.215.93:606036</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0240</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        5.37013e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.30799 </td><td style=\"text-align: right;\">0.692545</td><td style=\"text-align: right;\">0.33727 </td><td style=\"text-align: right;\">0.0983579</td></tr>\n",
       "<tr><td>FSR_Trainable_b963783f</td><td>TERMINATED</td><td>172.26.215.93:606222</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0b40</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.24179e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.863862</td><td style=\"text-align: right;\">0.70832 </td><td style=\"text-align: right;\">0.376492</td><td style=\"text-align: right;\">0.107889 </td></tr>\n",
       "<tr><td>FSR_Trainable_82c4071c</td><td>TERMINATED</td><td>172.26.215.93:606547</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__6740</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.28715e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.861999</td><td style=\"text-align: right;\">0.69849 </td><td style=\"text-align: right;\">0.365454</td><td style=\"text-align: right;\">0.105108 </td></tr>\n",
       "<tr><td>FSR_Trainable_42976b17</td><td>TERMINATED</td><td>172.26.215.93:606640</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7180</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.46406e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.756658</td><td style=\"text-align: right;\">2.42865 </td><td style=\"text-align: right;\">2.08839 </td><td style=\"text-align: right;\">0.326001 </td></tr>\n",
       "<tr><td>FSR_Trainable_60c199f7</td><td>TERMINATED</td><td>172.26.215.93:606956</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__8540</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.71174e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       52.2874  </td><td style=\"text-align: right;\">0.692254</td><td style=\"text-align: right;\">0.363343</td><td style=\"text-align: right;\">0.0961637</td></tr>\n",
       "<tr><td>FSR_Trainable_2057e300</td><td>TERMINATED</td><td>172.26.215.93:607052</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__7b80</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.03724e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       54.8637  </td><td style=\"text-align: right;\">0.692117</td><td style=\"text-align: right;\">0.357928</td><td style=\"text-align: right;\">0.095586 </td></tr>\n",
       "<tr><td>FSR_Trainable_022d2e6c</td><td>TERMINATED</td><td>172.26.215.93:607368</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__2d00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.6755e-05 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       27.8226  </td><td style=\"text-align: right;\">0.688187</td><td style=\"text-align: right;\">0.356571</td><td style=\"text-align: right;\">0.0947144</td></tr>\n",
       "<tr><td>FSR_Trainable_efef0f93</td><td>TERMINATED</td><td>172.26.215.93:607591</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__5e00</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.04866e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        4.21659 </td><td style=\"text-align: right;\">0.68811 </td><td style=\"text-align: right;\">0.321279</td><td style=\"text-align: right;\">0.0925323</td></tr>\n",
       "<tr><td>FSR_Trainable_a3be7ec3</td><td>TERMINATED</td><td>172.26.215.93:607842</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__3780</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.01858e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       46.038   </td><td style=\"text-align: right;\">0.691486</td><td style=\"text-align: right;\">0.35915 </td><td style=\"text-align: right;\">0.0936888</td></tr>\n",
       "<tr><td>FSR_Trainable_0aeb87c0</td><td>TERMINATED</td><td>172.26.215.93:608080</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__f700</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.66302e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       47.2934  </td><td style=\"text-align: right;\">0.681257</td><td style=\"text-align: right;\">0.341253</td><td style=\"text-align: right;\">0.0984356</td></tr>\n",
       "<tr><td>FSR_Trainable_24440a9b</td><td>TERMINATED</td><td>172.26.215.93:608314</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__0cc0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.25214e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       42.2955  </td><td style=\"text-align: right;\">0.671597</td><td style=\"text-align: right;\">0.321612</td><td style=\"text-align: right;\">0.0892672</td></tr>\n",
       "<tr><td>FSR_Trainable_e9ff9cd3</td><td>TERMINATED</td><td>172.26.215.93:608549</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_89f0</td><td>sklearn.impute._5250</td><td>mean  </td><td>FSR_for_coord</td><td>[&#x27;x_coord&#x27;, &#x27;y__22c0</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.19172e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.835093</td><td style=\"text-align: right;\">0.700165</td><td style=\"text-align: right;\">0.366793</td><td style=\"text-align: right;\">0.097432 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 12:32:38,001\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">     mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_022d2e6c</td><td>2023-07-19_12-51-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.356571</td><td style=\"text-align: right;\">0.0947144</td><td>172.26.215.93</td><td style=\"text-align: right;\">607368</td><td style=\"text-align: right;\">0.688187</td><td style=\"text-align: right;\">           27.8226  </td><td style=\"text-align: right;\">          0.841327</td><td style=\"text-align: right;\">     27.8226  </td><td style=\"text-align: right;\"> 1689738692</td><td style=\"text-align: right;\">                  32</td><td>022d2e6c  </td></tr>\n",
       "<tr><td>FSR_Trainable_05183844</td><td>2023-07-19_12-42-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.347267</td><td style=\"text-align: right;\">0.09601  </td><td>172.26.215.93</td><td style=\"text-align: right;\">596393</td><td style=\"text-align: right;\">0.687038</td><td style=\"text-align: right;\">           34.863   </td><td style=\"text-align: right;\">          0.27316 </td><td style=\"text-align: right;\">     34.863   </td><td style=\"text-align: right;\"> 1689738139</td><td style=\"text-align: right;\">                 100</td><td>05183844  </td></tr>\n",
       "<tr><td>FSR_Trainable_07d63ce3</td><td>2023-07-19_12-43-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.334862</td><td style=\"text-align: right;\">0.0951913</td><td>172.26.215.93</td><td style=\"text-align: right;\">598554</td><td style=\"text-align: right;\">0.692606</td><td style=\"text-align: right;\">            1.91434 </td><td style=\"text-align: right;\">          0.372276</td><td style=\"text-align: right;\">      1.91434 </td><td style=\"text-align: right;\"> 1689738184</td><td style=\"text-align: right;\">                   4</td><td>07d63ce3  </td></tr>\n",
       "<tr><td>FSR_Trainable_08ebf9ae</td><td>2023-07-19_12-48-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.364634</td><td style=\"text-align: right;\">0.0917729</td><td>172.26.215.93</td><td style=\"text-align: right;\">604051</td><td style=\"text-align: right;\">0.697067</td><td style=\"text-align: right;\">           28.8554  </td><td style=\"text-align: right;\">          0.962538</td><td style=\"text-align: right;\">     28.8554  </td><td style=\"text-align: right;\"> 1689738527</td><td style=\"text-align: right;\">                  32</td><td>08ebf9ae  </td></tr>\n",
       "<tr><td>FSR_Trainable_0aeb87c0</td><td>2023-07-19_12-52-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.341253</td><td style=\"text-align: right;\">0.0984356</td><td>172.26.215.93</td><td style=\"text-align: right;\">608080</td><td style=\"text-align: right;\">0.681257</td><td style=\"text-align: right;\">           47.2934  </td><td style=\"text-align: right;\">          0.488673</td><td style=\"text-align: right;\">     47.2934  </td><td style=\"text-align: right;\"> 1689738767</td><td style=\"text-align: right;\">                 100</td><td>0aeb87c0  </td></tr>\n",
       "<tr><td>FSR_Trainable_13376944</td><td>2023-07-19_12-38-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.377046</td><td style=\"text-align: right;\">0.104644 </td><td>172.26.215.93</td><td style=\"text-align: right;\">592850</td><td style=\"text-align: right;\">0.71014 </td><td style=\"text-align: right;\">            1.05478 </td><td style=\"text-align: right;\">          0.416132</td><td style=\"text-align: right;\">      1.05478 </td><td style=\"text-align: right;\"> 1689737896</td><td style=\"text-align: right;\">                   2</td><td>13376944  </td></tr>\n",
       "<tr><td>FSR_Trainable_14ce494c</td><td>2023-07-19_12-49-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.34173 </td><td style=\"text-align: right;\">0.104097 </td><td>172.26.215.93</td><td style=\"text-align: right;\">605104</td><td style=\"text-align: right;\">0.690624</td><td style=\"text-align: right;\">            1.8377  </td><td style=\"text-align: right;\">          0.613618</td><td style=\"text-align: right;\">      1.8377  </td><td style=\"text-align: right;\"> 1689738553</td><td style=\"text-align: right;\">                   2</td><td>14ce494c  </td></tr>\n",
       "<tr><td>FSR_Trainable_17171c9a</td><td>2023-07-19_12-49-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.338669</td><td style=\"text-align: right;\">0.0957882</td><td>172.26.215.93</td><td style=\"text-align: right;\">605289</td><td style=\"text-align: right;\">0.686689</td><td style=\"text-align: right;\">            3.8407  </td><td style=\"text-align: right;\">          0.535483</td><td style=\"text-align: right;\">      3.8407  </td><td style=\"text-align: right;\"> 1689738564</td><td style=\"text-align: right;\">                   4</td><td>17171c9a  </td></tr>\n",
       "<tr><td>FSR_Trainable_18bf0dfa</td><td>2023-07-19_12-46-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.412381</td><td style=\"text-align: right;\">0.107606 </td><td>172.26.215.93</td><td style=\"text-align: right;\">602234</td><td style=\"text-align: right;\">0.728544</td><td style=\"text-align: right;\">            0.607731</td><td style=\"text-align: right;\">          0.607731</td><td style=\"text-align: right;\">      0.607731</td><td style=\"text-align: right;\"> 1689738397</td><td style=\"text-align: right;\">                   1</td><td>18bf0dfa  </td></tr>\n",
       "<tr><td>FSR_Trainable_2057e300</td><td>2023-07-19_12-51-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.357928</td><td style=\"text-align: right;\">0.095586 </td><td>172.26.215.93</td><td style=\"text-align: right;\">607052</td><td style=\"text-align: right;\">0.692117</td><td style=\"text-align: right;\">           54.8637  </td><td style=\"text-align: right;\">          0.788531</td><td style=\"text-align: right;\">     54.8637  </td><td style=\"text-align: right;\"> 1689738709</td><td style=\"text-align: right;\">                  64</td><td>2057e300  </td></tr>\n",
       "<tr><td>FSR_Trainable_22bf881b</td><td>2023-07-19_12-42-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.364555</td><td style=\"text-align: right;\">0.0965191</td><td>172.26.215.93</td><td style=\"text-align: right;\">598233</td><td style=\"text-align: right;\">0.711317</td><td style=\"text-align: right;\">            0.770888</td><td style=\"text-align: right;\">          0.770888</td><td style=\"text-align: right;\">      0.770888</td><td style=\"text-align: right;\"> 1689738172</td><td style=\"text-align: right;\">                   1</td><td>22bf881b  </td></tr>\n",
       "<tr><td>FSR_Trainable_24440a9b</td><td>2023-07-19_12-52-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.321612</td><td style=\"text-align: right;\">0.0892672</td><td>172.26.215.93</td><td style=\"text-align: right;\">608314</td><td style=\"text-align: right;\">0.671597</td><td style=\"text-align: right;\">           42.2955  </td><td style=\"text-align: right;\">          0.290761</td><td style=\"text-align: right;\">     42.2955  </td><td style=\"text-align: right;\"> 1689738773</td><td style=\"text-align: right;\">                 100</td><td>24440a9b  </td></tr>\n",
       "<tr><td>FSR_Trainable_26092a16</td><td>2023-07-19_12-43-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.365088</td><td style=\"text-align: right;\">0.0946039</td><td>172.26.215.93</td><td style=\"text-align: right;\">598647</td><td style=\"text-align: right;\">0.692268</td><td style=\"text-align: right;\">           32.5503  </td><td style=\"text-align: right;\">          0.691506</td><td style=\"text-align: right;\">     32.5503  </td><td style=\"text-align: right;\"> 1689738229</td><td style=\"text-align: right;\">                  64</td><td>26092a16  </td></tr>\n",
       "<tr><td>FSR_Trainable_2800ec9e</td><td>2023-07-19_12-35-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.378241</td><td style=\"text-align: right;\">0.0927852</td><td>172.26.215.93</td><td style=\"text-align: right;\">589258</td><td style=\"text-align: right;\">0.721124</td><td style=\"text-align: right;\">           14.7173  </td><td style=\"text-align: right;\">          0.339669</td><td style=\"text-align: right;\">     14.7173  </td><td style=\"text-align: right;\"> 1689737717</td><td style=\"text-align: right;\">                  32</td><td>2800ec9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_2c7ae2c4</td><td>2023-07-19_12-48-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.363681</td><td style=\"text-align: right;\">0.0935615</td><td>172.26.215.93</td><td style=\"text-align: right;\">603816</td><td style=\"text-align: right;\">0.691259</td><td style=\"text-align: right;\">           17.3043  </td><td style=\"text-align: right;\">          0.982216</td><td style=\"text-align: right;\">     17.3043  </td><td style=\"text-align: right;\"> 1689738494</td><td style=\"text-align: right;\">                  16</td><td>2c7ae2c4  </td></tr>\n",
       "<tr><td>FSR_Trainable_306e4778</td><td>2023-07-19_12-49-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.33727 </td><td style=\"text-align: right;\">0.0983579</td><td>172.26.215.93</td><td style=\"text-align: right;\">606036</td><td style=\"text-align: right;\">0.692545</td><td style=\"text-align: right;\">            1.30799 </td><td style=\"text-align: right;\">          0.366119</td><td style=\"text-align: right;\">      1.30799 </td><td style=\"text-align: right;\"> 1689738597</td><td style=\"text-align: right;\">                   2</td><td>306e4778  </td></tr>\n",
       "<tr><td>FSR_Trainable_31faf724</td><td>2023-07-19_12-40-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.357717</td><td style=\"text-align: right;\">0.102063 </td><td>172.26.215.93</td><td style=\"text-align: right;\">594811</td><td style=\"text-align: right;\">0.693191</td><td style=\"text-align: right;\">            1.29734 </td><td style=\"text-align: right;\">          0.302891</td><td style=\"text-align: right;\">      1.29734 </td><td style=\"text-align: right;\"> 1689738025</td><td style=\"text-align: right;\">                   4</td><td>31faf724  </td></tr>\n",
       "<tr><td>FSR_Trainable_32a09485</td><td>2023-07-19_12-36-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.36719 </td><td style=\"text-align: right;\">0.0939628</td><td>172.26.215.93</td><td style=\"text-align: right;\">590657</td><td style=\"text-align: right;\">0.695122</td><td style=\"text-align: right;\">           38.8931  </td><td style=\"text-align: right;\">          0.524557</td><td style=\"text-align: right;\">     38.8931  </td><td style=\"text-align: right;\"> 1689737814</td><td style=\"text-align: right;\">                 100</td><td>32a09485  </td></tr>\n",
       "<tr><td>FSR_Trainable_38fc685f</td><td>2023-07-19_12-48-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.357923</td><td style=\"text-align: right;\">0.0950116</td><td>172.26.215.93</td><td style=\"text-align: right;\">604776</td><td style=\"text-align: right;\">0.694246</td><td style=\"text-align: right;\">            1.03449 </td><td style=\"text-align: right;\">          0.340546</td><td style=\"text-align: right;\">      1.03449 </td><td style=\"text-align: right;\"> 1689738536</td><td style=\"text-align: right;\">                   2</td><td>38fc685f  </td></tr>\n",
       "<tr><td>FSR_Trainable_3a0cddfa</td><td>2023-07-19_12-48-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.331436</td><td style=\"text-align: right;\">0.0945542</td><td>172.26.215.93</td><td style=\"text-align: right;\">604298</td><td style=\"text-align: right;\">0.690316</td><td style=\"text-align: right;\">            3.07842 </td><td style=\"text-align: right;\">          1.113   </td><td style=\"text-align: right;\">      3.07842 </td><td style=\"text-align: right;\"> 1689738514</td><td style=\"text-align: right;\">                   2</td><td>3a0cddfa  </td></tr>\n",
       "<tr><td>FSR_Trainable_3daf2b4f</td><td>2023-07-19_12-33-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.410457</td><td style=\"text-align: right;\">0.0910491</td><td>172.26.215.93</td><td style=\"text-align: right;\">586608</td><td style=\"text-align: right;\">0.748562</td><td style=\"text-align: right;\">            8.83764 </td><td style=\"text-align: right;\">          0.303026</td><td style=\"text-align: right;\">      8.83764 </td><td style=\"text-align: right;\"> 1689737599</td><td style=\"text-align: right;\">                  32</td><td>3daf2b4f  </td></tr>\n",
       "<tr><td>FSR_Trainable_3f89db78</td><td>2023-07-19_12-33-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.431788</td><td style=\"text-align: right;\">0.110562 </td><td>172.26.215.93</td><td style=\"text-align: right;\">586247</td><td style=\"text-align: right;\">0.738911</td><td style=\"text-align: right;\">           24.661   </td><td style=\"text-align: right;\">          0.194459</td><td style=\"text-align: right;\">     24.661   </td><td style=\"text-align: right;\"> 1689737607</td><td style=\"text-align: right;\">                 100</td><td>3f89db78  </td></tr>\n",
       "<tr><td>FSR_Trainable_3facbad1</td><td>2023-07-19_12-34-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.24194 </td><td style=\"text-align: right;\">0.350986 </td><td>172.26.215.93</td><td style=\"text-align: right;\">588774</td><td style=\"text-align: right;\">2.51697 </td><td style=\"text-align: right;\">            0.551296</td><td style=\"text-align: right;\">          0.551296</td><td style=\"text-align: right;\">      0.551296</td><td style=\"text-align: right;\"> 1689737678</td><td style=\"text-align: right;\">                   1</td><td>3facbad1  </td></tr>\n",
       "<tr><td>FSR_Trainable_40f0ffa6</td><td>2023-07-19_12-49-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.369309</td><td style=\"text-align: right;\">0.0983989</td><td>172.26.215.93</td><td style=\"text-align: right;\">605662</td><td style=\"text-align: right;\">0.691224</td><td style=\"text-align: right;\">            1.61476 </td><td style=\"text-align: right;\">          0.31664 </td><td style=\"text-align: right;\">      1.61476 </td><td style=\"text-align: right;\"> 1689738580</td><td style=\"text-align: right;\">                   2</td><td>40f0ffa6  </td></tr>\n",
       "<tr><td>FSR_Trainable_42976b17</td><td>2023-07-19_12-50-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.08839 </td><td style=\"text-align: right;\">0.326001 </td><td>172.26.215.93</td><td style=\"text-align: right;\">606640</td><td style=\"text-align: right;\">2.42865 </td><td style=\"text-align: right;\">            0.756658</td><td style=\"text-align: right;\">          0.756658</td><td style=\"text-align: right;\">      0.756658</td><td style=\"text-align: right;\"> 1689738620</td><td style=\"text-align: right;\">                   1</td><td>42976b17  </td></tr>\n",
       "<tr><td>FSR_Trainable_43cad568</td><td>2023-07-19_12-44-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.362458</td><td style=\"text-align: right;\">0.0929477</td><td>172.26.215.93</td><td style=\"text-align: right;\">599054</td><td style=\"text-align: right;\">0.689974</td><td style=\"text-align: right;\">           58.3205  </td><td style=\"text-align: right;\">          0.65696 </td><td style=\"text-align: right;\">     58.3205  </td><td style=\"text-align: right;\"> 1689738278</td><td style=\"text-align: right;\">                 100</td><td>43cad568  </td></tr>\n",
       "<tr><td>FSR_Trainable_43f6bf74</td><td>2023-07-19_12-42-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.402616</td><td style=\"text-align: right;\">0.11129  </td><td>172.26.215.93</td><td style=\"text-align: right;\">597078</td><td style=\"text-align: right;\">0.738966</td><td style=\"text-align: right;\">            0.364737</td><td style=\"text-align: right;\">          0.364737</td><td style=\"text-align: right;\">      0.364737</td><td style=\"text-align: right;\"> 1689738124</td><td style=\"text-align: right;\">                   1</td><td>43f6bf74  </td></tr>\n",
       "<tr><td>FSR_Trainable_44ce35cb</td><td>2023-07-19_12-49-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.387725</td><td style=\"text-align: right;\">0.104342 </td><td>172.26.215.93</td><td style=\"text-align: right;\">605849</td><td style=\"text-align: right;\">0.7159  </td><td style=\"text-align: right;\">            0.88661 </td><td style=\"text-align: right;\">          0.88661 </td><td style=\"text-align: right;\">      0.88661 </td><td style=\"text-align: right;\"> 1689738585</td><td style=\"text-align: right;\">                   1</td><td>44ce35cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_465577db</td><td>2023-07-19_12-41-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.33887 </td><td style=\"text-align: right;\">0.0974638</td><td>172.26.215.93</td><td style=\"text-align: right;\">595832</td><td style=\"text-align: right;\">0.69402 </td><td style=\"text-align: right;\">            2.16543 </td><td style=\"text-align: right;\">          0.559734</td><td style=\"text-align: right;\">      2.16543 </td><td style=\"text-align: right;\"> 1689738074</td><td style=\"text-align: right;\">                   4</td><td>465577db  </td></tr>\n",
       "<tr><td>FSR_Trainable_49470abf</td><td>2023-07-19_12-49-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.326714</td><td style=\"text-align: right;\">0.0926423</td><td>172.26.215.93</td><td style=\"text-align: right;\">605012</td><td style=\"text-align: right;\">0.689767</td><td style=\"text-align: right;\">            1.54175 </td><td style=\"text-align: right;\">          0.517856</td><td style=\"text-align: right;\">      1.54175 </td><td style=\"text-align: right;\"> 1689738545</td><td style=\"text-align: right;\">                   2</td><td>49470abf  </td></tr>\n",
       "<tr><td>FSR_Trainable_4ac2d8e6</td><td>2023-07-19_12-34-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.378713</td><td style=\"text-align: right;\">0.0907652</td><td>172.26.215.93</td><td style=\"text-align: right;\">588140</td><td style=\"text-align: right;\">0.716007</td><td style=\"text-align: right;\">           16.0406  </td><td style=\"text-align: right;\">          0.266205</td><td style=\"text-align: right;\">     16.0406  </td><td style=\"text-align: right;\"> 1689737673</td><td style=\"text-align: right;\">                  64</td><td>4ac2d8e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_4c065307</td><td>2023-07-19_12-33-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.368948</td><td style=\"text-align: right;\">0.0952491</td><td>172.26.215.93</td><td style=\"text-align: right;\">586175</td><td style=\"text-align: right;\">0.709447</td><td style=\"text-align: right;\">           43.287   </td><td style=\"text-align: right;\">          0.387044</td><td style=\"text-align: right;\">     43.287   </td><td style=\"text-align: right;\"> 1689737621</td><td style=\"text-align: right;\">                 100</td><td>4c065307  </td></tr>\n",
       "<tr><td>FSR_Trainable_516c3546</td><td>2023-07-19_12-37-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.360913</td><td style=\"text-align: right;\">0.0926143</td><td>172.26.215.93</td><td style=\"text-align: right;\">591659</td><td style=\"text-align: right;\">0.700656</td><td style=\"text-align: right;\">            2.0268  </td><td style=\"text-align: right;\">          0.462596</td><td style=\"text-align: right;\">      2.0268  </td><td style=\"text-align: right;\"> 1689737838</td><td style=\"text-align: right;\">                   4</td><td>516c3546  </td></tr>\n",
       "<tr><td>FSR_Trainable_52289b67</td><td>2023-07-19_12-48-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.378678</td><td style=\"text-align: right;\">0.0960584</td><td>172.26.215.93</td><td style=\"text-align: right;\">604534</td><td style=\"text-align: right;\">0.722536</td><td style=\"text-align: right;\">            0.491789</td><td style=\"text-align: right;\">          0.491789</td><td style=\"text-align: right;\">      0.491789</td><td style=\"text-align: right;\"> 1689738522</td><td style=\"text-align: right;\">                   1</td><td>52289b67  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a119cb1</td><td>2023-07-19_12-35-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.05894 </td><td style=\"text-align: right;\">0.346268 </td><td>172.26.215.93</td><td style=\"text-align: right;\">589959</td><td style=\"text-align: right;\">2.27949 </td><td style=\"text-align: right;\">            0.742979</td><td style=\"text-align: right;\">          0.742979</td><td style=\"text-align: right;\">      0.742979</td><td style=\"text-align: right;\"> 1689737731</td><td style=\"text-align: right;\">                   1</td><td>5a119cb1  </td></tr>\n",
       "<tr><td>FSR_Trainable_600c6b6c</td><td>2023-07-19_12-41-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.367849</td><td style=\"text-align: right;\">0.0934042</td><td>172.26.215.93</td><td style=\"text-align: right;\">595515</td><td style=\"text-align: right;\">0.698301</td><td style=\"text-align: right;\">           18.9437  </td><td style=\"text-align: right;\">          0.566098</td><td style=\"text-align: right;\">     18.9437  </td><td style=\"text-align: right;\"> 1689738077</td><td style=\"text-align: right;\">                  32</td><td>600c6b6c  </td></tr>\n",
       "<tr><td>FSR_Trainable_60c199f7</td><td>2023-07-19_12-51-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.363343</td><td style=\"text-align: right;\">0.0961637</td><td>172.26.215.93</td><td style=\"text-align: right;\">606956</td><td style=\"text-align: right;\">0.692254</td><td style=\"text-align: right;\">           52.2874  </td><td style=\"text-align: right;\">          0.878711</td><td style=\"text-align: right;\">     52.2874  </td><td style=\"text-align: right;\"> 1689738695</td><td style=\"text-align: right;\">                  64</td><td>60c199f7  </td></tr>\n",
       "<tr><td>FSR_Trainable_69f7d8fc</td><td>2023-07-19_12-43-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.364848</td><td style=\"text-align: right;\">0.10005  </td><td>172.26.215.93</td><td style=\"text-align: right;\">598963</td><td style=\"text-align: right;\">0.704142</td><td style=\"text-align: right;\">            0.419852</td><td style=\"text-align: right;\">          0.419852</td><td style=\"text-align: right;\">      0.419852</td><td style=\"text-align: right;\"> 1689738197</td><td style=\"text-align: right;\">                   1</td><td>69f7d8fc  </td></tr>\n",
       "<tr><td>FSR_Trainable_6c791bc8</td><td>2023-07-19_12-35-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.1848  </td><td style=\"text-align: right;\">0.353141 </td><td>172.26.215.93</td><td style=\"text-align: right;\">589487</td><td style=\"text-align: right;\">2.49943 </td><td style=\"text-align: right;\">            0.610034</td><td style=\"text-align: right;\">          0.610034</td><td style=\"text-align: right;\">      0.610034</td><td style=\"text-align: right;\"> 1689737709</td><td style=\"text-align: right;\">                   1</td><td>6c791bc8  </td></tr>\n",
       "<tr><td>FSR_Trainable_6cbf283a</td><td>2023-07-19_12-37-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.420866</td><td style=\"text-align: right;\">0.113081 </td><td>172.26.215.93</td><td style=\"text-align: right;\">592139</td><td style=\"text-align: right;\">0.741861</td><td style=\"text-align: right;\">            0.530205</td><td style=\"text-align: right;\">          0.530205</td><td style=\"text-align: right;\">      0.530205</td><td style=\"text-align: right;\"> 1689737858</td><td style=\"text-align: right;\">                   1</td><td>6cbf283a  </td></tr>\n",
       "<tr><td>FSR_Trainable_6fabf16c</td><td>2023-07-19_12-42-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.383967</td><td style=\"text-align: right;\">0.100731 </td><td>172.26.215.93</td><td style=\"text-align: right;\">597865</td><td style=\"text-align: right;\">0.708778</td><td style=\"text-align: right;\">            0.932467</td><td style=\"text-align: right;\">          0.932467</td><td style=\"text-align: right;\">      0.932467</td><td style=\"text-align: right;\"> 1689738158</td><td style=\"text-align: right;\">                   1</td><td>6fabf16c  </td></tr>\n",
       "<tr><td>FSR_Trainable_7223ea8f</td><td>2023-07-19_12-39-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.361229</td><td style=\"text-align: right;\">0.0943213</td><td>172.26.215.93</td><td style=\"text-align: right;\">593554</td><td style=\"text-align: right;\">0.692058</td><td style=\"text-align: right;\">           63.6396  </td><td style=\"text-align: right;\">          0.567587</td><td style=\"text-align: right;\">     63.6396  </td><td style=\"text-align: right;\"> 1689737997</td><td style=\"text-align: right;\">                 100</td><td>7223ea8f  </td></tr>\n",
       "<tr><td>FSR_Trainable_73b9107b</td><td>2023-07-19_12-33-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.367365</td><td style=\"text-align: right;\">0.108243 </td><td>172.26.215.93</td><td style=\"text-align: right;\">587434</td><td style=\"text-align: right;\">0.766272</td><td style=\"text-align: right;\">            2.60202 </td><td style=\"text-align: right;\">          1.13893 </td><td style=\"text-align: right;\">      2.60202 </td><td style=\"text-align: right;\"> 1689737626</td><td style=\"text-align: right;\">                   2</td><td>73b9107b  </td></tr>\n",
       "<tr><td>FSR_Trainable_74dbef32</td><td>2023-07-19_12-34-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.369542</td><td style=\"text-align: right;\">0.0939247</td><td>172.26.215.93</td><td style=\"text-align: right;\">588233</td><td style=\"text-align: right;\">0.697817</td><td style=\"text-align: right;\">           29.4636  </td><td style=\"text-align: right;\">          0.253388</td><td style=\"text-align: right;\">     29.4636  </td><td style=\"text-align: right;\"> 1689737696</td><td style=\"text-align: right;\">                 100</td><td>74dbef32  </td></tr>\n",
       "<tr><td>FSR_Trainable_7a508a51</td><td>2023-07-19_12-47-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.322995</td><td style=\"text-align: right;\">0.0945223</td><td>172.26.215.93</td><td style=\"text-align: right;\">603346</td><td style=\"text-align: right;\">0.692435</td><td style=\"text-align: right;\">            1.5096  </td><td style=\"text-align: right;\">          0.69567 </td><td style=\"text-align: right;\">      1.5096  </td><td style=\"text-align: right;\"> 1689738451</td><td style=\"text-align: right;\">                   2</td><td>7a508a51  </td></tr>\n",
       "<tr><td>FSR_Trainable_7ad4562c</td><td>2023-07-19_12-42-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.36389 </td><td style=\"text-align: right;\">0.0993147</td><td>172.26.215.93</td><td style=\"text-align: right;\">598050</td><td style=\"text-align: right;\">0.706983</td><td style=\"text-align: right;\">            0.789543</td><td style=\"text-align: right;\">          0.789543</td><td style=\"text-align: right;\">      0.789543</td><td style=\"text-align: right;\"> 1689738165</td><td style=\"text-align: right;\">                   1</td><td>7ad4562c  </td></tr>\n",
       "<tr><td>FSR_Trainable_7be5f46b</td><td>2023-07-19_12-42-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.98808 </td><td style=\"text-align: right;\">0.302478 </td><td>172.26.215.93</td><td style=\"text-align: right;\">597541</td><td style=\"text-align: right;\">2.23853 </td><td style=\"text-align: right;\">            1.23156 </td><td style=\"text-align: right;\">          1.23156 </td><td style=\"text-align: right;\">      1.23156 </td><td style=\"text-align: right;\"> 1689738144</td><td style=\"text-align: right;\">                   1</td><td>7be5f46b  </td></tr>\n",
       "<tr><td>FSR_Trainable_7d2620f2</td><td>2023-07-19_12-44-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.46885 </td><td style=\"text-align: right;\">0.399757 </td><td>172.26.215.93</td><td style=\"text-align: right;\">600307</td><td style=\"text-align: right;\">2.64992 </td><td style=\"text-align: right;\">            0.518257</td><td style=\"text-align: right;\">          0.518257</td><td style=\"text-align: right;\">      0.518257</td><td style=\"text-align: right;\"> 1689738260</td><td style=\"text-align: right;\">                   1</td><td>7d2620f2  </td></tr>\n",
       "<tr><td>FSR_Trainable_82c4071c</td><td>2023-07-19_12-50-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.365454</td><td style=\"text-align: right;\">0.105108 </td><td>172.26.215.93</td><td style=\"text-align: right;\">606547</td><td style=\"text-align: right;\">0.69849 </td><td style=\"text-align: right;\">            0.861999</td><td style=\"text-align: right;\">          0.861999</td><td style=\"text-align: right;\">      0.861999</td><td style=\"text-align: right;\"> 1689738612</td><td style=\"text-align: right;\">                   1</td><td>82c4071c  </td></tr>\n",
       "<tr><td>FSR_Trainable_8b3cb18b</td><td>2023-07-19_12-44-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.28835 </td><td style=\"text-align: right;\">0.37588  </td><td>172.26.215.93</td><td style=\"text-align: right;\">600069</td><td style=\"text-align: right;\">2.59378 </td><td style=\"text-align: right;\">            0.5456  </td><td style=\"text-align: right;\">          0.5456  </td><td style=\"text-align: right;\">      0.5456  </td><td style=\"text-align: right;\"> 1689738250</td><td style=\"text-align: right;\">                   1</td><td>8b3cb18b  </td></tr>\n",
       "<tr><td>FSR_Trainable_8be6c175</td><td>2023-07-19_12-45-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.364049</td><td style=\"text-align: right;\">0.0908353</td><td>172.26.215.93</td><td style=\"text-align: right;\">600531</td><td style=\"text-align: right;\">0.692058</td><td style=\"text-align: right;\">           39.4842  </td><td style=\"text-align: right;\">          0.685355</td><td style=\"text-align: right;\">     39.4842  </td><td style=\"text-align: right;\"> 1689738320</td><td style=\"text-align: right;\">                  64</td><td>8be6c175  </td></tr>\n",
       "<tr><td>FSR_Trainable_8c9518c7</td><td>2023-07-19_12-40-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.332044</td><td style=\"text-align: right;\">0.0950253</td><td>172.26.215.93</td><td style=\"text-align: right;\">594359</td><td style=\"text-align: right;\">0.690417</td><td style=\"text-align: right;\">            3.15526 </td><td style=\"text-align: right;\">          0.68465 </td><td style=\"text-align: right;\">      3.15526 </td><td style=\"text-align: right;\"> 1689738004</td><td style=\"text-align: right;\">                   4</td><td>8c9518c7  </td></tr>\n",
       "<tr><td>FSR_Trainable_9200f54b</td><td>2023-07-19_12-38-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.364037</td><td style=\"text-align: right;\">0.093427 </td><td>172.26.215.93</td><td style=\"text-align: right;\">592379</td><td style=\"text-align: right;\">0.697574</td><td style=\"text-align: right;\">           38.0979  </td><td style=\"text-align: right;\">          0.374921</td><td style=\"text-align: right;\">     38.0979  </td><td style=\"text-align: right;\"> 1689737921</td><td style=\"text-align: right;\">                 100</td><td>9200f54b  </td></tr>\n",
       "<tr><td>FSR_Trainable_926676f3</td><td>2023-07-19_12-41-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.351266</td><td style=\"text-align: right;\">0.0972942</td><td>172.26.215.93</td><td style=\"text-align: right;\">596610</td><td style=\"text-align: right;\">0.696644</td><td style=\"text-align: right;\">            1.04537 </td><td style=\"text-align: right;\">          0.421083</td><td style=\"text-align: right;\">      1.04537 </td><td style=\"text-align: right;\"> 1689738105</td><td style=\"text-align: right;\">                   2</td><td>926676f3  </td></tr>\n",
       "<tr><td>FSR_Trainable_93c5e213</td><td>2023-07-19_12-36-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.353356</td><td style=\"text-align: right;\">0.0974838</td><td>172.26.215.93</td><td style=\"text-align: right;\">589712</td><td style=\"text-align: right;\">0.681293</td><td style=\"text-align: right;\">           43.7829  </td><td style=\"text-align: right;\">          0.426733</td><td style=\"text-align: right;\">     43.7829  </td><td style=\"text-align: right;\"> 1689737777</td><td style=\"text-align: right;\">                 100</td><td>93c5e213  </td></tr>\n",
       "<tr><td>FSR_Trainable_93ffe5a7</td><td>2023-07-19_12-40-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.369852</td><td style=\"text-align: right;\">0.107496 </td><td>172.26.215.93</td><td style=\"text-align: right;\">594562</td><td style=\"text-align: right;\">0.698185</td><td style=\"text-align: right;\">            0.753346</td><td style=\"text-align: right;\">          0.28741 </td><td style=\"text-align: right;\">      0.753346</td><td style=\"text-align: right;\"> 1689738014</td><td style=\"text-align: right;\">                   2</td><td>93ffe5a7  </td></tr>\n",
       "<tr><td>FSR_Trainable_94477dcb</td><td>2023-07-19_12-33-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.556933</td><td style=\"text-align: right;\">0.116071 </td><td>172.26.215.93</td><td style=\"text-align: right;\">587666</td><td style=\"text-align: right;\">0.913409</td><td style=\"text-align: right;\">            0.687717</td><td style=\"text-align: right;\">          0.687717</td><td style=\"text-align: right;\">      0.687717</td><td style=\"text-align: right;\"> 1689737633</td><td style=\"text-align: right;\">                   1</td><td>94477dcb  </td></tr>\n",
       "<tr><td>FSR_Trainable_95726d4d</td><td>2023-07-19_12-41-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.364148</td><td style=\"text-align: right;\">0.0963233</td><td>172.26.215.93</td><td style=\"text-align: right;\">596830</td><td style=\"text-align: right;\">0.712543</td><td style=\"text-align: right;\">            0.544977</td><td style=\"text-align: right;\">          0.544977</td><td style=\"text-align: right;\">      0.544977</td><td style=\"text-align: right;\"> 1689738114</td><td style=\"text-align: right;\">                   1</td><td>95726d4d  </td></tr>\n",
       "<tr><td>FSR_Trainable_95b55524</td><td>2023-07-19_12-46-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.36098 </td><td style=\"text-align: right;\">0.0927179</td><td>172.26.215.93</td><td style=\"text-align: right;\">601500</td><td style=\"text-align: right;\">0.687858</td><td style=\"text-align: right;\">           62.7993  </td><td style=\"text-align: right;\">          0.381266</td><td style=\"text-align: right;\">     62.7993  </td><td style=\"text-align: right;\"> 1689738407</td><td style=\"text-align: right;\">                 100</td><td>95b55524  </td></tr>\n",
       "<tr><td>FSR_Trainable_975ec4d8</td><td>2023-07-19_12-34-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.384481</td><td style=\"text-align: right;\">0.0945442</td><td>172.26.215.93</td><td style=\"text-align: right;\">587181</td><td style=\"text-align: right;\">0.71998 </td><td style=\"text-align: right;\">           22.423   </td><td style=\"text-align: right;\">          0.168195</td><td style=\"text-align: right;\">     22.423   </td><td style=\"text-align: right;\"> 1689737641</td><td style=\"text-align: right;\">                 100</td><td>975ec4d8  </td></tr>\n",
       "<tr><td>FSR_Trainable_99a5a408</td><td>2023-07-19_12-46-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.383632</td><td style=\"text-align: right;\">0.101932 </td><td>172.26.215.93</td><td style=\"text-align: right;\">602479</td><td style=\"text-align: right;\">0.70417 </td><td style=\"text-align: right;\">            0.542787</td><td style=\"text-align: right;\">          0.542787</td><td style=\"text-align: right;\">      0.542787</td><td style=\"text-align: right;\"> 1689738408</td><td style=\"text-align: right;\">                   1</td><td>99a5a408  </td></tr>\n",
       "<tr><td>FSR_Trainable_99d79818</td><td>2023-07-19_12-43-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.370529</td><td style=\"text-align: right;\">0.106992 </td><td>172.26.215.93</td><td style=\"text-align: right;\">599368</td><td style=\"text-align: right;\">0.690215</td><td style=\"text-align: right;\">            2.54149 </td><td style=\"text-align: right;\">          0.595472</td><td style=\"text-align: right;\">      2.54149 </td><td style=\"text-align: right;\"> 1689738221</td><td style=\"text-align: right;\">                   4</td><td>99d79818  </td></tr>\n",
       "<tr><td>FSR_Trainable_99fe5057</td><td>2023-07-19_12-49-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.365063</td><td style=\"text-align: right;\">0.105731 </td><td>172.26.215.93</td><td style=\"text-align: right;\">605472</td><td style=\"text-align: right;\">0.69623 </td><td style=\"text-align: right;\">            1.22919 </td><td style=\"text-align: right;\">          0.408365</td><td style=\"text-align: right;\">      1.22919 </td><td style=\"text-align: right;\"> 1689738572</td><td style=\"text-align: right;\">                   2</td><td>99fe5057  </td></tr>\n",
       "<tr><td>FSR_Trainable_9d1f181e</td><td>2023-07-19_12-38-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.37488 </td><td style=\"text-align: right;\">0.104922 </td><td>172.26.215.93</td><td style=\"text-align: right;\">592624</td><td style=\"text-align: right;\">0.709229</td><td style=\"text-align: right;\">            1.00508 </td><td style=\"text-align: right;\">          0.436404</td><td style=\"text-align: right;\">      1.00508 </td><td style=\"text-align: right;\"> 1689737885</td><td style=\"text-align: right;\">                   2</td><td>9d1f181e  </td></tr>\n",
       "<tr><td>FSR_Trainable_9f640d1b</td><td>2023-07-19_12-41-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.364206</td><td style=\"text-align: right;\">0.0956895</td><td>172.26.215.93</td><td style=\"text-align: right;\">596151</td><td style=\"text-align: right;\">0.693141</td><td style=\"text-align: right;\">           25.9221  </td><td style=\"text-align: right;\">          0.821105</td><td style=\"text-align: right;\">     25.9221  </td><td style=\"text-align: right;\"> 1689738113</td><td style=\"text-align: right;\">                  32</td><td>9f640d1b  </td></tr>\n",
       "<tr><td>FSR_Trainable_a3be7ec3</td><td>2023-07-19_12-52-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.35915 </td><td style=\"text-align: right;\">0.0936888</td><td>172.26.215.93</td><td style=\"text-align: right;\">607842</td><td style=\"text-align: right;\">0.691486</td><td style=\"text-align: right;\">           46.038   </td><td style=\"text-align: right;\">          0.6215  </td><td style=\"text-align: right;\">     46.038   </td><td style=\"text-align: right;\"> 1689738752</td><td style=\"text-align: right;\">                  64</td><td>a3be7ec3  </td></tr>\n",
       "<tr><td>FSR_Trainable_ac021b55</td><td>2023-07-19_12-38-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.349101</td><td style=\"text-align: right;\">0.096672 </td><td>172.26.215.93</td><td style=\"text-align: right;\">591896</td><td style=\"text-align: right;\">0.677568</td><td style=\"text-align: right;\">           37.6297  </td><td style=\"text-align: right;\">          0.428063</td><td style=\"text-align: right;\">     37.6297  </td><td style=\"text-align: right;\"> 1689737896</td><td style=\"text-align: right;\">                 100</td><td>ac021b55  </td></tr>\n",
       "<tr><td>FSR_Trainable_ad9d1ca1</td><td>2023-07-19_12-45-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.362244</td><td style=\"text-align: right;\">0.0925566</td><td>172.26.215.93</td><td style=\"text-align: right;\">600758</td><td style=\"text-align: right;\">0.68989 </td><td style=\"text-align: right;\">           66.2802  </td><td style=\"text-align: right;\">          0.674616</td><td style=\"text-align: right;\">     66.2802  </td><td style=\"text-align: right;\"> 1689738358</td><td style=\"text-align: right;\">                 100</td><td>ad9d1ca1  </td></tr>\n",
       "<tr><td>FSR_Trainable_add6f9fc</td><td>2023-07-19_12-46-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.360398</td><td style=\"text-align: right;\">0.0920104</td><td>172.26.215.93</td><td style=\"text-align: right;\">601213</td><td style=\"text-align: right;\">0.690882</td><td style=\"text-align: right;\">           66.2366  </td><td style=\"text-align: right;\">          0.703891</td><td style=\"text-align: right;\">     66.2366  </td><td style=\"text-align: right;\"> 1689738381</td><td style=\"text-align: right;\">                 100</td><td>add6f9fc  </td></tr>\n",
       "<tr><td>FSR_Trainable_b963783f</td><td>2023-07-19_12-50-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.376492</td><td style=\"text-align: right;\">0.107889 </td><td>172.26.215.93</td><td style=\"text-align: right;\">606222</td><td style=\"text-align: right;\">0.70832 </td><td style=\"text-align: right;\">            0.863862</td><td style=\"text-align: right;\">          0.863862</td><td style=\"text-align: right;\">      0.863862</td><td style=\"text-align: right;\"> 1689738602</td><td style=\"text-align: right;\">                   1</td><td>b963783f  </td></tr>\n",
       "<tr><td>FSR_Trainable_ba0d5db0</td><td>2023-07-19_12-34-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.64216 </td><td style=\"text-align: right;\">0.450981 </td><td>172.26.215.93</td><td style=\"text-align: right;\">587900</td><td style=\"text-align: right;\">2.99788 </td><td style=\"text-align: right;\">            0.547637</td><td style=\"text-align: right;\">          0.547637</td><td style=\"text-align: right;\">      0.547637</td><td style=\"text-align: right;\"> 1689737642</td><td style=\"text-align: right;\">                   1</td><td>ba0d5db0  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb057b2a</td><td>2023-07-19_12-47-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.323187</td><td style=\"text-align: right;\">0.0936087</td><td>172.26.215.93</td><td style=\"text-align: right;\">603121</td><td style=\"text-align: right;\">0.686332</td><td style=\"text-align: right;\">            2.61701 </td><td style=\"text-align: right;\">          0.647753</td><td style=\"text-align: right;\">      2.61701 </td><td style=\"text-align: right;\"> 1689738440</td><td style=\"text-align: right;\">                   4</td><td>bb057b2a  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb1c2a03</td><td>2023-07-19_12-48-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.359678</td><td style=\"text-align: right;\">0.0912273</td><td>172.26.215.93</td><td style=\"text-align: right;\">602711</td><td style=\"text-align: right;\">0.688079</td><td style=\"text-align: right;\">           59.923   </td><td style=\"text-align: right;\">          0.63945 </td><td style=\"text-align: right;\">     59.923   </td><td style=\"text-align: right;\"> 1689738491</td><td style=\"text-align: right;\">                 100</td><td>bb1c2a03  </td></tr>\n",
       "<tr><td>FSR_Trainable_c0cac9a2</td><td>2023-07-19_12-46-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.363592</td><td style=\"text-align: right;\">0.0920025</td><td>172.26.215.93</td><td style=\"text-align: right;\">600986</td><td style=\"text-align: right;\">0.692614</td><td style=\"text-align: right;\">           66.0267  </td><td style=\"text-align: right;\">          0.663627</td><td style=\"text-align: right;\">     66.0267  </td><td style=\"text-align: right;\"> 1689738368</td><td style=\"text-align: right;\">                 100</td><td>c0cac9a2  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1020d1d</td><td>2023-07-19_12-39-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.332185</td><td style=\"text-align: right;\">0.0954011</td><td>172.26.215.93</td><td style=\"text-align: right;\">593089</td><td style=\"text-align: right;\">0.675797</td><td style=\"text-align: right;\">           40.5935  </td><td style=\"text-align: right;\">          0.411647</td><td style=\"text-align: right;\">     40.5935  </td><td style=\"text-align: right;\"> 1689737954</td><td style=\"text-align: right;\">                 100</td><td>c1020d1d  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1baadff</td><td>2023-07-19_12-39-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.362186</td><td style=\"text-align: right;\">0.0913388</td><td>172.26.215.93</td><td style=\"text-align: right;\">593330</td><td style=\"text-align: right;\">0.689377</td><td style=\"text-align: right;\">           62.3849  </td><td style=\"text-align: right;\">          0.592204</td><td style=\"text-align: right;\">     62.3849  </td><td style=\"text-align: right;\"> 1689737986</td><td style=\"text-align: right;\">                 100</td><td>c1baadff  </td></tr>\n",
       "<tr><td>FSR_Trainable_c4f4b7da</td><td>2023-07-19_12-40-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.372315</td><td style=\"text-align: right;\">0.102087 </td><td>172.26.215.93</td><td style=\"text-align: right;\">595046</td><td style=\"text-align: right;\">0.723395</td><td style=\"text-align: right;\">            0.428261</td><td style=\"text-align: right;\">          0.428261</td><td style=\"text-align: right;\">      0.428261</td><td style=\"text-align: right;\"> 1689738031</td><td style=\"text-align: right;\">                   1</td><td>c4f4b7da  </td></tr>\n",
       "<tr><td>FSR_Trainable_c9a9fa72</td><td>2023-07-19_12-40-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">0.367267</td><td style=\"text-align: right;\">0.0933371</td><td>172.26.215.93</td><td style=\"text-align: right;\">595279</td><td style=\"text-align: right;\">0.695517</td><td style=\"text-align: right;\">            6.55527 </td><td style=\"text-align: right;\">          0.388665</td><td style=\"text-align: right;\">      6.55527 </td><td style=\"text-align: right;\"> 1689738049</td><td style=\"text-align: right;\">                  16</td><td>c9a9fa72  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc674ded</td><td>2023-07-19_12-33-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.72287 </td><td style=\"text-align: right;\">0.276691 </td><td>172.26.215.93</td><td style=\"text-align: right;\">586426</td><td style=\"text-align: right;\">2.05699 </td><td style=\"text-align: right;\">            0.449091</td><td style=\"text-align: right;\">          0.449091</td><td style=\"text-align: right;\">      0.449091</td><td style=\"text-align: right;\"> 1689737580</td><td style=\"text-align: right;\">                   1</td><td>cc674ded  </td></tr>\n",
       "<tr><td>FSR_Trainable_cffe4d2f</td><td>2023-07-19_12-37-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.366264</td><td style=\"text-align: right;\">0.0924884</td><td>172.26.215.93</td><td style=\"text-align: right;\">591214</td><td style=\"text-align: right;\">0.69357 </td><td style=\"text-align: right;\">           38.1707  </td><td style=\"text-align: right;\">          0.504879</td><td style=\"text-align: right;\">     38.1707  </td><td style=\"text-align: right;\"> 1689737862</td><td style=\"text-align: right;\">                 100</td><td>cffe4d2f  </td></tr>\n",
       "<tr><td>FSR_Trainable_d545cc45</td><td>2023-07-19_12-47-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.367547</td><td style=\"text-align: right;\">0.0940652</td><td>172.26.215.93</td><td style=\"text-align: right;\">602806</td><td style=\"text-align: right;\">0.694589</td><td style=\"text-align: right;\">           32.3323  </td><td style=\"text-align: right;\">          0.501953</td><td style=\"text-align: right;\">     32.3323  </td><td style=\"text-align: right;\"> 1689738466</td><td style=\"text-align: right;\">                  64</td><td>d545cc45  </td></tr>\n",
       "<tr><td>FSR_Trainable_d5cd8ecb</td><td>2023-07-19_12-33-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.579424</td><td style=\"text-align: right;\">0.135847 </td><td>172.26.215.93</td><td style=\"text-align: right;\">586929</td><td style=\"text-align: right;\">0.858841</td><td style=\"text-align: right;\">            0.947242</td><td style=\"text-align: right;\">          0.32508 </td><td style=\"text-align: right;\">      0.947242</td><td style=\"text-align: right;\"> 1689737604</td><td style=\"text-align: right;\">                   2</td><td>d5cd8ecb  </td></tr>\n",
       "<tr><td>FSR_Trainable_d7846332</td><td>2023-07-19_12-46-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.362349</td><td style=\"text-align: right;\">0.0968753</td><td>172.26.215.93</td><td style=\"text-align: right;\">601987</td><td style=\"text-align: right;\">0.707034</td><td style=\"text-align: right;\">            0.672598</td><td style=\"text-align: right;\">          0.672598</td><td style=\"text-align: right;\">      0.672598</td><td style=\"text-align: right;\"> 1689738384</td><td style=\"text-align: right;\">                   1</td><td>d7846332  </td></tr>\n",
       "<tr><td>FSR_Trainable_d912899d</td><td>2023-07-19_12-46-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">0.358589</td><td style=\"text-align: right;\">0.0933797</td><td>172.26.215.93</td><td style=\"text-align: right;\">601781</td><td style=\"text-align: right;\">0.693966</td><td style=\"text-align: right;\">           21.0144  </td><td style=\"text-align: right;\">          0.519948</td><td style=\"text-align: right;\">     21.0144  </td><td style=\"text-align: right;\"> 1689738396</td><td style=\"text-align: right;\">                  32</td><td>d912899d  </td></tr>\n",
       "<tr><td>FSR_Trainable_d9b9342e</td><td>2023-07-19_12-34-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.450817</td><td style=\"text-align: right;\">0.108167 </td><td>172.26.215.93</td><td style=\"text-align: right;\">588552</td><td style=\"text-align: right;\">0.771801</td><td style=\"text-align: right;\">            0.720611</td><td style=\"text-align: right;\">          0.720611</td><td style=\"text-align: right;\">      0.720611</td><td style=\"text-align: right;\"> 1689737668</td><td style=\"text-align: right;\">                   1</td><td>d9b9342e  </td></tr>\n",
       "<tr><td>FSR_Trainable_dbeb54e6</td><td>2023-07-19_12-44-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">0.353965</td><td style=\"text-align: right;\">0.0966764</td><td>172.26.215.93</td><td style=\"text-align: right;\">599831</td><td style=\"text-align: right;\">0.688164</td><td style=\"text-align: right;\">            3.28711 </td><td style=\"text-align: right;\">          0.3531  </td><td style=\"text-align: right;\">      3.28711 </td><td style=\"text-align: right;\"> 1689738244</td><td style=\"text-align: right;\">                   8</td><td>dbeb54e6  </td></tr>\n",
       "<tr><td>FSR_Trainable_dce86eaa</td><td>2023-07-19_12-36-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.368148</td><td style=\"text-align: right;\">0.0936629</td><td>172.26.215.93</td><td style=\"text-align: right;\">590184</td><td style=\"text-align: right;\">0.695836</td><td style=\"text-align: right;\">           50.1152  </td><td style=\"text-align: right;\">          0.491298</td><td style=\"text-align: right;\">     50.1152  </td><td style=\"text-align: right;\"> 1689737804</td><td style=\"text-align: right;\">                 100</td><td>dce86eaa  </td></tr>\n",
       "<tr><td>FSR_Trainable_dea91bda</td><td>2023-07-19_12-41-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.353322</td><td style=\"text-align: right;\">0.10491  </td><td>172.26.215.93</td><td style=\"text-align: right;\">595744</td><td style=\"text-align: right;\">0.695942</td><td style=\"text-align: right;\">            0.923195</td><td style=\"text-align: right;\">          0.441982</td><td style=\"text-align: right;\">      0.923195</td><td style=\"text-align: right;\"> 1689738065</td><td style=\"text-align: right;\">                   2</td><td>dea91bda  </td></tr>\n",
       "<tr><td>FSR_Trainable_e4fcf99e</td><td>2023-07-19_12-40-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.361628</td><td style=\"text-align: right;\">0.0942743</td><td>172.26.215.93</td><td style=\"text-align: right;\">593792</td><td style=\"text-align: right;\">0.687492</td><td style=\"text-align: right;\">           63.1489  </td><td style=\"text-align: right;\">          0.484387</td><td style=\"text-align: right;\">     63.1489  </td><td style=\"text-align: right;\"> 1689738010</td><td style=\"text-align: right;\">                 100</td><td>e4fcf99e  </td></tr>\n",
       "<tr><td>FSR_Trainable_e547a93a</td><td>2023-07-19_12-42-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">2.36352 </td><td style=\"text-align: right;\">0.40444  </td><td>172.26.215.93</td><td style=\"text-align: right;\">597307</td><td style=\"text-align: right;\">2.82725 </td><td style=\"text-align: right;\">            0.300096</td><td style=\"text-align: right;\">          0.300096</td><td style=\"text-align: right;\">      0.300096</td><td style=\"text-align: right;\"> 1689738134</td><td style=\"text-align: right;\">                   1</td><td>e547a93a  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9ff9cd3</td><td>2023-07-19_12-52-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.366793</td><td style=\"text-align: right;\">0.097432 </td><td>172.26.215.93</td><td style=\"text-align: right;\">608549</td><td style=\"text-align: right;\">0.700165</td><td style=\"text-align: right;\">            0.835093</td><td style=\"text-align: right;\">          0.835093</td><td style=\"text-align: right;\">      0.835093</td><td style=\"text-align: right;\"> 1689738738</td><td style=\"text-align: right;\">                   1</td><td>e9ff9cd3  </td></tr>\n",
       "<tr><td>FSR_Trainable_eec8f5aa</td><td>2023-07-19_12-37-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.366887</td><td style=\"text-align: right;\">0.0933402</td><td>172.26.215.93</td><td style=\"text-align: right;\">590944</td><td style=\"text-align: right;\">0.695118</td><td style=\"text-align: right;\">           35.0984  </td><td style=\"text-align: right;\">          0.43312 </td><td style=\"text-align: right;\">     35.0984  </td><td style=\"text-align: right;\"> 1689737832</td><td style=\"text-align: right;\">                 100</td><td>eec8f5aa  </td></tr>\n",
       "<tr><td>FSR_Trainable_efef0f93</td><td>2023-07-19_12-51-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">0.321279</td><td style=\"text-align: right;\">0.0925323</td><td>172.26.215.93</td><td style=\"text-align: right;\">607591</td><td style=\"text-align: right;\">0.68811 </td><td style=\"text-align: right;\">            4.21659 </td><td style=\"text-align: right;\">          1.05672 </td><td style=\"text-align: right;\">      4.21659 </td><td style=\"text-align: right;\"> 1689738680</td><td style=\"text-align: right;\">                   4</td><td>efef0f93  </td></tr>\n",
       "<tr><td>FSR_Trainable_f084baac</td><td>2023-07-19_12-48-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">0.363642</td><td style=\"text-align: right;\">0.0934157</td><td>172.26.215.93</td><td style=\"text-align: right;\">603580</td><td style=\"text-align: right;\">0.692194</td><td style=\"text-align: right;\">           42.4947  </td><td style=\"text-align: right;\">          0.611026</td><td style=\"text-align: right;\">     42.4947  </td><td style=\"text-align: right;\"> 1689738514</td><td style=\"text-align: right;\">                  64</td><td>f084baac  </td></tr>\n",
       "<tr><td>FSR_Trainable_f0e8e935</td><td>2023-07-19_12-37-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.355519</td><td style=\"text-align: right;\">0.0973735</td><td>172.26.215.93</td><td style=\"text-align: right;\">591422</td><td style=\"text-align: right;\">0.68892 </td><td style=\"text-align: right;\">           36.196   </td><td style=\"text-align: right;\">          0.475991</td><td style=\"text-align: right;\">     36.196   </td><td style=\"text-align: right;\"> 1689737869</td><td style=\"text-align: right;\">                 100</td><td>f0e8e935  </td></tr>\n",
       "<tr><td>FSR_Trainable_f1c23855</td><td>2023-07-19_12-40-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.363373</td><td style=\"text-align: right;\">0.0958467</td><td>172.26.215.93</td><td style=\"text-align: right;\">594085</td><td style=\"text-align: right;\">0.687581</td><td style=\"text-align: right;\">           57.3284  </td><td style=\"text-align: right;\">          0.501914</td><td style=\"text-align: right;\">     57.3284  </td><td style=\"text-align: right;\"> 1689738036</td><td style=\"text-align: right;\">                 100</td><td>f1c23855  </td></tr>\n",
       "<tr><td>FSR_Trainable_f7a9c5df</td><td>2023-07-19_12-36-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.347616</td><td style=\"text-align: right;\">0.0980838</td><td>172.26.215.93</td><td style=\"text-align: right;\">590414</td><td style=\"text-align: right;\">0.686824</td><td style=\"text-align: right;\">           36.9426  </td><td style=\"text-align: right;\">          0.330101</td><td style=\"text-align: right;\">     36.9426  </td><td style=\"text-align: right;\"> 1689737800</td><td style=\"text-align: right;\">                 100</td><td>f7a9c5df  </td></tr>\n",
       "<tr><td>FSR_Trainable_f89b1452</td><td>2023-07-19_12-42-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.378499</td><td style=\"text-align: right;\">0.109805 </td><td>172.26.215.93</td><td style=\"text-align: right;\">597776</td><td style=\"text-align: right;\">0.709189</td><td style=\"text-align: right;\">            0.74753 </td><td style=\"text-align: right;\">          0.74753 </td><td style=\"text-align: right;\">      0.74753 </td><td style=\"text-align: right;\"> 1689738152</td><td style=\"text-align: right;\">                   1</td><td>f89b1452  </td></tr>\n",
       "<tr><td>FSR_Trainable_fb69d467</td><td>2023-07-19_12-35-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">0.377818</td><td style=\"text-align: right;\">0.0937043</td><td>172.26.215.93</td><td style=\"text-align: right;\">589025</td><td style=\"text-align: right;\">0.713594</td><td style=\"text-align: right;\">           49.5629  </td><td style=\"text-align: right;\">          0.581207</td><td style=\"text-align: right;\">     49.5629  </td><td style=\"text-align: right;\"> 1689737751</td><td style=\"text-align: right;\">                 100</td><td>fb69d467  </td></tr>\n",
       "<tr><td>FSR_Trainable_fffdff38</td><td>2023-07-19_12-43-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">0.376545</td><td style=\"text-align: right;\">0.0988597</td><td>172.26.215.93</td><td style=\"text-align: right;\">599592</td><td style=\"text-align: right;\">0.705822</td><td style=\"text-align: right;\">            0.779195</td><td style=\"text-align: right;\">          0.779195</td><td style=\"text-align: right;\">      0.779195</td><td style=\"text-align: right;\"> 1689738229</td><td style=\"text-align: right;\">                   1</td><td>fffdff38  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_4c065307_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-32-38/wandb/run-20230719_123249-4c065307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Syncing run FSR_Trainable_4c065307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4c065307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_3f89db78_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-32-43/wandb/run-20230719_123257-3f89db78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Syncing run FSR_Trainable_3f89db78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3f89db78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_cc674ded_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-32-51/wandb/run-20230719_123305-cc674ded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Syncing run FSR_Trainable_cc674ded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc674ded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                      mae 1.72287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                     mape 0.27669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                     rmse 2.05699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:       time_since_restore 0.44909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:         time_this_iter_s 0.44909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:             time_total_s 0.44909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:                timestamp 1689737580\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb:  View run FSR_Trainable_cc674ded at: https://wandb.ai/seokjin/FSR-prediction/runs/cc674ded\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586606)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123305-cc674ded/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_3daf2b4f_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-32-59/wandb/run-20230719_123314-3daf2b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Syncing run FSR_Trainable_3daf2b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3daf2b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                      mae 0.41046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                     mape 0.09105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                     rmse 0.74856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:       time_since_restore 8.83764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:         time_this_iter_s 0.30303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:             time_total_s 8.83764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:                timestamp 1689737599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb:  View run FSR_Trainable_3daf2b4f at: https://wandb.ai/seokjin/FSR-prediction/runs/3daf2b4f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586795)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123314-3daf2b4f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_d5cd8ecb_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-33-08/wandb/run-20230719_123327-d5cd8ecb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Syncing run FSR_Trainable_d5cd8ecb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d5cd8ecb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                      mae 0.43179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                     mape 0.11056\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                     rmse 0.73891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:       time_since_restore 24.66096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:         time_this_iter_s 0.19446\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:             time_total_s 24.66096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:                timestamp 1689737607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb:  View run FSR_Trainable_3f89db78 at: https://wandb.ai/seokjin/FSR-prediction/runs/3f89db78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586425)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123257-3f89db78/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123327-d5cd8ecb/logs\n",
      "2023-07-19 12:33:34,953\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.393 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:33:34,958\tWARNING util.py:315 -- The `process_trial_result` operation took 2.399 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:33:34,960\tWARNING util.py:315 -- Processing trial results took 2.401 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:33:34,962\tWARNING util.py:315 -- The `process_trial_result` operation took 2.402 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587047)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_975ec4d8_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-33-21/wandb/run-20230719_123337-975ec4d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: Syncing run FSR_Trainable_975ec4d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/975ec4d8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:33:45,651\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.516 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:33:45,655\tWARNING util.py:315 -- The `process_trial_result` operation took 1.520 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:33:45,658\tWARNING util.py:315 -- Processing trial results took 1.523 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:33:45,659\tWARNING util.py:315 -- The `process_trial_result` operation took 1.524 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                      mae 0.36895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                     mape 0.09525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                     rmse 0.70945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:       time_since_restore 43.287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:         time_this_iter_s 0.38704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:             time_total_s 43.287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:                timestamp 1689737621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb:  View run FSR_Trainable_4c065307 at: https://wandb.ai/seokjin/FSR-prediction/runs/4c065307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=586246)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123249-4c065307/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_73b9107b_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-33-32/wandb/run-20230719_123348-73b9107b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Syncing run FSR_Trainable_73b9107b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/73b9107b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                      mae 0.36736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                     mape 0.10824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                     rmse 0.76627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:       time_since_restore 2.60202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:         time_this_iter_s 1.13893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:             time_total_s 2.60202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:                timestamp 1689737626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb:  View run FSR_Trainable_73b9107b at: https://wandb.ai/seokjin/FSR-prediction/runs/73b9107b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587530)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123348-73b9107b/logs\n",
      "2023-07-19 12:33:55,323\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.082 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:33:55,327\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:33:55,331\tWARNING util.py:315 -- Processing trial results took 2.091 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:33:55,332\tWARNING util.py:315 -- The `process_trial_result` operation took 2.093 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_94477dcb_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-33-42/wandb/run-20230719_123357-94477dcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Syncing run FSR_Trainable_94477dcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/94477dcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                      mae 0.55693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                     mape 0.11607\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                     rmse 0.91341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:       time_since_restore 0.68772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:         time_this_iter_s 0.68772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:             time_total_s 0.68772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:                timestamp 1689737633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb:  View run FSR_Trainable_94477dcb at: https://wandb.ai/seokjin/FSR-prediction/runs/94477dcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587766)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123357-94477dcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:34:04,618\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.123 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:04,623\tWARNING util.py:315 -- The `process_trial_result` operation took 2.128 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:04,624\tWARNING util.py:315 -- Processing trial results took 2.129 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:34:04,626\tWARNING util.py:315 -- The `process_trial_result` operation took 2.131 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123337-975ec4d8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_ba0d5db0_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-07-19_12-33-52/wandb/run-20230719_123406-ba0d5db0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Syncing run FSR_Trainable_ba0d5db0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ba0d5db0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=587296)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                      mae 2.64216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                     mape 0.45098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                     rmse 2.99788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:       time_since_restore 0.54764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:         time_this_iter_s 0.54764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:             time_total_s 0.54764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:                timestamp 1689737642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb:  View run FSR_Trainable_ba0d5db0 at: https://wandb.ai/seokjin/FSR-prediction/runs/ba0d5db0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123406-ba0d5db0/logs\n",
      "2023-07-19 12:34:12,685\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.306 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:12,690\tWARNING util.py:315 -- The `process_trial_result` operation took 2.312 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:12,692\tWARNING util.py:315 -- Processing trial results took 2.314 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:34:12,694\tWARNING util.py:315 -- The `process_trial_result` operation took 2.315 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588003)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_4ac2d8e6_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-34-01/wandb/run-20230719_123415-4ac2d8e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Syncing run FSR_Trainable_4ac2d8e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4ac2d8e6\n",
      "2023-07-19 12:34:19,421\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.320 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:19,426\tWARNING util.py:315 -- The `process_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:19,427\tWARNING util.py:315 -- Processing trial results took 2.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:34:19,429\tWARNING util.py:315 -- The `process_trial_result` operation took 2.329 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-07-19 12:34:30,595\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.396 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:30,601\tWARNING util.py:315 -- The `process_trial_result` operation took 2.402 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:30,602\tWARNING util.py:315 -- Processing trial results took 2.404 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:34:30,604\tWARNING util.py:315 -- The `process_trial_result` operation took 2.406 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_d9b9342e_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-34-16/wandb/run-20230719_123433-d9b9342e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Syncing run FSR_Trainable_d9b9342e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d9b9342e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                      mae 0.37871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                     mape 0.09077\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                     rmse 0.71601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:       time_since_restore 16.04063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:         time_this_iter_s 0.26621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:             time_total_s 16.04063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:                timestamp 1689737673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb:  View run FSR_Trainable_4ac2d8e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/4ac2d8e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588232)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123415-4ac2d8e6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123433-d9b9342e/logs\n",
      "2023-07-19 12:34:40,875\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:40,878\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:40,881\tWARNING util.py:315 -- Processing trial results took 2.191 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:34:40,885\tWARNING util.py:315 -- The `process_trial_result` operation took 2.195 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588647)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_3facbad1_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-34-27/wandb/run-20230719_123443-3facbad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Syncing run FSR_Trainable_3facbad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3facbad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                      mae 2.24194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                     mape 0.35099\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                     rmse 2.51697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:       time_since_restore 0.5513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:         time_this_iter_s 0.5513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:             time_total_s 0.5513\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:                timestamp 1689737678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb:  View run FSR_Trainable_3facbad1 at: https://wandb.ai/seokjin/FSR-prediction/runs/3facbad1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123443-3facbad1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588890)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 12:34:50,549\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:50,553\tWARNING util.py:315 -- The `process_trial_result` operation took 2.070 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:34:50,555\tWARNING util.py:315 -- Processing trial results took 2.073 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:34:50,557\tWARNING util.py:315 -- The `process_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_fb69d467_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-34-38/wandb/run-20230719_123453-fb69d467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Syncing run FSR_Trainable_fb69d467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb69d467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 12:35:01,127\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.135 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:01,132\tWARNING util.py:315 -- The `process_trial_result` operation took 2.141 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:01,134\tWARNING util.py:315 -- Processing trial results took 2.143 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:35:01,139\tWARNING util.py:315 -- The `process_trial_result` operation took 2.147 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                      mae 0.36954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                     mape 0.09392\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                     rmse 0.69782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:       time_since_restore 29.46364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:         time_this_iter_s 0.25339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:             time_total_s 29.46364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:                timestamp 1689737696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb:  View run FSR_Trainable_74dbef32 at: https://wandb.ai/seokjin/FSR-prediction/runs/74dbef32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=588401)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123422-74dbef32/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_2800ec9e_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-34-47/wandb/run-20230719_123504-2800ec9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Syncing run FSR_Trainable_2800ec9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2800ec9e\n",
      "2023-07-19 12:35:12,020\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.133 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:12,024\tWARNING util.py:315 -- The `process_trial_result` operation took 2.138 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:12,026\tWARNING util.py:315 -- Processing trial results took 2.140 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:35:12,028\tWARNING util.py:315 -- The `process_trial_result` operation took 2.142 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...580)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_6c791bc8_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-34-58/wandb/run-20230719_123515-6c791bc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Syncing run FSR_Trainable_6c791bc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6c791bc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                      mae 2.1848\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                     mape 0.35314\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                     rmse 2.49943\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:       time_since_restore 0.61003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:         time_this_iter_s 0.61003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:             time_total_s 0.61003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:                timestamp 1689737709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb:  View run FSR_Trainable_6c791bc8 at: https://wandb.ai/seokjin/FSR-prediction/runs/6c791bc8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589580)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123515-6c791bc8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123515-6c791bc8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 12:35:22,666\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:22,670\tWARNING util.py:315 -- The `process_trial_result` operation took 2.228 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:22,672\tWARNING util.py:315 -- Processing trial results took 2.230 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:35:22,674\tWARNING util.py:315 -- The `process_trial_result` operation took 2.232 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589352)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_93c5e213_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-35-09/wandb/run-20230719_123525-93c5e213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Syncing run FSR_Trainable_93c5e213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/93c5e213\n",
      "2023-07-19 12:35:33,763\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:33,768\tWARNING util.py:315 -- The `process_trial_result` operation took 2.233 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:33,770\tWARNING util.py:315 -- Processing trial results took 2.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:35:33,774\tWARNING util.py:315 -- The `process_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...051)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_5a119cb1_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-35-19/wandb/run-20230719_123537-5a119cb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Syncing run FSR_Trainable_5a119cb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a119cb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:35:45,136\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.333 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:45,139\tWARNING util.py:315 -- The `process_trial_result` operation took 2.336 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:45,140\tWARNING util.py:315 -- Processing trial results took 2.338 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:35:45,142\tWARNING util.py:315 -- The `process_trial_result` operation took 2.340 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                      mae 2.05894\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                     mape 0.34627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                     rmse 2.27949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:       time_since_restore 0.74298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:         time_this_iter_s 0.74298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:             time_total_s 0.74298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:                timestamp 1689737731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb:  View run FSR_Trainable_5a119cb1 at: https://wandb.ai/seokjin/FSR-prediction/runs/5a119cb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590051)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123537-5a119cb1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_dce86eaa_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-35-30/wandb/run-20230719_123548-dce86eaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Syncing run FSR_Trainable_dce86eaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dce86eaa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                      mae 0.37782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                     mape 0.0937\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                     rmse 0.71359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:       time_since_restore 49.56292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:         time_this_iter_s 0.58121\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:             time_total_s 49.56292\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:                timestamp 1689737751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb:  View run FSR_Trainable_fb69d467 at: https://wandb.ai/seokjin/FSR-prediction/runs/fb69d467\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589121)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123453-fb69d467/logs\n",
      "2023-07-19 12:35:56,851\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.324 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:56,853\tWARNING util.py:315 -- The `process_trial_result` operation took 2.328 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:35:56,856\tWARNING util.py:315 -- Processing trial results took 2.330 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:35:56,862\tWARNING util.py:315 -- The `process_trial_result` operation took 2.337 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_f7a9c5df_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-35-42/wandb/run-20230719_123600-f7a9c5df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Syncing run FSR_Trainable_f7a9c5df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f7a9c5df\n",
      "2023-07-19 12:36:09,796\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.850 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:36:09,799\tWARNING util.py:315 -- The `process_trial_result` operation took 2.854 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:36:09,801\tWARNING util.py:315 -- Processing trial results took 2.856 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:36:09,804\tWARNING util.py:315 -- The `process_trial_result` operation took 2.860 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_32a09485_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-35-54/wandb/run-20230719_123614-32a09485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Syncing run FSR_Trainable_32a09485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/32a09485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                      mae 0.35336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                     mape 0.09748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                     rmse 0.68129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:       time_since_restore 43.78294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:         time_this_iter_s 0.42673\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:             time_total_s 43.78294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:                timestamp 1689737777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb:  View run FSR_Trainable_93c5e213 at: https://wandb.ai/seokjin/FSR-prediction/runs/93c5e213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=589813)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123525-93c5e213/logs\n",
      "2023-07-19 12:36:31,134\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:36:31,137\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:36:31,138\tWARNING util.py:315 -- Processing trial results took 2.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:36:31,141\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_eec8f5aa_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-36-06/wandb/run-20230719_123634-eec8f5aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Syncing run FSR_Trainable_eec8f5aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/eec8f5aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                      mae 0.34762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                     mape 0.09808\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                     rmse 0.68682\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:       time_since_restore 36.94263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:         time_this_iter_s 0.3301\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:             time_total_s 36.94263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:                timestamp 1689737800\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb:  View run FSR_Trainable_f7a9c5df at: https://wandb.ai/seokjin/FSR-prediction/runs/f7a9c5df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590519)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123600-f7a9c5df/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123548-dce86eaa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123548-dce86eaa/logs\n",
      "2023-07-19 12:36:53,519\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.379 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:36:53,524\tWARNING util.py:315 -- The `process_trial_result` operation took 2.384 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:36:53,525\tWARNING util.py:315 -- Processing trial results took 2.386 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:36:53,528\tWARNING util.py:315 -- The `process_trial_result` operation took 2.389 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590281)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_cffe4d2f_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-36-28/wandb/run-20230719_123656-cffe4d2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Syncing run FSR_Trainable_cffe4d2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cffe4d2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                      mae 0.36719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                     mape 0.09396\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                     rmse 0.69512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:       time_since_restore 38.89315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:         time_this_iter_s 0.52456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:             time_total_s 38.89315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:                timestamp 1689737814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb:  View run FSR_Trainable_32a09485 at: https://wandb.ai/seokjin/FSR-prediction/runs/32a09485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123614-32a09485/logs\n",
      "2023-07-19 12:37:04,950\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.271 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:04,952\tWARNING util.py:315 -- The `process_trial_result` operation took 2.274 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:04,954\tWARNING util.py:315 -- Processing trial results took 2.276 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:37:04,957\tWARNING util.py:315 -- The `process_trial_result` operation took 2.279 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=590752)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_f0e8e935_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-36-50/wandb/run-20230719_123708-f0e8e935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Syncing run FSR_Trainable_f0e8e935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f0e8e935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:37:17,121\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.363 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:17,123\tWARNING util.py:315 -- The `process_trial_result` operation took 2.367 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:17,127\tWARNING util.py:315 -- Processing trial results took 2.370 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:37:17,131\tWARNING util.py:315 -- The `process_trial_result` operation took 2.374 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                      mae 0.36689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                     mape 0.09334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                     rmse 0.69512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:       time_since_restore 35.0984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:         time_this_iter_s 0.43312\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:             time_total_s 35.0984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:                timestamp 1689737832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb:  View run FSR_Trainable_eec8f5aa at: https://wandb.ai/seokjin/FSR-prediction/runs/eec8f5aa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591016)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123634-eec8f5aa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_516c3546_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-37-02/wandb/run-20230719_123720-516c3546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Syncing run FSR_Trainable_516c3546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/516c3546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                      mae 0.36091\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                     mape 0.09261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                     rmse 0.70066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:       time_since_restore 2.0268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:         time_this_iter_s 0.4626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:             time_total_s 2.0268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:                timestamp 1689737838\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb:  View run FSR_Trainable_516c3546 at: https://wandb.ai/seokjin/FSR-prediction/runs/516c3546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591759)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123720-516c3546/logs\n",
      "2023-07-19 12:37:28,584\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.266 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:28,587\tWARNING util.py:315 -- The `process_trial_result` operation took 2.270 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:28,589\tWARNING util.py:315 -- Processing trial results took 2.271 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:37:28,592\tWARNING util.py:315 -- The `process_trial_result` operation took 2.274 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_ac021b55_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-37-14/wandb/run-20230719_123731-ac021b55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Syncing run FSR_Trainable_ac021b55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ac021b55\n",
      "2023-07-19 12:37:40,405\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.115 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:40,410\tWARNING util.py:315 -- The `process_trial_result` operation took 2.120 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:40,416\tWARNING util.py:315 -- Processing trial results took 2.126 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:37:40,418\tWARNING util.py:315 -- The `process_trial_result` operation took 2.128 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_6cbf283a_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-37-25/wandb/run-20230719_123743-6cbf283a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: Syncing run FSR_Trainable_6cbf283a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6cbf283a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                      mae 0.36626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                     mape 0.09249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                     rmse 0.69357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:       time_since_restore 38.1707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:         time_this_iter_s 0.50488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:             time_total_s 38.1707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:                timestamp 1689737862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb:  View run FSR_Trainable_cffe4d2f at: https://wandb.ai/seokjin/FSR-prediction/runs/cffe4d2f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591291)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123656-cffe4d2f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592237)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:37:53,385\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.278 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:53,389\tWARNING util.py:315 -- The `process_trial_result` operation took 2.283 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:37:53,391\tWARNING util.py:315 -- Processing trial results took 2.285 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:37:53,393\tWARNING util.py:315 -- The `process_trial_result` operation took 2.287 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                      mae 0.35552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                     mape 0.09737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                     rmse 0.68892\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:       time_since_restore 36.19599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:         time_this_iter_s 0.47599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:             time_total_s 36.19599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:                timestamp 1689737869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb:  View run FSR_Trainable_f0e8e935 at: https://wandb.ai/seokjin/FSR-prediction/runs/f0e8e935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123708-f0e8e935/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123708-f0e8e935/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_9200f54b_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-37-37/wandb/run-20230719_123756-9200f54b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Syncing run FSR_Trainable_9200f54b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9200f54b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=591530)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 12:38:04,551\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.290 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:04,555\tWARNING util.py:315 -- The `process_trial_result` operation took 2.295 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:04,557\tWARNING util.py:315 -- Processing trial results took 2.297 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:38:04,560\tWARNING util.py:315 -- The `process_trial_result` operation took 2.300 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_9d1f181e_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-37-50/wandb/run-20230719_123807-9d1f181e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Syncing run FSR_Trainable_9d1f181e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d1f181e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                      mae 0.37488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                     mape 0.10492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                     rmse 0.70923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:       time_since_restore 1.00508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:         time_this_iter_s 0.4364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:             time_total_s 1.00508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:                timestamp 1689737885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb:  View run FSR_Trainable_9d1f181e at: https://wandb.ai/seokjin/FSR-prediction/runs/9d1f181e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123807-9d1f181e/logs\n",
      "2023-07-19 12:38:15,907\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.350 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:15,912\tWARNING util.py:315 -- The `process_trial_result` operation took 2.355 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:15,916\tWARNING util.py:315 -- Processing trial results took 2.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:38:15,921\tWARNING util.py:315 -- The `process_trial_result` operation took 2.365 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_13376944_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-38-01/wandb/run-20230719_123818-13376944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Syncing run FSR_Trainable_13376944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/13376944\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                      mae 0.3491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                     mape 0.09667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                     rmse 0.67757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:       time_since_restore 37.62972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:         time_this_iter_s 0.42806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:             time_total_s 37.62972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:                timestamp 1689737896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb:  View run FSR_Trainable_ac021b55 at: https://wandb.ai/seokjin/FSR-prediction/runs/ac021b55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592002)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123731-ac021b55/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123818-13376944/logs\n",
      "2023-07-19 12:38:25,740\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.287 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:25,745\tWARNING util.py:315 -- The `process_trial_result` operation took 2.292 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:25,746\tWARNING util.py:315 -- Processing trial results took 2.293 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:38:25,747\tWARNING util.py:315 -- The `process_trial_result` operation took 2.294 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_c1020d1d_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-38-12/wandb/run-20230719_123828-c1020d1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Syncing run FSR_Trainable_c1020d1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1020d1d\n",
      "2023-07-19 12:38:36,914\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.142 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:36,915\tWARNING util.py:315 -- The `process_trial_result` operation took 2.144 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:36,919\tWARNING util.py:315 -- Processing trial results took 2.148 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:38:36,922\tWARNING util.py:315 -- The `process_trial_result` operation took 2.151 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_c1baadff_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-38-23/wandb/run-20230719_123840-c1baadff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Syncing run FSR_Trainable_c1baadff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1baadff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                      mae 0.36404\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                     mape 0.09343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                     rmse 0.69757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:       time_since_restore 38.09788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:         time_this_iter_s 0.37492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:             time_total_s 38.09788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:                timestamp 1689737921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb:  View run FSR_Trainable_9200f54b at: https://wandb.ai/seokjin/FSR-prediction/runs/9200f54b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=592491)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123756-9200f54b/logs\n",
      "2023-07-19 12:38:48,920\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.179 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:48,921\tWARNING util.py:315 -- The `process_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:38:48,924\tWARNING util.py:315 -- Processing trial results took 2.185 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:38:48,926\tWARNING util.py:315 -- The `process_trial_result` operation took 2.186 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_7223ea8f_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-38-34/wandb/run-20230719_123852-7223ea8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Syncing run FSR_Trainable_7223ea8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7223ea8f\n",
      "2023-07-19 12:39:01,695\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.179 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:39:01,700\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:39:01,704\tWARNING util.py:315 -- Processing trial results took 2.188 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:39:01,705\tWARNING util.py:315 -- The `process_trial_result` operation took 2.190 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_e4fcf99e_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-38-45/wandb/run-20230719_123905-e4fcf99e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Syncing run FSR_Trainable_e4fcf99e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e4fcf99e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                      mae 0.33219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                     mape 0.0954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                     rmse 0.6758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:       time_since_restore 40.59351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:         time_this_iter_s 0.41165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:             time_total_s 40.59351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:                timestamp 1689737954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb:  View run FSR_Trainable_c1020d1d at: https://wandb.ai/seokjin/FSR-prediction/runs/c1020d1d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593196)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123828-c1020d1d/logs\n",
      "2023-07-19 12:39:28,869\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:39:28,872\tWARNING util.py:315 -- The `process_trial_result` operation took 2.015 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:39:28,874\tWARNING util.py:315 -- Processing trial results took 2.017 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:39:28,877\tWARNING util.py:315 -- The `process_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_f1c23855_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-38-58/wandb/run-20230719_123932-f1c23855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Syncing run FSR_Trainable_f1c23855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f1c23855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                      mae 0.36219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                     mape 0.09134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                     rmse 0.68938\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:       time_since_restore 62.38489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:         time_this_iter_s 0.5922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:             time_total_s 62.38489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:                timestamp 1689737986\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb:  View run FSR_Trainable_c1baadff at: https://wandb.ai/seokjin/FSR-prediction/runs/c1baadff\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593422)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123840-c1baadff/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:40:01,743\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.186 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:01,747\tWARNING util.py:315 -- The `process_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:01,750\tWARNING util.py:315 -- Processing trial results took 2.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:40:01,752\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                      mae 0.36123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                     mape 0.09432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                     rmse 0.69206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:       time_since_restore 63.63961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:         time_this_iter_s 0.56759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:             time_total_s 63.63961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:                timestamp 1689737997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb:  View run FSR_Trainable_7223ea8f at: https://wandb.ai/seokjin/FSR-prediction/runs/7223ea8f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593662)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123852-7223ea8f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_8c9518c7_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-39-25/wandb/run-20230719_124005-8c9518c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Syncing run FSR_Trainable_8c9518c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8c9518c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                      mae 0.33204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                     mape 0.09503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                     rmse 0.69042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:       time_since_restore 3.15526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:         time_this_iter_s 0.68465\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:             time_total_s 3.15526\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:                timestamp 1689738004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb:  View run FSR_Trainable_8c9518c7 at: https://wandb.ai/seokjin/FSR-prediction/runs/8c9518c7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594426)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124005-8c9518c7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:40:13,915\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.467 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:13,919\tWARNING util.py:315 -- The `process_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:13,920\tWARNING util.py:315 -- Processing trial results took 2.473 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:40:13,922\tWARNING util.py:315 -- The `process_trial_result` operation took 2.475 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123905-e4fcf99e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123905-e4fcf99e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=593888)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_93ffe5a7_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-39-58/wandb/run-20230719_124016-93ffe5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Syncing run FSR_Trainable_93ffe5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/93ffe5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                      mae 0.36985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                     mape 0.1075\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                     rmse 0.69819\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:       time_since_restore 0.75335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:         time_this_iter_s 0.28741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:             time_total_s 0.75335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:                timestamp 1689738014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb:  View run FSR_Trainable_93ffe5a7 at: https://wandb.ai/seokjin/FSR-prediction/runs/93ffe5a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124016-93ffe5a7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594674)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 12:40:24,145\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.452 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:24,148\tWARNING util.py:315 -- The `process_trial_result` operation took 2.456 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:24,150\tWARNING util.py:315 -- Processing trial results took 2.458 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:40:24,154\tWARNING util.py:315 -- The `process_trial_result` operation took 2.462 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_31faf724_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-40-11/wandb/run-20230719_124027-31faf724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Syncing run FSR_Trainable_31faf724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/31faf724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                      mae 0.35772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                     mape 0.10206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                     rmse 0.69319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:       time_since_restore 1.29734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:         time_this_iter_s 0.30289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:             time_total_s 1.29734\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:                timestamp 1689738025\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb:  View run FSR_Trainable_31faf724 at: https://wandb.ai/seokjin/FSR-prediction/runs/31faf724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594909)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124027-31faf724/logs\n",
      "2023-07-19 12:40:34,359\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.426 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:34,364\tWARNING util.py:315 -- The `process_trial_result` operation took 2.432 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:34,369\tWARNING util.py:315 -- Processing trial results took 2.436 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:40:34,371\tWARNING util.py:315 -- The `process_trial_result` operation took 2.439 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_c4f4b7da_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-40-21/wandb/run-20230719_124036-c4f4b7da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Syncing run FSR_Trainable_c4f4b7da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c4f4b7da\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                      mae 0.36337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                     mape 0.09585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                     rmse 0.68758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:       time_since_restore 57.32837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:         time_this_iter_s 0.50191\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:             time_total_s 57.32837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:                timestamp 1689738036\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb:  View run FSR_Trainable_f1c23855 at: https://wandb.ai/seokjin/FSR-prediction/runs/f1c23855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=594152)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_123932-f1c23855/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 12:40:43,220\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:43,223\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:43,224\tWARNING util.py:315 -- Processing trial results took 2.084 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:40:43,225\tWARNING util.py:315 -- The `process_trial_result` operation took 2.085 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595141)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_c9a9fa72_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-40-31/wandb/run-20230719_124045-c9a9fa72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Syncing run FSR_Trainable_c9a9fa72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c9a9fa72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:40:53,518\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:53,522\tWARNING util.py:315 -- The `process_trial_result` operation took 2.092 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:40:53,524\tWARNING util.py:315 -- Processing trial results took 2.094 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:40:53,526\tWARNING util.py:315 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                      mae 0.36727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                     mape 0.09334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                     rmse 0.69552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:       time_since_restore 6.55527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:         time_this_iter_s 0.38866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:             time_total_s 6.55527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:                timestamp 1689738049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb:  View run FSR_Trainable_c9a9fa72 at: https://wandb.ai/seokjin/FSR-prediction/runs/c9a9fa72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595373)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124045-c9a9fa72/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_600c6b6c_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-40-40/wandb/run-20230719_124056-600c6b6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Syncing run FSR_Trainable_600c6b6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/600c6b6c\n",
      "2023-07-19 12:41:04,619\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.979 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:04,622\tWARNING util.py:315 -- The `process_trial_result` operation took 2.983 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:04,625\tWARNING util.py:315 -- Processing trial results took 2.986 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:41:04,627\tWARNING util.py:315 -- The `process_trial_result` operation took 2.988 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_dea91bda_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-40-50/wandb/run-20230719_124107-dea91bda\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Syncing run FSR_Trainable_dea91bda\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dea91bda\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:41:12,996\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.094 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:13,003\tWARNING util.py:315 -- The `process_trial_result` operation took 3.103 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:13,005\tWARNING util.py:315 -- Processing trial results took 3.105 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:41:13,007\tWARNING util.py:315 -- The `process_trial_result` operation took 3.107 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                      mae 0.35332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                     mape 0.10491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                     rmse 0.69594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:       time_since_restore 0.92319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:         time_this_iter_s 0.44198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:             time_total_s 0.92319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:                timestamp 1689738065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb:  View run FSR_Trainable_dea91bda at: https://wandb.ai/seokjin/FSR-prediction/runs/dea91bda\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595830)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124107-dea91bda/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_465577db_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-41-01/wandb/run-20230719_124116-465577db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Syncing run FSR_Trainable_465577db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/465577db\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                      mae 0.36785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                     mape 0.0934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                     rmse 0.6983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:       time_since_restore 18.94367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:         time_this_iter_s 0.5661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:             time_total_s 18.94367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:                timestamp 1689738077\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb:  View run FSR_Trainable_600c6b6c at: https://wandb.ai/seokjin/FSR-prediction/runs/600c6b6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=595605)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124056-600c6b6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 12:41:23,729\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:23,732\tWARNING util.py:315 -- The `process_trial_result` operation took 1.691 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:23,734\tWARNING util.py:315 -- Processing trial results took 1.693 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:41:23,735\tWARNING util.py:315 -- The `process_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596011)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_9f640d1b_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-41-09/wandb/run-20230719_124126-9f640d1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Syncing run FSR_Trainable_9f640d1b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9f640d1b\n",
      "2023-07-19 12:41:33,590\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.333 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:33,596\tWARNING util.py:315 -- The `process_trial_result` operation took 2.340 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:33,599\tWARNING util.py:315 -- Processing trial results took 2.343 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:41:33,601\tWARNING util.py:315 -- The `process_trial_result` operation took 2.345 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_05183844_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-41-20/wandb/run-20230719_124136-05183844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Syncing run FSR_Trainable_05183844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/05183844\n",
      "2023-07-19 12:41:45,048\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.254 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:45,051\tWARNING util.py:315 -- The `process_trial_result` operation took 2.258 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:45,053\tWARNING util.py:315 -- Processing trial results took 2.260 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:41:45,055\tWARNING util.py:315 -- The `process_trial_result` operation took 2.262 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_926676f3_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-41-30/wandb/run-20230719_124148-926676f3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Syncing run FSR_Trainable_926676f3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/926676f3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                      mae 0.35127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                     mape 0.09729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                     rmse 0.69664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:       time_since_restore 1.04537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:         time_this_iter_s 0.42108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:             time_total_s 1.04537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:                timestamp 1689738105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb:  View run FSR_Trainable_926676f3 at: https://wandb.ai/seokjin/FSR-prediction/runs/926676f3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596700)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124148-926676f3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124126-9f640d1b/logs\n",
      "2023-07-19 12:41:56,772\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.327 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:56,776\tWARNING util.py:315 -- The `process_trial_result` operation took 2.331 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:41:56,777\tWARNING util.py:315 -- Processing trial results took 2.332 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:41:56,778\tWARNING util.py:315 -- The `process_trial_result` operation took 2.334 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596253)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_95726d4d_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-41-42/wandb/run-20230719_124159-95726d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Syncing run FSR_Trainable_95726d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/95726d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                      mae 0.36415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                     mape 0.09632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                     rmse 0.71254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:       time_since_restore 0.54498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:         time_this_iter_s 0.54498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:             time_total_s 0.54498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:                timestamp 1689738114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb:  View run FSR_Trainable_95726d4d at: https://wandb.ai/seokjin/FSR-prediction/runs/95726d4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124159-95726d4d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596944)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 12:42:06,696\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.380 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:06,701\tWARNING util.py:315 -- The `process_trial_result` operation took 2.385 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:06,702\tWARNING util.py:315 -- Processing trial results took 2.387 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:42:06,704\tWARNING util.py:315 -- The `process_trial_result` operation took 2.388 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_43f6bf74_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-41-53/wandb/run-20230719_124209-43f6bf74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Syncing run FSR_Trainable_43f6bf74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/43f6bf74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                      mae 0.40262\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                     mape 0.11129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                     rmse 0.73897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:       time_since_restore 0.36474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:         time_this_iter_s 0.36474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:             time_total_s 0.36474\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:                timestamp 1689738124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb:  View run FSR_Trainable_43f6bf74 at: https://wandb.ai/seokjin/FSR-prediction/runs/43f6bf74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597173)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124209-43f6bf74/logs\n",
      "2023-07-19 12:42:16,686\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.516 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:16,691\tWARNING util.py:315 -- The `process_trial_result` operation took 2.522 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:16,693\tWARNING util.py:315 -- Processing trial results took 2.524 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:42:16,694\tWARNING util.py:315 -- The `process_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_e547a93a_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-42-04/wandb/run-20230719_124219-e547a93a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Syncing run FSR_Trainable_e547a93a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e547a93a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                      mae 0.34727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                     mape 0.09601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                     rmse 0.68704\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:       time_since_restore 34.86297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:         time_this_iter_s 0.27316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:             time_total_s 34.86297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:                timestamp 1689738139\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb:  View run FSR_Trainable_05183844 at: https://wandb.ai/seokjin/FSR-prediction/runs/05183844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=596478)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124136-05183844/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 12:42:25,826\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.670 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:25,833\tWARNING util.py:315 -- The `process_trial_result` operation took 1.677 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:25,835\tWARNING util.py:315 -- Processing trial results took 1.680 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:42:25,839\tWARNING util.py:315 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_7be5f46b_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-42-13/wandb/run-20230719_124228-7be5f46b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Syncing run FSR_Trainable_7be5f46b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7be5f46b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597404)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                      mae 1.98808\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                     mape 0.30248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                     rmse 2.23853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:       time_since_restore 1.23156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:         time_this_iter_s 1.23156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:             time_total_s 1.23156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:                timestamp 1689738144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb:  View run FSR_Trainable_7be5f46b at: https://wandb.ai/seokjin/FSR-prediction/runs/7be5f46b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124228-7be5f46b/logs\n",
      "2023-07-19 12:42:34,444\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.351 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:34,449\tWARNING util.py:315 -- The `process_trial_result` operation took 2.356 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:34,450\tWARNING util.py:315 -- Processing trial results took 2.357 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:42:34,451\tWARNING util.py:315 -- The `process_trial_result` operation took 2.359 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597637)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_f89b1452_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-42-22/wandb/run-20230719_124236-f89b1452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Syncing run FSR_Trainable_f89b1452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f89b1452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:42:41,289\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.317 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:41,294\tWARNING util.py:315 -- The `process_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:41,296\tWARNING util.py:315 -- Processing trial results took 2.324 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:42:41,299\tWARNING util.py:315 -- The `process_trial_result` operation took 2.327 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                      mae 0.3785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                     mape 0.10981\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                     rmse 0.70919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:       time_since_restore 0.74753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:         time_this_iter_s 0.74753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:             time_total_s 0.74753\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:                timestamp 1689738152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb:  View run FSR_Trainable_f89b1452 at: https://wandb.ai/seokjin/FSR-prediction/runs/f89b1452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124236-f89b1452/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=597864)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_6fabf16c_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-42-31/wandb/run-20230719_124243-6fabf16c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Syncing run FSR_Trainable_6fabf16c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6fabf16c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:42:48,092\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.515 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:48,096\tWARNING util.py:315 -- The `process_trial_result` operation took 2.521 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:48,098\tWARNING util.py:315 -- Processing trial results took 2.523 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:42:48,099\tWARNING util.py:315 -- The `process_trial_result` operation took 2.524 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124243-6fabf16c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598049)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_7ad4562c_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-42-38/wandb/run-20230719_124250-7ad4562c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Syncing run FSR_Trainable_7ad4562c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7ad4562c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:42:54,618\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:54,621\tWARNING util.py:315 -- The `process_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:42:54,623\tWARNING util.py:315 -- Processing trial results took 2.065 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:42:54,625\tWARNING util.py:315 -- The `process_trial_result` operation took 2.066 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                      mae 0.36389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                     mape 0.09931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                     rmse 0.70698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:       time_since_restore 0.78954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:         time_this_iter_s 0.78954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:             time_total_s 0.78954\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:                timestamp 1689738165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb:  View run FSR_Trainable_7ad4562c at: https://wandb.ai/seokjin/FSR-prediction/runs/7ad4562c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598232)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124250-7ad4562c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_22bf881b_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-42-44/wandb/run-20230719_124256-22bf881b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Syncing run FSR_Trainable_22bf881b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/22bf881b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                      mae 0.36456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                     mape 0.09652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                     rmse 0.71132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:       time_since_restore 0.77089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:         time_this_iter_s 0.77089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:             time_total_s 0.77089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:                timestamp 1689738172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb:  View run FSR_Trainable_22bf881b at: https://wandb.ai/seokjin/FSR-prediction/runs/22bf881b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598420)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124256-22bf881b/logs\n",
      "2023-07-19 12:43:03,350\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.414 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:03,354\tWARNING util.py:315 -- The `process_trial_result` operation took 2.419 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:03,356\tWARNING util.py:315 -- Processing trial results took 2.421 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:43:03,358\tWARNING util.py:315 -- The `process_trial_result` operation took 2.423 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_07d63ce3_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-42-51/wandb/run-20230719_124305-07d63ce3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Syncing run FSR_Trainable_07d63ce3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/07d63ce3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:43:10,041\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:10,046\tWARNING util.py:315 -- The `process_trial_result` operation took 1.848 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:10,047\tWARNING util.py:315 -- Processing trial results took 1.849 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:43:10,049\tWARNING util.py:315 -- The `process_trial_result` operation took 1.851 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                      mae 0.33486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                     mape 0.09519\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                     rmse 0.69261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:       time_since_restore 1.91434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:         time_this_iter_s 0.37228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:             time_total_s 1.91434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:                timestamp 1689738184\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb:  View run FSR_Trainable_07d63ce3 at: https://wandb.ai/seokjin/FSR-prediction/runs/07d63ce3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598646)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124305-07d63ce3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_26092a16_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-43-00/wandb/run-20230719_124312-26092a16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Syncing run FSR_Trainable_26092a16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/26092a16\n",
      "2023-07-19 12:43:20,158\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.687 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:20,163\tWARNING util.py:315 -- The `process_trial_result` operation took 2.692 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:20,164\tWARNING util.py:315 -- Processing trial results took 2.694 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:43:20,166\tWARNING util.py:315 -- The `process_trial_result` operation took 2.695 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_69f7d8fc_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-43-07/wandb/run-20230719_124322-69f7d8fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Syncing run FSR_Trainable_69f7d8fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/69f7d8fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:43:27,857\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:27,861\tWARNING util.py:315 -- The `process_trial_result` operation took 2.113 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:27,862\tWARNING util.py:315 -- Processing trial results took 2.114 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:43:27,864\tWARNING util.py:315 -- The `process_trial_result` operation took 2.116 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                      mae 0.36485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                     mape 0.10005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                     rmse 0.70414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:       time_since_restore 0.41985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:         time_this_iter_s 0.41985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:             time_total_s 0.41985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:                timestamp 1689738197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb:  View run FSR_Trainable_69f7d8fc at: https://wandb.ai/seokjin/FSR-prediction/runs/69f7d8fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599053)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124322-69f7d8fc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_43cad568_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-43-17/wandb/run-20230719_124330-43cad568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Syncing run FSR_Trainable_43cad568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/43cad568\n",
      "2023-07-19 12:43:39,550\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:39,552\tWARNING util.py:315 -- The `process_trial_result` operation took 2.328 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:39,553\tWARNING util.py:315 -- Processing trial results took 2.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:43:39,555\tWARNING util.py:315 -- The `process_trial_result` operation took 2.331 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_99d79818_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-43-24/wandb/run-20230719_124343-99d79818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Syncing run FSR_Trainable_99d79818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/99d79818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                      mae 0.37053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                     mape 0.10699\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                     rmse 0.69021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:       time_since_restore 2.54149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:         time_this_iter_s 0.59547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:             time_total_s 2.54149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:                timestamp 1689738221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb:  View run FSR_Trainable_99d79818 at: https://wandb.ai/seokjin/FSR-prediction/runs/99d79818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599461)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124343-99d79818/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:43:51,751\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:51,755\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:43:51,757\tWARNING util.py:315 -- Processing trial results took 2.197 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:43:51,759\tWARNING util.py:315 -- The `process_trial_result` operation took 2.200 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124312-26092a16/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=598829)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_fffdff38_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-43-36/wandb/run-20230719_124354-fffdff38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Syncing run FSR_Trainable_fffdff38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fffdff38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                      mae 0.37654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                     mape 0.09886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                     rmse 0.70582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:       time_since_restore 0.7792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:         time_this_iter_s 0.7792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:             time_total_s 0.7792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:                timestamp 1689738229\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb:  View run FSR_Trainable_fffdff38 at: https://wandb.ai/seokjin/FSR-prediction/runs/fffdff38\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599691)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124354-fffdff38/logs\n",
      "2023-07-19 12:44:02,037\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.405 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:02,040\tWARNING util.py:315 -- The `process_trial_result` operation took 2.409 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:02,042\tWARNING util.py:315 -- Processing trial results took 2.411 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:44:02,043\tWARNING util.py:315 -- The `process_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_dbeb54e6_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-43-48/wandb/run-20230719_124405-dbeb54e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Syncing run FSR_Trainable_dbeb54e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/dbeb54e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                      mae 0.35397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                     mape 0.09668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                     rmse 0.68816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:       time_since_restore 3.28711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:         time_this_iter_s 0.3531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:             time_total_s 3.28711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:                timestamp 1689738244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb:  View run FSR_Trainable_dbeb54e6 at: https://wandb.ai/seokjin/FSR-prediction/runs/dbeb54e6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599932)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124405-dbeb54e6/logs\n",
      "2023-07-19 12:44:12,706\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.395 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:12,709\tWARNING util.py:315 -- The `process_trial_result` operation took 2.400 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:12,713\tWARNING util.py:315 -- Processing trial results took 2.403 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:44:12,714\tWARNING util.py:315 -- The `process_trial_result` operation took 2.405 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_8b3cb18b_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-43-59/wandb/run-20230719_124415-8b3cb18b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Syncing run FSR_Trainable_8b3cb18b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8b3cb18b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                      mae 2.28835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                     mape 0.37588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                     rmse 2.59378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:       time_since_restore 0.5456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:         time_this_iter_s 0.5456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:             time_total_s 0.5456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:                timestamp 1689738250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb:  View run FSR_Trainable_8b3cb18b at: https://wandb.ai/seokjin/FSR-prediction/runs/8b3cb18b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600164)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124415-8b3cb18b/logs\n",
      "2023-07-19 12:44:23,049\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.351 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:23,051\tWARNING util.py:315 -- The `process_trial_result` operation took 2.354 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:23,053\tWARNING util.py:315 -- Processing trial results took 2.356 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:44:23,054\tWARNING util.py:315 -- The `process_trial_result` operation took 2.357 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_7d2620f2_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-44-09/wandb/run-20230719_124425-7d2620f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Syncing run FSR_Trainable_7d2620f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7d2620f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                      mae 2.46885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                     mape 0.39976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                     rmse 2.64992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:       time_since_restore 0.51826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:         time_this_iter_s 0.51826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:             time_total_s 0.51826\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:                timestamp 1689738260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb:  View run FSR_Trainable_7d2620f2 at: https://wandb.ai/seokjin/FSR-prediction/runs/7d2620f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600394)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124425-7d2620f2/logs\n",
      "2023-07-19 12:44:33,281\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:33,284\tWARNING util.py:315 -- The `process_trial_result` operation took 2.196 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:33,286\tWARNING util.py:315 -- Processing trial results took 2.197 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:44:33,287\tWARNING util.py:315 -- The `process_trial_result` operation took 2.199 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_8be6c175_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-44-20/wandb/run-20230719_124436-8be6c175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Syncing run FSR_Trainable_8be6c175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8be6c175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:44:44,262\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.126 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:44,264\tWARNING util.py:315 -- The `process_trial_result` operation took 2.128 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:44,265\tWARNING util.py:315 -- Processing trial results took 2.130 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:44:44,266\tWARNING util.py:315 -- The `process_trial_result` operation took 2.131 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                      mae 0.36246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                     mape 0.09295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                     rmse 0.68997\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:       time_since_restore 58.3205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:         time_this_iter_s 0.65696\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:             time_total_s 58.3205\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:                timestamp 1689738278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb:  View run FSR_Trainable_43cad568 at: https://wandb.ai/seokjin/FSR-prediction/runs/43cad568\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=599234)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124330-43cad568/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_ad9d1ca1_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-44-30/wandb/run-20230719_124447-ad9d1ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Syncing run FSR_Trainable_ad9d1ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad9d1ca1\n",
      "2023-07-19 12:44:56,043\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:56,044\tWARNING util.py:315 -- The `process_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:44:56,046\tWARNING util.py:315 -- Processing trial results took 2.169 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:44:56,047\tWARNING util.py:315 -- The `process_trial_result` operation took 2.170 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_c0cac9a2_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-44-41/wandb/run-20230719_124459-c0cac9a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Syncing run FSR_Trainable_c0cac9a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c0cac9a2\n",
      "2023-07-19 12:45:09,182\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.123 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:45:09,187\tWARNING util.py:315 -- The `process_trial_result` operation took 2.128 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:45:09,188\tWARNING util.py:315 -- Processing trial results took 2.130 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:45:09,190\tWARNING util.py:315 -- The `process_trial_result` operation took 2.132 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_add6f9fc_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-44-53/wandb/run-20230719_124512-add6f9fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Syncing run FSR_Trainable_add6f9fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/add6f9fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                      mae 0.36405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                     mape 0.09084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                     rmse 0.69206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:       time_since_restore 39.48416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:         time_this_iter_s 0.68536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:             time_total_s 39.48416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:                timestamp 1689738320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb:  View run FSR_Trainable_8be6c175 at: https://wandb.ai/seokjin/FSR-prediction/runs/8be6c175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600621)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124436-8be6c175/logs\n",
      "2023-07-19 12:45:35,085\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.170 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:45:35,088\tWARNING util.py:315 -- The `process_trial_result` operation took 2.174 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:45:35,090\tWARNING util.py:315 -- Processing trial results took 2.175 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:45:35,092\tWARNING util.py:315 -- The `process_trial_result` operation took 2.178 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_95b55524_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-45-06/wandb/run-20230719_124538-95b55524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Syncing run FSR_Trainable_95b55524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/95b55524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                      mae 0.36224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                     mape 0.09256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                     rmse 0.68989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:       time_since_restore 66.28023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:         time_this_iter_s 0.67462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:             time_total_s 66.28023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:                timestamp 1689738358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb:  View run FSR_Trainable_ad9d1ca1 at: https://wandb.ai/seokjin/FSR-prediction/runs/ad9d1ca1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=600853)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124447-ad9d1ca1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:46:13,175\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.111 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:13,177\tWARNING util.py:315 -- The `process_trial_result` operation took 2.114 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:13,179\tWARNING util.py:315 -- Processing trial results took 2.117 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:46:13,181\tWARNING util.py:315 -- The `process_trial_result` operation took 2.118 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                      mae 0.36359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                     mape 0.092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                     rmse 0.69261\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:       time_since_restore 66.02672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:         time_this_iter_s 0.66363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:             time_total_s 66.02672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:                timestamp 1689738368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb:  View run FSR_Trainable_c0cac9a2 at: https://wandb.ai/seokjin/FSR-prediction/runs/c0cac9a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601084)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124459-c0cac9a2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_d912899d_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-45-32/wandb/run-20230719_124616-d912899d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Syncing run FSR_Trainable_d912899d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d912899d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                      mae 0.3604\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                     mape 0.09201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                     rmse 0.69088\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:       time_since_restore 66.23659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:         time_this_iter_s 0.70389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:             time_total_s 66.23659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:                timestamp 1689738381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb:  View run FSR_Trainable_add6f9fc at: https://wandb.ai/seokjin/FSR-prediction/runs/add6f9fc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601307)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124512-add6f9fc/logs\n",
      "2023-07-19 12:46:27,618\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.899 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:27,622\tWARNING util.py:315 -- The `process_trial_result` operation took 2.904 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:27,624\tWARNING util.py:315 -- Processing trial results took 2.906 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:46:27,626\tWARNING util.py:315 -- The `process_trial_result` operation took 2.907 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_d7846332_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-46-10/wandb/run-20230719_124630-d7846332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Syncing run FSR_Trainable_d7846332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d7846332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                      mae 0.36235\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                     mape 0.09688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                     rmse 0.70703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:       time_since_restore 0.6726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:         time_this_iter_s 0.6726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:             time_total_s 0.6726\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:                timestamp 1689738384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb:  View run FSR_Trainable_d7846332 at: https://wandb.ai/seokjin/FSR-prediction/runs/d7846332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602094)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124630-d7846332/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:46:39,881\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.460 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:39,886\tWARNING util.py:315 -- The `process_trial_result` operation took 2.466 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:39,888\tWARNING util.py:315 -- Processing trial results took 2.468 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:46:39,890\tWARNING util.py:315 -- The `process_trial_result` operation took 2.470 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601853)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_18bf0dfa_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-46-24/wandb/run-20230719_124642-18bf0dfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Syncing run FSR_Trainable_18bf0dfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/18bf0dfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                      mae 0.41238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                     mape 0.10761\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                     rmse 0.72854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:       time_since_restore 0.60773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:         time_this_iter_s 0.60773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:             time_total_s 0.60773\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:                timestamp 1689738397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb:  View run FSR_Trainable_18bf0dfa at: https://wandb.ai/seokjin/FSR-prediction/runs/18bf0dfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124642-18bf0dfa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602335)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:46:50,457\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.427 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:50,460\tWARNING util.py:315 -- The `process_trial_result` operation took 2.431 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:50,461\tWARNING util.py:315 -- Processing trial results took 2.432 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:46:50,462\tWARNING util.py:315 -- The `process_trial_result` operation took 2.433 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124538-95b55524/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124538-95b55524/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124538-95b55524/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_99a5a408_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-46-36/wandb/run-20230719_124652-99a5a408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Syncing run FSR_Trainable_99a5a408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/99a5a408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=601566)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                      mae 0.38363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                     mape 0.10193\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                     rmse 0.70417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:       time_since_restore 0.54279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:         time_this_iter_s 0.54279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:             time_total_s 0.54279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:                timestamp 1689738408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb:  View run FSR_Trainable_99a5a408 at: https://wandb.ai/seokjin/FSR-prediction/runs/99a5a408\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124652-99a5a408/logs\n",
      "2023-07-19 12:46:59,382\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.309 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:59,386\tWARNING util.py:315 -- The `process_trial_result` operation took 2.314 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:46:59,387\tWARNING util.py:315 -- Processing trial results took 2.316 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:46:59,389\tWARNING util.py:315 -- The `process_trial_result` operation took 2.317 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602574)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_bb1c2a03_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-46-47/wandb/run-20230719_124702-bb1c2a03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Syncing run FSR_Trainable_bb1c2a03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb1c2a03\n",
      "2023-07-19 12:47:06,983\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.214 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:06,985\tWARNING util.py:315 -- The `process_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:06,987\tWARNING util.py:315 -- Processing trial results took 2.219 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:47:06,990\tWARNING util.py:315 -- The `process_trial_result` operation took 2.222 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_d545cc45_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-46-56/wandb/run-20230719_124710-d545cc45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Syncing run FSR_Trainable_d545cc45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d545cc45\n",
      "2023-07-19 12:47:18,677\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.217 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:18,679\tWARNING util.py:315 -- The `process_trial_result` operation took 2.220 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:18,681\tWARNING util.py:315 -- Processing trial results took 2.222 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:47:18,682\tWARNING util.py:315 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_bb057b2a_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-47-04/wandb/run-20230719_124722-bb057b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Syncing run FSR_Trainable_bb057b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb057b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                      mae 0.32319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                     mape 0.09361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                     rmse 0.68633\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:       time_since_restore 2.61701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:         time_this_iter_s 0.64775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:             time_total_s 2.61701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:                timestamp 1689738440\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb:  View run FSR_Trainable_bb057b2a at: https://wandb.ai/seokjin/FSR-prediction/runs/bb057b2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603214)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124722-bb057b2a/logs\n",
      "2023-07-19 12:47:30,936\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:30,939\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:30,940\tWARNING util.py:315 -- Processing trial results took 2.107 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:47:30,941\tWARNING util.py:315 -- The `process_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_7a508a51_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-47-15/wandb/run-20230719_124734-7a508a51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Syncing run FSR_Trainable_7a508a51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7a508a51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                      mae 0.323\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                     mape 0.09452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                     rmse 0.69243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:       time_since_restore 1.5096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:         time_this_iter_s 0.69567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:             time_total_s 1.5096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:                timestamp 1689738451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb:  View run FSR_Trainable_7a508a51 at: https://wandb.ai/seokjin/FSR-prediction/runs/7a508a51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603451)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124734-7a508a51/logs\n",
      "2023-07-19 12:47:43,206\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:43,208\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:43,210\tWARNING util.py:315 -- Processing trial results took 2.150 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:47:43,212\tWARNING util.py:315 -- The `process_trial_result` operation took 2.153 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_f084baac_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-47-28/wandb/run-20230719_124746-f084baac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Syncing run FSR_Trainable_f084baac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f084baac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                      mae 0.36755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                     mape 0.09407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                     rmse 0.69459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:       time_since_restore 32.33226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:         time_this_iter_s 0.50195\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:             time_total_s 32.33226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:                timestamp 1689738466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb:  View run FSR_Trainable_d545cc45 at: https://wandb.ai/seokjin/FSR-prediction/runs/d545cc45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602992)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124710-d545cc45/logs\n",
      "2023-07-19 12:47:56,071\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.046 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:56,074\tWARNING util.py:315 -- The `process_trial_result` operation took 2.050 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:47:56,075\tWARNING util.py:315 -- Processing trial results took 2.052 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:47:56,076\tWARNING util.py:315 -- The `process_trial_result` operation took 2.053 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_2c7ae2c4_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-47-40/wandb/run-20230719_124801-2c7ae2c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Syncing run FSR_Trainable_2c7ae2c4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2c7ae2c4\n",
      "2023-07-19 12:48:13,165\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.047 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:13,169\tWARNING util.py:315 -- The `process_trial_result` operation took 2.052 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:13,171\tWARNING util.py:315 -- Processing trial results took 2.054 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:48:13,173\tWARNING util.py:315 -- The `process_trial_result` operation took 2.056 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                      mae 0.35968\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                     mape 0.09123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                     rmse 0.68808\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:       time_since_restore 59.923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:         time_this_iter_s 0.63945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:             time_total_s 59.923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:                timestamp 1689738491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb:  View run FSR_Trainable_bb1c2a03 at: https://wandb.ai/seokjin/FSR-prediction/runs/bb1c2a03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=602805)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124702-bb1c2a03/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124801-2c7ae2c4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_08ebf9ae_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-47-52/wandb/run-20230719_124817-08ebf9ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Syncing run FSR_Trainable_08ebf9ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/08ebf9ae\n",
      "2023-07-19 12:48:33,310\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.266 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:33,313\tWARNING util.py:315 -- The `process_trial_result` operation took 3.270 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:33,314\tWARNING util.py:315 -- Processing trial results took 3.271 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:48:33,315\tWARNING util.py:315 -- The `process_trial_result` operation took 3.272 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603920)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_3a0cddfa_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-48-09/wandb/run-20230719_124836-3a0cddfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Syncing run FSR_Trainable_3a0cddfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3a0cddfa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                      mae 0.36364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                     mape 0.09342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                     rmse 0.69219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:       time_since_restore 42.49468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:         time_this_iter_s 0.61103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:             time_total_s 42.49468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:                timestamp 1689738514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb:  View run FSR_Trainable_f084baac at: https://wandb.ai/seokjin/FSR-prediction/runs/f084baac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124746-f084baac/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=603685)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124836-3a0cddfa/logs\n",
      "2023-07-19 12:48:45,341\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.785 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:45,347\tWARNING util.py:315 -- The `process_trial_result` operation took 2.791 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:45,348\tWARNING util.py:315 -- Processing trial results took 2.793 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:48:45,349\tWARNING util.py:315 -- The `process_trial_result` operation took 2.794 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604402)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_52289b67_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-48-28/wandb/run-20230719_124848-52289b67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Syncing run FSR_Trainable_52289b67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/52289b67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                      mae 0.36463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                     mape 0.09177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                     rmse 0.69707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:       time_since_restore 28.85542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:         time_this_iter_s 0.96254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:             time_total_s 28.85542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:                timestamp 1689738527\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb:  View run FSR_Trainable_08ebf9ae at: https://wandb.ai/seokjin/FSR-prediction/runs/08ebf9ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604149)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124817-08ebf9ae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124848-52289b67/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "2023-07-19 12:48:56,194\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.233 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:56,196\tWARNING util.py:315 -- The `process_trial_result` operation took 2.236 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:48:56,198\tWARNING util.py:315 -- Processing trial results took 2.238 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:48:56,202\tWARNING util.py:315 -- The `process_trial_result` operation took 2.242 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_38fc685f_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-48-42/wandb/run-20230719_124858-38fc685f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Syncing run FSR_Trainable_38fc685f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/38fc685f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                      mae 0.35792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                     mape 0.09501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                     rmse 0.69425\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:       time_since_restore 1.03449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:         time_this_iter_s 0.34055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:             time_total_s 1.03449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:                timestamp 1689738536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb:  View run FSR_Trainable_38fc685f at: https://wandb.ai/seokjin/FSR-prediction/runs/38fc685f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=604879)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124858-38fc685f/logs\n",
      "2023-07-19 12:49:05,295\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.178 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:05,301\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:05,303\tWARNING util.py:315 -- Processing trial results took 2.187 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:49:05,304\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_49470abf_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-48-53/wandb/run-20230719_124908-49470abf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Syncing run FSR_Trainable_49470abf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/49470abf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:49:13,339\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:13,344\tWARNING util.py:315 -- The `process_trial_result` operation took 2.153 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:13,346\tWARNING util.py:315 -- Processing trial results took 2.155 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:49:13,347\tWARNING util.py:315 -- The `process_trial_result` operation took 2.157 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                      mae 0.32671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                     mape 0.09264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                     rmse 0.68977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:       time_since_restore 1.54175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:         time_this_iter_s 0.51786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:             time_total_s 1.54175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:                timestamp 1689738545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb:  View run FSR_Trainable_49470abf at: https://wandb.ai/seokjin/FSR-prediction/runs/49470abf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605103)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124908-49470abf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_14ce494c_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-49-02/wandb/run-20230719_124915-14ce494c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Syncing run FSR_Trainable_14ce494c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/14ce494c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                      mae 0.34173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                     mape 0.1041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                     rmse 0.69062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:       time_since_restore 1.8377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:         time_this_iter_s 0.61362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:             time_total_s 1.8377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:                timestamp 1689738553\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb:  View run FSR_Trainable_14ce494c at: https://wandb.ai/seokjin/FSR-prediction/runs/14ce494c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605288)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124915-14ce494c/logs\n",
      "2023-07-19 12:49:22,972\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.702 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:22,977\tWARNING util.py:315 -- The `process_trial_result` operation took 3.709 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:22,979\tWARNING util.py:315 -- Processing trial results took 3.711 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:49:22,981\tWARNING util.py:315 -- The `process_trial_result` operation took 3.713 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_17171c9a_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-49-10/wandb/run-20230719_124926-17171c9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Syncing run FSR_Trainable_17171c9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/17171c9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:49:31,799\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.728 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:31,802\tWARNING util.py:315 -- The `process_trial_result` operation took 2.732 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:31,804\tWARNING util.py:315 -- Processing trial results took 2.734 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:49:31,808\tWARNING util.py:315 -- The `process_trial_result` operation took 2.738 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                      mae 0.33867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                     mape 0.09579\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                     rmse 0.68669\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:       time_since_restore 3.8407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:         time_this_iter_s 0.53548\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:             time_total_s 3.8407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:                timestamp 1689738564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb:  View run FSR_Trainable_17171c9a at: https://wandb.ai/seokjin/FSR-prediction/runs/17171c9a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605471)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124926-17171c9a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_99fe5057_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-49-17/wandb/run-20230719_124934-99fe5057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Syncing run FSR_Trainable_99fe5057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/99fe5057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                      mae 0.36506\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                     mape 0.10573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                     rmse 0.69623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:       time_since_restore 1.22919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:         time_this_iter_s 0.40836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:             time_total_s 1.22919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:                timestamp 1689738572\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb:  View run FSR_Trainable_99fe5057 at: https://wandb.ai/seokjin/FSR-prediction/runs/99fe5057\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605661)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124934-99fe5057/logs\n",
      "2023-07-19 12:49:40,520\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.200 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:40,523\tWARNING util.py:315 -- The `process_trial_result` operation took 3.210 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:40,525\tWARNING util.py:315 -- Processing trial results took 3.211 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:49:40,526\tWARNING util.py:315 -- The `process_trial_result` operation took 3.212 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_40f0ffa6_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-49-28/wandb/run-20230719_124943-40f0ffa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Syncing run FSR_Trainable_40f0ffa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/40f0ffa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:49:48,652\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.989 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:48,659\tWARNING util.py:315 -- The `process_trial_result` operation took 2.997 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:48,668\tWARNING util.py:315 -- Processing trial results took 3.006 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:49:48,688\tWARNING util.py:315 -- The `process_trial_result` operation took 3.026 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                      mae 0.36931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                     mape 0.0984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                     rmse 0.69122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:       time_since_restore 1.61476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:         time_this_iter_s 0.31664\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:             time_total_s 1.61476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:                timestamp 1689738580\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb:  View run FSR_Trainable_40f0ffa6 at: https://wandb.ai/seokjin/FSR-prediction/runs/40f0ffa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=605848)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124943-40f0ffa6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_44ce35cb_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-49-36/wandb/run-20230719_124951-44ce35cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Syncing run FSR_Trainable_44ce35cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/44ce35cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:49:56,783\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.030 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:56,788\tWARNING util.py:315 -- The `process_trial_result` operation took 3.036 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:49:56,790\tWARNING util.py:315 -- Processing trial results took 3.038 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:49:56,793\tWARNING util.py:315 -- The `process_trial_result` operation took 3.040 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                      mae 0.38772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                     mape 0.10434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                     rmse 0.7159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:       time_since_restore 0.88661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:         time_this_iter_s 0.88661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:             time_total_s 0.88661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:                timestamp 1689738585\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb:  View run FSR_Trainable_44ce35cb at: https://wandb.ai/seokjin/FSR-prediction/runs/44ce35cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606035)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_124951-44ce35cb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_306e4778_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-49-44/wandb/run-20230719_125000-306e4778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Syncing run FSR_Trainable_306e4778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/306e4778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:50:04,606\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.209 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:04,609\tWARNING util.py:315 -- The `process_trial_result` operation took 2.213 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:04,613\tWARNING util.py:315 -- Processing trial results took 2.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:50:04,617\tWARNING util.py:315 -- The `process_trial_result` operation took 2.221 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                      mae 0.33727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                     mape 0.09836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                     rmse 0.69254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:       time_since_restore 1.30799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:         time_this_iter_s 0.36612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:             time_total_s 1.30799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:                timestamp 1689738597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb:  View run FSR_Trainable_306e4778 at: https://wandb.ai/seokjin/FSR-prediction/runs/306e4778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606221)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125000-306e4778/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_b963783f_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-49-52/wandb/run-20230719_125007-b963783f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Syncing run FSR_Trainable_b963783f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b963783f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                      mae 0.37649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                     mape 0.10789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                     rmse 0.70832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:       time_since_restore 0.86386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:         time_this_iter_s 0.86386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:             time_total_s 0.86386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:                timestamp 1689738602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb:  View run FSR_Trainable_b963783f at: https://wandb.ai/seokjin/FSR-prediction/runs/b963783f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606411)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125007-b963783f/logs\n",
      "2023-07-19 12:50:15,017\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.520 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:15,024\tWARNING util.py:315 -- The `process_trial_result` operation took 2.528 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:15,027\tWARNING util.py:315 -- Processing trial results took 2.530 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:50:15,029\tWARNING util.py:315 -- The `process_trial_result` operation took 2.533 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_82c4071c_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-50-01/wandb/run-20230719_125017-82c4071c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Syncing run FSR_Trainable_82c4071c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/82c4071c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 12:50:22,301\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:22,305\tWARNING util.py:315 -- The `process_trial_result` operation took 2.232 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:22,307\tWARNING util.py:315 -- Processing trial results took 2.234 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:50:22,309\tWARNING util.py:315 -- The `process_trial_result` operation took 2.237 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                      mae 0.36545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                     mape 0.10511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                     rmse 0.69849\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:       time_since_restore 0.862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:         time_this_iter_s 0.862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:             time_total_s 0.862\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:                timestamp 1689738612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb:  View run FSR_Trainable_82c4071c at: https://wandb.ai/seokjin/FSR-prediction/runs/82c4071c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606639)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125017-82c4071c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_42976b17_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-50-11/wandb/run-20230719_125024-42976b17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Syncing run FSR_Trainable_42976b17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/42976b17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                      mae 2.08839\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                     mape 0.326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                     rmse 2.42865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:       time_since_restore 0.75666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:         time_this_iter_s 0.75666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:             time_total_s 0.75666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:                timestamp 1689738620\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb:  View run FSR_Trainable_42976b17 at: https://wandb.ai/seokjin/FSR-prediction/runs/42976b17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=606824)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125024-42976b17/logs\n",
      "2023-07-19 12:50:34,373\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.292 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:34,386\tWARNING util.py:315 -- The `process_trial_result` operation took 4.308 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:34,389\tWARNING util.py:315 -- Processing trial results took 4.310 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:50:34,391\tWARNING util.py:315 -- The `process_trial_result` operation took 4.313 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_60c199f7_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-50-19/wandb/run-20230719_125038-60c199f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Syncing run FSR_Trainable_60c199f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/60c199f7\n",
      "2023-07-19 12:50:46,022\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.920 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:46,027\tWARNING util.py:315 -- The `process_trial_result` operation took 2.926 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:50:46,028\tWARNING util.py:315 -- Processing trial results took 2.927 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:50:46,030\tWARNING util.py:315 -- The `process_trial_result` operation took 2.928 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_2057e300_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-50-28/wandb/run-20230719_125049-2057e300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Syncing run FSR_Trainable_2057e300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2057e300\n",
      "2023-07-19 12:51:00,820\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.603 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:00,825\tWARNING util.py:315 -- The `process_trial_result` operation took 2.608 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:00,827\tWARNING util.py:315 -- Processing trial results took 2.610 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:51:00,829\tWARNING util.py:315 -- The `process_trial_result` operation took 2.613 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_022d2e6c_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-50-41/wandb/run-20230719_125104-022d2e6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Syncing run FSR_Trainable_022d2e6c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/022d2e6c\n",
      "2023-07-19 12:51:17,321\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.083 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:17,324\tWARNING util.py:315 -- The `process_trial_result` operation took 3.088 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:17,326\tWARNING util.py:315 -- Processing trial results took 3.090 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:51:17,328\tWARNING util.py:315 -- The `process_trial_result` operation took 3.092 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_efef0f93_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-50-57/wandb/run-20230719_125122-efef0f93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Syncing run FSR_Trainable_efef0f93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/efef0f93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                      mae 0.32128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                     mape 0.09253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                     rmse 0.68811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:       time_since_restore 4.21659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:         time_this_iter_s 1.05672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:             time_total_s 4.21659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:                timestamp 1689738680\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb:  View run FSR_Trainable_efef0f93 at: https://wandb.ai/seokjin/FSR-prediction/runs/efef0f93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607684)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125122-efef0f93/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125104-022d2e6c/logs\n",
      "2023-07-19 12:51:38,458\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.593 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:38,461\tWARNING util.py:315 -- The `process_trial_result` operation took 2.597 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:38,464\tWARNING util.py:315 -- Processing trial results took 2.599 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:51:38,465\tWARNING util.py:315 -- The `process_trial_result` operation took 2.601 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                      mae 0.36334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                     mape 0.09616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                     rmse 0.69225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:       time_since_restore 52.28736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:         time_this_iter_s 0.87871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:             time_total_s 52.28736\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:                timestamp 1689738695\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb:  View run FSR_Trainable_60c199f7 at: https://wandb.ai/seokjin/FSR-prediction/runs/60c199f7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607050)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125038-60c199f7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_a3be7ec3_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-51-13/wandb/run-20230719_125142-a3be7ec3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Syncing run FSR_Trainable_a3be7ec3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/a3be7ec3\n",
      "2023-07-19 12:51:52,526\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.675 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:52,531\tWARNING util.py:315 -- The `process_trial_result` operation took 2.681 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:51:52,534\tWARNING util.py:315 -- Processing trial results took 2.683 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:51:52,536\tWARNING util.py:315 -- The `process_trial_result` operation took 2.685 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                      mae 0.35793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                     mape 0.09559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                     rmse 0.69212\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:       time_since_restore 54.86371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:         time_this_iter_s 0.78853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:             time_total_s 54.86371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:                timestamp 1689738709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb:  View run FSR_Trainable_2057e300 at: https://wandb.ai/seokjin/FSR-prediction/runs/2057e300\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607224)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125049-2057e300/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_0aeb87c0_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-51-35/wandb/run-20230719_125156-0aeb87c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Syncing run FSR_Trainable_0aeb87c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0aeb87c0\n",
      "2023-07-19 12:52:06,219\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.657 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:52:06,222\tWARNING util.py:315 -- The `process_trial_result` operation took 2.661 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:52:06,223\tWARNING util.py:315 -- Processing trial results took 2.662 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:52:06,224\tWARNING util.py:315 -- The `process_trial_result` operation took 2.664 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_24440a9b_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-07-19_12-51-49/wandb/run-20230719_125209-24440a9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Syncing run FSR_Trainable_24440a9b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/24440a9b\n",
      "2023-07-19 12:52:21,247\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.518 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:52:21,251\tWARNING util.py:315 -- The `process_trial_result` operation took 2.523 s, which may be a performance bottleneck.\n",
      "2023-07-19 12:52:21,253\tWARNING util.py:315 -- Processing trial results took 2.525 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 12:52:21,255\tWARNING util.py:315 -- The `process_trial_result` operation took 2.527 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-19_12-32-34/FSR_Trainable_e9ff9cd3_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-07-19_12-52-02/wandb/run-20230719_125224-e9ff9cd3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Syncing run FSR_Trainable_e9ff9cd3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9ff9cd3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                      mae 0.36679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                     mape 0.09743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                     rmse 0.70017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:       time_since_restore 0.83509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:         time_this_iter_s 0.83509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:             time_total_s 0.83509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:                timestamp 1689738738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb:  View run FSR_Trainable_e9ff9cd3 at: https://wandb.ai/seokjin/FSR-prediction/runs/e9ff9cd3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608648)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125224-e9ff9cd3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                      mae 0.35915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                     mape 0.09369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                     rmse 0.69149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:       time_since_restore 46.03801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:         time_this_iter_s 0.6215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:             time_total_s 46.03801\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:                timestamp 1689738752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb:  View run FSR_Trainable_a3be7ec3 at: https://wandb.ai/seokjin/FSR-prediction/runs/a3be7ec3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=607940)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125142-a3be7ec3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                      mae 0.34125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                     mape 0.09844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                     rmse 0.68126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:       time_since_restore 47.2934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:         time_this_iter_s 0.48867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:             time_total_s 47.2934\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:                timestamp 1689738767\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb:  View run FSR_Trainable_0aeb87c0 at: https://wandb.ai/seokjin/FSR-prediction/runs/0aeb87c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608184)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125156-0aeb87c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=608418)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_125209-24440a9b/logs\n",
      "2023-07-19 12:52:58,201\tINFO tune.py:1111 -- Total run time: 1220.23 seconds (1215.67 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
