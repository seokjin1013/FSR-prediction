{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2\n",
    "\n",
    "Index_X = FSR_for_force\n",
    "\n",
    "Index_y = force\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-08-11_03-36-28/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-08-11_03-36-28\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "200.127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force'],\n",
    "        'index_y': ['force'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-11 05:09:34,074] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search \n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='metric',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 05:09:36,238\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-08-11 05:09:37,987\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-11 05:28:38</td></tr>\n",
       "<tr><td>Running for: </td><td>00:19:00.63        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.9/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -0.07165064470438755 | Iter 32.000: -0.0725315087819599 | Iter 16.000: -0.07271977613189878 | Iter 8.000: -0.07614157579515977 | Iter 4.000: -0.08604865640152404 | Iter 2.000: -0.10579927953950136 | Iter 1.000: -0.13877393070807365<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>index_X          </th><th>index_y  </th><th>model        </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  trmse_force</th><th style=\"text-align: right;\">   tmape_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_066a794c</td><td>TERMINATED</td><td>172.26.215.93:249052</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00046296 </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       84.7024  </td><td style=\"text-align: right;\">   0.302529 </td><td style=\"text-align: right;\">    0.542226 </td><td style=\"text-align: right;\">   1.79636    </td></tr>\n",
       "<tr><td>FSR_Trainable_7dbc4a1e</td><td>TERMINATED</td><td>172.26.215.93:249122</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00524338 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       89.3008  </td><td style=\"text-align: right;\">   0.0560612</td><td style=\"text-align: right;\">    0.0904434</td><td style=\"text-align: right;\">   6.37317e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_4226873b</td><td>TERMINATED</td><td>172.26.215.93:249294</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00851046 </td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.772551</td><td style=\"text-align: right;\">   1.45985  </td><td style=\"text-align: right;\">    3.90003  </td><td style=\"text-align: right;\">   1.19374e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_9bf7eb16</td><td>TERMINATED</td><td>172.26.215.93:249468</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000139785</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       78.0229  </td><td style=\"text-align: right;\">   0.0412636</td><td style=\"text-align: right;\">    0.0690789</td><td style=\"text-align: right;\">   4.52468e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_701b36be</td><td>TERMINATED</td><td>172.26.215.93:249774</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        2.63562e-05</td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.942149</td><td style=\"text-align: right;\">   0.769007 </td><td style=\"text-align: right;\">    1.21279  </td><td style=\"text-align: right;\">   2.08017    </td></tr>\n",
       "<tr><td>FSR_Trainable_44c0d485</td><td>TERMINATED</td><td>172.26.215.93:250011</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0201904  </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.40346 </td><td style=\"text-align: right;\">   0.425797 </td><td style=\"text-align: right;\">    0.660663 </td><td style=\"text-align: right;\">   2.40597    </td></tr>\n",
       "<tr><td>FSR_Trainable_b5687929</td><td>TERMINATED</td><td>172.26.215.93:250243</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0348678  </td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.946659</td><td style=\"text-align: right;\">   1.56477  </td><td style=\"text-align: right;\">    3.67662  </td><td style=\"text-align: right;\">   1.74719e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_576c6c49</td><td>TERMINATED</td><td>172.26.215.93:250473</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00517268 </td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.13852 </td><td style=\"text-align: right;\">   1.84047  </td><td style=\"text-align: right;\">    4.74584  </td><td style=\"text-align: right;\">   1.37206e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_17d11d30</td><td>TERMINATED</td><td>172.26.215.93:250702</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0120846  </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.5798  </td><td style=\"text-align: right;\">   0.407918 </td><td style=\"text-align: right;\">    0.751758 </td><td style=\"text-align: right;\">   2.16413    </td></tr>\n",
       "<tr><td>FSR_Trainable_2c2d0384</td><td>TERMINATED</td><td>172.26.215.93:250939</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0373808  </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.75518 </td><td style=\"text-align: right;\">   2.53357  </td><td style=\"text-align: right;\">    3.80998  </td><td style=\"text-align: right;\">  15.6028     </td></tr>\n",
       "<tr><td>FSR_Trainable_b4ab27a4</td><td>TERMINATED</td><td>172.26.215.93:251154</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.02263    </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.42276 </td><td style=\"text-align: right;\"> 601.589    </td><td style=\"text-align: right;\"> 1019.64     </td><td style=\"text-align: right;\">3839.34       </td></tr>\n",
       "<tr><td>FSR_Trainable_f9e02361</td><td>TERMINATED</td><td>172.26.215.93:251389</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000147586</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.99218 </td><td style=\"text-align: right;\">   0.153425 </td><td style=\"text-align: right;\">    0.194588 </td><td style=\"text-align: right;\">   2.97621e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_140519ef</td><td>TERMINATED</td><td>172.26.215.93:251643</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000640004</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       80.3434  </td><td style=\"text-align: right;\">   0.041688 </td><td style=\"text-align: right;\">    0.0702235</td><td style=\"text-align: right;\">   4.40195e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_1e9c13e0</td><td>TERMINATED</td><td>172.26.215.93:251732</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00185289 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       82.128   </td><td style=\"text-align: right;\">   0.0463165</td><td style=\"text-align: right;\">    0.0773766</td><td style=\"text-align: right;\">   5.07666e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_12d5687d</td><td>TERMINATED</td><td>172.26.215.93:251909</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00177988 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       53.6394  </td><td style=\"text-align: right;\">   0.0477783</td><td style=\"text-align: right;\">    0.079045 </td><td style=\"text-align: right;\">   5.41892e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_ad27d160</td><td>TERMINATED</td><td>172.26.215.93:252212</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00033329 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.62041 </td><td style=\"text-align: right;\">   0.0639394</td><td style=\"text-align: right;\">    0.099359 </td><td style=\"text-align: right;\">   9.51491e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_4712e9ed</td><td>TERMINATED</td><td>172.26.215.93:252470</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0025474  </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       12.1235  </td><td style=\"text-align: right;\">   0.0551115</td><td style=\"text-align: right;\">    0.0855795</td><td style=\"text-align: right;\">   7.9372e+13 </td></tr>\n",
       "<tr><td>FSR_Trainable_1a10a85b</td><td>TERMINATED</td><td>172.26.215.93:252730</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00161898 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       80.2348  </td><td style=\"text-align: right;\">   0.0441611</td><td style=\"text-align: right;\">    0.0741876</td><td style=\"text-align: right;\">   4.73322e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_f7f51bca</td><td>TERMINATED</td><td>172.26.215.93:252968</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00152929 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       80.4897  </td><td style=\"text-align: right;\">   0.04484  </td><td style=\"text-align: right;\">    0.0746233</td><td style=\"text-align: right;\">   4.85148e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_69867db4</td><td>TERMINATED</td><td>172.26.215.93:253243</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00104441 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       81.7701  </td><td style=\"text-align: right;\">   0.0422479</td><td style=\"text-align: right;\">    0.0715563</td><td style=\"text-align: right;\">   4.39387e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_ea42184f</td><td>TERMINATED</td><td>172.26.215.93:253447</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        7.9549e-05 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.957803</td><td style=\"text-align: right;\">   0.258023 </td><td style=\"text-align: right;\">    0.320983 </td><td style=\"text-align: right;\">   4.15169e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_03c3dfec</td><td>TERMINATED</td><td>172.26.215.93:253685</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        7.74613e-05</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.07117 </td><td style=\"text-align: right;\">   0.133545 </td><td style=\"text-align: right;\">    0.174838 </td><td style=\"text-align: right;\">   2.69508e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_7eecfc37</td><td>TERMINATED</td><td>172.26.215.93:253932</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000585966</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.9758  </td><td style=\"text-align: right;\">   0.10131  </td><td style=\"text-align: right;\">    0.143131 </td><td style=\"text-align: right;\">   1.82737e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_86266f50</td><td>TERMINATED</td><td>172.26.215.93:254170</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000687746</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       83.4918  </td><td style=\"text-align: right;\">   0.0416671</td><td style=\"text-align: right;\">    0.0706926</td><td style=\"text-align: right;\">   4.29733e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_e880e411</td><td>TERMINATED</td><td>172.26.215.93:254459</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        1.33344e-05</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.39108 </td><td style=\"text-align: right;\">   0.161571 </td><td style=\"text-align: right;\">    0.228659 </td><td style=\"text-align: right;\">   2.41639e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_84973430</td><td>TERMINATED</td><td>172.26.215.93:254667</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000904399</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       82.3411  </td><td style=\"text-align: right;\">   0.042137 </td><td style=\"text-align: right;\">    0.0709367</td><td style=\"text-align: right;\">   4.47847e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_3ce133e1</td><td>TERMINATED</td><td>172.26.215.93:254886</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000825782</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       81.0078  </td><td style=\"text-align: right;\">   0.0424363</td><td style=\"text-align: right;\">    0.0714944</td><td style=\"text-align: right;\">   4.39931e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_2f18c218</td><td>TERMINATED</td><td>172.26.215.93:255154</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0890658  </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.27287 </td><td style=\"text-align: right;\">   1.02852  </td><td style=\"text-align: right;\">    1.53123  </td><td style=\"text-align: right;\">   2.36924e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_5d427b91</td><td>TERMINATED</td><td>172.26.215.93:255383</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000727053</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.94662 </td><td style=\"text-align: right;\">   0.0693117</td><td style=\"text-align: right;\">    0.105799 </td><td style=\"text-align: right;\">   1.05136e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_0f05f885</td><td>TERMINATED</td><td>172.26.215.93:255627</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000759944</td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.94211 </td><td style=\"text-align: right;\">   1.98122  </td><td style=\"text-align: right;\">    4.96632  </td><td style=\"text-align: right;\">   1.55767e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_43b74dad</td><td>TERMINATED</td><td>172.26.215.93:255862</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000195886</td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.971685</td><td style=\"text-align: right;\">   3.19431  </td><td style=\"text-align: right;\">    6.99051  </td><td style=\"text-align: right;\">   1.89757e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_7debcdf3</td><td>TERMINATED</td><td>172.26.215.93:256093</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00027843 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.877184</td><td style=\"text-align: right;\">   0.259698 </td><td style=\"text-align: right;\">    0.315566 </td><td style=\"text-align: right;\">   5.25336e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_37accafc</td><td>TERMINATED</td><td>172.26.215.93:256314</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000350786</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.15204 </td><td style=\"text-align: right;\">   0.148958 </td><td style=\"text-align: right;\">    0.211409 </td><td style=\"text-align: right;\">   2.04576e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_cc8df67d</td><td>TERMINATED</td><td>172.26.215.93:256545</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00048176 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.09357 </td><td style=\"text-align: right;\">   0.115234 </td><td style=\"text-align: right;\">    0.165141 </td><td style=\"text-align: right;\">   1.84543e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_f719134f</td><td>TERMINATED</td><td>172.26.215.93:256798</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000428361</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.92012 </td><td style=\"text-align: right;\">   0.0879727</td><td style=\"text-align: right;\">    0.13786  </td><td style=\"text-align: right;\">   1.33343e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_4035f2d3</td><td>TERMINATED</td><td>172.26.215.93:257032</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00330453 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.22026 </td><td style=\"text-align: right;\">   0.0582975</td><td style=\"text-align: right;\">    0.0942032</td><td style=\"text-align: right;\">   7.13911e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_891b6b9f</td><td>TERMINATED</td><td>172.26.215.93:257120</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000145665</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.00434 </td><td style=\"text-align: right;\">   0.0898181</td><td style=\"text-align: right;\">    0.143273 </td><td style=\"text-align: right;\">   1.30631e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_461f3c40</td><td>TERMINATED</td><td>172.26.215.93:257299</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000141727</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.63162 </td><td style=\"text-align: right;\">   0.0845507</td><td style=\"text-align: right;\">    0.130151 </td><td style=\"text-align: right;\">   1.34028e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_87366ad5</td><td>TERMINATED</td><td>172.26.215.93:257484</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00443385 </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.41831 </td><td style=\"text-align: right;\">   0.310264 </td><td style=\"text-align: right;\">    0.562654 </td><td style=\"text-align: right;\">   1.95593    </td></tr>\n",
       "<tr><td>FSR_Trainable_d1434ee3</td><td>TERMINATED</td><td>172.26.215.93:257803</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00119282 </td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.40397 </td><td style=\"text-align: right;\">   1.94162  </td><td style=\"text-align: right;\">    5.10449  </td><td style=\"text-align: right;\">   1.10359e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_405a9de5</td><td>TERMINATED</td><td>172.26.215.93:257898</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00108783 </td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.49952 </td><td style=\"text-align: right;\">   1.84587  </td><td style=\"text-align: right;\">    4.90812  </td><td style=\"text-align: right;\">   1.07307e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_972b67b6</td><td>TERMINATED</td><td>172.26.215.93:258228</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00101333 </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.75564 </td><td style=\"text-align: right;\">   0.388989 </td><td style=\"text-align: right;\">    0.693979 </td><td style=\"text-align: right;\">   1.95147    </td></tr>\n",
       "<tr><td>FSR_Trainable_1b8c9dd5</td><td>TERMINATED</td><td>172.26.215.93:258321</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000737987</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       80.9922  </td><td style=\"text-align: right;\">   0.0422561</td><td style=\"text-align: right;\">    0.0708448</td><td style=\"text-align: right;\">   4.51797e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_6d759f9e</td><td>TERMINATED</td><td>172.26.215.93:258500</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00058972 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.22382 </td><td style=\"text-align: right;\">   0.0586808</td><td style=\"text-align: right;\">    0.0917419</td><td style=\"text-align: right;\">   8.20048e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_3f62f2b8</td><td>TERMINATED</td><td>172.26.215.93:258681</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000665831</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       83.8791  </td><td style=\"text-align: right;\">   0.0417671</td><td style=\"text-align: right;\">    0.0705685</td><td style=\"text-align: right;\">   4.38952e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_4d669d51</td><td>TERMINATED</td><td>172.26.215.93:258990</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000719643</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.81211 </td><td style=\"text-align: right;\">   0.0642206</td><td style=\"text-align: right;\">    0.107008 </td><td style=\"text-align: right;\">   7.6158e+13 </td></tr>\n",
       "<tr><td>FSR_Trainable_21cd5e27</td><td>TERMINATED</td><td>172.26.215.93:259211</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00298463 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        7.18334 </td><td style=\"text-align: right;\">   0.0484175</td><td style=\"text-align: right;\">    0.0785875</td><td style=\"text-align: right;\">   6.13416e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_ae6f907f</td><td>TERMINATED</td><td>172.26.215.93:259452</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00246007 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.16567 </td><td style=\"text-align: right;\">   0.062523 </td><td style=\"text-align: right;\">    0.106157 </td><td style=\"text-align: right;\">   5.81034e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_fed368c0</td><td>TERMINATED</td><td>172.26.215.93:259679</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000301834</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       81.2409  </td><td style=\"text-align: right;\">   0.0419013</td><td style=\"text-align: right;\">    0.0704898</td><td style=\"text-align: right;\">   4.39956e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_71d25cbf</td><td>TERMINATED</td><td>172.26.215.93:259932</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0065469  </td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.15688 </td><td style=\"text-align: right;\">   0.488153 </td><td style=\"text-align: right;\">    0.805558 </td><td style=\"text-align: right;\">   2.37171    </td></tr>\n",
       "<tr><td>FSR_Trainable_f9f5f046</td><td>TERMINATED</td><td>172.26.215.93:260160</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000265052</td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.25605 </td><td style=\"text-align: right;\">   0.398659 </td><td style=\"text-align: right;\">    0.664579 </td><td style=\"text-align: right;\">   2.11018    </td></tr>\n",
       "<tr><td>FSR_Trainable_350a53a7</td><td>TERMINATED</td><td>172.26.215.93:260404</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000287442</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.92851 </td><td style=\"text-align: right;\">   0.0854025</td><td style=\"text-align: right;\">    0.134573 </td><td style=\"text-align: right;\">   1.30201e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_05707371</td><td>TERMINATED</td><td>172.26.215.93:260637</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0003981  </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.93254 </td><td style=\"text-align: right;\">   0.0720448</td><td style=\"text-align: right;\">    0.11614  </td><td style=\"text-align: right;\">   1.02808e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_22c63583</td><td>TERMINATED</td><td>172.26.215.93:260888</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000505308</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.58021 </td><td style=\"text-align: right;\">   0.0727988</td><td style=\"text-align: right;\">    0.11719  </td><td style=\"text-align: right;\">   1.00015e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_816e65ef</td><td>TERMINATED</td><td>172.26.215.93:261135</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00147613 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        6.35228 </td><td style=\"text-align: right;\">   0.0516845</td><td style=\"text-align: right;\">    0.0809611</td><td style=\"text-align: right;\">   7.38421e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_d502195a</td><td>TERMINATED</td><td>172.26.215.93:261359</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00143552 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        5.39616 </td><td style=\"text-align: right;\">   0.0492345</td><td style=\"text-align: right;\">    0.0798937</td><td style=\"text-align: right;\">   6.41256e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_84dd00bc</td><td>TERMINATED</td><td>172.26.215.93:261586</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00203649 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        7.23032 </td><td style=\"text-align: right;\">   0.0512643</td><td style=\"text-align: right;\">    0.0801731</td><td style=\"text-align: right;\">   7.22801e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_48404c5c</td><td>TERMINATED</td><td>172.26.215.93:261809</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00196904 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.10006 </td><td style=\"text-align: right;\">   0.124324 </td><td style=\"text-align: right;\">    0.175015 </td><td style=\"text-align: right;\">   1.98075e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_70ed32b7</td><td>TERMINATED</td><td>172.26.215.93:262066</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000514855</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.980382</td><td style=\"text-align: right;\">   0.138407 </td><td style=\"text-align: right;\">    0.190241 </td><td style=\"text-align: right;\">   1.95608e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_11597b57</td><td>TERMINATED</td><td>172.26.215.93:262146</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000192716</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.91121 </td><td style=\"text-align: right;\">   0.0714123</td><td style=\"text-align: right;\">    0.121577 </td><td style=\"text-align: right;\">   8.55344e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_d4d2b4e8</td><td>TERMINATED</td><td>172.26.215.93:262324</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000215274</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.95009 </td><td style=\"text-align: right;\">   0.0732465</td><td style=\"text-align: right;\">    0.119473 </td><td style=\"text-align: right;\">   9.7177e+13 </td></tr>\n",
       "<tr><td>FSR_Trainable_f521736c</td><td>TERMINATED</td><td>172.26.215.93:262508</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        8.82182e-05</td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.42404 </td><td style=\"text-align: right;\">   1.81966  </td><td style=\"text-align: right;\">    5.04244  </td><td style=\"text-align: right;\">   8.17121e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_e3a71d59</td><td>TERMINATED</td><td>172.26.215.93:262830</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000978763</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.80557 </td><td style=\"text-align: right;\">   0.0925322</td><td style=\"text-align: right;\">    0.139821 </td><td style=\"text-align: right;\">   1.53481e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_d06442e2</td><td>TERMINATED</td><td>172.26.215.93:262914</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000842702</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.14272 </td><td style=\"text-align: right;\">   0.0838368</td><td style=\"text-align: right;\">    0.151322 </td><td style=\"text-align: right;\">   7.29915e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_4109b910</td><td>TERMINATED</td><td>172.26.215.93:263226</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000366371</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.75508 </td><td style=\"text-align: right;\">   0.0759073</td><td style=\"text-align: right;\">    0.13032  </td><td style=\"text-align: right;\">   9.08222e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_d759cdd5</td><td>TERMINATED</td><td>172.26.215.93:263316</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000360866</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.88648 </td><td style=\"text-align: right;\">   0.0794289</td><td style=\"text-align: right;\">    0.130788 </td><td style=\"text-align: right;\">   1.1043e+14 </td></tr>\n",
       "<tr><td>FSR_Trainable_dbe2f24c</td><td>TERMINATED</td><td>172.26.215.93:263631</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000630503</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.92331 </td><td style=\"text-align: right;\">   0.0676761</td><td style=\"text-align: right;\">    0.108655 </td><td style=\"text-align: right;\">   9.16203e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_5ed4e384</td><td>TERMINATED</td><td>172.26.215.93:263723</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000607303</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.19451 </td><td style=\"text-align: right;\">   0.0517762</td><td style=\"text-align: right;\">    0.0878513</td><td style=\"text-align: right;\">   5.59636e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_92ba28cc</td><td>TERMINATED</td><td>172.26.215.93:264036</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000661407</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.234   </td><td style=\"text-align: right;\">   0.112215 </td><td style=\"text-align: right;\">    0.16798  </td><td style=\"text-align: right;\">   1.76055e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_10045936</td><td>TERMINATED</td><td>172.26.215.93:264129</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000461544</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.13052 </td><td style=\"text-align: right;\">   0.198258 </td><td style=\"text-align: right;\">    0.275953 </td><td style=\"text-align: right;\">   2.98802e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_61601394</td><td>TERMINATED</td><td>172.26.215.93:264312</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000429075</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.12172 </td><td style=\"text-align: right;\">   0.116116 </td><td style=\"text-align: right;\">    0.173664 </td><td style=\"text-align: right;\">   1.50918e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_48d91995</td><td>TERMINATED</td><td>172.26.215.93:264629</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00139888 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        4.09957 </td><td style=\"text-align: right;\">   0.0583976</td><td style=\"text-align: right;\">    0.093891 </td><td style=\"text-align: right;\">   6.72684e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_2eda9114</td><td>TERMINATED</td><td>172.26.215.93:264718</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000821896</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       87.779   </td><td style=\"text-align: right;\">   0.0418759</td><td style=\"text-align: right;\">    0.0710262</td><td style=\"text-align: right;\">   4.30276e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_c7fcd88e</td><td>TERMINATED</td><td>172.26.215.93:265034</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000849427</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       88.6218  </td><td style=\"text-align: right;\">   0.0421582</td><td style=\"text-align: right;\">    0.0710329</td><td style=\"text-align: right;\">   4.4442e+13 </td></tr>\n",
       "<tr><td>FSR_Trainable_788c3fca</td><td>TERMINATED</td><td>172.26.215.93:265124</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000825042</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.86893 </td><td style=\"text-align: right;\">   0.0584193</td><td style=\"text-align: right;\">    0.0911966</td><td style=\"text-align: right;\">   8.38975e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_39f6fe44</td><td>TERMINATED</td><td>172.26.215.93:265431</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000965948</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       16.3547  </td><td style=\"text-align: right;\">   0.042415 </td><td style=\"text-align: right;\">    0.0730533</td><td style=\"text-align: right;\">   4.29716e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_ce34eb03</td><td>TERMINATED</td><td>172.26.215.93:265657</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000961412</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       32.8927  </td><td style=\"text-align: right;\">   0.0431565</td><td style=\"text-align: right;\">    0.0728557</td><td style=\"text-align: right;\">   4.47016e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_98ec7841</td><td>TERMINATED</td><td>172.26.215.93:266164</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000955142</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       30.6326  </td><td style=\"text-align: right;\">   0.042847 </td><td style=\"text-align: right;\">    0.0728378</td><td style=\"text-align: right;\">   4.35873e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_072f7c51</td><td>TERMINATED</td><td>172.26.215.93:266411</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00125839 </td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.13233 </td><td style=\"text-align: right;\">   1.62005  </td><td style=\"text-align: right;\">    4.50291  </td><td style=\"text-align: right;\">   1.00964e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_b3901d26</td><td>TERMINATED</td><td>172.26.215.93:266610</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00128189 </td><td>sklearn.preproc_ced0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.976343</td><td style=\"text-align: right;\">   1.85988  </td><td style=\"text-align: right;\">    5.06583  </td><td style=\"text-align: right;\">   8.78693e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_537b7970</td><td>TERMINATED</td><td>172.26.215.93:266858</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000678357</td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.13974 </td><td style=\"text-align: right;\">   0.625094 </td><td style=\"text-align: right;\">    0.990651 </td><td style=\"text-align: right;\">   1.52485    </td></tr>\n",
       "<tr><td>FSR_Trainable_988be0f4</td><td>TERMINATED</td><td>172.26.215.93:267106</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000540837</td><td>sklearn.preproc_ce10</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.80263 </td><td style=\"text-align: right;\">   0.379179 </td><td style=\"text-align: right;\">    0.643209 </td><td style=\"text-align: right;\">   2.01335    </td></tr>\n",
       "<tr><td>FSR_Trainable_5d520eab</td><td>TERMINATED</td><td>172.26.215.93:267224</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000544582</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.14832 </td><td style=\"text-align: right;\">   0.0614922</td><td style=\"text-align: right;\">    0.106046 </td><td style=\"text-align: right;\">   6.73448e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_2e7e4879</td><td>TERMINATED</td><td>172.26.215.93:267424</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000311561</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.25537 </td><td style=\"text-align: right;\">   0.0710424</td><td style=\"text-align: right;\">    0.117212 </td><td style=\"text-align: right;\">   9.66491e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_76a437b0</td><td>TERMINATED</td><td>172.26.215.93:267621</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000289812</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.95523 </td><td style=\"text-align: right;\">   0.0779973</td><td style=\"text-align: right;\">    0.122849 </td><td style=\"text-align: right;\">   1.18566e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_8f5659b5</td><td>TERMINATED</td><td>172.26.215.93:267822</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000774563</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.33677 </td><td style=\"text-align: right;\">   0.0648362</td><td style=\"text-align: right;\">    0.106597 </td><td style=\"text-align: right;\">   8.24642e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_8067426e</td><td>TERMINATED</td><td>172.26.215.93:268035</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000747873</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       69.6883  </td><td style=\"text-align: right;\">   0.0421488</td><td style=\"text-align: right;\">    0.0702086</td><td style=\"text-align: right;\">   4.59168e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_c9309d56</td><td>TERMINATED</td><td>172.26.215.93:268247</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000399214</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.48554 </td><td style=\"text-align: right;\">   0.0794544</td><td style=\"text-align: right;\">    0.12499  </td><td style=\"text-align: right;\">   1.17995e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_dbec4dcf</td><td>TERMINATED</td><td>172.26.215.93:268474</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00186624 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       13.2532  </td><td style=\"text-align: right;\">   0.0436019</td><td style=\"text-align: right;\">    0.0742187</td><td style=\"text-align: right;\">   4.54507e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_1a9699d3</td><td>TERMINATED</td><td>172.26.215.93:268796</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00155165 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.97006 </td><td style=\"text-align: right;\">   0.12563  </td><td style=\"text-align: right;\">    0.200032 </td><td style=\"text-align: right;\">   1.05016e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_28609f96</td><td>TERMINATED</td><td>172.26.215.93:269016</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00175981 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.10674 </td><td style=\"text-align: right;\">   0.12385  </td><td style=\"text-align: right;\">    0.186017 </td><td style=\"text-align: right;\">   1.51543e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_12e8ba4d</td><td>TERMINATED</td><td>172.26.215.93:269255</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00119708 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.80479 </td><td style=\"text-align: right;\">   0.0725862</td><td style=\"text-align: right;\">    0.125945 </td><td style=\"text-align: right;\">   8.47331e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_27ced3c6</td><td>TERMINATED</td><td>172.26.215.93:269478</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00114268 </td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.02776 </td><td style=\"text-align: right;\">   0.133243 </td><td style=\"text-align: right;\">    0.1789   </td><td style=\"text-align: right;\">   2.40534e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_c7d66c81</td><td>TERMINATED</td><td>172.26.215.93:269719</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000670247</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       68.9551  </td><td style=\"text-align: right;\">   0.042169 </td><td style=\"text-align: right;\">    0.0711004</td><td style=\"text-align: right;\">   4.40945e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_854bb80b</td><td>TERMINATED</td><td>172.26.215.93:269943</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000720052</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        4.39825 </td><td style=\"text-align: right;\">   0.0557921</td><td style=\"text-align: right;\">    0.0878811</td><td style=\"text-align: right;\">   7.72161e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_e2a4c7eb</td><td>TERMINATED</td><td>172.26.215.93:270160</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000783662</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        5.22745 </td><td style=\"text-align: right;\">   0.0558343</td><td style=\"text-align: right;\">    0.0862344</td><td style=\"text-align: right;\">   7.9434e+13 </td></tr>\n",
       "<tr><td>FSR_Trainable_87c5d6ce</td><td>TERMINATED</td><td>172.26.215.93:270401</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000499608</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.07698 </td><td style=\"text-align: right;\">   0.120139 </td><td style=\"text-align: right;\">    0.206667 </td><td style=\"text-align: right;\">   1.14302e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_30951e7e</td><td>TERMINATED</td><td>172.26.215.93:270640</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000494052</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.905593</td><td style=\"text-align: right;\">   0.0927812</td><td style=\"text-align: right;\">    0.145176 </td><td style=\"text-align: right;\">   1.49501e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_f7d46d05</td><td>TERMINATED</td><td>172.26.215.93:270869</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000415727</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.18027 </td><td style=\"text-align: right;\">   0.0710396</td><td style=\"text-align: right;\">    0.118345 </td><td style=\"text-align: right;\">   9.07133e+13</td></tr>\n",
       "<tr><td>FSR_Trainable_090f5019</td><td>TERMINATED</td><td>172.26.215.93:271105</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_cf30</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000595042</td><td>sklearn.preproc_ce70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        5.17846 </td><td style=\"text-align: right;\">   0.0465737</td><td style=\"text-align: right;\">    0.0761416</td><td style=\"text-align: right;\">   5.11471e+13</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-11 05:09:38,046\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  mae_force</th><th style=\"text-align: right;\">  mape_force</th><th style=\"text-align: right;\">      metric</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">  rmse_force</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">   tmape_force</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th><th style=\"text-align: right;\">  trmse_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_03c3dfec</td><td>2023-08-11_05-14-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    311.648</td><td style=\"text-align: right;\"> 4.92432e+17</td><td style=\"text-align: right;\">   0.174838 </td><td>172.26.215.93</td><td style=\"text-align: right;\">253685</td><td style=\"text-align: right;\">     501.278</td><td style=\"text-align: right;\">            2.07117 </td><td style=\"text-align: right;\">          1.07559 </td><td style=\"text-align: right;\">      2.07117 </td><td style=\"text-align: right;\"> 1691698458</td><td style=\"text-align: right;\">   0.133545 </td><td style=\"text-align: right;\">   2.69508e+14</td><td style=\"text-align: right;\">                   2</td><td>03c3dfec  </td><td style=\"text-align: right;\">    0.174838 </td></tr>\n",
       "<tr><td>FSR_Trainable_05707371</td><td>2023-08-11_05-20-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    201.99 </td><td style=\"text-align: right;\"> 2.74243e+17</td><td style=\"text-align: right;\">   0.11614  </td><td>172.26.215.93</td><td style=\"text-align: right;\">260637</td><td style=\"text-align: right;\">     366.755</td><td style=\"text-align: right;\">            1.93254 </td><td style=\"text-align: right;\">          0.879484</td><td style=\"text-align: right;\">      1.93254 </td><td style=\"text-align: right;\"> 1691698818</td><td style=\"text-align: right;\">   0.0720448</td><td style=\"text-align: right;\">   1.02808e+14</td><td style=\"text-align: right;\">                   2</td><td>05707371  </td><td style=\"text-align: right;\">    0.11614  </td></tr>\n",
       "<tr><td>FSR_Trainable_066a794c</td><td>2023-08-11_05-11-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    106.639</td><td style=\"text-align: right;\"> 2.15676e+08</td><td style=\"text-align: right;\">   0.542226 </td><td>172.26.215.93</td><td style=\"text-align: right;\">249052</td><td style=\"text-align: right;\">     203.433</td><td style=\"text-align: right;\">           84.7024  </td><td style=\"text-align: right;\">          0.75792 </td><td style=\"text-align: right;\">     84.7024  </td><td style=\"text-align: right;\"> 1691698285</td><td style=\"text-align: right;\">   0.302529 </td><td style=\"text-align: right;\">   1.79636    </td><td style=\"text-align: right;\">                 100</td><td>066a794c  </td><td style=\"text-align: right;\">    0.542226 </td></tr>\n",
       "<tr><td>FSR_Trainable_072f7c51</td><td>2023-08-11_05-24-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    215.311</td><td style=\"text-align: right;\"> 2.2081e+16 </td><td style=\"text-align: right;\">   4.50291  </td><td>172.26.215.93</td><td style=\"text-align: right;\">266411</td><td style=\"text-align: right;\">     445.292</td><td style=\"text-align: right;\">            1.13233 </td><td style=\"text-align: right;\">          1.13233 </td><td style=\"text-align: right;\">      1.13233 </td><td style=\"text-align: right;\"> 1691699072</td><td style=\"text-align: right;\">   1.62005  </td><td style=\"text-align: right;\">   1.00964e+15</td><td style=\"text-align: right;\">                   1</td><td>072f7c51  </td><td style=\"text-align: right;\">    4.50291  </td></tr>\n",
       "<tr><td>FSR_Trainable_090f5019</td><td>2023-08-11_05-28-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    124.241</td><td style=\"text-align: right;\"> 1.13881e+17</td><td style=\"text-align: right;\">   0.0761416</td><td>172.26.215.93</td><td style=\"text-align: right;\">271105</td><td style=\"text-align: right;\">     235.983</td><td style=\"text-align: right;\">            5.17846 </td><td style=\"text-align: right;\">          0.491376</td><td style=\"text-align: right;\">      5.17846 </td><td style=\"text-align: right;\"> 1691699310</td><td style=\"text-align: right;\">   0.0465737</td><td style=\"text-align: right;\">   5.11471e+13</td><td style=\"text-align: right;\">                   8</td><td>090f5019  </td><td style=\"text-align: right;\">    0.0761416</td></tr>\n",
       "<tr><td>FSR_Trainable_0f05f885</td><td>2023-08-11_05-16-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    268.35 </td><td style=\"text-align: right;\"> 2.72339e+16</td><td style=\"text-align: right;\">   4.96632  </td><td>172.26.215.93</td><td style=\"text-align: right;\">255627</td><td style=\"text-align: right;\">     552.389</td><td style=\"text-align: right;\">            1.94211 </td><td style=\"text-align: right;\">          1.94211 </td><td style=\"text-align: right;\">      1.94211 </td><td style=\"text-align: right;\"> 1691698575</td><td style=\"text-align: right;\">   1.98122  </td><td style=\"text-align: right;\">   1.55767e+15</td><td style=\"text-align: right;\">                   1</td><td>0f05f885  </td><td style=\"text-align: right;\">    4.96632  </td></tr>\n",
       "<tr><td>FSR_Trainable_10045936</td><td>2023-08-11_05-22-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    592.785</td><td style=\"text-align: right;\"> 7.53553e+17</td><td style=\"text-align: right;\">   0.275953 </td><td>172.26.215.93</td><td style=\"text-align: right;\">264129</td><td style=\"text-align: right;\">     995.626</td><td style=\"text-align: right;\">            1.13052 </td><td style=\"text-align: right;\">          1.13052 </td><td style=\"text-align: right;\">      1.13052 </td><td style=\"text-align: right;\"> 1691698953</td><td style=\"text-align: right;\">   0.198258 </td><td style=\"text-align: right;\">   2.98802e+14</td><td style=\"text-align: right;\">                   1</td><td>10045936  </td><td style=\"text-align: right;\">    0.275953 </td></tr>\n",
       "<tr><td>FSR_Trainable_11597b57</td><td>2023-08-11_05-21-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    202.114</td><td style=\"text-align: right;\"> 2.37995e+17</td><td style=\"text-align: right;\">   0.121577 </td><td>172.26.215.93</td><td style=\"text-align: right;\">262146</td><td style=\"text-align: right;\">     378.425</td><td style=\"text-align: right;\">            1.91121 </td><td style=\"text-align: right;\">          0.738586</td><td style=\"text-align: right;\">      1.91121 </td><td style=\"text-align: right;\"> 1691698884</td><td style=\"text-align: right;\">   0.0714123</td><td style=\"text-align: right;\">   8.55344e+13</td><td style=\"text-align: right;\">                   2</td><td>11597b57  </td><td style=\"text-align: right;\">    0.121577 </td></tr>\n",
       "<tr><td>FSR_Trainable_12d5687d</td><td>2023-08-11_05-13-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">    124.081</td><td style=\"text-align: right;\"> 1.17056e+17</td><td style=\"text-align: right;\">   0.079045 </td><td>172.26.215.93</td><td style=\"text-align: right;\">251909</td><td style=\"text-align: right;\">     233.714</td><td style=\"text-align: right;\">           53.6394  </td><td style=\"text-align: right;\">          0.911983</td><td style=\"text-align: right;\">     53.6394  </td><td style=\"text-align: right;\"> 1691698402</td><td style=\"text-align: right;\">   0.0477783</td><td style=\"text-align: right;\">   5.41892e+13</td><td style=\"text-align: right;\">                  64</td><td>12d5687d  </td><td style=\"text-align: right;\">    0.079045 </td></tr>\n",
       "<tr><td>FSR_Trainable_12e8ba4d</td><td>2023-08-11_05-26-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    217.158</td><td style=\"text-align: right;\"> 2.82053e+17</td><td style=\"text-align: right;\">   0.125945 </td><td>172.26.215.93</td><td style=\"text-align: right;\">269255</td><td style=\"text-align: right;\">     410.82 </td><td style=\"text-align: right;\">            1.80479 </td><td style=\"text-align: right;\">          0.945862</td><td style=\"text-align: right;\">      1.80479 </td><td style=\"text-align: right;\"> 1691699215</td><td style=\"text-align: right;\">   0.0725862</td><td style=\"text-align: right;\">   8.47331e+13</td><td style=\"text-align: right;\">                   2</td><td>12e8ba4d  </td><td style=\"text-align: right;\">    0.125945 </td></tr>\n",
       "<tr><td>FSR_Trainable_140519ef</td><td>2023-08-11_05-13-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    109.87 </td><td style=\"text-align: right;\"> 1.04478e+17</td><td style=\"text-align: right;\">   0.0702235</td><td>172.26.215.93</td><td style=\"text-align: right;\">251643</td><td style=\"text-align: right;\">     207.585</td><td style=\"text-align: right;\">           80.3434  </td><td style=\"text-align: right;\">          0.718947</td><td style=\"text-align: right;\">     80.3434  </td><td style=\"text-align: right;\"> 1691698421</td><td style=\"text-align: right;\">   0.041688 </td><td style=\"text-align: right;\">   4.40195e+13</td><td style=\"text-align: right;\">                 100</td><td>140519ef  </td><td style=\"text-align: right;\">    0.0702235</td></tr>\n",
       "<tr><td>FSR_Trainable_17d11d30</td><td>2023-08-11_05-11-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    163.833</td><td style=\"text-align: right;\"> 4.30145e+08</td><td style=\"text-align: right;\">   0.751758 </td><td>172.26.215.93</td><td style=\"text-align: right;\">250702</td><td style=\"text-align: right;\">     328.152</td><td style=\"text-align: right;\">            2.5798  </td><td style=\"text-align: right;\">          1.03749 </td><td style=\"text-align: right;\">      2.5798  </td><td style=\"text-align: right;\"> 1691698281</td><td style=\"text-align: right;\">   0.407918 </td><td style=\"text-align: right;\">   2.16413    </td><td style=\"text-align: right;\">                   2</td><td>17d11d30  </td><td style=\"text-align: right;\">    0.751758 </td></tr>\n",
       "<tr><td>FSR_Trainable_1a10a85b</td><td>2023-08-11_05-14-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    116.428</td><td style=\"text-align: right;\"> 1.11268e+17</td><td style=\"text-align: right;\">   0.0741876</td><td>172.26.215.93</td><td style=\"text-align: right;\">252730</td><td style=\"text-align: right;\">     219.083</td><td style=\"text-align: right;\">           80.2348  </td><td style=\"text-align: right;\">          0.791324</td><td style=\"text-align: right;\">     80.2348  </td><td style=\"text-align: right;\"> 1691698492</td><td style=\"text-align: right;\">   0.0441611</td><td style=\"text-align: right;\">   4.73322e+13</td><td style=\"text-align: right;\">                 100</td><td>1a10a85b  </td><td style=\"text-align: right;\">    0.0741876</td></tr>\n",
       "<tr><td>FSR_Trainable_1a9699d3</td><td>2023-08-11_05-26-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    365.644</td><td style=\"text-align: right;\"> 2.62773e+17</td><td style=\"text-align: right;\">   0.200032 </td><td>172.26.215.93</td><td style=\"text-align: right;\">268796</td><td style=\"text-align: right;\">     658.921</td><td style=\"text-align: right;\">            1.97006 </td><td style=\"text-align: right;\">          1.97006 </td><td style=\"text-align: right;\">      1.97006 </td><td style=\"text-align: right;\"> 1691699192</td><td style=\"text-align: right;\">   0.12563  </td><td style=\"text-align: right;\">   1.05016e+14</td><td style=\"text-align: right;\">                   1</td><td>1a9699d3  </td><td style=\"text-align: right;\">    0.200032 </td></tr>\n",
       "<tr><td>FSR_Trainable_1b8c9dd5</td><td>2023-08-11_05-19-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    111.279</td><td style=\"text-align: right;\"> 1.06247e+17</td><td style=\"text-align: right;\">   0.0708448</td><td>172.26.215.93</td><td style=\"text-align: right;\">258321</td><td style=\"text-align: right;\">     209.703</td><td style=\"text-align: right;\">           80.9922  </td><td style=\"text-align: right;\">          0.740369</td><td style=\"text-align: right;\">     80.9922  </td><td style=\"text-align: right;\"> 1691698795</td><td style=\"text-align: right;\">   0.0422561</td><td style=\"text-align: right;\">   4.51797e+13</td><td style=\"text-align: right;\">                 100</td><td>1b8c9dd5  </td><td style=\"text-align: right;\">    0.0708448</td></tr>\n",
       "<tr><td>FSR_Trainable_1e9c13e0</td><td>2023-08-11_05-13-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    122.145</td><td style=\"text-align: right;\"> 1.18882e+17</td><td style=\"text-align: right;\">   0.0773766</td><td>172.26.215.93</td><td style=\"text-align: right;\">251732</td><td style=\"text-align: right;\">     228.186</td><td style=\"text-align: right;\">           82.128   </td><td style=\"text-align: right;\">          0.670511</td><td style=\"text-align: right;\">     82.128   </td><td style=\"text-align: right;\"> 1691698428</td><td style=\"text-align: right;\">   0.0463165</td><td style=\"text-align: right;\">   5.07666e+13</td><td style=\"text-align: right;\">                 100</td><td>1e9c13e0  </td><td style=\"text-align: right;\">    0.0773766</td></tr>\n",
       "<tr><td>FSR_Trainable_21cd5e27</td><td>2023-08-11_05-19-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    128.096</td><td style=\"text-align: right;\"> 1.46093e+17</td><td style=\"text-align: right;\">   0.0785875</td><td>172.26.215.93</td><td style=\"text-align: right;\">259211</td><td style=\"text-align: right;\">     233.393</td><td style=\"text-align: right;\">            7.18334 </td><td style=\"text-align: right;\">          0.657331</td><td style=\"text-align: right;\">      7.18334 </td><td style=\"text-align: right;\"> 1691698743</td><td style=\"text-align: right;\">   0.0484175</td><td style=\"text-align: right;\">   6.13416e+13</td><td style=\"text-align: right;\">                   8</td><td>21cd5e27  </td><td style=\"text-align: right;\">    0.0785875</td></tr>\n",
       "<tr><td>FSR_Trainable_22c63583</td><td>2023-08-11_05-20-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    203.817</td><td style=\"text-align: right;\"> 2.76216e+17</td><td style=\"text-align: right;\">   0.11719  </td><td>172.26.215.93</td><td style=\"text-align: right;\">260888</td><td style=\"text-align: right;\">     369.277</td><td style=\"text-align: right;\">            1.58021 </td><td style=\"text-align: right;\">          0.77325 </td><td style=\"text-align: right;\">      1.58021 </td><td style=\"text-align: right;\"> 1691698829</td><td style=\"text-align: right;\">   0.0727988</td><td style=\"text-align: right;\">   1.00015e+14</td><td style=\"text-align: right;\">                   2</td><td>22c63583  </td><td style=\"text-align: right;\">    0.11719  </td></tr>\n",
       "<tr><td>FSR_Trainable_27ced3c6</td><td>2023-08-11_05-27-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    316.856</td><td style=\"text-align: right;\"> 5.02213e+17</td><td style=\"text-align: right;\">   0.1789   </td><td>172.26.215.93</td><td style=\"text-align: right;\">269478</td><td style=\"text-align: right;\">     490.867</td><td style=\"text-align: right;\">            1.02776 </td><td style=\"text-align: right;\">          1.02776 </td><td style=\"text-align: right;\">      1.02776 </td><td style=\"text-align: right;\"> 1691699224</td><td style=\"text-align: right;\">   0.133243 </td><td style=\"text-align: right;\">   2.40534e+14</td><td style=\"text-align: right;\">                   1</td><td>27ced3c6  </td><td style=\"text-align: right;\">    0.1789   </td></tr>\n",
       "<tr><td>FSR_Trainable_28609f96</td><td>2023-08-11_05-26-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    330.143</td><td style=\"text-align: right;\"> 3.63168e+17</td><td style=\"text-align: right;\">   0.186017 </td><td>172.26.215.93</td><td style=\"text-align: right;\">269016</td><td style=\"text-align: right;\">     557.295</td><td style=\"text-align: right;\">            1.10674 </td><td style=\"text-align: right;\">          1.10674 </td><td style=\"text-align: right;\">      1.10674 </td><td style=\"text-align: right;\"> 1691699203</td><td style=\"text-align: right;\">   0.12385  </td><td style=\"text-align: right;\">   1.51543e+14</td><td style=\"text-align: right;\">                   1</td><td>28609f96  </td><td style=\"text-align: right;\">    0.186017 </td></tr>\n",
       "<tr><td>FSR_Trainable_2c2d0384</td><td>2023-08-11_05-11-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    980.939</td><td style=\"text-align: right;\"> 3.58168e+09</td><td style=\"text-align: right;\">   3.80998  </td><td>172.26.215.93</td><td style=\"text-align: right;\">250939</td><td style=\"text-align: right;\">    1802.58 </td><td style=\"text-align: right;\">            1.75518 </td><td style=\"text-align: right;\">          1.75518 </td><td style=\"text-align: right;\">      1.75518 </td><td style=\"text-align: right;\"> 1691698293</td><td style=\"text-align: right;\">   2.53357  </td><td style=\"text-align: right;\">  15.6028     </td><td style=\"text-align: right;\">                   1</td><td>2c2d0384  </td><td style=\"text-align: right;\">    3.80998  </td></tr>\n",
       "<tr><td>FSR_Trainable_2e7e4879</td><td>2023-08-11_05-25-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    200.656</td><td style=\"text-align: right;\"> 2.71728e+17</td><td style=\"text-align: right;\">   0.117212 </td><td>172.26.215.93</td><td style=\"text-align: right;\">267424</td><td style=\"text-align: right;\">     365.724</td><td style=\"text-align: right;\">            2.25537 </td><td style=\"text-align: right;\">          0.996534</td><td style=\"text-align: right;\">      2.25537 </td><td style=\"text-align: right;\"> 1691699134</td><td style=\"text-align: right;\">   0.0710424</td><td style=\"text-align: right;\">   9.66491e+13</td><td style=\"text-align: right;\">                   2</td><td>2e7e4879  </td><td style=\"text-align: right;\">    0.117212 </td></tr>\n",
       "<tr><td>FSR_Trainable_2eda9114</td><td>2023-08-11_05-24-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    111.401</td><td style=\"text-align: right;\"> 1.05389e+17</td><td style=\"text-align: right;\">   0.0710262</td><td>172.26.215.93</td><td style=\"text-align: right;\">264718</td><td style=\"text-align: right;\">     210.229</td><td style=\"text-align: right;\">           87.779   </td><td style=\"text-align: right;\">          0.959974</td><td style=\"text-align: right;\">     87.779   </td><td style=\"text-align: right;\"> 1691699081</td><td style=\"text-align: right;\">   0.0418759</td><td style=\"text-align: right;\">   4.30276e+13</td><td style=\"text-align: right;\">                 100</td><td>2eda9114  </td><td style=\"text-align: right;\">    0.0710262</td></tr>\n",
       "<tr><td>FSR_Trainable_2f18c218</td><td>2023-08-11_05-15-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   2444.73 </td><td style=\"text-align: right;\"> 5.24874e+18</td><td style=\"text-align: right;\">   1.53123  </td><td>172.26.215.93</td><td style=\"text-align: right;\">255154</td><td style=\"text-align: right;\">    4574.24 </td><td style=\"text-align: right;\">            1.27287 </td><td style=\"text-align: right;\">          1.27287 </td><td style=\"text-align: right;\">      1.27287 </td><td style=\"text-align: right;\"> 1691698544</td><td style=\"text-align: right;\">   1.02852  </td><td style=\"text-align: right;\">   2.36924e+15</td><td style=\"text-align: right;\">                   1</td><td>2f18c218  </td><td style=\"text-align: right;\">    1.53123  </td></tr>\n",
       "<tr><td>FSR_Trainable_30951e7e</td><td>2023-08-11_05-28-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    268.617</td><td style=\"text-align: right;\"> 4.26328e+17</td><td style=\"text-align: right;\">   0.145176 </td><td>172.26.215.93</td><td style=\"text-align: right;\">270640</td><td style=\"text-align: right;\">     482.584</td><td style=\"text-align: right;\">            0.905593</td><td style=\"text-align: right;\">          0.905593</td><td style=\"text-align: right;\">      0.905593</td><td style=\"text-align: right;\"> 1691699281</td><td style=\"text-align: right;\">   0.0927812</td><td style=\"text-align: right;\">   1.49501e+14</td><td style=\"text-align: right;\">                   1</td><td>30951e7e  </td><td style=\"text-align: right;\">    0.145176 </td></tr>\n",
       "<tr><td>FSR_Trainable_350a53a7</td><td>2023-08-11_05-20-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    240.455</td><td style=\"text-align: right;\"> 3.52652e+17</td><td style=\"text-align: right;\">   0.134573 </td><td>172.26.215.93</td><td style=\"text-align: right;\">260404</td><td style=\"text-align: right;\">     427.153</td><td style=\"text-align: right;\">            1.92851 </td><td style=\"text-align: right;\">          0.822973</td><td style=\"text-align: right;\">      1.92851 </td><td style=\"text-align: right;\"> 1691698805</td><td style=\"text-align: right;\">   0.0854025</td><td style=\"text-align: right;\">   1.30201e+14</td><td style=\"text-align: right;\">                   2</td><td>350a53a7  </td><td style=\"text-align: right;\">    0.134573 </td></tr>\n",
       "<tr><td>FSR_Trainable_37accafc</td><td>2023-08-11_05-16-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    378.548</td><td style=\"text-align: right;\"> 3.61285e+17</td><td style=\"text-align: right;\">   0.211409 </td><td>172.26.215.93</td><td style=\"text-align: right;\">256314</td><td style=\"text-align: right;\">     641.652</td><td style=\"text-align: right;\">            1.15204 </td><td style=\"text-align: right;\">          1.15204 </td><td style=\"text-align: right;\">      1.15204 </td><td style=\"text-align: right;\"> 1691698610</td><td style=\"text-align: right;\">   0.148958 </td><td style=\"text-align: right;\">   2.04576e+14</td><td style=\"text-align: right;\">                   1</td><td>37accafc  </td><td style=\"text-align: right;\">    0.211409 </td></tr>\n",
       "<tr><td>FSR_Trainable_39f6fe44</td><td>2023-08-11_05-23-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    112.14 </td><td style=\"text-align: right;\"> 9.53508e+16</td><td style=\"text-align: right;\">   0.0730533</td><td>172.26.215.93</td><td style=\"text-align: right;\">265431</td><td style=\"text-align: right;\">     221.099</td><td style=\"text-align: right;\">           16.3547  </td><td style=\"text-align: right;\">          1.20381 </td><td style=\"text-align: right;\">     16.3547  </td><td style=\"text-align: right;\"> 1691699027</td><td style=\"text-align: right;\">   0.042415 </td><td style=\"text-align: right;\">   4.29716e+13</td><td style=\"text-align: right;\">                  16</td><td>39f6fe44  </td><td style=\"text-align: right;\">    0.0730533</td></tr>\n",
       "<tr><td>FSR_Trainable_3ce133e1</td><td>2023-08-11_05-17-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    112.625</td><td style=\"text-align: right;\"> 1.07871e+17</td><td style=\"text-align: right;\">   0.0714944</td><td>172.26.215.93</td><td style=\"text-align: right;\">254886</td><td style=\"text-align: right;\">     210.918</td><td style=\"text-align: right;\">           81.0078  </td><td style=\"text-align: right;\">          0.483022</td><td style=\"text-align: right;\">     81.0078  </td><td style=\"text-align: right;\"> 1691698627</td><td style=\"text-align: right;\">   0.0424363</td><td style=\"text-align: right;\">   4.39931e+13</td><td style=\"text-align: right;\">                 100</td><td>3ce133e1  </td><td style=\"text-align: right;\">    0.0714944</td></tr>\n",
       "<tr><td>FSR_Trainable_3f62f2b8</td><td>2023-08-11_05-20-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    109.81 </td><td style=\"text-align: right;\"> 1.01693e+17</td><td style=\"text-align: right;\">   0.0705685</td><td>172.26.215.93</td><td style=\"text-align: right;\">258681</td><td style=\"text-align: right;\">     208.84 </td><td style=\"text-align: right;\">           83.8791  </td><td style=\"text-align: right;\">          0.614776</td><td style=\"text-align: right;\">     83.8791  </td><td style=\"text-align: right;\"> 1691698813</td><td style=\"text-align: right;\">   0.0417671</td><td style=\"text-align: right;\">   4.38952e+13</td><td style=\"text-align: right;\">                 100</td><td>3f62f2b8  </td><td style=\"text-align: right;\">    0.0705685</td></tr>\n",
       "<tr><td>FSR_Trainable_4035f2d3</td><td>2023-08-11_05-17-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    147.802</td><td style=\"text-align: right;\"> 1.33546e+17</td><td style=\"text-align: right;\">   0.0942032</td><td>172.26.215.93</td><td style=\"text-align: right;\">257032</td><td style=\"text-align: right;\">     283.298</td><td style=\"text-align: right;\">            3.22026 </td><td style=\"text-align: right;\">          0.53841 </td><td style=\"text-align: right;\">      3.22026 </td><td style=\"text-align: right;\"> 1691698644</td><td style=\"text-align: right;\">   0.0582975</td><td style=\"text-align: right;\">   7.13911e+13</td><td style=\"text-align: right;\">                   4</td><td>4035f2d3  </td><td style=\"text-align: right;\">    0.0942032</td></tr>\n",
       "<tr><td>FSR_Trainable_405a9de5</td><td>2023-08-11_05-17-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    228.012</td><td style=\"text-align: right;\"> 1.98064e+16</td><td style=\"text-align: right;\">   4.90812  </td><td>172.26.215.93</td><td style=\"text-align: right;\">257898</td><td style=\"text-align: right;\">     441.08 </td><td style=\"text-align: right;\">            1.49952 </td><td style=\"text-align: right;\">          1.49952 </td><td style=\"text-align: right;\">      1.49952 </td><td style=\"text-align: right;\"> 1691698678</td><td style=\"text-align: right;\">   1.84587  </td><td style=\"text-align: right;\">   1.07307e+15</td><td style=\"text-align: right;\">                   1</td><td>405a9de5  </td><td style=\"text-align: right;\">    4.90812  </td></tr>\n",
       "<tr><td>FSR_Trainable_4109b910</td><td>2023-08-11_05-22-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    222.434</td><td style=\"text-align: right;\"> 2.79717e+17</td><td style=\"text-align: right;\">   0.13032  </td><td>172.26.215.93</td><td style=\"text-align: right;\">263226</td><td style=\"text-align: right;\">     418.449</td><td style=\"text-align: right;\">            1.75508 </td><td style=\"text-align: right;\">          0.647395</td><td style=\"text-align: right;\">      1.75508 </td><td style=\"text-align: right;\"> 1691698920</td><td style=\"text-align: right;\">   0.0759073</td><td style=\"text-align: right;\">   9.08222e+13</td><td style=\"text-align: right;\">                   2</td><td>4109b910  </td><td style=\"text-align: right;\">    0.13032  </td></tr>\n",
       "<tr><td>FSR_Trainable_4226873b</td><td>2023-08-11_05-09-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    181.944</td><td style=\"text-align: right;\"> 2.15573e+16</td><td style=\"text-align: right;\">   3.90003  </td><td>172.26.215.93</td><td style=\"text-align: right;\">249294</td><td style=\"text-align: right;\">     390.884</td><td style=\"text-align: right;\">            0.772551</td><td style=\"text-align: right;\">          0.772551</td><td style=\"text-align: right;\">      0.772551</td><td style=\"text-align: right;\"> 1691698199</td><td style=\"text-align: right;\">   1.45985  </td><td style=\"text-align: right;\">   1.19374e+15</td><td style=\"text-align: right;\">                   1</td><td>4226873b  </td><td style=\"text-align: right;\">    3.90003  </td></tr>\n",
       "<tr><td>FSR_Trainable_43b74dad</td><td>2023-08-11_05-16-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    703.215</td><td style=\"text-align: right;\"> 5.17029e+16</td><td style=\"text-align: right;\">   6.99051  </td><td>172.26.215.93</td><td style=\"text-align: right;\">255862</td><td style=\"text-align: right;\">    2016.82 </td><td style=\"text-align: right;\">            0.971685</td><td style=\"text-align: right;\">          0.971685</td><td style=\"text-align: right;\">      0.971685</td><td style=\"text-align: right;\"> 1691698588</td><td style=\"text-align: right;\">   3.19431  </td><td style=\"text-align: right;\">   1.89757e+15</td><td style=\"text-align: right;\">                   1</td><td>43b74dad  </td><td style=\"text-align: right;\">    6.99051  </td></tr>\n",
       "<tr><td>FSR_Trainable_44c0d485</td><td>2023-08-11_05-10-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    160.121</td><td style=\"text-align: right;\"> 4.43166e+08</td><td style=\"text-align: right;\">   0.660663 </td><td>172.26.215.93</td><td style=\"text-align: right;\">250011</td><td style=\"text-align: right;\">     270.371</td><td style=\"text-align: right;\">            1.40346 </td><td style=\"text-align: right;\">          0.621045</td><td style=\"text-align: right;\">      1.40346 </td><td style=\"text-align: right;\"> 1691698237</td><td style=\"text-align: right;\">   0.425797 </td><td style=\"text-align: right;\">   2.40597    </td><td style=\"text-align: right;\">                   2</td><td>44c0d485  </td><td style=\"text-align: right;\">    0.660663 </td></tr>\n",
       "<tr><td>FSR_Trainable_461f3c40</td><td>2023-08-11_05-17-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    233.119</td><td style=\"text-align: right;\"> 3.52079e+17</td><td style=\"text-align: right;\">   0.130151 </td><td>172.26.215.93</td><td style=\"text-align: right;\">257299</td><td style=\"text-align: right;\">     411.314</td><td style=\"text-align: right;\">            1.63162 </td><td style=\"text-align: right;\">          0.573177</td><td style=\"text-align: right;\">      1.63162 </td><td style=\"text-align: right;\"> 1691698656</td><td style=\"text-align: right;\">   0.0845507</td><td style=\"text-align: right;\">   1.34028e+14</td><td style=\"text-align: right;\">                   2</td><td>461f3c40  </td><td style=\"text-align: right;\">    0.130151 </td></tr>\n",
       "<tr><td>FSR_Trainable_4712e9ed</td><td>2023-08-11_05-13-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    144.897</td><td style=\"text-align: right;\"> 1.82533e+17</td><td style=\"text-align: right;\">   0.0855795</td><td>172.26.215.93</td><td style=\"text-align: right;\">252470</td><td style=\"text-align: right;\">     257.081</td><td style=\"text-align: right;\">           12.1235  </td><td style=\"text-align: right;\">          0.724091</td><td style=\"text-align: right;\">     12.1235  </td><td style=\"text-align: right;\"> 1691698385</td><td style=\"text-align: right;\">   0.0551115</td><td style=\"text-align: right;\">   7.9372e+13 </td><td style=\"text-align: right;\">                  16</td><td>4712e9ed  </td><td style=\"text-align: right;\">    0.0855795</td></tr>\n",
       "<tr><td>FSR_Trainable_48404c5c</td><td>2023-08-11_05-21-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    328.235</td><td style=\"text-align: right;\"> 5.29582e+17</td><td style=\"text-align: right;\">   0.175015 </td><td>172.26.215.93</td><td style=\"text-align: right;\">261809</td><td style=\"text-align: right;\">     512.722</td><td style=\"text-align: right;\">            1.10006 </td><td style=\"text-align: right;\">          1.10006 </td><td style=\"text-align: right;\">      1.10006 </td><td style=\"text-align: right;\"> 1691698867</td><td style=\"text-align: right;\">   0.124324 </td><td style=\"text-align: right;\">   1.98075e+14</td><td style=\"text-align: right;\">                   1</td><td>48404c5c  </td><td style=\"text-align: right;\">    0.175015 </td></tr>\n",
       "<tr><td>FSR_Trainable_48d91995</td><td>2023-08-11_05-22-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    162.284</td><td style=\"text-align: right;\"> 1.5123e+17 </td><td style=\"text-align: right;\">   0.093891 </td><td>172.26.215.93</td><td style=\"text-align: right;\">264629</td><td style=\"text-align: right;\">     308.458</td><td style=\"text-align: right;\">            4.09957 </td><td style=\"text-align: right;\">          0.693598</td><td style=\"text-align: right;\">      4.09957 </td><td style=\"text-align: right;\"> 1691698974</td><td style=\"text-align: right;\">   0.0583976</td><td style=\"text-align: right;\">   6.72684e+13</td><td style=\"text-align: right;\">                   4</td><td>48d91995  </td><td style=\"text-align: right;\">    0.093891 </td></tr>\n",
       "<tr><td>FSR_Trainable_4d669d51</td><td>2023-08-11_05-18-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    173.983</td><td style=\"text-align: right;\"> 1.9537e+17 </td><td style=\"text-align: right;\">   0.107008 </td><td>172.26.215.93</td><td style=\"text-align: right;\">258990</td><td style=\"text-align: right;\">     321.312</td><td style=\"text-align: right;\">            1.81211 </td><td style=\"text-align: right;\">          0.892825</td><td style=\"text-align: right;\">      1.81211 </td><td style=\"text-align: right;\"> 1691698725</td><td style=\"text-align: right;\">   0.0642206</td><td style=\"text-align: right;\">   7.6158e+13 </td><td style=\"text-align: right;\">                   2</td><td>4d669d51  </td><td style=\"text-align: right;\">    0.107008 </td></tr>\n",
       "<tr><td>FSR_Trainable_537b7970</td><td>2023-08-11_05-24-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    244.424</td><td style=\"text-align: right;\"> 8.72664e+08</td><td style=\"text-align: right;\">   0.990651 </td><td>172.26.215.93</td><td style=\"text-align: right;\">266858</td><td style=\"text-align: right;\">     427.405</td><td style=\"text-align: right;\">            1.13974 </td><td style=\"text-align: right;\">          1.13974 </td><td style=\"text-align: right;\">      1.13974 </td><td style=\"text-align: right;\"> 1691699098</td><td style=\"text-align: right;\">   0.625094 </td><td style=\"text-align: right;\">   1.52485    </td><td style=\"text-align: right;\">                   1</td><td>537b7970  </td><td style=\"text-align: right;\">    0.990651 </td></tr>\n",
       "<tr><td>FSR_Trainable_576c6c49</td><td>2023-08-11_05-11-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    241.064</td><td style=\"text-align: right;\"> 2.44426e+16</td><td style=\"text-align: right;\">   4.74584  </td><td>172.26.215.93</td><td style=\"text-align: right;\">250473</td><td style=\"text-align: right;\">     487.486</td><td style=\"text-align: right;\">            1.13852 </td><td style=\"text-align: right;\">          1.13852 </td><td style=\"text-align: right;\">      1.13852 </td><td style=\"text-align: right;\"> 1691698264</td><td style=\"text-align: right;\">   1.84047  </td><td style=\"text-align: right;\">   1.37206e+15</td><td style=\"text-align: right;\">                   1</td><td>576c6c49  </td><td style=\"text-align: right;\">    4.74584  </td></tr>\n",
       "<tr><td>FSR_Trainable_5d427b91</td><td>2023-08-11_05-16-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    185.93 </td><td style=\"text-align: right;\"> 2.54243e+17</td><td style=\"text-align: right;\">   0.105799 </td><td>172.26.215.93</td><td style=\"text-align: right;\">255383</td><td style=\"text-align: right;\">     328.387</td><td style=\"text-align: right;\">            1.94662 </td><td style=\"text-align: right;\">          0.908744</td><td style=\"text-align: right;\">      1.94662 </td><td style=\"text-align: right;\"> 1691698561</td><td style=\"text-align: right;\">   0.0693117</td><td style=\"text-align: right;\">   1.05136e+14</td><td style=\"text-align: right;\">                   2</td><td>5d427b91  </td><td style=\"text-align: right;\">    0.105799 </td></tr>\n",
       "<tr><td>FSR_Trainable_5d520eab</td><td>2023-08-11_05-25-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    174.453</td><td style=\"text-align: right;\"> 1.87614e+17</td><td style=\"text-align: right;\">   0.106046 </td><td>172.26.215.93</td><td style=\"text-align: right;\">267224</td><td style=\"text-align: right;\">     338.316</td><td style=\"text-align: right;\">            3.14832 </td><td style=\"text-align: right;\">          1.03005 </td><td style=\"text-align: right;\">      3.14832 </td><td style=\"text-align: right;\"> 1691699124</td><td style=\"text-align: right;\">   0.0614922</td><td style=\"text-align: right;\">   6.73448e+13</td><td style=\"text-align: right;\">                   2</td><td>5d520eab  </td><td style=\"text-align: right;\">    0.106046 </td></tr>\n",
       "<tr><td>FSR_Trainable_5ed4e384</td><td>2023-08-11_05-22-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    141.552</td><td style=\"text-align: right;\"> 1.41391e+17</td><td style=\"text-align: right;\">   0.0878513</td><td>172.26.215.93</td><td style=\"text-align: right;\">263723</td><td style=\"text-align: right;\">     268.691</td><td style=\"text-align: right;\">            3.19451 </td><td style=\"text-align: right;\">          0.575419</td><td style=\"text-align: right;\">      3.19451 </td><td style=\"text-align: right;\"> 1691698942</td><td style=\"text-align: right;\">   0.0517762</td><td style=\"text-align: right;\">   5.59636e+13</td><td style=\"text-align: right;\">                   4</td><td>5ed4e384  </td><td style=\"text-align: right;\">    0.0878513</td></tr>\n",
       "<tr><td>FSR_Trainable_61601394</td><td>2023-08-11_05-22-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    295.914</td><td style=\"text-align: right;\"> 2.6596e+17 </td><td style=\"text-align: right;\">   0.173664 </td><td>172.26.215.93</td><td style=\"text-align: right;\">264312</td><td style=\"text-align: right;\">     526.666</td><td style=\"text-align: right;\">            1.12172 </td><td style=\"text-align: right;\">          1.12172 </td><td style=\"text-align: right;\">      1.12172 </td><td style=\"text-align: right;\"> 1691698960</td><td style=\"text-align: right;\">   0.116116 </td><td style=\"text-align: right;\">   1.50918e+14</td><td style=\"text-align: right;\">                   1</td><td>61601394  </td><td style=\"text-align: right;\">    0.173664 </td></tr>\n",
       "<tr><td>FSR_Trainable_69867db4</td><td>2023-08-11_05-15-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    111.621</td><td style=\"text-align: right;\"> 1.01746e+17</td><td style=\"text-align: right;\">   0.0715563</td><td>172.26.215.93</td><td style=\"text-align: right;\">253243</td><td style=\"text-align: right;\">     213.139</td><td style=\"text-align: right;\">           81.7701  </td><td style=\"text-align: right;\">          0.878769</td><td style=\"text-align: right;\">     81.7701  </td><td style=\"text-align: right;\"> 1691698528</td><td style=\"text-align: right;\">   0.0422479</td><td style=\"text-align: right;\">   4.39387e+13</td><td style=\"text-align: right;\">                 100</td><td>69867db4  </td><td style=\"text-align: right;\">    0.0715563</td></tr>\n",
       "<tr><td>FSR_Trainable_6d759f9e</td><td>2023-08-11_05-18-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    157.855</td><td style=\"text-align: right;\"> 2.05606e+17</td><td style=\"text-align: right;\">   0.0917419</td><td>172.26.215.93</td><td style=\"text-align: right;\">258500</td><td style=\"text-align: right;\">     277.083</td><td style=\"text-align: right;\">            3.22382 </td><td style=\"text-align: right;\">          0.73693 </td><td style=\"text-align: right;\">      3.22382 </td><td style=\"text-align: right;\"> 1691698709</td><td style=\"text-align: right;\">   0.0586808</td><td style=\"text-align: right;\">   8.20048e+13</td><td style=\"text-align: right;\">                   4</td><td>6d759f9e  </td><td style=\"text-align: right;\">    0.0917419</td></tr>\n",
       "<tr><td>FSR_Trainable_701b36be</td><td>2023-08-11_05-10-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    301.814</td><td style=\"text-align: right;\"> 1.15103e+09</td><td style=\"text-align: right;\">   1.21279  </td><td>172.26.215.93</td><td style=\"text-align: right;\">249774</td><td style=\"text-align: right;\">     540.499</td><td style=\"text-align: right;\">            0.942149</td><td style=\"text-align: right;\">          0.942149</td><td style=\"text-align: right;\">      0.942149</td><td style=\"text-align: right;\"> 1691698220</td><td style=\"text-align: right;\">   0.769007 </td><td style=\"text-align: right;\">   2.08017    </td><td style=\"text-align: right;\">                   1</td><td>701b36be  </td><td style=\"text-align: right;\">    1.21279  </td></tr>\n",
       "<tr><td>FSR_Trainable_70ed32b7</td><td>2023-08-11_05-21-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    332.654</td><td style=\"text-align: right;\"> 3.36418e+17</td><td style=\"text-align: right;\">   0.190241 </td><td>172.26.215.93</td><td style=\"text-align: right;\">262066</td><td style=\"text-align: right;\">     547.479</td><td style=\"text-align: right;\">            0.980382</td><td style=\"text-align: right;\">          0.980382</td><td style=\"text-align: right;\">      0.980382</td><td style=\"text-align: right;\"> 1691698875</td><td style=\"text-align: right;\">   0.138407 </td><td style=\"text-align: right;\">   1.95608e+14</td><td style=\"text-align: right;\">                   1</td><td>70ed32b7  </td><td style=\"text-align: right;\">    0.190241 </td></tr>\n",
       "<tr><td>FSR_Trainable_71d25cbf</td><td>2023-08-11_05-19-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    182.217</td><td style=\"text-align: right;\"> 4.85221e+08</td><td style=\"text-align: right;\">   0.805558 </td><td>172.26.215.93</td><td style=\"text-align: right;\">259932</td><td style=\"text-align: right;\">     325.291</td><td style=\"text-align: right;\">            1.15688 </td><td style=\"text-align: right;\">          1.15688 </td><td style=\"text-align: right;\">      1.15688 </td><td style=\"text-align: right;\"> 1691698772</td><td style=\"text-align: right;\">   0.488153 </td><td style=\"text-align: right;\">   2.37171    </td><td style=\"text-align: right;\">                   1</td><td>71d25cbf  </td><td style=\"text-align: right;\">    0.805558 </td></tr>\n",
       "<tr><td>FSR_Trainable_76a437b0</td><td>2023-08-11_05-25-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    219.181</td><td style=\"text-align: right;\"> 3.27136e+17</td><td style=\"text-align: right;\">   0.122849 </td><td>172.26.215.93</td><td style=\"text-align: right;\">267621</td><td style=\"text-align: right;\">     384.486</td><td style=\"text-align: right;\">            2.95523 </td><td style=\"text-align: right;\">          1.31416 </td><td style=\"text-align: right;\">      2.95523 </td><td style=\"text-align: right;\"> 1691699143</td><td style=\"text-align: right;\">   0.0779973</td><td style=\"text-align: right;\">   1.18566e+14</td><td style=\"text-align: right;\">                   2</td><td>76a437b0  </td><td style=\"text-align: right;\">    0.122849 </td></tr>\n",
       "<tr><td>FSR_Trainable_788c3fca</td><td>2023-08-11_05-23-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    158.384</td><td style=\"text-align: right;\"> 2.04597e+17</td><td style=\"text-align: right;\">   0.0911966</td><td>172.26.215.93</td><td style=\"text-align: right;\">265124</td><td style=\"text-align: right;\">     286.081</td><td style=\"text-align: right;\">            3.86893 </td><td style=\"text-align: right;\">          0.764355</td><td style=\"text-align: right;\">      3.86893 </td><td style=\"text-align: right;\"> 1691699001</td><td style=\"text-align: right;\">   0.0584193</td><td style=\"text-align: right;\">   8.38975e+13</td><td style=\"text-align: right;\">                   4</td><td>788c3fca  </td><td style=\"text-align: right;\">    0.0911966</td></tr>\n",
       "<tr><td>FSR_Trainable_7dbc4a1e</td><td>2023-08-11_05-11-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    149.736</td><td style=\"text-align: right;\"> 1.49477e+17</td><td style=\"text-align: right;\">   0.0904434</td><td>172.26.215.93</td><td style=\"text-align: right;\">249122</td><td style=\"text-align: right;\">     272.875</td><td style=\"text-align: right;\">           89.3008  </td><td style=\"text-align: right;\">          0.842681</td><td style=\"text-align: right;\">     89.3008  </td><td style=\"text-align: right;\"> 1691698299</td><td style=\"text-align: right;\">   0.0560612</td><td style=\"text-align: right;\">   6.37317e+13</td><td style=\"text-align: right;\">                 100</td><td>7dbc4a1e  </td><td style=\"text-align: right;\">    0.0904434</td></tr>\n",
       "<tr><td>FSR_Trainable_7debcdf3</td><td>2023-08-11_05-16-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    678.268</td><td style=\"text-align: right;\"> 1.08562e+18</td><td style=\"text-align: right;\">   0.315566 </td><td>172.26.215.93</td><td style=\"text-align: right;\">256093</td><td style=\"text-align: right;\">    1029.81 </td><td style=\"text-align: right;\">            0.877184</td><td style=\"text-align: right;\">          0.877184</td><td style=\"text-align: right;\">      0.877184</td><td style=\"text-align: right;\"> 1691698599</td><td style=\"text-align: right;\">   0.259698 </td><td style=\"text-align: right;\">   5.25336e+14</td><td style=\"text-align: right;\">                   1</td><td>7debcdf3  </td><td style=\"text-align: right;\">    0.315566 </td></tr>\n",
       "<tr><td>FSR_Trainable_7eecfc37</td><td>2023-08-11_05-14-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    276.324</td><td style=\"text-align: right;\"> 4.74335e+17</td><td style=\"text-align: right;\">   0.143131 </td><td>172.26.215.93</td><td style=\"text-align: right;\">253932</td><td style=\"text-align: right;\">     445.989</td><td style=\"text-align: right;\">            1.9758  </td><td style=\"text-align: right;\">          0.994529</td><td style=\"text-align: right;\">      1.9758  </td><td style=\"text-align: right;\"> 1691698473</td><td style=\"text-align: right;\">   0.10131  </td><td style=\"text-align: right;\">   1.82737e+14</td><td style=\"text-align: right;\">                   2</td><td>7eecfc37  </td><td style=\"text-align: right;\">    0.143131 </td></tr>\n",
       "<tr><td>FSR_Trainable_8067426e</td><td>2023-08-11_05-27-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    109.91 </td><td style=\"text-align: right;\"> 1.05704e+17</td><td style=\"text-align: right;\">   0.0702086</td><td>172.26.215.93</td><td style=\"text-align: right;\">268035</td><td style=\"text-align: right;\">     206.664</td><td style=\"text-align: right;\">           69.6883  </td><td style=\"text-align: right;\">          0.730762</td><td style=\"text-align: right;\">     69.6883  </td><td style=\"text-align: right;\"> 1691699250</td><td style=\"text-align: right;\">   0.0421488</td><td style=\"text-align: right;\">   4.59168e+13</td><td style=\"text-align: right;\">                 100</td><td>8067426e  </td><td style=\"text-align: right;\">    0.0702086</td></tr>\n",
       "<tr><td>FSR_Trainable_816e65ef</td><td>2023-08-11_05-20-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    137.942</td><td style=\"text-align: right;\"> 1.79206e+17</td><td style=\"text-align: right;\">   0.0809611</td><td>172.26.215.93</td><td style=\"text-align: right;\">261135</td><td style=\"text-align: right;\">     244.99 </td><td style=\"text-align: right;\">            6.35228 </td><td style=\"text-align: right;\">          0.783645</td><td style=\"text-align: right;\">      6.35228 </td><td style=\"text-align: right;\"> 1691698843</td><td style=\"text-align: right;\">   0.0516845</td><td style=\"text-align: right;\">   7.38421e+13</td><td style=\"text-align: right;\">                   8</td><td>816e65ef  </td><td style=\"text-align: right;\">    0.0809611</td></tr>\n",
       "<tr><td>FSR_Trainable_84973430</td><td>2023-08-11_05-16-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    111.125</td><td style=\"text-align: right;\"> 1.06103e+17</td><td style=\"text-align: right;\">   0.0709367</td><td>172.26.215.93</td><td style=\"text-align: right;\">254667</td><td style=\"text-align: right;\">     209.389</td><td style=\"text-align: right;\">           82.3411  </td><td style=\"text-align: right;\">          0.627789</td><td style=\"text-align: right;\">     82.3411  </td><td style=\"text-align: right;\"> 1691698616</td><td style=\"text-align: right;\">   0.042137 </td><td style=\"text-align: right;\">   4.47847e+13</td><td style=\"text-align: right;\">                 100</td><td>84973430  </td><td style=\"text-align: right;\">    0.0709367</td></tr>\n",
       "<tr><td>FSR_Trainable_84dd00bc</td><td>2023-08-11_05-21-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    134.812</td><td style=\"text-align: right;\"> 1.6918e+17 </td><td style=\"text-align: right;\">   0.0801731</td><td>172.26.215.93</td><td style=\"text-align: right;\">261586</td><td style=\"text-align: right;\">     238.812</td><td style=\"text-align: right;\">            7.23032 </td><td style=\"text-align: right;\">          0.477232</td><td style=\"text-align: right;\">      7.23032 </td><td style=\"text-align: right;\"> 1691698864</td><td style=\"text-align: right;\">   0.0512643</td><td style=\"text-align: right;\">   7.22801e+13</td><td style=\"text-align: right;\">                   8</td><td>84dd00bc  </td><td style=\"text-align: right;\">    0.0801731</td></tr>\n",
       "<tr><td>FSR_Trainable_854bb80b</td><td>2023-08-11_05-27-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    146.956</td><td style=\"text-align: right;\"> 1.71359e+17</td><td style=\"text-align: right;\">   0.0878811</td><td>172.26.215.93</td><td style=\"text-align: right;\">269943</td><td style=\"text-align: right;\">     269.709</td><td style=\"text-align: right;\">            4.39825 </td><td style=\"text-align: right;\">          0.786388</td><td style=\"text-align: right;\">      4.39825 </td><td style=\"text-align: right;\"> 1691699250</td><td style=\"text-align: right;\">   0.0557921</td><td style=\"text-align: right;\">   7.72161e+13</td><td style=\"text-align: right;\">                   4</td><td>854bb80b  </td><td style=\"text-align: right;\">    0.0878811</td></tr>\n",
       "<tr><td>FSR_Trainable_86266f50</td><td>2023-08-11_05-16-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    110.274</td><td style=\"text-align: right;\"> 1.03254e+17</td><td style=\"text-align: right;\">   0.0706926</td><td>172.26.215.93</td><td style=\"text-align: right;\">254170</td><td style=\"text-align: right;\">     208.935</td><td style=\"text-align: right;\">           83.4918  </td><td style=\"text-align: right;\">          0.755876</td><td style=\"text-align: right;\">     83.4918  </td><td style=\"text-align: right;\"> 1691698583</td><td style=\"text-align: right;\">   0.0416671</td><td style=\"text-align: right;\">   4.29733e+13</td><td style=\"text-align: right;\">                 100</td><td>86266f50  </td><td style=\"text-align: right;\">    0.0706926</td></tr>\n",
       "<tr><td>FSR_Trainable_87366ad5</td><td>2023-08-11_05-17-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    109.413</td><td style=\"text-align: right;\"> 1.98258e+08</td><td style=\"text-align: right;\">   0.562654 </td><td>172.26.215.93</td><td style=\"text-align: right;\">257484</td><td style=\"text-align: right;\">     210.613</td><td style=\"text-align: right;\">            1.41831 </td><td style=\"text-align: right;\">          1.41831 </td><td style=\"text-align: right;\">      1.41831 </td><td style=\"text-align: right;\"> 1691698661</td><td style=\"text-align: right;\">   0.310264 </td><td style=\"text-align: right;\">   1.95593    </td><td style=\"text-align: right;\">                   1</td><td>87366ad5  </td><td style=\"text-align: right;\">    0.562654 </td></tr>\n",
       "<tr><td>FSR_Trainable_87c5d6ce</td><td>2023-08-11_05-27-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    374.974</td><td style=\"text-align: right;\"> 2.97003e+17</td><td style=\"text-align: right;\">   0.206667 </td><td>172.26.215.93</td><td style=\"text-align: right;\">270401</td><td style=\"text-align: right;\">     768.948</td><td style=\"text-align: right;\">            1.07698 </td><td style=\"text-align: right;\">          1.07698 </td><td style=\"text-align: right;\">      1.07698 </td><td style=\"text-align: right;\"> 1691699270</td><td style=\"text-align: right;\">   0.120139 </td><td style=\"text-align: right;\">   1.14302e+14</td><td style=\"text-align: right;\">                   1</td><td>87c5d6ce  </td><td style=\"text-align: right;\">    0.206667 </td></tr>\n",
       "<tr><td>FSR_Trainable_891b6b9f</td><td>2023-08-11_05-17-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    254.513</td><td style=\"text-align: right;\"> 3.68133e+17</td><td style=\"text-align: right;\">   0.143273 </td><td>172.26.215.93</td><td style=\"text-align: right;\">257120</td><td style=\"text-align: right;\">     447.02 </td><td style=\"text-align: right;\">            2.00434 </td><td style=\"text-align: right;\">          0.723774</td><td style=\"text-align: right;\">      2.00434 </td><td style=\"text-align: right;\"> 1691698649</td><td style=\"text-align: right;\">   0.0898181</td><td style=\"text-align: right;\">   1.30631e+14</td><td style=\"text-align: right;\">                   2</td><td>891b6b9f  </td><td style=\"text-align: right;\">    0.143273 </td></tr>\n",
       "<tr><td>FSR_Trainable_8f5659b5</td><td>2023-08-11_05-25-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    178.818</td><td style=\"text-align: right;\"> 2.0101e+17 </td><td style=\"text-align: right;\">   0.106597 </td><td>172.26.215.93</td><td style=\"text-align: right;\">267822</td><td style=\"text-align: right;\">     338.71 </td><td style=\"text-align: right;\">            3.33677 </td><td style=\"text-align: right;\">          1.11306 </td><td style=\"text-align: right;\">      3.33677 </td><td style=\"text-align: right;\"> 1691699153</td><td style=\"text-align: right;\">   0.0648362</td><td style=\"text-align: right;\">   8.24642e+13</td><td style=\"text-align: right;\">                   2</td><td>8f5659b5  </td><td style=\"text-align: right;\">    0.106597 </td></tr>\n",
       "<tr><td>FSR_Trainable_92ba28cc</td><td>2023-08-11_05-22-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    288.871</td><td style=\"text-align: right;\"> 3.55557e+17</td><td style=\"text-align: right;\">   0.16798  </td><td>172.26.215.93</td><td style=\"text-align: right;\">264036</td><td style=\"text-align: right;\">     502.015</td><td style=\"text-align: right;\">            1.234   </td><td style=\"text-align: right;\">          1.234   </td><td style=\"text-align: right;\">      1.234   </td><td style=\"text-align: right;\"> 1691698947</td><td style=\"text-align: right;\">   0.112215 </td><td style=\"text-align: right;\">   1.76055e+14</td><td style=\"text-align: right;\">                   1</td><td>92ba28cc  </td><td style=\"text-align: right;\">    0.16798  </td></tr>\n",
       "<tr><td>FSR_Trainable_972b67b6</td><td>2023-08-11_05-18-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    138.875</td><td style=\"text-align: right;\"> 2.16575e+08</td><td style=\"text-align: right;\">   0.693979 </td><td>172.26.215.93</td><td style=\"text-align: right;\">258228</td><td style=\"text-align: right;\">     264.243</td><td style=\"text-align: right;\">            1.75564 </td><td style=\"text-align: right;\">          1.75564 </td><td style=\"text-align: right;\">      1.75564 </td><td style=\"text-align: right;\"> 1691698689</td><td style=\"text-align: right;\">   0.388989 </td><td style=\"text-align: right;\">   1.95147    </td><td style=\"text-align: right;\">                   1</td><td>972b67b6  </td><td style=\"text-align: right;\">    0.693979 </td></tr>\n",
       "<tr><td>FSR_Trainable_988be0f4</td><td>2023-08-11_05-25-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    132.973</td><td style=\"text-align: right;\"> 2.36961e+08</td><td style=\"text-align: right;\">   0.643209 </td><td>172.26.215.93</td><td style=\"text-align: right;\">267106</td><td style=\"text-align: right;\">     238.156</td><td style=\"text-align: right;\">            1.80263 </td><td style=\"text-align: right;\">          1.80263 </td><td style=\"text-align: right;\">      1.80263 </td><td style=\"text-align: right;\"> 1691699111</td><td style=\"text-align: right;\">   0.379179 </td><td style=\"text-align: right;\">   2.01335    </td><td style=\"text-align: right;\">                   1</td><td>988be0f4  </td><td style=\"text-align: right;\">    0.643209 </td></tr>\n",
       "<tr><td>FSR_Trainable_98ec7841</td><td>2023-08-11_05-24-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    113.13 </td><td style=\"text-align: right;\"> 9.86658e+16</td><td style=\"text-align: right;\">   0.0728378</td><td>172.26.215.93</td><td style=\"text-align: right;\">266164</td><td style=\"text-align: right;\">     218.297</td><td style=\"text-align: right;\">           30.6326  </td><td style=\"text-align: right;\">          1.29948 </td><td style=\"text-align: right;\">     30.6326  </td><td style=\"text-align: right;\"> 1691699077</td><td style=\"text-align: right;\">   0.042847 </td><td style=\"text-align: right;\">   4.35873e+13</td><td style=\"text-align: right;\">                  32</td><td>98ec7841  </td><td style=\"text-align: right;\">    0.0728378</td></tr>\n",
       "<tr><td>FSR_Trainable_9bf7eb16</td><td>2023-08-11_05-11-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    107.529</td><td style=\"text-align: right;\"> 1.04303e+17</td><td style=\"text-align: right;\">   0.0690789</td><td>172.26.215.93</td><td style=\"text-align: right;\">249468</td><td style=\"text-align: right;\">     203.148</td><td style=\"text-align: right;\">           78.0229  </td><td style=\"text-align: right;\">          0.695372</td><td style=\"text-align: right;\">     78.0229  </td><td style=\"text-align: right;\"> 1691698299</td><td style=\"text-align: right;\">   0.0412636</td><td style=\"text-align: right;\">   4.52468e+13</td><td style=\"text-align: right;\">                 100</td><td>9bf7eb16  </td><td style=\"text-align: right;\">    0.0690789</td></tr>\n",
       "<tr><td>FSR_Trainable_ad27d160</td><td>2023-08-11_05-12-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    173.163</td><td style=\"text-align: right;\"> 2.45369e+17</td><td style=\"text-align: right;\">   0.099359 </td><td>172.26.215.93</td><td style=\"text-align: right;\">252212</td><td style=\"text-align: right;\">     300.81 </td><td style=\"text-align: right;\">            3.62041 </td><td style=\"text-align: right;\">          0.77756 </td><td style=\"text-align: right;\">      3.62041 </td><td style=\"text-align: right;\"> 1691698357</td><td style=\"text-align: right;\">   0.0639394</td><td style=\"text-align: right;\">   9.51491e+13</td><td style=\"text-align: right;\">                   4</td><td>ad27d160  </td><td style=\"text-align: right;\">    0.099359 </td></tr>\n",
       "<tr><td>FSR_Trainable_ae6f907f</td><td>2023-08-11_05-19-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    177.072</td><td style=\"text-align: right;\"> 1.44877e+17</td><td style=\"text-align: right;\">   0.106157 </td><td>172.26.215.93</td><td style=\"text-align: right;\">259452</td><td style=\"text-align: right;\">     339.076</td><td style=\"text-align: right;\">            2.16567 </td><td style=\"text-align: right;\">          1.12587 </td><td style=\"text-align: right;\">      2.16567 </td><td style=\"text-align: right;\"> 1691698750</td><td style=\"text-align: right;\">   0.062523 </td><td style=\"text-align: right;\">   5.81034e+13</td><td style=\"text-align: right;\">                   2</td><td>ae6f907f  </td><td style=\"text-align: right;\">    0.106157 </td></tr>\n",
       "<tr><td>FSR_Trainable_b3901d26</td><td>2023-08-11_05-24-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    229.567</td><td style=\"text-align: right;\"> 1.31589e+16</td><td style=\"text-align: right;\">   5.06583  </td><td>172.26.215.93</td><td style=\"text-align: right;\">266610</td><td style=\"text-align: right;\">     450.597</td><td style=\"text-align: right;\">            0.976343</td><td style=\"text-align: right;\">          0.976343</td><td style=\"text-align: right;\">      0.976343</td><td style=\"text-align: right;\"> 1691699087</td><td style=\"text-align: right;\">   1.85988  </td><td style=\"text-align: right;\">   8.78693e+14</td><td style=\"text-align: right;\">                   1</td><td>b3901d26  </td><td style=\"text-align: right;\">    5.06583  </td></tr>\n",
       "<tr><td>FSR_Trainable_b4ab27a4</td><td>2023-08-11_05-11-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 206678    </td><td style=\"text-align: right;\"> 5.62439e+11</td><td style=\"text-align: right;\">1019.64     </td><td>172.26.215.93</td><td style=\"text-align: right;\">251154</td><td style=\"text-align: right;\">  421890    </td><td style=\"text-align: right;\">            1.42276 </td><td style=\"text-align: right;\">          1.42276 </td><td style=\"text-align: right;\">      1.42276 </td><td style=\"text-align: right;\"> 1691698302</td><td style=\"text-align: right;\"> 601.589    </td><td style=\"text-align: right;\">3839.34       </td><td style=\"text-align: right;\">                   1</td><td>b4ab27a4  </td><td style=\"text-align: right;\"> 1019.64     </td></tr>\n",
       "<tr><td>FSR_Trainable_b5687929</td><td>2023-08-11_05-10-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    223.427</td><td style=\"text-align: right;\"> 3.70347e+16</td><td style=\"text-align: right;\">   3.67662  </td><td>172.26.215.93</td><td style=\"text-align: right;\">250243</td><td style=\"text-align: right;\">     538.144</td><td style=\"text-align: right;\">            0.946659</td><td style=\"text-align: right;\">          0.946659</td><td style=\"text-align: right;\">      0.946659</td><td style=\"text-align: right;\"> 1691698249</td><td style=\"text-align: right;\">   1.56477  </td><td style=\"text-align: right;\">   1.74719e+15</td><td style=\"text-align: right;\">                   1</td><td>b5687929  </td><td style=\"text-align: right;\">    3.67662  </td></tr>\n",
       "<tr><td>FSR_Trainable_c7d66c81</td><td>2023-08-11_05-28-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    110.61 </td><td style=\"text-align: right;\"> 1.01819e+17</td><td style=\"text-align: right;\">   0.0711004</td><td>172.26.215.93</td><td style=\"text-align: right;\">269719</td><td style=\"text-align: right;\">     210.167</td><td style=\"text-align: right;\">           68.9551  </td><td style=\"text-align: right;\">          0.327903</td><td style=\"text-align: right;\">     68.9551  </td><td style=\"text-align: right;\"> 1691699318</td><td style=\"text-align: right;\">   0.042169 </td><td style=\"text-align: right;\">   4.40945e+13</td><td style=\"text-align: right;\">                 100</td><td>c7d66c81  </td><td style=\"text-align: right;\">    0.0711004</td></tr>\n",
       "<tr><td>FSR_Trainable_c7fcd88e</td><td>2023-08-11_05-24-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    110.905</td><td style=\"text-align: right;\"> 1.04109e+17</td><td style=\"text-align: right;\">   0.0710329</td><td>172.26.215.93</td><td style=\"text-align: right;\">265034</td><td style=\"text-align: right;\">     209.816</td><td style=\"text-align: right;\">           88.6218  </td><td style=\"text-align: right;\">          0.710604</td><td style=\"text-align: right;\">     88.6218  </td><td style=\"text-align: right;\"> 1691699094</td><td style=\"text-align: right;\">   0.0421582</td><td style=\"text-align: right;\">   4.4442e+13 </td><td style=\"text-align: right;\">                 100</td><td>c7fcd88e  </td><td style=\"text-align: right;\">    0.0710329</td></tr>\n",
       "<tr><td>FSR_Trainable_c9309d56</td><td>2023-08-11_05-26-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    224.589</td><td style=\"text-align: right;\"> 3.24888e+17</td><td style=\"text-align: right;\">   0.12499  </td><td>172.26.215.93</td><td style=\"text-align: right;\">268247</td><td style=\"text-align: right;\">     398.376</td><td style=\"text-align: right;\">            2.48554 </td><td style=\"text-align: right;\">          0.926049</td><td style=\"text-align: right;\">      2.48554 </td><td style=\"text-align: right;\"> 1691699175</td><td style=\"text-align: right;\">   0.0794544</td><td style=\"text-align: right;\">   1.17995e+14</td><td style=\"text-align: right;\">                   2</td><td>c9309d56  </td><td style=\"text-align: right;\">    0.12499  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc8df67d</td><td>2023-08-11_05-17-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    313.196</td><td style=\"text-align: right;\"> 4.5415e+17 </td><td style=\"text-align: right;\">   0.165141 </td><td>172.26.215.93</td><td style=\"text-align: right;\">256545</td><td style=\"text-align: right;\">     502.105</td><td style=\"text-align: right;\">            2.09357 </td><td style=\"text-align: right;\">          0.648778</td><td style=\"text-align: right;\">      2.09357 </td><td style=\"text-align: right;\"> 1691698625</td><td style=\"text-align: right;\">   0.115234 </td><td style=\"text-align: right;\">   1.84543e+14</td><td style=\"text-align: right;\">                   2</td><td>cc8df67d  </td><td style=\"text-align: right;\">    0.165141 </td></tr>\n",
       "<tr><td>FSR_Trainable_ce34eb03</td><td>2023-08-11_05-24-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">    113.627</td><td style=\"text-align: right;\"> 1.00799e+17</td><td style=\"text-align: right;\">   0.0728557</td><td>172.26.215.93</td><td style=\"text-align: right;\">265657</td><td style=\"text-align: right;\">     217.434</td><td style=\"text-align: right;\">           32.8927  </td><td style=\"text-align: right;\">          1.07116 </td><td style=\"text-align: right;\">     32.8927  </td><td style=\"text-align: right;\"> 1691699059</td><td style=\"text-align: right;\">   0.0431565</td><td style=\"text-align: right;\">   4.47016e+13</td><td style=\"text-align: right;\">                  32</td><td>ce34eb03  </td><td style=\"text-align: right;\">    0.0728557</td></tr>\n",
       "<tr><td>FSR_Trainable_d06442e2</td><td>2023-08-11_05-21-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    236.348</td><td style=\"text-align: right;\"> 1.79625e+17</td><td style=\"text-align: right;\">   0.151322 </td><td>172.26.215.93</td><td style=\"text-align: right;\">262914</td><td style=\"text-align: right;\">     482.275</td><td style=\"text-align: right;\">            1.14272 </td><td style=\"text-align: right;\">          1.14272 </td><td style=\"text-align: right;\">      1.14272 </td><td style=\"text-align: right;\"> 1691698910</td><td style=\"text-align: right;\">   0.0838368</td><td style=\"text-align: right;\">   7.29915e+13</td><td style=\"text-align: right;\">                   1</td><td>d06442e2  </td><td style=\"text-align: right;\">    0.151322 </td></tr>\n",
       "<tr><td>FSR_Trainable_d1434ee3</td><td>2023-08-11_05-17-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    283.978</td><td style=\"text-align: right;\"> 2.4253e+16 </td><td style=\"text-align: right;\">   5.10449  </td><td>172.26.215.93</td><td style=\"text-align: right;\">257803</td><td style=\"text-align: right;\">     613.246</td><td style=\"text-align: right;\">            1.40397 </td><td style=\"text-align: right;\">          1.40397 </td><td style=\"text-align: right;\">      1.40397 </td><td style=\"text-align: right;\"> 1691698670</td><td style=\"text-align: right;\">   1.94162  </td><td style=\"text-align: right;\">   1.10359e+15</td><td style=\"text-align: right;\">                   1</td><td>d1434ee3  </td><td style=\"text-align: right;\">    5.10449  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4d2b4e8</td><td>2023-08-11_05-21-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    205.991</td><td style=\"text-align: right;\"> 2.68854e+17</td><td style=\"text-align: right;\">   0.119473 </td><td>172.26.215.93</td><td style=\"text-align: right;\">262324</td><td style=\"text-align: right;\">     372.858</td><td style=\"text-align: right;\">            1.95009 </td><td style=\"text-align: right;\">          0.768235</td><td style=\"text-align: right;\">      1.95009 </td><td style=\"text-align: right;\"> 1691698891</td><td style=\"text-align: right;\">   0.0732465</td><td style=\"text-align: right;\">   9.7177e+13 </td><td style=\"text-align: right;\">                   2</td><td>d4d2b4e8  </td><td style=\"text-align: right;\">    0.119473 </td></tr>\n",
       "<tr><td>FSR_Trainable_d502195a</td><td>2023-08-11_05-20-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">    132.286</td><td style=\"text-align: right;\"> 1.57999e+17</td><td style=\"text-align: right;\">   0.0798937</td><td>172.26.215.93</td><td style=\"text-align: right;\">261359</td><td style=\"text-align: right;\">     242.167</td><td style=\"text-align: right;\">            5.39616 </td><td style=\"text-align: right;\">          0.574696</td><td style=\"text-align: right;\">      5.39616 </td><td style=\"text-align: right;\"> 1691698853</td><td style=\"text-align: right;\">   0.0492345</td><td style=\"text-align: right;\">   6.41256e+13</td><td style=\"text-align: right;\">                   8</td><td>d502195a  </td><td style=\"text-align: right;\">    0.0798937</td></tr>\n",
       "<tr><td>FSR_Trainable_d759cdd5</td><td>2023-08-11_05-22-07</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    226.045</td><td style=\"text-align: right;\"> 3.13374e+17</td><td style=\"text-align: right;\">   0.130788 </td><td>172.26.215.93</td><td style=\"text-align: right;\">263316</td><td style=\"text-align: right;\">     409.903</td><td style=\"text-align: right;\">            1.88648 </td><td style=\"text-align: right;\">          0.618613</td><td style=\"text-align: right;\">      1.88648 </td><td style=\"text-align: right;\"> 1691698927</td><td style=\"text-align: right;\">   0.0794289</td><td style=\"text-align: right;\">   1.1043e+14 </td><td style=\"text-align: right;\">                   2</td><td>d759cdd5  </td><td style=\"text-align: right;\">    0.130788 </td></tr>\n",
       "<tr><td>FSR_Trainable_dbe2f24c</td><td>2023-08-11_05-22-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    186.501</td><td style=\"text-align: right;\"> 2.40237e+17</td><td style=\"text-align: right;\">   0.108655 </td><td>172.26.215.93</td><td style=\"text-align: right;\">263631</td><td style=\"text-align: right;\">     337.691</td><td style=\"text-align: right;\">            1.92331 </td><td style=\"text-align: right;\">          0.7379  </td><td style=\"text-align: right;\">      1.92331 </td><td style=\"text-align: right;\"> 1691698935</td><td style=\"text-align: right;\">   0.0676761</td><td style=\"text-align: right;\">   9.16203e+13</td><td style=\"text-align: right;\">                   2</td><td>dbe2f24c  </td><td style=\"text-align: right;\">    0.108655 </td></tr>\n",
       "<tr><td>FSR_Trainable_dbec4dcf</td><td>2023-08-11_05-26-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">    114.971</td><td style=\"text-align: right;\"> 1.02759e+17</td><td style=\"text-align: right;\">   0.0742187</td><td>172.26.215.93</td><td style=\"text-align: right;\">268474</td><td style=\"text-align: right;\">     222.181</td><td style=\"text-align: right;\">           13.2532  </td><td style=\"text-align: right;\">          0.88681 </td><td style=\"text-align: right;\">     13.2532  </td><td style=\"text-align: right;\"> 1691699196</td><td style=\"text-align: right;\">   0.0436019</td><td style=\"text-align: right;\">   4.54507e+13</td><td style=\"text-align: right;\">                  16</td><td>dbec4dcf  </td><td style=\"text-align: right;\">    0.0742187</td></tr>\n",
       "<tr><td>FSR_Trainable_e2a4c7eb</td><td>2023-08-11_05-27-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    148.644</td><td style=\"text-align: right;\"> 1.93911e+17</td><td style=\"text-align: right;\">   0.0862344</td><td>172.26.215.93</td><td style=\"text-align: right;\">270160</td><td style=\"text-align: right;\">     257.694</td><td style=\"text-align: right;\">            5.22745 </td><td style=\"text-align: right;\">          1.7603  </td><td style=\"text-align: right;\">      5.22745 </td><td style=\"text-align: right;\"> 1691699263</td><td style=\"text-align: right;\">   0.0558343</td><td style=\"text-align: right;\">   7.9434e+13 </td><td style=\"text-align: right;\">                   4</td><td>e2a4c7eb  </td><td style=\"text-align: right;\">    0.0862344</td></tr>\n",
       "<tr><td>FSR_Trainable_e3a71d59</td><td>2023-08-11_05-21-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    257.342</td><td style=\"text-align: right;\"> 4.07678e+17</td><td style=\"text-align: right;\">   0.139821 </td><td>172.26.215.93</td><td style=\"text-align: right;\">262830</td><td style=\"text-align: right;\">     438.549</td><td style=\"text-align: right;\">            1.80557 </td><td style=\"text-align: right;\">          0.679577</td><td style=\"text-align: right;\">      1.80557 </td><td style=\"text-align: right;\"> 1691698906</td><td style=\"text-align: right;\">   0.0925322</td><td style=\"text-align: right;\">   1.53481e+14</td><td style=\"text-align: right;\">                   2</td><td>e3a71d59  </td><td style=\"text-align: right;\">    0.139821 </td></tr>\n",
       "<tr><td>FSR_Trainable_e880e411</td><td>2023-08-11_05-15-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    467.369</td><td style=\"text-align: right;\"> 5.40688e+17</td><td style=\"text-align: right;\">   0.228659 </td><td>172.26.215.93</td><td style=\"text-align: right;\">254459</td><td style=\"text-align: right;\">     810.84 </td><td style=\"text-align: right;\">            1.39108 </td><td style=\"text-align: right;\">          1.39108 </td><td style=\"text-align: right;\">      1.39108 </td><td style=\"text-align: right;\"> 1691698505</td><td style=\"text-align: right;\">   0.161571 </td><td style=\"text-align: right;\">   2.41639e+14</td><td style=\"text-align: right;\">                   1</td><td>e880e411  </td><td style=\"text-align: right;\">    0.228659 </td></tr>\n",
       "<tr><td>FSR_Trainable_ea42184f</td><td>2023-08-11_05-14-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    635.332</td><td style=\"text-align: right;\"> 8.50593e+17</td><td style=\"text-align: right;\">   0.320983 </td><td>172.26.215.93</td><td style=\"text-align: right;\">253447</td><td style=\"text-align: right;\">     864.188</td><td style=\"text-align: right;\">            0.957803</td><td style=\"text-align: right;\">          0.957803</td><td style=\"text-align: right;\">      0.957803</td><td style=\"text-align: right;\"> 1691698443</td><td style=\"text-align: right;\">   0.258023 </td><td style=\"text-align: right;\">   4.15169e+14</td><td style=\"text-align: right;\">                   1</td><td>ea42184f  </td><td style=\"text-align: right;\">    0.320983 </td></tr>\n",
       "<tr><td>FSR_Trainable_f521736c</td><td>2023-08-11_05-21-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    288.347</td><td style=\"text-align: right;\"> 1.57711e+16</td><td style=\"text-align: right;\">   5.04244  </td><td>172.26.215.93</td><td style=\"text-align: right;\">262508</td><td style=\"text-align: right;\">     640.834</td><td style=\"text-align: right;\">            1.42404 </td><td style=\"text-align: right;\">          1.42404 </td><td style=\"text-align: right;\">      1.42404 </td><td style=\"text-align: right;\"> 1691698895</td><td style=\"text-align: right;\">   1.81966  </td><td style=\"text-align: right;\">   8.17121e+14</td><td style=\"text-align: right;\">                   1</td><td>f521736c  </td><td style=\"text-align: right;\">    5.04244  </td></tr>\n",
       "<tr><td>FSR_Trainable_f719134f</td><td>2023-08-11_05-17-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    243.896</td><td style=\"text-align: right;\"> 3.45025e+17</td><td style=\"text-align: right;\">   0.13786  </td><td>172.26.215.93</td><td style=\"text-align: right;\">256798</td><td style=\"text-align: right;\">     431.264</td><td style=\"text-align: right;\">            1.92012 </td><td style=\"text-align: right;\">          0.706811</td><td style=\"text-align: right;\">      1.92012 </td><td style=\"text-align: right;\"> 1691698634</td><td style=\"text-align: right;\">   0.0879727</td><td style=\"text-align: right;\">   1.33343e+14</td><td style=\"text-align: right;\">                   2</td><td>f719134f  </td><td style=\"text-align: right;\">    0.13786  </td></tr>\n",
       "<tr><td>FSR_Trainable_f7d46d05</td><td>2023-08-11_05-28-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">    203.601</td><td style=\"text-align: right;\"> 2.60208e+17</td><td style=\"text-align: right;\">   0.118345 </td><td>172.26.215.93</td><td style=\"text-align: right;\">270869</td><td style=\"text-align: right;\">     374.906</td><td style=\"text-align: right;\">            2.18027 </td><td style=\"text-align: right;\">          1.15468 </td><td style=\"text-align: right;\">      2.18027 </td><td style=\"text-align: right;\"> 1691699295</td><td style=\"text-align: right;\">   0.0710396</td><td style=\"text-align: right;\">   9.07133e+13</td><td style=\"text-align: right;\">                   2</td><td>f7d46d05  </td><td style=\"text-align: right;\">    0.118345 </td></tr>\n",
       "<tr><td>FSR_Trainable_f7f51bca</td><td>2023-08-11_05-15-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    117.629</td><td style=\"text-align: right;\"> 1.11412e+17</td><td style=\"text-align: right;\">   0.0746233</td><td>172.26.215.93</td><td style=\"text-align: right;\">252968</td><td style=\"text-align: right;\">     220.384</td><td style=\"text-align: right;\">           80.4897  </td><td style=\"text-align: right;\">          0.745647</td><td style=\"text-align: right;\">     80.4897  </td><td style=\"text-align: right;\"> 1691698508</td><td style=\"text-align: right;\">   0.04484  </td><td style=\"text-align: right;\">   4.85148e+13</td><td style=\"text-align: right;\">                 100</td><td>f7f51bca  </td><td style=\"text-align: right;\">    0.0746233</td></tr>\n",
       "<tr><td>FSR_Trainable_f9e02361</td><td>2023-08-11_05-12-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">    389.077</td><td style=\"text-align: right;\"> 6.87829e+17</td><td style=\"text-align: right;\">   0.194588 </td><td>172.26.215.93</td><td style=\"text-align: right;\">251389</td><td style=\"text-align: right;\">     566.697</td><td style=\"text-align: right;\">            3.99218 </td><td style=\"text-align: right;\">          0.762076</td><td style=\"text-align: right;\">      3.99218 </td><td style=\"text-align: right;\"> 1691698320</td><td style=\"text-align: right;\">   0.153425 </td><td style=\"text-align: right;\">   2.97621e+14</td><td style=\"text-align: right;\">                   4</td><td>f9e02361  </td><td style=\"text-align: right;\">    0.194588 </td></tr>\n",
       "<tr><td>FSR_Trainable_f9f5f046</td><td>2023-08-11_05-19-48</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">    148.083</td><td style=\"text-align: right;\"> 3.64834e+08</td><td style=\"text-align: right;\">   0.664579 </td><td>172.26.215.93</td><td style=\"text-align: right;\">260160</td><td style=\"text-align: right;\">     259.336</td><td style=\"text-align: right;\">            1.25605 </td><td style=\"text-align: right;\">          1.25605 </td><td style=\"text-align: right;\">      1.25605 </td><td style=\"text-align: right;\"> 1691698788</td><td style=\"text-align: right;\">   0.398659 </td><td style=\"text-align: right;\">   2.11018    </td><td style=\"text-align: right;\">                   1</td><td>f9f5f046  </td><td style=\"text-align: right;\">    0.664579 </td></tr>\n",
       "<tr><td>FSR_Trainable_fed368c0</td><td>2023-08-11_05-21-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">    110.512</td><td style=\"text-align: right;\"> 1.04904e+17</td><td style=\"text-align: right;\">   0.0704898</td><td>172.26.215.93</td><td style=\"text-align: right;\">259679</td><td style=\"text-align: right;\">     207.821</td><td style=\"text-align: right;\">           81.2409  </td><td style=\"text-align: right;\">          1.28062 </td><td style=\"text-align: right;\">     81.2409  </td><td style=\"text-align: right;\"> 1691698860</td><td style=\"text-align: right;\">   0.0419013</td><td style=\"text-align: right;\">   4.39956e+13</td><td style=\"text-align: right;\">                 100</td><td>fed368c0  </td><td style=\"text-align: right;\">    0.0704898</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_066a794c_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-09-38/wandb/run-20230811_050948-066a794c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Syncing run FSR_Trainable_066a794c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/066a794c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_7dbc4a1e_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-09-43/wandb/run-20230811_050957-7dbc4a1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Syncing run FSR_Trainable_7dbc4a1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7dbc4a1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_4226873b_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-09-50/wandb/run-20230811_051005-4226873b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Syncing run FSR_Trainable_4226873b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4226873b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:                mae_force 181.94358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:               mape_force 2.1557313564952916e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:                   metric 3.90003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:               rmse_force 390.8835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:       time_since_restore 0.77255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:         time_this_iter_s 0.77255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:             time_total_s 0.77255\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:                timestamp 1691698199\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:               tmae_force 1.45985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:              tmape_force 1193742046654679.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb:              trmse_force 3.90003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: 🚀 View run FSR_Trainable_4226873b at: https://wandb.ai/seokjin/FSR-prediction/runs/4226873b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249467)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051005-4226873b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_9bf7eb16_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-09-59/wandb/run-20230811_051013-9bf7eb16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Syncing run FSR_Trainable_9bf7eb16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9bf7eb16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_701b36be_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-10-07/wandb/run-20230811_051024-701b36be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Syncing run FSR_Trainable_701b36be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/701b36be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:                mae_force 301.81357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:               mape_force 1151034791.7371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:                   metric 1.21279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:               rmse_force 540.49909\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:       time_since_restore 0.94215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:         time_this_iter_s 0.94215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:             time_total_s 0.94215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:                timestamp 1691698220\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:               tmae_force 0.76901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:              tmape_force 2.08017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb:              trmse_force 1.21279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: 🚀 View run FSR_Trainable_701b36be at: https://wandb.ai/seokjin/FSR-prediction/runs/701b36be\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249867)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051024-701b36be/logs\n",
      "2023-08-11 05:10:36,449\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.099 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:10:36,453\tWARNING util.py:315 -- The `process_trial_result` operation took 2.105 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:10:36,456\tWARNING util.py:315 -- Processing trial results took 2.108 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:10:36,459\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_44c0d485_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-10-19/wandb/run-20230811_051039-44c0d485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Syncing run FSR_Trainable_44c0d485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/44c0d485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:                mae_force 160.12052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:               mape_force 443166064.69142\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:                   metric 0.66066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:               rmse_force 270.37053\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:       time_since_restore 1.40346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:         time_this_iter_s 0.62105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:             time_total_s 1.40346\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:                timestamp 1691698237\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:               tmae_force 0.4258\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:              tmape_force 2.40597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb:              trmse_force 0.66066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: 🚀 View run FSR_Trainable_44c0d485 at: https://wandb.ai/seokjin/FSR-prediction/runs/44c0d485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250099)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051039-44c0d485/logs\n",
      "2023-08-11 05:10:51,104\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:10:51,109\tWARNING util.py:315 -- The `process_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:10:51,110\tWARNING util.py:315 -- Processing trial results took 1.873 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:10:51,112\tWARNING util.py:315 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_b5687929_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-10-33/wandb/run-20230811_051054-b5687929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Syncing run FSR_Trainable_b5687929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b5687929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:                mae_force 223.42706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:               mape_force 3.7034719152696744e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:                   metric 3.67662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:               rmse_force 538.1444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:       time_since_restore 0.94666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:         time_this_iter_s 0.94666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:             time_total_s 0.94666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:                timestamp 1691698249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:               tmae_force 1.56477\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:              tmape_force 1747190159782958.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb:              trmse_force 3.67662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: 🚀 View run FSR_Trainable_b5687929 at: https://wandb.ai/seokjin/FSR-prediction/runs/b5687929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250331)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051054-b5687929/logs\n",
      "2023-08-11 05:11:06,898\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.355 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:06,902\tWARNING util.py:315 -- The `process_trial_result` operation took 2.360 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:06,905\tWARNING util.py:315 -- Processing trial results took 2.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:11:06,906\tWARNING util.py:315 -- The `process_trial_result` operation took 2.364 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_576c6c49_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-10-48/wandb/run-20230811_051113-576c6c49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Syncing run FSR_Trainable_576c6c49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/576c6c49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:                mae_force 241.06402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:               mape_force 2.4442602554408332e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:                   metric 4.74584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:               rmse_force 487.48573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:       time_since_restore 1.13852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:         time_this_iter_s 1.13852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:             time_total_s 1.13852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:                timestamp 1691698264\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:               tmae_force 1.84047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:              tmape_force 1372056317660989.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb:              trmse_force 4.74584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: 🚀 View run FSR_Trainable_576c6c49 at: https://wandb.ai/seokjin/FSR-prediction/runs/576c6c49\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250565)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051113-576c6c49/logs\n",
      "2023-08-11 05:11:20,570\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.625 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:20,572\tWARNING util.py:315 -- The `process_trial_result` operation took 1.629 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:20,576\tWARNING util.py:315 -- Processing trial results took 1.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:11:20,578\tWARNING util.py:315 -- The `process_trial_result` operation took 1.635 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_17d11d30_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-08-11_05-11-03/wandb/run-20230811_051123-17d11d30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Syncing run FSR_Trainable_17d11d30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/17d11d30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:                mae_force 163.833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:               mape_force 430144531.25731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:                   metric 0.75176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:               rmse_force 328.15189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:       time_since_restore 2.5798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:         time_this_iter_s 1.03749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:             time_total_s 2.5798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:                timestamp 1691698281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:               tmae_force 0.40792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:              tmape_force 2.16413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb:              trmse_force 0.75176\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: 🚀 View run FSR_Trainable_17d11d30 at: https://wandb.ai/seokjin/FSR-prediction/runs/17d11d30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=250787)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051123-17d11d30/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:                mae_force █▆▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:               mape_force █▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:                   metric █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:               rmse_force █▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:         time_this_iter_s ▅▄▁█▃▁▄▃▂▄▅▃▂▃▃▃▃▃▃▄▆▃▄▃▃▃▅▃▃▃▅▅▅▃▃▃▄▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:               tmae_force █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:              tmape_force ▁▃███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb:              trmse_force █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: \n",
      "2023-08-11 05:11:35,459\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:35,467\tWARNING util.py:315 -- The `process_trial_result` operation took 2.193 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:35,469\tWARNING util.py:315 -- Processing trial results took 2.195 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:11:35,471\tWARNING util.py:315 -- The `process_trial_result` operation took 2.197 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249121)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_2c2d0384_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-11-17/wandb/run-20230811_051138-2c2d0384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Syncing run FSR_Trainable_2c2d0384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2c2d0384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:                mae_force █▇▄▄▃▃▃▄▃▃▃▄▃▃▃▃▃▂▂▃▃▃▃▂▁▂▂▂▂▃▃▁▄▄▄▄▄▃▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:               mape_force ▇▃▄▄▂▃▃▄▄▄▄▄▄▄▃▃▃▂▂▃▃▂▂▂▁▂▃▃▃▄▄▄▅▅▅▅▄▄▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:                   metric █▆▃▄▄▃▃▄▃▄▃▄▃▃▃▄▃▃▃▄▄▃▃▃▂▃▃▂▂▂▃▁▄▃▄▃▄▃▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:               rmse_force █▆▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▂▃▂▂▂▂▁▂▂▂▁▃▂▃▃▃▂▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:         time_this_iter_s █▂▁▄▂▂▃▄▃▂▃▂▄▂▃▄▅▂▄▂▂▃▅▄▃▃▃▄▃▂▂▃▂▃▂▃▂▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:               tmae_force █▇▃▄▃▃▃▄▃▃▃▃▃▃▃▄▃▂▂▄▄▃▃▂▁▂▂▂▂▂▃▁▄▃▅▄▄▅▅▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:              tmape_force ▆▃▃▄▃▃▃▄▃▄▄▃▃▄▃▃▃▂▂▃▃▃▂▂▁▂▂▃▃▃▄▃▄▄▅▅▄▅▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:              trmse_force █▆▃▄▄▃▃▄▃▄▃▄▃▃▃▄▃▃▃▄▄▃▃▃▂▃▃▂▂▂▃▁▄▃▄▃▄▃▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:                mae_force 149.73595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:               mape_force 1.4947673018909978e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:                   metric 0.09044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:               rmse_force 272.87505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:       time_since_restore 89.30081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:         time_this_iter_s 0.84268\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:             time_total_s 89.30081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:                timestamp 1691698299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:               tmae_force 0.05606\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:              tmape_force 63731675552858.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb:              trmse_force 0.09044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: 🚀 View run FSR_Trainable_7dbc4a1e at: https://wandb.ai/seokjin/FSR-prediction/runs/7dbc4a1e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249293)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_050957-7dbc4a1e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:                mae_force ██▆▅▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:               mape_force ▄█▆▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:                   metric █▇▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:               rmse_force █▇▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:         time_this_iter_s ▆▅▂▄▄▇▄▅▃▄▂▄▅▄▇▄▄▂▂█▄▄▅▇▅▇▃▂▄▄▇▃▃▃▄▁▂▆▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:               tmae_force ██▆▅▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:              tmape_force ▄█▆▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb:              trmse_force █▇▅▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:              trmse_force ▁\n",
      "2023-08-11 05:11:45,361\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.457 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:45,366\tWARNING util.py:315 -- The `process_trial_result` operation took 2.462 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:45,368\tWARNING util.py:315 -- Processing trial results took 2.464 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:11:45,372\tWARNING util.py:315 -- The `process_trial_result` operation took 2.468 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_b4ab27a4_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-11-31/wandb/run-20230811_051148-b4ab27a4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Syncing run FSR_Trainable_b4ab27a4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b4ab27a4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=249645)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: iterations_since_restore 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:                mae_force 980.93902\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:               mape_force 3581675050.50399\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:                   metric 3.80998\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:               rmse_force 1802.58446\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:       time_since_restore 1.75518\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:         time_this_iter_s 1.75518\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:             time_total_s 1.75518\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:                timestamp 1691698293\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:               tmae_force 2.53357\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:              tmape_force 15.60278\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:       training_iteration 1\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb:              trmse_force 3.80998\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: 🚀 View run FSR_Trainable_2c2d0384 at: https://wandb.ai/seokjin/FSR-prediction/runs/2c2d0384\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251040)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051138-2c2d0384/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251270)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051148-b4ab27a4/logs\n",
      "2023-08-11 05:11:58,068\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.632 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:58,071\tWARNING util.py:315 -- The `process_trial_result` operation took 2.637 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:11:58,078\tWARNING util.py:315 -- Processing trial results took 2.643 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:11:58,080\tWARNING util.py:315 -- The `process_trial_result` operation took 2.645 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_f9e02361_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-11-41/wandb/run-20230811_051201-f9e02361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Syncing run FSR_Trainable_f9e02361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9e02361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:                mae_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:               mape_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:                   metric █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:               rmse_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:         time_this_iter_s █▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:               tmae_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:              tmape_force █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:              trmse_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:                mae_force 389.07689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:               mape_force 6.878293661814301e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:                   metric 0.19459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:               rmse_force 566.69685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:       time_since_restore 3.99218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:         time_this_iter_s 0.76208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:             time_total_s 3.99218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:                timestamp 1691698320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:               tmae_force 0.15342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:              tmape_force 297621056476648.44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb:              trmse_force 0.19459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: 🚀 View run FSR_Trainable_f9e02361 at: https://wandb.ai/seokjin/FSR-prediction/runs/f9e02361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251510)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051201-f9e02361/logs\n",
      "2023-08-11 05:12:08,584\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.011 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:08,588\tWARNING util.py:315 -- The `process_trial_result` operation took 2.016 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:08,592\tWARNING util.py:315 -- Processing trial results took 2.019 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:12:08,594\tWARNING util.py:315 -- The `process_trial_result` operation took 2.022 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_140519ef_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-11-54/wandb/run-20230811_051211-140519ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Syncing run FSR_Trainable_140519ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/140519ef\n",
      "2023-08-11 05:12:15,594\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:15,599\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:15,601\tWARNING util.py:315 -- Processing trial results took 1.910 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:12:15,602\tWARNING util.py:315 -- The `process_trial_result` operation took 1.911 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_1e9c13e0_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-12-05/wandb/run-20230811_051218-1e9c13e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Syncing run FSR_Trainable_1e9c13e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1e9c13e0\n",
      "2023-08-11 05:12:22,915\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.828 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:22,917\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:22,919\tWARNING util.py:315 -- Processing trial results took 1.833 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:12:22,922\tWARNING util.py:315 -- The `process_trial_result` operation took 1.836 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_12d5687d_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-12-12/wandb/run-20230811_051225-12d5687d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Syncing run FSR_Trainable_12d5687d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/12d5687d\n",
      "2023-08-11 05:12:34,660\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:34,664\tWARNING util.py:315 -- The `process_trial_result` operation took 1.887 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:34,665\tWARNING util.py:315 -- Processing trial results took 1.888 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:12:34,667\tWARNING util.py:315 -- The `process_trial_result` operation took 1.890 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_ad27d160_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-12-20/wandb/run-20230811_051237-ad27d160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Syncing run FSR_Trainable_ad27d160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad27d160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:                mae_force █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:               mape_force █▅▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:                   metric █▆▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:               rmse_force █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:         time_this_iter_s █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:               tmae_force █▆▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:              tmape_force █▅▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:              trmse_force █▆▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:                mae_force 173.16269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:               mape_force 2.453689371066971e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:                   metric 0.09936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:               rmse_force 300.80974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:       time_since_restore 3.62041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:         time_this_iter_s 0.77756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:             time_total_s 3.62041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:                timestamp 1691698357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:               tmae_force 0.06394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:              tmape_force 95149142882333.77\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb:              trmse_force 0.09936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: 🚀 View run FSR_Trainable_ad27d160 at: https://wandb.ai/seokjin/FSR-prediction/runs/ad27d160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252298)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051237-ad27d160/logs\n",
      "2023-08-11 05:12:54,152\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:54,155\tWARNING util.py:315 -- The `process_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:12:54,158\tWARNING util.py:315 -- Processing trial results took 1.874 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:12:54,159\tWARNING util.py:315 -- The `process_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_4712e9ed_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-12-31/wandb/run-20230811_051257-4712e9ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Syncing run FSR_Trainable_4712e9ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4712e9ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:                mae_force █▄▃▂▂▂▃▃▃▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:               mape_force █▁▁▃▄▅▆▇▇▆▆▆▅▅▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:                   metric █▆▅▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:               rmse_force █▇▇▄▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:         time_this_iter_s █▃▃▄▂▂▂▃▂▂▁▂▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:                timestamp ▁▂▃▃▄▄▄▅▅▆▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:               tmae_force █▃▃▄▃▄▅▅▅▄▃▃▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:              tmape_force ▆▁▂▄▅▇███▇▆▆▅▅▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:              trmse_force █▆▅▃▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:                mae_force 144.89672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:               mape_force 1.8253262215989718e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:                   metric 0.08558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:               rmse_force 257.08074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:       time_since_restore 12.12352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:         time_this_iter_s 0.72409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:             time_total_s 12.12352\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:                timestamp 1691698385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:               tmae_force 0.05511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:              tmape_force 79372007186132.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb:              trmse_force 0.08558\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: 🚀 View run FSR_Trainable_4712e9ed at: https://wandb.ai/seokjin/FSR-prediction/runs/4712e9ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252555)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051257-4712e9ed/logs\n",
      "2023-08-11 05:13:19,909\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:13:19,911\tWARNING util.py:315 -- The `process_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:13:19,917\tWARNING util.py:315 -- Processing trial results took 2.083 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:13:19,918\tWARNING util.py:315 -- The `process_trial_result` operation took 2.085 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:                mae_force █▅▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:               mape_force █▂▃▃▄▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:                   metric █▇▅▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:               rmse_force █▇▅▄▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:         time_this_iter_s ▄▄▂▃▂▂▂▃▃▃▁▂▃▂▃▃█▇▂▃▂▃▁▂▂▂▂▂▂▂▁▃▄▂▁▃▅▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:               tmae_force █▅▅▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:              tmape_force █▂▄▃▄▅▄▅▅▄▄▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:              trmse_force █▇▅▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:                mae_force 124.08107\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:               mape_force 1.1705612863884261e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:                   metric 0.07904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:               rmse_force 233.71372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:       time_since_restore 53.6394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:         time_this_iter_s 0.91198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:             time_total_s 53.6394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:                timestamp 1691698402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:               tmae_force 0.04778\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:              tmape_force 54189150382981.625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb:              trmse_force 0.07904\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: 🚀 View run FSR_Trainable_12d5687d at: https://wandb.ai/seokjin/FSR-prediction/runs/12d5687d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252082)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051225-12d5687d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_1a10a85b_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-12-51/wandb/run-20230811_051327-1a10a85b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Syncing run FSR_Trainable_1a10a85b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a10a85b\n",
      "2023-08-11 05:13:37,546\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.985 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:13:37,553\tWARNING util.py:315 -- The `process_trial_result` operation took 2.993 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:13:37,555\tWARNING util.py:315 -- Processing trial results took 2.995 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:13:37,557\tWARNING util.py:315 -- The `process_trial_result` operation took 2.997 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_f7f51bca_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-13-16/wandb/run-20230811_051341-f7f51bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Syncing run FSR_Trainable_f7f51bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f7f51bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:13:54,080\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.945 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:13:54,083\tWARNING util.py:315 -- The `process_trial_result` operation took 1.949 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:13:54,085\tWARNING util.py:315 -- Processing trial results took 1.951 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:13:54,088\tWARNING util.py:315 -- The `process_trial_result` operation took 1.954 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:                mae_force █▅▃▃▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:               mape_force █▂▃▄▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:                   metric █▇▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:               rmse_force █▆▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:         time_this_iter_s ▃▂▁▂▃▂▂▂▃▃▂▄▃█▅▃▂▁▂▂▂▂▁▂▂▂▅▃▄▂▃▃▂▂▃▆▂▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:               tmae_force █▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:              tmape_force █▂▄▅▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:              trmse_force █▇▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:                mae_force 122.1447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:               mape_force 1.1888162289280418e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:                   metric 0.07738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:               rmse_force 228.18563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:       time_since_restore 82.12802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:         time_this_iter_s 0.67051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:             time_total_s 82.12802\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:                timestamp 1691698428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:               tmae_force 0.04632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:              tmape_force 50766640009785.74\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb:              trmse_force 0.07738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: 🚀 View run FSR_Trainable_1e9c13e0 at: https://wandb.ai/seokjin/FSR-prediction/runs/1e9c13e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251908)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051218-1e9c13e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_69867db4_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-13-33/wandb/run-20230811_051357-69867db4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Syncing run FSR_Trainable_69867db4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/69867db4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:14:04,908\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.731 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:04,911\tWARNING util.py:315 -- The `process_trial_result` operation took 1.736 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:04,914\tWARNING util.py:315 -- Processing trial results took 1.738 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:14:04,918\tWARNING util.py:315 -- The `process_trial_result` operation took 1.742 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_ea42184f_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-13-51/wandb/run-20230811_051407-ea42184f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Syncing run FSR_Trainable_ea42184f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ea42184f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:                mae_force █▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:               mape_force █▆▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:                   metric █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:               rmse_force █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:         time_this_iter_s █▁▂▂▂▁▂▄▃▃▃▃▃▂▄▄▄█▂▃▄▂▂▂▂▂▂▄▂▃▄▃▄▂▃▄▂▃▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:               tmae_force █▅▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:              tmape_force █▇▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:              trmse_force █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:                mae_force 109.87048\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:               mape_force 1.0447796781302046e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:                   metric 0.07022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:               rmse_force 207.5851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:       time_since_restore 80.34339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:         time_this_iter_s 0.71895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:             time_total_s 80.34339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:                timestamp 1691698421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:               tmae_force 0.04169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:              tmape_force 44019525361895.91\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb:              trmse_force 0.07022\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: 🚀 View run FSR_Trainable_140519ef at: https://wandb.ai/seokjin/FSR-prediction/runs/140519ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=251731)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051211-140519ef/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:14:17,817\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:17,820\tWARNING util.py:315 -- The `process_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:17,824\tWARNING util.py:315 -- Processing trial results took 1.968 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:14:17,826\tWARNING util.py:315 -- The `process_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_03c3dfec_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-14-02/wandb/run-20230811_051420-03c3dfec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Syncing run FSR_Trainable_03c3dfec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/03c3dfec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:                mae_force 311.64821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:               mape_force 4.924324998521487e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:                   metric 0.17484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:               rmse_force 501.27833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:       time_since_restore 2.07117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:         time_this_iter_s 1.07559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:             time_total_s 2.07117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:                timestamp 1691698458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:               tmae_force 0.13355\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:              tmape_force 269508073087526.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb:              trmse_force 0.17484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: 🚀 View run FSR_Trainable_03c3dfec at: https://wandb.ai/seokjin/FSR-prediction/runs/03c3dfec\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253779)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051420-03c3dfec/logs\n",
      "2023-08-11 05:14:32,497\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.926 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:32,502\tWARNING util.py:315 -- The `process_trial_result` operation took 1.932 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:32,505\tWARNING util.py:315 -- Processing trial results took 1.934 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:14:32,506\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_7eecfc37_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-14-14/wandb/run-20230811_051436-7eecfc37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Syncing run FSR_Trainable_7eecfc37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7eecfc37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:                mae_force 276.32407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:               mape_force 4.7433459959219475e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:                   metric 0.14313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:               rmse_force 445.98914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:       time_since_restore 1.9758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:         time_this_iter_s 0.99453\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:             time_total_s 1.9758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:                timestamp 1691698473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:               tmae_force 0.10131\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:              tmape_force 182737450392219.84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb:              trmse_force 0.14313\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: 🚀 View run FSR_Trainable_7eecfc37 at: https://wandb.ai/seokjin/FSR-prediction/runs/7eecfc37\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254024)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051436-7eecfc37/logs\n",
      "2023-08-11 05:14:49,513\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:49,516\tWARNING util.py:315 -- The `process_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:14:49,519\tWARNING util.py:315 -- Processing trial results took 1.872 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:14:49,520\tWARNING util.py:315 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_86266f50_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-14-29/wandb/run-20230811_051452-86266f50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Syncing run FSR_Trainable_86266f50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/86266f50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:                mae_force █▅▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:               mape_force █▂▃▄▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:                   metric █▆▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:               rmse_force █▆▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:         time_this_iter_s ▅▄▂▃▅▃▄▅█▂▄▂▁▁▁▃▂▂▂▂▂▃▂▂▂▄▃▃▂▂▂▄▄▄▇▂▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:               tmae_force █▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:              tmape_force █▂▃▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:              trmse_force █▆▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:                mae_force 116.42754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:               mape_force 1.1126760415232029e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:                   metric 0.07419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:               rmse_force 219.08293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:       time_since_restore 80.23476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:         time_this_iter_s 0.79132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:             time_total_s 80.23476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:                timestamp 1691698492\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:               tmae_force 0.04416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:              tmape_force 47332209526912.18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb:              trmse_force 0.07419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: 🚀 View run FSR_Trainable_1a10a85b at: https://wandb.ai/seokjin/FSR-prediction/runs/1a10a85b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=252813)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051327-1a10a85b/logs\n",
      "2023-08-11 05:15:07,311\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:07,314\tWARNING util.py:315 -- The `process_trial_result` operation took 2.086 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:07,333\tWARNING util.py:315 -- Processing trial results took 2.104 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:15:07,335\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_e880e411_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-14-46/wandb/run-20230811_051514-e880e411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: Syncing run FSR_Trainable_e880e411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e880e411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:15:19,621\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:19,625\tWARNING util.py:315 -- The `process_trial_result` operation took 1.773 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:19,627\tWARNING util.py:315 -- Processing trial results took 1.775 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:15:19,628\tWARNING util.py:315 -- The `process_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:                mae_force █▅▂▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:               mape_force █▁▃▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:                   metric █▆▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:               rmse_force █▇▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:         time_this_iter_s ▄▆▂▃▂▂▁▁▂▂▂▂▂▂▂▂▂▂▃▄▂▄▂▃▂▂▃█▂▃▃▃▃▃▃▂▂▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:               tmae_force █▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:              tmape_force █▂▄▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:              trmse_force █▆▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:                mae_force 117.62883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:               mape_force 1.1141217157802562e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:                   metric 0.07462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:               rmse_force 220.38411\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:       time_since_restore 80.48974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:         time_this_iter_s 0.74565\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:             time_total_s 80.48974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:                timestamp 1691698508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:               tmae_force 0.04484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:              tmape_force 48514778720934.484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb:              trmse_force 0.07462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: 🚀 View run FSR_Trainable_f7f51bca at: https://wandb.ai/seokjin/FSR-prediction/runs/f7f51bca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051341-f7f51bca/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253054)\u001b[0m wandb: W&B API key is configured. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254521)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_84973430_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-15-03/wandb/run-20230811_051527-84973430\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Syncing run FSR_Trainable_84973430\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/84973430\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:15:33,834\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:33,838\tWARNING util.py:315 -- The `process_trial_result` operation took 1.840 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:33,839\tWARNING util.py:315 -- Processing trial results took 1.841 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:15:33,841\tWARNING util.py:315 -- The `process_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:                mae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:               mape_force █▆▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:                   metric █▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:               rmse_force █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:         time_this_iter_s ▄▃▂▁▂▄▃▂▂▂▄▅▂▃▂▂▄▅█▃▂▂▅▄▂▅▂▂▂▆▃▁▂▁▁▂▄▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:               tmae_force █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:              tmape_force ██▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:              trmse_force █▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:                mae_force 111.62116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:               mape_force 1.0174574284717698e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:                   metric 0.07156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:               rmse_force 213.13941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:       time_since_restore 81.77012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:         time_this_iter_s 0.87877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:             time_total_s 81.77012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:                timestamp 1691698528\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:               tmae_force 0.04225\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:              tmape_force 43938663540950.67\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb:              trmse_force 0.07156\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: 🚀 View run FSR_Trainable_69867db4 at: https://wandb.ai/seokjin/FSR-prediction/runs/69867db4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051357-69867db4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=253309)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_3ce133e1_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-15-16/wandb/run-20230811_051537-3ce133e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Syncing run FSR_Trainable_3ce133e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3ce133e1\n",
      "2023-08-11 05:15:46,767\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:46,771\tWARNING util.py:315 -- The `process_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:15:46,773\tWARNING util.py:315 -- Processing trial results took 1.886 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:15:46,775\tWARNING util.py:315 -- The `process_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_2f18c218_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-15-29/wandb/run-20230811_051549-2f18c218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Syncing run FSR_Trainable_2f18c218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2f18c218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:                mae_force 2444.7251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:               mape_force 5.248735460216647e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:                   metric 1.53123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:               rmse_force 4574.24263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:       time_since_restore 1.27287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:         time_this_iter_s 1.27287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:             time_total_s 1.27287\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:                timestamp 1691698544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:               tmae_force 1.02852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:              tmape_force 2369239540619365.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb:              trmse_force 1.53123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: 🚀 View run FSR_Trainable_2f18c218 at: https://wandb.ai/seokjin/FSR-prediction/runs/2f18c218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255243)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051549-2f18c218/logs\n",
      "2023-08-11 05:16:00,347\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:00,350\tWARNING util.py:315 -- The `process_trial_result` operation took 1.737 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:00,355\tWARNING util.py:315 -- Processing trial results took 1.741 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:16:00,357\tWARNING util.py:315 -- The `process_trial_result` operation took 1.744 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_5d427b91_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-15-43/wandb/run-20230811_051603-5d427b91\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Syncing run FSR_Trainable_5d427b91\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5d427b91\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:              tmape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:                mae_force 185.92996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:               mape_force 2.5424264054682525e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:                   metric 0.1058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:               rmse_force 328.38741\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:       time_since_restore 1.94662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:         time_this_iter_s 0.90874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:             time_total_s 1.94662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:                timestamp 1691698561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:               tmae_force 0.06931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:              tmape_force 105135596656936.78\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb:              trmse_force 0.1058\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: 🚀 View run FSR_Trainable_5d427b91 at: https://wandb.ai/seokjin/FSR-prediction/runs/5d427b91\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255475)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051603-5d427b91/logs\n",
      "2023-08-11 05:16:17,662\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.575 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:17,667\tWARNING util.py:315 -- The `process_trial_result` operation took 2.580 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:17,669\tWARNING util.py:315 -- Processing trial results took 2.582 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:16:17,671\tWARNING util.py:315 -- The `process_trial_result` operation took 2.584 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_0f05f885_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-15-57/wandb/run-20230811_051621-0f05f885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Syncing run FSR_Trainable_0f05f885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0f05f885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:                mae_force 268.35028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:               mape_force 2.7233932368401348e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:                   metric 4.96632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:               rmse_force 552.38878\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:       time_since_restore 1.94211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:         time_this_iter_s 1.94211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:             time_total_s 1.94211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:                timestamp 1691698575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:               tmae_force 1.98122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:              tmape_force 1557669600274696.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb:              trmse_force 4.96632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: 🚀 View run FSR_Trainable_0f05f885 at: https://wandb.ai/seokjin/FSR-prediction/runs/0f05f885\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255709)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051621-0f05f885/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:                mae_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:               mape_force █▆▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:                   metric █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:               rmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:         time_this_iter_s █▅▃▅▂▂▂▄▂▂▃▂▁▁▃▄▂▄▄▄▆▅▂▂▆▄▄▂▂▅▂▃▄▅▂▄▄▅▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:               tmae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:              tmape_force █▇▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb:              trmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051452-86266f50/logs\n",
      "2023-08-11 05:16:30,749\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.994 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:30,753\tWARNING util.py:315 -- The `process_trial_result` operation took 1.998 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:30,756\tWARNING util.py:315 -- Processing trial results took 2.001 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:16:30,758\tWARNING util.py:315 -- The `process_trial_result` operation took 2.003 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_43b74dad_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-16-13/wandb/run-20230811_051636-43b74dad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Syncing run FSR_Trainable_43b74dad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/43b74dad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:16:41,522\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.102 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:41,525\tWARNING util.py:315 -- The `process_trial_result` operation took 2.106 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:41,528\tWARNING util.py:315 -- Processing trial results took 2.109 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:16:41,529\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:                mae_force 703.21485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:               mape_force 5.1702864461309336e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:                   metric 6.99051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:               rmse_force 2016.82434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:       time_since_restore 0.97169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:         time_this_iter_s 0.97169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:             time_total_s 0.97169\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:                timestamp 1691698588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:               tmae_force 3.19431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:              tmape_force 1897566842891547.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb:              trmse_force 6.99051\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: 🚀 View run FSR_Trainable_43b74dad at: https://wandb.ai/seokjin/FSR-prediction/runs/43b74dad\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051636-43b74dad/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=255959)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_7debcdf3_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-16-27/wandb/run-20230811_051644-7debcdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Syncing run FSR_Trainable_7debcdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7debcdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:                mae_force 678.26809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:               mape_force 1.0856211866542326e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:                   metric 0.31557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:               rmse_force 1029.81076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:       time_since_restore 0.87718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:         time_this_iter_s 0.87718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:             time_total_s 0.87718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:                timestamp 1691698599\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:               tmae_force 0.2597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:              tmape_force 525336337039558.06\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb:              trmse_force 0.31557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: 🚀 View run FSR_Trainable_7debcdf3 at: https://wandb.ai/seokjin/FSR-prediction/runs/7debcdf3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256181)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051644-7debcdf3/logs\n",
      "2023-08-11 05:16:52,665\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:52,671\tWARNING util.py:315 -- The `process_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:16:52,672\tWARNING util.py:315 -- Processing trial results took 2.044 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:16:52,673\tWARNING util.py:315 -- The `process_trial_result` operation took 2.045 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_37accafc_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-16-38/wandb/run-20230811_051655-37accafc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Syncing run FSR_Trainable_37accafc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/37accafc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:                mae_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:               mape_force █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:                   metric █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:               rmse_force █▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:         time_this_iter_s ▃▃▂▃▂█▃▃▂▂▃▂▃▂▂▂▂▂▃▂▂▂▇▂▁▁▁▁▂▂▁▂▂▃▁▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:               tmae_force █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:              tmape_force █▇▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:              trmse_force █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:                mae_force 111.12483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:               mape_force 1.06103313425845e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:                   metric 0.07094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:               rmse_force 209.38871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:       time_since_restore 82.34105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:         time_this_iter_s 0.62779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:             time_total_s 82.34105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:                timestamp 1691698616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:               tmae_force 0.04214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:              tmape_force 44784724681327.016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb:              trmse_force 0.07094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: 🚀 View run FSR_Trainable_84973430 at: https://wandb.ai/seokjin/FSR-prediction/runs/84973430\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254751)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051527-84973430/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb:              trmse_force ▁\n",
      "2023-08-11 05:17:04,849\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:04,852\tWARNING util.py:315 -- The `process_trial_result` operation took 1.715 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:04,856\tWARNING util.py:315 -- Processing trial results took 1.720 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:17:04,859\tWARNING util.py:315 -- The `process_trial_result` operation took 1.723 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256411)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_cc8df67d_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-16-49/wandb/run-20230811_051707-cc8df67d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Syncing run FSR_Trainable_cc8df67d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc8df67d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:                mae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:               mape_force █▆▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:                   metric █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:               rmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:         time_this_iter_s █▃▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▆▂▂▂▂▂▃▂▁▂▂▂▁▂▂▁▂▂▁▁▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:               tmae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:              tmape_force ██▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:              trmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:                mae_force 112.62498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:               mape_force 1.0787121443960394e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:                   metric 0.07149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:               rmse_force 210.91806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:       time_since_restore 81.00779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:         time_this_iter_s 0.48302\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:             time_total_s 81.00779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:                timestamp 1691698627\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:               tmae_force 0.04244\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:              tmape_force 43993134861258.57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb:              trmse_force 0.07149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: 🚀 View run FSR_Trainable_3ce133e1 at: https://wandb.ai/seokjin/FSR-prediction/runs/3ce133e1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=254979)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051537-3ce133e1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb:              trmse_force █▁\n",
      "2023-08-11 05:17:13,375\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.588 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:13,380\tWARNING util.py:315 -- The `process_trial_result` operation took 1.594 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:13,381\tWARNING util.py:315 -- Processing trial results took 1.595 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:17:13,383\tWARNING util.py:315 -- The `process_trial_result` operation took 1.597 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_f719134f_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-01/wandb/run-20230811_051716-f719134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Syncing run FSR_Trainable_f719134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f719134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256668)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:               mape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:                mae_force 243.89621\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:               mape_force 3.4502511282935584e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:                   metric 0.13786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:               rmse_force 431.26361\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:       time_since_restore 1.92012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:         time_this_iter_s 0.70681\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:             time_total_s 1.92012\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:                timestamp 1691698634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:               tmae_force 0.08797\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:              tmape_force 133342953067331.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb:              trmse_force 0.13786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: 🚀 View run FSR_Trainable_f719134f at: https://wandb.ai/seokjin/FSR-prediction/runs/f719134f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051716-f719134f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=256893)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-11 05:17:22,298\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:22,301\tWARNING util.py:315 -- The `process_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:22,304\tWARNING util.py:315 -- Processing trial results took 1.623 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:17:22,308\tWARNING util.py:315 -- The `process_trial_result` operation took 1.626 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_4035f2d3_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-10/wandb/run-20230811_051724-4035f2d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Syncing run FSR_Trainable_4035f2d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4035f2d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:17:29,077\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:29,082\tWARNING util.py:315 -- The `process_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:29,084\tWARNING util.py:315 -- Processing trial results took 1.965 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:17:29,086\tWARNING util.py:315 -- The `process_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:                mae_force █▇▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:               mape_force ██▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:                   metric █▆▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:               rmse_force █▅▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:         time_this_iter_s █▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:               tmae_force ▇▆█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:              tmape_force ▄▅█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:              trmse_force █▆▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:                mae_force 147.80173\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:               mape_force 1.3354597516219723e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:                   metric 0.0942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:               rmse_force 283.29799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:       time_since_restore 3.22026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:         time_this_iter_s 0.53841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:             time_total_s 3.22026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:                timestamp 1691698644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:               tmae_force 0.0583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:              tmape_force 71391113694854.05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb:              trmse_force 0.0942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: 🚀 View run FSR_Trainable_4035f2d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/4035f2d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257119)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051724-4035f2d3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_891b6b9f_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-19/wandb/run-20230811_051731-891b6b9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Syncing run FSR_Trainable_891b6b9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/891b6b9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:17:36,195\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.426 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:36,201\tWARNING util.py:315 -- The `process_trial_result` operation took 2.432 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:36,203\tWARNING util.py:315 -- Processing trial results took 2.434 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:17:36,205\tWARNING util.py:315 -- The `process_trial_result` operation took 2.437 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:               mape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:              tmape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:                mae_force 254.51341\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:               mape_force 3.681333178106097e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:                   metric 0.14327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:               rmse_force 447.02015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:       time_since_restore 2.00434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:         time_this_iter_s 0.72377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:             time_total_s 2.00434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:                timestamp 1691698649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:               tmae_force 0.08982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:              tmape_force 130630752640528.84\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb:              trmse_force 0.14327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: 🚀 View run FSR_Trainable_891b6b9f at: https://wandb.ai/seokjin/FSR-prediction/runs/891b6b9f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257298)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051731-891b6b9f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_461f3c40_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-25/wandb/run-20230811_051738-461f3c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Syncing run FSR_Trainable_461f3c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/461f3c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:17:43,651\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:43,656\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:43,657\tWARNING util.py:315 -- Processing trial results took 1.883 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:17:43,658\tWARNING util.py:315 -- The `process_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:               mape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:                mae_force 233.11895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:               mape_force 3.520788262960382e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:                   metric 0.13015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:               rmse_force 411.31367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:       time_since_restore 1.63162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:         time_this_iter_s 0.57318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:             time_total_s 1.63162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:                timestamp 1691698656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:               tmae_force 0.08455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:              tmape_force 134028098326390.73\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb:              trmse_force 0.13015\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: 🚀 View run FSR_Trainable_461f3c40 at: https://wandb.ai/seokjin/FSR-prediction/runs/461f3c40\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257483)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051738-461f3c40/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_87366ad5_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-32/wandb/run-20230811_051746-87366ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Syncing run FSR_Trainable_87366ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/87366ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:                mae_force 109.41251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:               mape_force 198258373.88132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:                   metric 0.56265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:               rmse_force 210.61294\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:       time_since_restore 1.41831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:         time_this_iter_s 1.41831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:             time_total_s 1.41831\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:                timestamp 1691698661\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:               tmae_force 0.31026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:              tmape_force 1.95593\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb:              trmse_force 0.56265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: 🚀 View run FSR_Trainable_87366ad5 at: https://wandb.ai/seokjin/FSR-prediction/runs/87366ad5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257673)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051746-87366ad5/logs\n",
      "2023-08-11 05:17:53,040\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.294 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:53,045\tWARNING util.py:315 -- The `process_trial_result` operation took 2.300 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:17:53,046\tWARNING util.py:315 -- Processing trial results took 2.301 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:17:53,048\tWARNING util.py:315 -- The `process_trial_result` operation took 2.303 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_d1434ee3_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-40/wandb/run-20230811_051755-d1434ee3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Syncing run FSR_Trainable_d1434ee3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d1434ee3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:                mae_force 283.97828\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:               mape_force 2.425304000811368e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:                   metric 5.10449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:               rmse_force 613.2461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:       time_since_restore 1.40397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:         time_this_iter_s 1.40397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:             time_total_s 1.40397\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:                timestamp 1691698670\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:               tmae_force 1.94162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:              tmape_force 1103586221930525.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb:              trmse_force 5.10449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: 🚀 View run FSR_Trainable_d1434ee3 at: https://wandb.ai/seokjin/FSR-prediction/runs/d1434ee3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=257896)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051755-d1434ee3/logs\n",
      "2023-08-11 05:18:01,299\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.589 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:01,303\tWARNING util.py:315 -- The `process_trial_result` operation took 2.594 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:01,305\tWARNING util.py:315 -- Processing trial results took 2.597 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:18:01,308\tWARNING util.py:315 -- The `process_trial_result` operation took 2.600 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_405a9de5_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-49/wandb/run-20230811_051804-405a9de5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Syncing run FSR_Trainable_405a9de5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/405a9de5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:                mae_force 228.01187\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:               mape_force 1.9806425416540348e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:                   metric 4.90812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:               rmse_force 441.07958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:       time_since_restore 1.49952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:         time_this_iter_s 1.49952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:             time_total_s 1.49952\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:                timestamp 1691698678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:               tmae_force 1.84587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:              tmape_force 1073069192101303.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb:              trmse_force 4.90812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: 🚀 View run FSR_Trainable_405a9de5 at: https://wandb.ai/seokjin/FSR-prediction/runs/405a9de5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258088)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051804-405a9de5/logs\n",
      "2023-08-11 05:18:11,964\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.043 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:11,969\tWARNING util.py:315 -- The `process_trial_result` operation took 2.050 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:11,970\tWARNING util.py:315 -- Processing trial results took 2.051 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:18:11,972\tWARNING util.py:315 -- The `process_trial_result` operation took 2.053 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_972b67b6_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-17-57/wandb/run-20230811_051814-972b67b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Syncing run FSR_Trainable_972b67b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/972b67b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:18:19,268\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.078 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:19,272\tWARNING util.py:315 -- The `process_trial_result` operation took 2.083 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:19,274\tWARNING util.py:315 -- Processing trial results took 2.086 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:18:19,277\tWARNING util.py:315 -- The `process_trial_result` operation took 2.088 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:                mae_force 138.87491\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:               mape_force 216575135.81047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:                   metric 0.69398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:               rmse_force 264.24289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:       time_since_restore 1.75564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:         time_this_iter_s 1.75564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:             time_total_s 1.75564\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:                timestamp 1691698689\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:               tmae_force 0.38899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:              tmape_force 1.95147\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb:              trmse_force 0.69398\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: 🚀 View run FSR_Trainable_972b67b6 at: https://wandb.ai/seokjin/FSR-prediction/runs/972b67b6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258320)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051814-972b67b6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_1b8c9dd5_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-18-08/wandb/run-20230811_051821-1b8c9dd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Syncing run FSR_Trainable_1b8c9dd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1b8c9dd5\n",
      "2023-08-11 05:18:26,719\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.201 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:26,724\tWARNING util.py:315 -- The `process_trial_result` operation took 2.206 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:26,727\tWARNING util.py:315 -- Processing trial results took 2.209 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:18:26,730\tWARNING util.py:315 -- The `process_trial_result` operation took 2.212 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_6d759f9e_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-18-15/wandb/run-20230811_051829-6d759f9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Syncing run FSR_Trainable_6d759f9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6d759f9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:18:33,938\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:33,940\tWARNING util.py:315 -- The `process_trial_result` operation took 1.956 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:33,942\tWARNING util.py:315 -- Processing trial results took 1.958 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:18:33,943\tWARNING util.py:315 -- The `process_trial_result` operation took 1.959 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:                mae_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:               mape_force █▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:                   metric █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:               rmse_force █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:         time_this_iter_s █▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:               tmae_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:              tmape_force █▂▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:              trmse_force █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:                mae_force 157.85482\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:               mape_force 2.0560629132903478e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:                   metric 0.09174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:               rmse_force 277.08285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:       time_since_restore 3.22382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:         time_this_iter_s 0.73693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:             time_total_s 3.22382\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:                timestamp 1691698709\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:               tmae_force 0.05868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:              tmape_force 82004763136725.19\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb:              trmse_force 0.09174\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: 🚀 View run FSR_Trainable_6d759f9e at: https://wandb.ai/seokjin/FSR-prediction/runs/6d759f9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258679)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051829-6d759f9e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_3f62f2b8_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-18-23/wandb/run-20230811_051836-3f62f2b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Syncing run FSR_Trainable_3f62f2b8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3f62f2b8\n",
      "2023-08-11 05:18:44,349\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.993 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:44,351\tWARNING util.py:315 -- The `process_trial_result` operation took 1.996 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:44,353\tWARNING util.py:315 -- Processing trial results took 1.999 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:18:44,357\tWARNING util.py:315 -- The `process_trial_result` operation took 2.002 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_4d669d51_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-18-31/wandb/run-20230811_051847-4d669d51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Syncing run FSR_Trainable_4d669d51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4d669d51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:                mae_force 173.98275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:               mape_force 1.9536955161108234e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:                   metric 0.10701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:               rmse_force 321.31222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:       time_since_restore 1.81211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:         time_this_iter_s 0.89282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:             time_total_s 1.81211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:                timestamp 1691698725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:               tmae_force 0.06422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:              tmape_force 76157996631199.36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb:              trmse_force 0.10701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: 🚀 View run FSR_Trainable_4d669d51 at: https://wandb.ai/seokjin/FSR-prediction/runs/4d669d51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259080)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051847-4d669d51/logs\n",
      "2023-08-11 05:18:57,881\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.899 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:57,883\tWARNING util.py:315 -- The `process_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:18:57,884\tWARNING util.py:315 -- Processing trial results took 1.903 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:18:57,885\tWARNING util.py:315 -- The `process_trial_result` operation took 1.904 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_21cd5e27_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-18-41/wandb/run-20230811_051901-21cd5e27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Syncing run FSR_Trainable_21cd5e27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/21cd5e27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:19:09,758\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.130 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:09,761\tWARNING util.py:315 -- The `process_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:09,763\tWARNING util.py:315 -- Processing trial results took 2.136 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:19:09,766\tWARNING util.py:315 -- The `process_trial_result` operation took 2.139 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:                mae_force █▆▅▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:               mape_force █▃▂▁▂▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:                   metric █▆▅▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:               rmse_force █▇▆▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:         time_this_iter_s █▃▃▄▄▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:               tmae_force █▅▄▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:              tmape_force █▁▂▁▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:              trmse_force █▆▅▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:                mae_force 128.09632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:               mape_force 1.4609258640768022e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:                   metric 0.07859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:               rmse_force 233.39337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:       time_since_restore 7.18334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:         time_this_iter_s 0.65733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:             time_total_s 7.18334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:                timestamp 1691698743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:               tmae_force 0.04842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:              tmape_force 61341611014961.01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb:              trmse_force 0.07859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: 🚀 View run FSR_Trainable_21cd5e27 at: https://wandb.ai/seokjin/FSR-prediction/runs/21cd5e27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259322)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051901-21cd5e27/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_ae6f907f_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-18-54/wandb/run-20230811_051913-ae6f907f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Syncing run FSR_Trainable_ae6f907f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ae6f907f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:                mae_force 177.07239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:               mape_force 1.448766837865818e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:                   metric 0.10616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:               rmse_force 339.07594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:       time_since_restore 2.16567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:         time_this_iter_s 1.12587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:             time_total_s 2.16567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:                timestamp 1691698750\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:               tmae_force 0.06252\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:              tmape_force 58103356782483.88\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb:              trmse_force 0.10616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: 🚀 View run FSR_Trainable_ae6f907f at: https://wandb.ai/seokjin/FSR-prediction/runs/ae6f907f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259546)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051913-ae6f907f/logs\n",
      "2023-08-11 05:19:22,686\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.139 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:22,691\tWARNING util.py:315 -- The `process_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:22,693\tWARNING util.py:315 -- Processing trial results took 2.147 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:19:22,695\tWARNING util.py:315 -- The `process_trial_result` operation took 2.149 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_fed368c0_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-19-06/wandb/run-20230811_051926-fed368c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Syncing run FSR_Trainable_fed368c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fed368c0\n",
      "2023-08-11 05:19:34,905\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.961 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:34,913\tWARNING util.py:315 -- The `process_trial_result` operation took 1.971 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:34,915\tWARNING util.py:315 -- Processing trial results took 1.973 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:19:34,917\tWARNING util.py:315 -- The `process_trial_result` operation took 1.975 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_71d25cbf_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-19-19/wandb/run-20230811_051939-71d25cbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Syncing run FSR_Trainable_71d25cbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/71d25cbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:                mae_force 182.21738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:               mape_force 485220974.23433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:                   metric 0.80556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:               rmse_force 325.29076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:       time_since_restore 1.15688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:         time_this_iter_s 1.15688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:             time_total_s 1.15688\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:                timestamp 1691698772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:               tmae_force 0.48815\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:              tmape_force 2.37171\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb:              trmse_force 0.80556\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: 🚀 View run FSR_Trainable_71d25cbf at: https://wandb.ai/seokjin/FSR-prediction/runs/71d25cbf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260017)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051939-71d25cbf/logs\n",
      "2023-08-11 05:19:50,430\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.642 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:50,433\tWARNING util.py:315 -- The `process_trial_result` operation took 1.646 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:19:50,434\tWARNING util.py:315 -- Processing trial results took 1.647 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:19:50,436\tWARNING util.py:315 -- The `process_trial_result` operation took 1.649 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_f9f5f046_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-19-31/wandb/run-20230811_051953-f9f5f046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Syncing run FSR_Trainable_f9f5f046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f9f5f046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:                mae_force 148.08269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:               mape_force 364833851.27277\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:                   metric 0.66458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:               rmse_force 259.33615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:       time_since_restore 1.25605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:         time_this_iter_s 1.25605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:             time_total_s 1.25605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:                timestamp 1691698788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:               tmae_force 0.39866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:              tmape_force 2.11018\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb:              trmse_force 0.66458\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: 🚀 View run FSR_Trainable_f9f5f046 at: https://wandb.ai/seokjin/FSR-prediction/runs/f9f5f046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260264)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051953-f9f5f046/logs\n",
      "wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)02 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:                mae_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:               mape_force █▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:                   metric █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:               rmse_force █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:         time_this_iter_s ▇▃▁▂▃▁▂▄▁▂▁▂▄▃▃█▆▅▅▃▁▅█▄▂▂▃▄▃▄▂▄▄▃▃▃▃▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:               tmae_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:              tmape_force ██▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb:              trmse_force █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051821-1b8c9dd5/logs\n",
      "2023-08-11 05:20:04,439\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.229 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:04,442\tWARNING util.py:315 -- The `process_trial_result` operation took 2.233 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:04,444\tWARNING util.py:315 -- Processing trial results took 2.235 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:20:04,447\tWARNING util.py:315 -- The `process_trial_result` operation took 2.238 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258497)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_350a53a7_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-19-47/wandb/run-20230811_052007-350a53a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Syncing run FSR_Trainable_350a53a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/350a53a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:                mae_force 240.45522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:               mape_force 3.526515640128864e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:                   metric 0.13457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:               rmse_force 427.15307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:       time_since_restore 1.92851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:         time_this_iter_s 0.82297\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:             time_total_s 1.92851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:                timestamp 1691698805\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:               tmae_force 0.0854\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:              tmape_force 130201387097975.36\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb:              trmse_force 0.13457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: 🚀 View run FSR_Trainable_350a53a7 at: https://wandb.ai/seokjin/FSR-prediction/runs/350a53a7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260502)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052007-350a53a7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:20:17,266\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.770 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:17,268\tWARNING util.py:315 -- The `process_trial_result` operation took 1.774 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:17,271\tWARNING util.py:315 -- Processing trial results took 1.777 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:20:17,275\tWARNING util.py:315 -- The `process_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:                mae_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:               mape_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:                   metric █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:               rmse_force █▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:         time_this_iter_s ▃▂▁▁▂▃▂▂▄▂▃▃▂▁▂▆▃▁▂▃▃▂▂▂▃█▂▂▂▃▂▃▂▂▁▂▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:               tmae_force █▄▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:              tmape_force █▇▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb:              trmse_force █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051836-3f62f2b8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051836-3f62f2b8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=258856)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_05707371_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-20-01/wandb/run-20230811_052020-05707371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Syncing run FSR_Trainable_05707371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/05707371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:                mae_force 201.98969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:               mape_force 2.7424253844342566e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:                   metric 0.11614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:               rmse_force 366.75472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:       time_since_restore 1.93254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:         time_this_iter_s 0.87948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:             time_total_s 1.93254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:                timestamp 1691698818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:               tmae_force 0.07204\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:              tmape_force 102807929364270.66\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb:              trmse_force 0.11614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: 🚀 View run FSR_Trainable_05707371 at: https://wandb.ai/seokjin/FSR-prediction/runs/05707371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052020-05707371/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260756)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-11 05:20:28,619\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.383 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:28,625\tWARNING util.py:315 -- The `process_trial_result` operation took 3.389 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:28,627\tWARNING util.py:315 -- Processing trial results took 3.392 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:20:28,629\tWARNING util.py:315 -- The `process_trial_result` operation took 3.394 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_22c63583_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-20-14/wandb/run-20230811_052031-22c63583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Syncing run FSR_Trainable_22c63583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/22c63583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:                mae_force 203.81701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:               mape_force 2.76215542816309e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:                   metric 0.11719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:               rmse_force 369.27708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:       time_since_restore 1.58021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:         time_this_iter_s 0.77325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:             time_total_s 1.58021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:                timestamp 1691698829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:               tmae_force 0.0728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:              tmape_force 100015375843277.86\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb:              trmse_force 0.11719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: 🚀 View run FSR_Trainable_22c63583 at: https://wandb.ai/seokjin/FSR-prediction/runs/22c63583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=260985)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052031-22c63583/logs\n",
      "2023-08-11 05:20:38,526\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.090 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:38,532\tWARNING util.py:315 -- The `process_trial_result` operation took 2.097 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:38,534\tWARNING util.py:315 -- Processing trial results took 2.099 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:20:38,535\tWARNING util.py:315 -- The `process_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_816e65ef_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-20-24/wandb/run-20230811_052041-816e65ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Syncing run FSR_Trainable_816e65ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/816e65ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:                mae_force █▃▂▁▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:               mape_force █▁▁▂▃▄▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:                   metric █▅▄▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:               rmse_force █▅▄▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:         time_this_iter_s █▄▂▂▁▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:               tmae_force █▃▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:              tmape_force █▁▂▂▄▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:              trmse_force █▅▄▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:                mae_force 137.94241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:               mape_force 1.792057113902441e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:                   metric 0.08096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:               rmse_force 244.98976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:       time_since_restore 6.35228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:         time_this_iter_s 0.78364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:             time_total_s 6.35228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:                timestamp 1691698843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:               tmae_force 0.05168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:              tmape_force 73842094541765.55\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb:              trmse_force 0.08096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: 🚀 View run FSR_Trainable_816e65ef at: https://wandb.ai/seokjin/FSR-prediction/runs/816e65ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261223)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052041-816e65ef/logs\n",
      "2023-08-11 05:20:48,617\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:48,621\tWARNING util.py:315 -- The `process_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:48,624\tWARNING util.py:315 -- Processing trial results took 1.892 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:20:48,625\tWARNING util.py:315 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_d502195a_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-20-35/wandb/run-20230811_052051-d502195a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Syncing run FSR_Trainable_d502195a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d502195a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:                mae_force █▄▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:               mape_force █▁▁▃▄▅▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:                   metric █▇▄▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:               rmse_force █▇▄▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:         time_this_iter_s █▅▆▄▂▁▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:               tmae_force █▄▂▂▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:              tmape_force █▁▁▂▄▅▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:              trmse_force █▇▄▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:                mae_force 132.28602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:               mape_force 1.579986143585926e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:                   metric 0.07989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:               rmse_force 242.16698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:       time_since_restore 5.39616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:         time_this_iter_s 0.5747\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:             time_total_s 5.39616\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:                timestamp 1691698853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:               tmae_force 0.04923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:              tmape_force 64125601665694.02\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb:              trmse_force 0.07989\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: 🚀 View run FSR_Trainable_d502195a at: https://wandb.ai/seokjin/FSR-prediction/runs/d502195a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261447)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052051-d502195a/logs\n",
      "2023-08-11 05:20:57,974\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:57,977\tWARNING util.py:315 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:20:57,980\tWARNING util.py:315 -- Processing trial results took 1.876 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:20:57,981\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_84dd00bc_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-20-45/wandb/run-20230811_052102-84dd00bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Syncing run FSR_Trainable_84dd00bc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/84dd00bc\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:                mae_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:               mape_force █▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:                   metric █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:               rmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:         time_this_iter_s ▅▄▃▃▃█▆▂▃▄▄▄▂▂▂▃▅▄▂▂▄▁▂▁▂▃▂▁▁▄▂▂▂▁▂▂▂▁▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:               tmae_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:              tmape_force █▇▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:              trmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:                mae_force 110.51186\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:               mape_force 1.049038415103244e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:                   metric 0.07049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:               rmse_force 207.82127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:       time_since_restore 81.24087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:         time_this_iter_s 1.28062\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:             time_total_s 81.24087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:                timestamp 1691698860\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:               tmae_force 0.0419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:              tmape_force 43995608130169.05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb:              trmse_force 0.07049\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: 🚀 View run FSR_Trainable_fed368c0 at: https://wandb.ai/seokjin/FSR-prediction/runs/fed368c0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=259795)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_051926-fed368c0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:                mae_force █▅▂▁▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:               mape_force █▃▁▁▄▆▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:                   metric █▆▄▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:               rmse_force █▆▄▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:       time_since_restore ▁▂▃▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:         time_this_iter_s ▃▅▄█▅▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:             time_total_s ▁▂▃▅▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:                timestamp ▁▄▅▅▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:               tmae_force █▅▂▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:              tmape_force █▃▁▁▄▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb:              trmse_force █▆▄▃▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052102-84dd00bc/logs\n",
      "2023-08-11 05:21:09,747\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.913 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:09,752\tWARNING util.py:315 -- The `process_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:09,754\tWARNING util.py:315 -- Processing trial results took 1.921 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:21:09,757\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261674)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_48404c5c_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-20-55/wandb/run-20230811_052111-48404c5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Syncing run FSR_Trainable_48404c5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48404c5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:                mae_force 328.23512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:               mape_force 5.295816026293861e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:                   metric 0.17501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:               rmse_force 512.72208\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:       time_since_restore 1.10006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:         time_this_iter_s 1.10006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:             time_total_s 1.10006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:                timestamp 1691698867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:               tmae_force 0.12432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:              tmape_force 198075223360347.94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb:              trmse_force 0.17501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: 🚀 View run FSR_Trainable_48404c5c at: https://wandb.ai/seokjin/FSR-prediction/runs/48404c5c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052111-48404c5c/logs\n",
      "2023-08-11 05:21:17,770\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:17,773\tWARNING util.py:315 -- The `process_trial_result` operation took 1.934 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:17,775\tWARNING util.py:315 -- Processing trial results took 1.936 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:21:17,778\tWARNING util.py:315 -- The `process_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=261924)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_70ed32b7_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-06/wandb/run-20230811_052119-70ed32b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Syncing run FSR_Trainable_70ed32b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/70ed32b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:21:24,149\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:24,153\tWARNING util.py:315 -- The `process_trial_result` operation took 1.851 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:24,155\tWARNING util.py:315 -- Processing trial results took 1.854 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:21:24,158\tWARNING util.py:315 -- The `process_trial_result` operation took 1.856 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:                mae_force 332.65441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:               mape_force 3.364179169217111e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:                   metric 0.19024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:               rmse_force 547.47896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:       time_since_restore 0.98038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:         time_this_iter_s 0.98038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:             time_total_s 0.98038\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:                timestamp 1691698875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:               tmae_force 0.13841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:              tmape_force 195608004662467.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb:              trmse_force 0.19024\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: 🚀 View run FSR_Trainable_70ed32b7 at: https://wandb.ai/seokjin/FSR-prediction/runs/70ed32b7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052119-70ed32b7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262144)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_11597b57_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-14/wandb/run-20230811_052126-11597b57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Syncing run FSR_Trainable_11597b57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/11597b57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:21:30,781\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:30,786\tWARNING util.py:315 -- The `process_trial_result` operation took 2.027 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:30,788\tWARNING util.py:315 -- Processing trial results took 2.029 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:21:30,791\tWARNING util.py:315 -- The `process_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052126-11597b57/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262323)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_d4d2b4e8_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-21/wandb/run-20230811_052133-d4d2b4e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Syncing run FSR_Trainable_d4d2b4e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d4d2b4e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:21:37,552\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.690 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:37,556\tWARNING util.py:315 -- The `process_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:37,562\tWARNING util.py:315 -- Processing trial results took 1.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:21:37,564\tWARNING util.py:315 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:                mae_force 205.99135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:               mape_force 2.6885402454889718e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:                   metric 0.11947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:               rmse_force 372.85803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:       time_since_restore 1.95009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:         time_this_iter_s 0.76823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:             time_total_s 1.95009\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:                timestamp 1691698891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:               tmae_force 0.07325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:              tmape_force 97176998181437.81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb:              trmse_force 0.11947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: 🚀 View run FSR_Trainable_d4d2b4e8 at: https://wandb.ai/seokjin/FSR-prediction/runs/d4d2b4e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262506)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052133-d4d2b4e8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_f521736c_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-27/wandb/run-20230811_052139-f521736c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Syncing run FSR_Trainable_f521736c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f521736c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262690)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052139-f521736c/logs\n",
      "2023-08-11 05:21:45,853\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:45,858\tWARNING util.py:315 -- The `process_trial_result` operation took 1.972 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:45,863\tWARNING util.py:315 -- Processing trial results took 1.977 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:21:45,865\tWARNING util.py:315 -- The `process_trial_result` operation took 1.979 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_e3a71d59_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-34/wandb/run-20230811_052148-e3a71d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Syncing run FSR_Trainable_e3a71d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e3a71d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:21:52,165\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.767 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:52,170\tWARNING util.py:315 -- The `process_trial_result` operation took 1.773 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:21:52,171\tWARNING util.py:315 -- Processing trial results took 1.774 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:21:52,173\tWARNING util.py:315 -- The `process_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:                mae_force 257.3424\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:               mape_force 4.0767842992261024e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:                   metric 0.13982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:               rmse_force 438.54891\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:       time_since_restore 1.80557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:         time_this_iter_s 0.67958\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:             time_total_s 1.80557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:                timestamp 1691698906\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:               tmae_force 0.09253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:              tmape_force 153481197604711.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb:              trmse_force 0.13982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: 🚀 View run FSR_Trainable_e3a71d59 at: https://wandb.ai/seokjin/FSR-prediction/runs/e3a71d59\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=262913)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052148-e3a71d59/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_d06442e2_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-42/wandb/run-20230811_052154-d06442e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Syncing run FSR_Trainable_d06442e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d06442e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263094)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052154-d06442e2/logs\n",
      "2023-08-11 05:22:00,058\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:00,062\tWARNING util.py:315 -- The `process_trial_result` operation took 1.921 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:00,064\tWARNING util.py:315 -- Processing trial results took 1.923 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:00,065\tWARNING util.py:315 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_4109b910_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-49/wandb/run-20230811_052202-4109b910\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Syncing run FSR_Trainable_4109b910\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4109b910\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:22:06,441\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.546 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:06,445\tWARNING util.py:315 -- The `process_trial_result` operation took 1.551 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:06,447\tWARNING util.py:315 -- Processing trial results took 1.554 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:06,448\tWARNING util.py:315 -- The `process_trial_result` operation took 1.555 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:                mae_force 222.43427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:               mape_force 2.7971704265897258e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:                   metric 0.13032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:               rmse_force 418.44856\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:       time_since_restore 1.75508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:         time_this_iter_s 0.64739\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:             time_total_s 1.75508\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:                timestamp 1691698920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:               tmae_force 0.07591\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:              tmape_force 90822215096235.7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb:              trmse_force 0.13032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: 🚀 View run FSR_Trainable_4109b910 at: https://wandb.ai/seokjin/FSR-prediction/runs/4109b910\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263315)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052202-4109b910/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_d759cdd5_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-21-57/wandb/run-20230811_052208-d759cdd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Syncing run FSR_Trainable_d759cdd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d759cdd5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263499)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052208-d759cdd5/logs\n",
      "2023-08-11 05:22:14,691\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:14,695\tWARNING util.py:315 -- The `process_trial_result` operation took 1.839 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:14,696\tWARNING util.py:315 -- Processing trial results took 1.841 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:14,698\tWARNING util.py:315 -- The `process_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_dbe2f24c_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-03/wandb/run-20230811_052217-dbe2f24c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Syncing run FSR_Trainable_dbe2f24c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dbe2f24c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:22:20,922\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.614 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:20,927\tWARNING util.py:315 -- The `process_trial_result` operation took 1.620 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:20,928\tWARNING util.py:315 -- Processing trial results took 1.621 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:20,930\tWARNING util.py:315 -- The `process_trial_result` operation took 1.624 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:              tmape_force ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:                mae_force 186.50128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:               mape_force 2.402372897514396e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:                   metric 0.10865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:               rmse_force 337.69126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:       time_since_restore 1.92331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:         time_this_iter_s 0.7379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:             time_total_s 1.92331\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:                timestamp 1691698935\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:               tmae_force 0.06768\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:              tmape_force 91620281455154.22\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb:              trmse_force 0.10865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: 🚀 View run FSR_Trainable_dbe2f24c at: https://wandb.ai/seokjin/FSR-prediction/runs/dbe2f24c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263722)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052217-dbe2f24c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_5ed4e384_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-11/wandb/run-20230811_052223-5ed4e384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Syncing run FSR_Trainable_5ed4e384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5ed4e384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:22:29,479\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:29,483\tWARNING util.py:315 -- The `process_trial_result` operation took 1.624 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:29,487\tWARNING util.py:315 -- Processing trial results took 1.627 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:29,489\tWARNING util.py:315 -- The `process_trial_result` operation took 1.630 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:                mae_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:               mape_force █▃▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:                   metric █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:               rmse_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:         time_this_iter_s █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:                timestamp ▁▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:               tmae_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:              tmape_force █▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:              trmse_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:                mae_force 141.55249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:               mape_force 1.4139129087334906e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:                   metric 0.08785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:               rmse_force 268.69074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:       time_since_restore 3.19451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:         time_this_iter_s 0.57542\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:             time_total_s 3.19451\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:                timestamp 1691698942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:               tmae_force 0.05178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:              tmape_force 55963567665640.26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb:              trmse_force 0.08785\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: 🚀 View run FSR_Trainable_5ed4e384 at: https://wandb.ai/seokjin/FSR-prediction/runs/5ed4e384\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=263904)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052223-5ed4e384/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_92ba28cc_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-18/wandb/run-20230811_052231-92ba28cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Syncing run FSR_Trainable_92ba28cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/92ba28cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:22:35,901\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:35,909\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:35,910\tWARNING util.py:315 -- Processing trial results took 1.927 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:35,912\tWARNING util.py:315 -- The `process_trial_result` operation took 1.928 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:                mae_force 288.87134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:               mape_force 3.555567830869106e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:                   metric 0.16798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:               rmse_force 502.01481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:       time_since_restore 1.234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:         time_this_iter_s 1.234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:             time_total_s 1.234\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:                timestamp 1691698947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:               tmae_force 0.11222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:              tmape_force 176055482516168.03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb:              trmse_force 0.16798\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: 🚀 View run FSR_Trainable_92ba28cc at: https://wandb.ai/seokjin/FSR-prediction/runs/92ba28cc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264128)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052231-92ba28cc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_10045936_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-26/wandb/run-20230811_052238-10045936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Syncing run FSR_Trainable_10045936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/10045936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:22:42,564\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:42,570\tWARNING util.py:315 -- The `process_trial_result` operation took 1.791 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:42,571\tWARNING util.py:315 -- Processing trial results took 1.792 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:42,573\tWARNING util.py:315 -- The `process_trial_result` operation took 1.794 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264311)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052238-10045936/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_61601394_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-32/wandb/run-20230811_052244-61601394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Syncing run FSR_Trainable_61601394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/61601394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:                mae_force 295.91448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:               mape_force 2.659597576296473e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:                   metric 0.17366\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:               rmse_force 526.66595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:       time_since_restore 1.12172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:         time_this_iter_s 1.12172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:             time_total_s 1.12172\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:                timestamp 1691698960\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:               tmae_force 0.11612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:              tmape_force 150918109560385.72\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb:              trmse_force 0.17366\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: 🚀 View run FSR_Trainable_61601394 at: https://wandb.ai/seokjin/FSR-prediction/runs/61601394\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264495)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052244-61601394/logs\n",
      "2023-08-11 05:22:51,411\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:51,415\tWARNING util.py:315 -- The `process_trial_result` operation took 1.955 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:51,416\tWARNING util.py:315 -- Processing trial results took 1.957 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:51,418\tWARNING util.py:315 -- The `process_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_48d91995_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-39/wandb/run-20230811_052254-48d91995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Syncing run FSR_Trainable_48d91995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48d91995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:22:58,947\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.070 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:58,951\tWARNING util.py:315 -- The `process_trial_result` operation took 2.075 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:22:58,954\tWARNING util.py:315 -- Processing trial results took 2.078 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:22:58,957\tWARNING util.py:315 -- The `process_trial_result` operation took 2.081 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:                mae_force █▆▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:               mape_force ▃█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:                   metric █▆▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:               rmse_force █▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:         time_this_iter_s █▃▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:               tmae_force ▇█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:              tmape_force ▃█▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:              trmse_force █▆▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:                mae_force 162.28436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:               mape_force 1.5122984277971942e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:                   metric 0.09389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:               rmse_force 308.45817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:       time_since_restore 4.09957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:         time_this_iter_s 0.6936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:             time_total_s 4.09957\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:                timestamp 1691698974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:               tmae_force 0.0584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:              tmape_force 67268390597662.87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb:              trmse_force 0.09389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: 🚀 View run FSR_Trainable_48d91995 at: https://wandb.ai/seokjin/FSR-prediction/runs/48d91995\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264716)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052254-48d91995/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_2eda9114_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-48/wandb/run-20230811_052301-2eda9114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Syncing run FSR_Trainable_2eda9114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2eda9114\n",
      "2023-08-11 05:23:09,556\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.519 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:09,562\tWARNING util.py:315 -- The `process_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:09,563\tWARNING util.py:315 -- Processing trial results took 2.526 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:23:09,564\tWARNING util.py:315 -- The `process_trial_result` operation took 2.528 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_c7fcd88e_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-22-55/wandb/run-20230811_052312-c7fcd88e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Syncing run FSR_Trainable_c7fcd88e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7fcd88e\n",
      "2023-08-11 05:23:18,646\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.435 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:18,649\tWARNING util.py:315 -- The `process_trial_result` operation took 2.439 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:18,652\tWARNING util.py:315 -- Processing trial results took 2.441 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:23:18,653\tWARNING util.py:315 -- The `process_trial_result` operation took 2.443 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_788c3fca_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-23-06/wandb/run-20230811_052321-788c3fca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Syncing run FSR_Trainable_788c3fca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/788c3fca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:                mae_force █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:               mape_force ▄█▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:                   metric █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:               rmse_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:         time_this_iter_s █▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:               tmae_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:              tmape_force ▁█▇▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:              trmse_force █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:                mae_force 158.38375\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:               mape_force 2.0459728058850493e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:                   metric 0.0912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:               rmse_force 286.08145\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:       time_since_restore 3.86893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:         time_this_iter_s 0.76436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:             time_total_s 3.86893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:                timestamp 1691699001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:               tmae_force 0.05842\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:              tmape_force 83897479108365.75\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb:              trmse_force 0.0912\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: 🚀 View run FSR_Trainable_788c3fca at: https://wandb.ai/seokjin/FSR-prediction/runs/788c3fca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265299)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052321-788c3fca/logs\n",
      "2023-08-11 05:23:29,359\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.860 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:29,360\tWARNING util.py:315 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:29,361\tWARNING util.py:315 -- Processing trial results took 1.864 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:23:29,362\tWARNING util.py:315 -- The `process_trial_result` operation took 1.864 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_39f6fe44_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-23-14/wandb/run-20230811_052332-39f6fe44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Syncing run FSR_Trainable_39f6fe44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/39f6fe44\n",
      "2023-08-11 05:23:43,701\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.245 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:43,704\tWARNING util.py:315 -- The `process_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:23:43,706\tWARNING util.py:315 -- Processing trial results took 2.250 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:23:43,708\tWARNING util.py:315 -- The `process_trial_result` operation took 2.252 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_ce34eb03_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-23-26/wandb/run-20230811_052347-ce34eb03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Syncing run FSR_Trainable_ce34eb03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ce34eb03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:                mae_force █▇▅▃▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:               mape_force █▇▄▂▃▃▃▃▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:                   metric █▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:               rmse_force █▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▄▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:         time_this_iter_s ▄▂▂▃█▆▅▂▃▂▂▁▃▇▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▄▄▄▅▅▆▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:                timestamp ▁▂▂▂▃▄▄▅▅▅▅▆▆▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:               tmae_force █▇▅▃▃▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:              tmape_force ██▆▄▄▃▃▃▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:              trmse_force █▆▄▃▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:                mae_force 112.14039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:               mape_force 9.535075807236624e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:                   metric 0.07305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:               rmse_force 221.09875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:       time_since_restore 16.35468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:         time_this_iter_s 1.20381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:             time_total_s 16.35468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:                timestamp 1691699027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:               tmae_force 0.04241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:              tmape_force 42971623309490.01\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb:              trmse_force 0.07305\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: 🚀 View run FSR_Trainable_39f6fe44 at: https://wandb.ai/seokjin/FSR-prediction/runs/39f6fe44\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265519)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052332-39f6fe44/logs\n",
      "2023-08-11 05:24:03,993\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:24:03,997\tWARNING util.py:315 -- The `process_trial_result` operation took 2.164 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:24:04,000\tWARNING util.py:315 -- Processing trial results took 2.167 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:24:04,003\tWARNING util.py:315 -- The `process_trial_result` operation took 2.169 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_98ec7841_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-23-40/wandb/run-20230811_052407-98ec7841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Syncing run FSR_Trainable_98ec7841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/98ec7841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:                mae_force █▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:               mape_force █▆▃▂▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:                   metric █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:               rmse_force █▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:       time_since_restore ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:         time_this_iter_s ▄█▅▅▅▃▃▃▃▃▄▃▄▃▁▂▅▃▂▂▄▅▄▂▅▂▂▄▆▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:             time_total_s ▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:               tmae_force █▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:              tmape_force █▇▄▃▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:              trmse_force █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:                mae_force 113.62748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:               mape_force 1.0079933707765978e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:                   metric 0.07286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:               rmse_force 217.43388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:       time_since_restore 32.89275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:         time_this_iter_s 1.07116\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:             time_total_s 32.89275\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:                timestamp 1691699059\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:               tmae_force 0.04316\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:              tmape_force 44701567209735.875\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb:              trmse_force 0.07286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: 🚀 View run FSR_Trainable_ce34eb03 at: https://wandb.ai/seokjin/FSR-prediction/runs/ce34eb03\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265989)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052347-ce34eb03/logs\n",
      "2023-08-11 05:24:34,967\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.029 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:24:34,970\tWARNING util.py:315 -- The `process_trial_result` operation took 2.034 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:24:34,971\tWARNING util.py:315 -- Processing trial results took 2.035 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:24:34,973\tWARNING util.py:315 -- The `process_trial_result` operation took 2.037 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_072f7c51_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-24-00/wandb/run-20230811_052438-072f7c51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: Syncing run FSR_Trainable_072f7c51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/072f7c51\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:                mae_force █▇▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:               mape_force ██▅▃▃▄▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:                   metric █▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:               rmse_force █▆▅▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:         time_this_iter_s ▅▂▄▃▆▇▄▇▄▄▆▇▅▆▆▂▂▄▂▅▄▄▂▂▁▁▃▅▅▆▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:                timestamp ▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:               tmae_force █▇▆▄▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:              tmape_force ██▆▅▅▅▄▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:              trmse_force █▆▅▄▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:                mae_force 113.13017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:               mape_force 9.866579817474939e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:                   metric 0.07284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:               rmse_force 218.29662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:       time_since_restore 30.63263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:         time_this_iter_s 1.29948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:             time_total_s 30.63263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:                timestamp 1691699077\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:               tmae_force 0.04285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:              tmape_force 43587310084250.19\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb:              trmse_force 0.07284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: 🚀 View run FSR_Trainable_98ec7841 at: https://wandb.ai/seokjin/FSR-prediction/runs/98ec7841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266223)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052407-98ec7841/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266468)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:                mae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:               mape_force █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:                   metric █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:               rmse_force █▄▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:         time_this_iter_s ▃▁▂▁▃▁▂▁▄▂▂▁▁▃▂█▃▂▂▄▃▃▃▃▂▃▃▄▃▃▃▃▂▃▂▂▃▃▅▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:               tmae_force █▆▃▂▁▁▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:              tmape_force █▇▃▂▁▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:              trmse_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "2023-08-11 05:24:49,221\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.187 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:24:49,224\tWARNING util.py:315 -- The `process_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:24:49,226\tWARNING util.py:315 -- Processing trial results took 2.193 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:24:49,228\tWARNING util.py:315 -- The `process_trial_result` operation took 2.194 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:                mae_force 111.40073\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:               mape_force 1.0538870918893398e+17\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:                   metric 0.07103\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:               rmse_force 210.22901\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:       time_since_restore 87.77899\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:         time_this_iter_s 0.95997\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:             time_total_s 87.77899\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:                timestamp 1691699081\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:               tmae_force 0.04188\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:              tmape_force 43027586686375.55\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb:              trmse_force 0.07103\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: 🚀 View run FSR_Trainable_2eda9114 at: https://wandb.ai/seokjin/FSR-prediction/runs/2eda9114\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052301-2eda9114/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=264899)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_b3901d26_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-24-31/wandb/run-20230811_052452-b3901d26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Syncing run FSR_Trainable_b3901d26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b3901d26\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:                mae_force █▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:               mape_force ██▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:                   metric █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:               rmse_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:         time_this_iter_s ▃▃▂▆▃▃▁▂▄▃█▄▂▇▆▅▃▄▄▅▃▅▄▄▆▆▄▄▃▂▅▅▆▃▄▁▁▄▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:               tmae_force █▆▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:              tmape_force ▇█▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:              trmse_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:                mae_force 110.90543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:               mape_force 1.0410918513643896e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:                   metric 0.07103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:               rmse_force 209.81612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:       time_since_restore 88.62175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:         time_this_iter_s 0.7106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:             time_total_s 88.62175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:                timestamp 1691699094\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:               tmae_force 0.04216\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:              tmape_force 44441959461399.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb:              trmse_force 0.07103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: 🚀 View run FSR_Trainable_c7fcd88e at: https://wandb.ai/seokjin/FSR-prediction/runs/c7fcd88e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052312-c7fcd88e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052312-c7fcd88e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb:              trmse_force ▁\n",
      "2023-08-11 05:25:01,185\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.412 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:01,190\tWARNING util.py:315 -- The `process_trial_result` operation took 2.418 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:01,193\tWARNING util.py:315 -- Processing trial results took 2.420 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:25:01,195\tWARNING util.py:315 -- The `process_trial_result` operation took 2.422 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=265123)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266720)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_537b7970_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-24-46/wandb/run-20230811_052504-537b7970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Syncing run FSR_Trainable_537b7970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/537b7970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:                mae_force 244.42428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:               mape_force 872663607.84095\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:                   metric 0.99065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:               rmse_force 427.40533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:       time_since_restore 1.13974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:         time_this_iter_s 1.13974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:             time_total_s 1.13974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:                timestamp 1691699098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:               tmae_force 0.62509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:              tmape_force 1.52485\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb:              trmse_force 0.99065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: 🚀 View run FSR_Trainable_537b7970 at: https://wandb.ai/seokjin/FSR-prediction/runs/537b7970\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052504-537b7970/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=266965)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-11 05:25:13,255\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.239 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:13,262\tWARNING util.py:315 -- The `process_trial_result` operation took 2.246 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:13,265\tWARNING util.py:315 -- Processing trial results took 2.249 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:25:13,267\tWARNING util.py:315 -- The `process_trial_result` operation took 2.251 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_988be0f4_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-24-57/wandb/run-20230811_052516-988be0f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Syncing run FSR_Trainable_988be0f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/988be0f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:                mae_force 132.97337\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:               mape_force 236961384.0269\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:                   metric 0.64321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:               rmse_force 238.15571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:       time_since_restore 1.80263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:         time_this_iter_s 1.80263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:             time_total_s 1.80263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:                timestamp 1691699111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:               tmae_force 0.37918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:              tmape_force 2.01335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb:              trmse_force 0.64321\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: 🚀 View run FSR_Trainable_988be0f4 at: https://wandb.ai/seokjin/FSR-prediction/runs/988be0f4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267223)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052516-988be0f4/logs\n",
      "2023-08-11 05:25:23,552\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.926 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:23,558\tWARNING util.py:315 -- The `process_trial_result` operation took 2.933 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:23,560\tWARNING util.py:315 -- Processing trial results took 2.935 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:25:23,563\tWARNING util.py:315 -- The `process_trial_result` operation took 2.938 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_5d520eab_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-25-09/wandb/run-20230811_052526-5d520eab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Syncing run FSR_Trainable_5d520eab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5d520eab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:                mae_force 174.4531\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:               mape_force 1.876139124453506e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:                   metric 0.10605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:               rmse_force 338.31561\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:       time_since_restore 3.14832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:         time_this_iter_s 1.03005\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:             time_total_s 3.14832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:                timestamp 1691699124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:               tmae_force 0.06149\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:              tmape_force 67344813993720.125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb:              trmse_force 0.10605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: 🚀 View run FSR_Trainable_5d520eab at: https://wandb.ai/seokjin/FSR-prediction/runs/5d520eab\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267423)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052526-5d520eab/logs\n",
      "2023-08-11 05:25:33,089\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.414 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:33,097\tWARNING util.py:315 -- The `process_trial_result` operation took 3.423 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:33,100\tWARNING util.py:315 -- Processing trial results took 3.425 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:25:33,102\tWARNING util.py:315 -- The `process_trial_result` operation took 3.428 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_2e7e4879_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-25-18/wandb/run-20230811_052535-2e7e4879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Syncing run FSR_Trainable_2e7e4879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2e7e4879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:                mae_force 200.65586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:               mape_force 2.717281501764862e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:                   metric 0.11721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:               rmse_force 365.72442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:       time_since_restore 2.25537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:         time_this_iter_s 0.99653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:             time_total_s 2.25537\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:                timestamp 1691699134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:               tmae_force 0.07104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:              tmape_force 96649087884551.1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb:              trmse_force 0.11721\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: 🚀 View run FSR_Trainable_2e7e4879 at: https://wandb.ai/seokjin/FSR-prediction/runs/2e7e4879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267620)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052535-2e7e4879/logs\n",
      "2023-08-11 05:25:42,382\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.519 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:42,388\tWARNING util.py:315 -- The `process_trial_result` operation took 2.525 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:42,390\tWARNING util.py:315 -- Processing trial results took 2.528 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:25:42,392\tWARNING util.py:315 -- The `process_trial_result` operation took 2.529 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_76a437b0_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-25-28/wandb/run-20230811_052545-76a437b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Syncing run FSR_Trainable_76a437b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/76a437b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:                mae_force 219.18106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:               mape_force 3.2713593679709357e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:                   metric 0.12285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:               rmse_force 384.48619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:       time_since_restore 2.95523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:         time_this_iter_s 1.31416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:             time_total_s 2.95523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:                timestamp 1691699143\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:               tmae_force 0.078\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:              tmape_force 118565734641979.27\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb:              trmse_force 0.12285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: 🚀 View run FSR_Trainable_76a437b0 at: https://wandb.ai/seokjin/FSR-prediction/runs/76a437b0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=267821)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052545-76a437b0/logs\n",
      "2023-08-11 05:25:52,161\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.577 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:52,169\tWARNING util.py:315 -- The `process_trial_result` operation took 2.586 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:25:52,171\tWARNING util.py:315 -- Processing trial results took 2.589 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:25:52,173\tWARNING util.py:315 -- The `process_trial_result` operation took 2.591 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_8f5659b5_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-25-38/wandb/run-20230811_052555-8f5659b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Syncing run FSR_Trainable_8f5659b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8f5659b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:                mae_force 178.81837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:               mape_force 2.010097318320301e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:                   metric 0.1066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:               rmse_force 338.70982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:       time_since_restore 3.33677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:         time_this_iter_s 1.11306\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:             time_total_s 3.33677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:                timestamp 1691699153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:               tmae_force 0.06484\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:              tmape_force 82464246973296.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb:              trmse_force 0.1066\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: 🚀 View run FSR_Trainable_8f5659b5 at: https://wandb.ai/seokjin/FSR-prediction/runs/8f5659b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268031)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052555-8f5659b5/logs\n",
      "2023-08-11 05:26:03,061\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.385 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:03,065\tWARNING util.py:315 -- The `process_trial_result` operation took 3.390 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:03,067\tWARNING util.py:315 -- Processing trial results took 3.391 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:26:03,068\tWARNING util.py:315 -- The `process_trial_result` operation took 3.393 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_8067426e_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-25-47/wandb/run-20230811_052606-8067426e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Syncing run FSR_Trainable_8067426e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8067426e\n",
      "2023-08-11 05:26:14,193\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.679 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:14,198\tWARNING util.py:315 -- The `process_trial_result` operation took 3.686 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:14,199\tWARNING util.py:315 -- Processing trial results took 3.688 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:26:14,201\tWARNING util.py:315 -- The `process_trial_result` operation took 3.690 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_c9309d56_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-25-57/wandb/run-20230811_052617-c9309d56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Syncing run FSR_Trainable_c9309d56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c9309d56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-11 05:26:21,874\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.944 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:21,876\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:21,877\tWARNING util.py:315 -- Processing trial results took 1.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:26:21,880\tWARNING util.py:315 -- The `process_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:                mae_force 224.58868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:               mape_force 3.248877343519066e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:                   metric 0.12499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:               rmse_force 398.37602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:       time_since_restore 2.48554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:         time_this_iter_s 0.92605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:             time_total_s 2.48554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:                timestamp 1691699175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:               tmae_force 0.07945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:              tmape_force 117994667008194.3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb:              trmse_force 0.12499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: 🚀 View run FSR_Trainable_c9309d56 at: https://wandb.ai/seokjin/FSR-prediction/runs/c9309d56\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268473)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052617-c9309d56/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_dbec4dcf_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-26-09/wandb/run-20230811_052625-dbec4dcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Syncing run FSR_Trainable_dbec4dcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/dbec4dcf\n",
      "2023-08-11 05:26:35,256\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.389 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:35,262\tWARNING util.py:315 -- The `process_trial_result` operation took 2.396 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:35,263\tWARNING util.py:315 -- Processing trial results took 2.397 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:26:35,265\tWARNING util.py:315 -- The `process_trial_result` operation took 2.398 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_1a9699d3_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-26-19/wandb/run-20230811_052638-1a9699d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Syncing run FSR_Trainable_1a9699d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a9699d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:                mae_force █▅▄▂▂▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:               mape_force █▂▂▂▃▄▃▂▂▂▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:                   metric █▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:               rmse_force █▆▄▃▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:       time_since_restore ▁▁▂▃▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:         time_this_iter_s ▄▂▃▅▁▃▃▂▄▁▁▂▃█▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:             time_total_s ▁▁▂▃▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:                timestamp ▁▂▃▃▃▃▄▄▅▅▅▆▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:               tmae_force █▅▄▃▃▂▂▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:              tmape_force █▃▃▄▅▄▃▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:              trmse_force █▆▄▃▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:                mae_force 114.97079\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:               mape_force 1.0275876959351235e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:                   metric 0.07422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:               rmse_force 222.18076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:       time_since_restore 13.25318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:         time_this_iter_s 0.88681\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:             time_total_s 13.25318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:                timestamp 1691699196\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:               tmae_force 0.0436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:              tmape_force 45450733778459.81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb:              trmse_force 0.07422\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: 🚀 View run FSR_Trainable_dbec4dcf at: https://wandb.ai/seokjin/FSR-prediction/runs/dbec4dcf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268664)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052625-dbec4dcf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb:              trmse_force ▁\n",
      "2023-08-11 05:26:45,643\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.751 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:45,647\tWARNING util.py:315 -- The `process_trial_result` operation took 1.756 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:45,649\tWARNING util.py:315 -- Processing trial results took 1.758 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:26:45,651\tWARNING util.py:315 -- The `process_trial_result` operation took 1.760 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268887)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_28609f96_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-26-30/wandb/run-20230811_052648-28609f96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Syncing run FSR_Trainable_28609f96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/28609f96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                mae_force 330.14338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:               mape_force 3.631684546080384e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                   metric 0.18602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:               rmse_force 557.2949\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       time_since_restore 1.10674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:         time_this_iter_s 1.10674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:             time_total_s 1.10674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:                timestamp 1691699203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:               tmae_force 0.12385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:              tmape_force 151543377908542.47\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb:              trmse_force 0.18602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: 🚀 View run FSR_Trainable_28609f96 at: https://wandb.ai/seokjin/FSR-prediction/runs/28609f96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052648-28609f96/logs\n",
      "2023-08-11 05:26:54,818\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:54,821\tWARNING util.py:315 -- The `process_trial_result` operation took 1.974 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:26:54,823\tWARNING util.py:315 -- Processing trial results took 1.976 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:26:54,824\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269120)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_12e8ba4d_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-26-42/wandb/run-20230811_052657-12e8ba4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Syncing run FSR_Trainable_12e8ba4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/12e8ba4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:                mae_force 217.15836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:               mape_force 2.82052911841588e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:                   metric 0.12595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:               rmse_force 410.8203\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:       time_since_restore 1.80479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:         time_this_iter_s 0.94586\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:             time_total_s 1.80479\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:                timestamp 1691699215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:               tmae_force 0.07259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:              tmape_force 84733097745711.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb:              trmse_force 0.12595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: 🚀 View run FSR_Trainable_12e8ba4d at: https://wandb.ai/seokjin/FSR-prediction/runs/12e8ba4d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052657-12e8ba4d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269343)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-11 05:27:05,780\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.776 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:05,783\tWARNING util.py:315 -- The `process_trial_result` operation took 1.779 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:05,785\tWARNING util.py:315 -- Processing trial results took 1.781 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:27:05,788\tWARNING util.py:315 -- The `process_trial_result` operation took 1.784 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_27ced3c6_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-26-52/wandb/run-20230811_052708-27ced3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Syncing run FSR_Trainable_27ced3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/27ced3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:                mae_force 316.85554\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:               mape_force 5.0221328856428934e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:                   metric 0.1789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:               rmse_force 490.86743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:       time_since_restore 1.02776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:         time_this_iter_s 1.02776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:             time_total_s 1.02776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:                timestamp 1691699224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:               tmae_force 0.13324\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:              tmape_force 240534096342427.62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb:              trmse_force 0.1789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: 🚀 View run FSR_Trainable_27ced3c6 at: https://wandb.ai/seokjin/FSR-prediction/runs/27ced3c6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269585)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052708-27ced3c6/logs\n",
      "2023-08-11 05:27:14,684\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:14,687\tWARNING util.py:315 -- The `process_trial_result` operation took 1.857 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:14,688\tWARNING util.py:315 -- Processing trial results took 1.858 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:27:14,692\tWARNING util.py:315 -- The `process_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_c7d66c81_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-27-03/wandb/run-20230811_052717-c7d66c81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Syncing run FSR_Trainable_c7d66c81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7d66c81\n",
      "2023-08-11 05:27:27,760\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.318 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:27,764\tWARNING util.py:315 -- The `process_trial_result` operation took 2.322 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:27,765\tWARNING util.py:315 -- Processing trial results took 2.323 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:27:27,768\tWARNING util.py:315 -- The `process_trial_result` operation took 2.326 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_854bb80b_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-27-12/wandb/run-20230811_052731-854bb80b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Syncing run FSR_Trainable_854bb80b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/854bb80b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:                mae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:               mape_force █▇▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:                   metric █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:               rmse_force █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:         time_this_iter_s █▄▄▂▄▁▂▂▂▂▂▂▃▂▂▂▁▂▂▁▁▁▂▃▁▂▂▂▂▁▁▁▁▂▂▄▂▂▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:               tmae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:              tmape_force ██▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:              trmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:                mae_force 109.90982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:               mape_force 1.0570350278410083e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:                   metric 0.07021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:               rmse_force 206.66362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:       time_since_restore 69.68829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:         time_this_iter_s 0.73076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:             time_total_s 69.68829\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:                timestamp 1691699250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:               tmae_force 0.04215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:              tmape_force 45916848486296.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb:              trmse_force 0.07021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: 🚀 View run FSR_Trainable_8067426e at: https://wandb.ai/seokjin/FSR-prediction/runs/8067426e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=268246)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052606-8067426e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:                mae_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:               mape_force ██▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:                   metric █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:               rmse_force █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:         time_this_iter_s █▄▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:               tmae_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:              tmape_force ▃█▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb:              trmse_force █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052731-854bb80b/logs\n",
      "2023-08-11 05:27:38,971\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.846 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:38,977\tWARNING util.py:315 -- The `process_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:38,979\tWARNING util.py:315 -- Processing trial results took 1.855 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:27:38,981\tWARNING util.py:315 -- The `process_trial_result` operation took 1.857 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270029)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_e2a4c7eb_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-27-24/wandb/run-20230811_052746-e2a4c7eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Syncing run FSR_Trainable_e2a4c7eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2a4c7eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:                mae_force █▅▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:               mape_force ▅█▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:                   metric █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:               rmse_force █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:         time_this_iter_s ▁▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:                timestamp ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:               tmae_force █▆▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:              tmape_force ▁█▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:              trmse_force █▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:                mae_force 148.64449\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:               mape_force 1.9391079703369523e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:                   metric 0.08623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:               rmse_force 257.69367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:       time_since_restore 5.22745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:         time_this_iter_s 1.7603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:             time_total_s 5.22745\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:                timestamp 1691699263\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:               tmae_force 0.05583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:              tmape_force 79433962769066.25\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb:              trmse_force 0.08623\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: 🚀 View run FSR_Trainable_e2a4c7eb at: https://wandb.ai/seokjin/FSR-prediction/runs/e2a4c7eb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052746-e2a4c7eb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270265)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-11 05:27:52,470\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.396 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:52,476\tWARNING util.py:315 -- The `process_trial_result` operation took 2.403 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:27:52,479\tWARNING util.py:315 -- Processing trial results took 2.405 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:27:52,480\tWARNING util.py:315 -- The `process_trial_result` operation took 2.407 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_87c5d6ce_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-27-36/wandb/run-20230811_052755-87c5d6ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Syncing run FSR_Trainable_87c5d6ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/87c5d6ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:                mae_force 374.97374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:               mape_force 2.9700318055319776e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:                   metric 0.20667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:               rmse_force 768.94823\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:       time_since_restore 1.07698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:         time_this_iter_s 1.07698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:             time_total_s 1.07698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:                timestamp 1691699270\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:               tmae_force 0.12014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:              tmape_force 114301856121874.95\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb:              trmse_force 0.20667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: 🚀 View run FSR_Trainable_87c5d6ce at: https://wandb.ai/seokjin/FSR-prediction/runs/87c5d6ce\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270501)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052755-87c5d6ce/logs\n",
      "2023-08-11 05:28:03,402\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.353 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:28:03,406\tWARNING util.py:315 -- The `process_trial_result` operation took 2.357 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:28:03,407\tWARNING util.py:315 -- Processing trial results took 2.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:28:03,408\tWARNING util.py:315 -- The `process_trial_result` operation took 2.360 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_30951e7e_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-27-49/wandb/run-20230811_052806-30951e7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Syncing run FSR_Trainable_30951e7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/30951e7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:                mae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:               mape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:                   metric ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:               rmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:               tmae_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:              tmape_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:              trmse_force ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:                mae_force 268.61715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:               mape_force 4.263278646409747e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:                   metric 0.14518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:               rmse_force 482.58417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:       time_since_restore 0.90559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:         time_this_iter_s 0.90559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:             time_total_s 0.90559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:                timestamp 1691699281\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:               tmae_force 0.09278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:              tmape_force 149500704477422.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb:              trmse_force 0.14518\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: 🚀 View run FSR_Trainable_30951e7e at: https://wandb.ai/seokjin/FSR-prediction/runs/30951e7e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270733)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052806-30951e7e/logs\n",
      "2023-08-11 05:28:14,489\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.010 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:28:14,496\tWARNING util.py:315 -- The `process_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:28:14,499\tWARNING util.py:315 -- Processing trial results took 2.021 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:28:14,501\tWARNING util.py:315 -- The `process_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_f7d46d05_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-08-11_05-28-00/wandb/run-20230811_052817-f7d46d05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Syncing run FSR_Trainable_f7d46d05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f7d46d05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:                mae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:               mape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:                   metric █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:               rmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:               tmae_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:              tmape_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:              trmse_force █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:                mae_force 203.60135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:               mape_force 2.6020770216278314e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:                   metric 0.11835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:               rmse_force 374.90583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:       time_since_restore 2.18027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:         time_this_iter_s 1.15468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:             time_total_s 2.18027\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:                timestamp 1691699295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:               tmae_force 0.07104\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:              tmape_force 90713265334288.61\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb:              trmse_force 0.11835\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: 🚀 View run FSR_Trainable_f7d46d05 at: https://wandb.ai/seokjin/FSR-prediction/runs/f7d46d05\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=270971)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052817-f7d46d05/logs\n",
      "2023-08-11 05:28:26,251\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.002 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:28:26,253\tWARNING util.py:315 -- The `process_trial_result` operation took 2.005 s, which may be a performance bottleneck.\n",
      "2023-08-11 05:28:26,256\tWARNING util.py:315 -- Processing trial results took 2.008 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-11 05:28:26,258\tWARNING util.py:315 -- The `process_trial_result` operation took 2.010 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-11_05-09-34/FSR_Trainable_090f5019_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y_2023-08-11_05-28-11/wandb/run-20230811_052829-090f5019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Syncing run FSR_Trainable_090f5019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/090f5019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:                mae_force █▄▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:               mape_force █▅▂▁▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:                   metric █▃▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:               rmse_force █▃▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:       time_since_restore ▁▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:         time_this_iter_s ██▄▄▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:             time_total_s ▁▃▄▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:                timestamp ▁▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:               tmae_force █▄▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:              tmape_force ██▂▁▂▃▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:              trmse_force █▃▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:                mae_force 124.24127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:               mape_force 1.1388141298282366e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:                   metric 0.07614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:               rmse_force 235.98265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:       time_since_restore 5.17846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:         time_this_iter_s 0.49138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:             time_total_s 5.17846\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:                timestamp 1691699310\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:               tmae_force 0.04657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:              tmape_force 51147104728089.85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb:              trmse_force 0.07614\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: 🚀 View run FSR_Trainable_090f5019 at: https://wandb.ai/seokjin/FSR-prediction/runs/090f5019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=271202)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052829-090f5019/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:                mae_force █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:               mape_force █▇▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:                   metric █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:               rmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:         time_this_iter_s ▄▅▆▅▃▇▅▄▂▂▄█▃▂▃▆▂▂▃▄▄▅▂▄▃▆▃▄▄▄▄▂▂▁▂▂▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:               tmae_force █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:              tmape_force ██▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:              trmse_force █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:                mae_force 110.60992\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:               mape_force 1.0181875730779091e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:                   metric 0.0711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:               rmse_force 210.16667\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:       time_since_restore 68.95505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:         time_this_iter_s 0.3279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:             time_total_s 68.95505\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:                timestamp 1691699318\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:               tmae_force 0.04217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:              tmape_force 44094475505825.086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb:              trmse_force 0.0711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: 🚀 View run FSR_Trainable_c7d66c81 at: https://wandb.ai/seokjin/FSR-prediction/runs/c7d66c81\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=269807)\u001b[0m wandb: Find logs at: ./wandb/run-20230811_052717-c7d66c81/logs\n",
      "2023-08-11 05:28:43,040\tINFO tune.py:1111 -- Total run time: 1145.05 seconds (1140.61 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
