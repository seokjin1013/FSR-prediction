{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2\n",
    "\n",
    "Index_X = FSR_for_force\n",
    "\n",
    "Index_y = force\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-07-18_23-55-45/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-07-18_23-55-45\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "200.127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.ANN'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force'],\n",
    "        'index_y': ['force'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_time'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-18 23:55:45,652] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:55:47,796\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-18 23:55:48,946\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-19 00:09:42</td></tr>\n",
       "<tr><td>Running for: </td><td>00:13:53.37        </td></tr>\n",
       "<tr><td>Memory:      </td><td>3.1/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -200.15404644941256 | Iter 32.000: -200.23926468583974 | Iter 16.000: -200.55608668677078 | Iter 8.000: -201.6026145098111 | Iter 4.000: -203.91644750745746 | Iter 2.000: -219.0101496984485 | Iter 1.000: -296.4972863073967<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>index_X          </th><th>index_y  </th><th>model        </th><th style=\"text-align: right;\">    model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">    mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_ec504580</td><td>TERMINATED</td><td>172.26.215.93:287505</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00592881 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       33.6234  </td><td style=\"text-align: right;\"> 200.424</td><td style=\"text-align: right;\">101.554</td><td style=\"text-align: right;\">1.6638e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_328be942</td><td>TERMINATED</td><td>172.26.215.93:287569</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0697382  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.775067</td><td style=\"text-align: right;\"> 356.09 </td><td style=\"text-align: right;\">190.25 </td><td style=\"text-align: right;\">4.52793e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_24444a45</td><td>TERMINATED</td><td>172.26.215.93:287747</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        9.48006e-05</td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.717424</td><td style=\"text-align: right;\"> 472.056</td><td style=\"text-align: right;\">235.118</td><td style=\"text-align: right;\">1.50472e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_0d137c87</td><td>TERMINATED</td><td>172.26.215.93:287922</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0164405  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.917137</td><td style=\"text-align: right;\"> 362.301</td><td style=\"text-align: right;\">195.882</td><td style=\"text-align: right;\">2.07104e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_63871c2d</td><td>TERMINATED</td><td>172.26.215.93:288238</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        6.90719e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.600638</td><td style=\"text-align: right;\"> 553.328</td><td style=\"text-align: right;\">336.791</td><td style=\"text-align: right;\">4.50171e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_6031f8ea</td><td>TERMINATED</td><td>172.26.215.93:288468</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0152123  </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.886219</td><td style=\"text-align: right;\">1885.75 </td><td style=\"text-align: right;\">608.594</td><td style=\"text-align: right;\">3.1081e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_110bbd1c</td><td>TERMINATED</td><td>172.26.215.93:288696</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        7.64564e-05</td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.541197</td><td style=\"text-align: right;\"> 589.979</td><td style=\"text-align: right;\">473.164</td><td style=\"text-align: right;\">9.22652e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_b57d8118</td><td>TERMINATED</td><td>172.26.215.93:288923</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00019745 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.91307 </td><td style=\"text-align: right;\"> 364.949</td><td style=\"text-align: right;\">212.089</td><td style=\"text-align: right;\">6.62232e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_bb61f7f2</td><td>TERMINATED</td><td>172.26.215.93:289137</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00601575 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.17868 </td><td style=\"text-align: right;\"> 335.752</td><td style=\"text-align: right;\">179.293</td><td style=\"text-align: right;\">2.12014e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_4bb595dd</td><td>TERMINATED</td><td>172.26.215.93:289376</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00114123 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.6006  </td><td style=\"text-align: right;\"> 255.403</td><td style=\"text-align: right;\">146.561</td><td style=\"text-align: right;\">4.26307e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_541dbe91</td><td>TERMINATED</td><td>172.26.215.93:289467</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0128533  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.804943</td><td style=\"text-align: right;\"> 547.261</td><td style=\"text-align: right;\">353.444</td><td style=\"text-align: right;\">4.88218e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_0871b7e5</td><td>TERMINATED</td><td>172.26.215.93:289783</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.57878e-05</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.7138  </td><td style=\"text-align: right;\"> 457.276</td><td style=\"text-align: right;\">269.765</td><td style=\"text-align: right;\">9.37367e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_43751b04</td><td>TERMINATED</td><td>172.26.215.93:289874</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00103339 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.86114 </td><td style=\"text-align: right;\"> 218.968</td><td style=\"text-align: right;\">116.444</td><td style=\"text-align: right;\">1.94503e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_f5a53c4b</td><td>TERMINATED</td><td>172.26.215.93:290184</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00159911 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.08704 </td><td style=\"text-align: right;\"> 229.183</td><td style=\"text-align: right;\">118.734</td><td style=\"text-align: right;\">1.23227e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_02b54729</td><td>TERMINATED</td><td>172.26.215.93:290414</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00156372 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.95993 </td><td style=\"text-align: right;\"> 243.016</td><td style=\"text-align: right;\">133.441</td><td style=\"text-align: right;\">2.60757e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_12c423a0</td><td>TERMINATED</td><td>172.26.215.93:290641</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000479344</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.745055</td><td style=\"text-align: right;\"> 626.005</td><td style=\"text-align: right;\">350.69 </td><td style=\"text-align: right;\">1.19713e+09</td></tr>\n",
       "<tr><td>FSR_Trainable_2bb087cb</td><td>TERMINATED</td><td>172.26.215.93:290857</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000365879</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.726568</td><td style=\"text-align: right;\"> 545.026</td><td style=\"text-align: right;\">305.725</td><td style=\"text-align: right;\">8.71271e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_01aba93e</td><td>TERMINATED</td><td>172.26.215.93:290947</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00381232 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        8.21014 </td><td style=\"text-align: right;\"> 200.947</td><td style=\"text-align: right;\">101.756</td><td style=\"text-align: right;\">1.62207e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_65bccec6</td><td>TERMINATED</td><td>172.26.215.93:291256</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00451158 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.516261</td><td style=\"text-align: right;\"> 470.591</td><td style=\"text-align: right;\">223.663</td><td style=\"text-align: right;\">2.10726e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_828467e0</td><td>TERMINATED</td><td>172.26.215.93:291477</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00504661 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.41421 </td><td style=\"text-align: right;\"> 373.869</td><td style=\"text-align: right;\">175.673</td><td style=\"text-align: right;\">2.77093e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_8a745a2d</td><td>TERMINATED</td><td>172.26.215.93:291705</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0681405  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.48743 </td><td style=\"text-align: right;\"> 237.254</td><td style=\"text-align: right;\">122.432</td><td style=\"text-align: right;\">1.45459e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_8ecd4b10</td><td>TERMINATED</td><td>172.26.215.93:291798</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0958895  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.20149 </td><td style=\"text-align: right;\"> 223.915</td><td style=\"text-align: right;\">112.591</td><td style=\"text-align: right;\">1.14584e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_82615fae</td><td>TERMINATED</td><td>172.26.215.93:292112</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00199609 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.14338 </td><td style=\"text-align: right;\"> 278.664</td><td style=\"text-align: right;\">151.615</td><td style=\"text-align: right;\">4.45666e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c552165f</td><td>TERMINATED</td><td>172.26.215.93:292208</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0025826  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.07983 </td><td style=\"text-align: right;\"> 320.32 </td><td style=\"text-align: right;\">179.811</td><td style=\"text-align: right;\">4.43148e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_144e715c</td><td>TERMINATED</td><td>172.26.215.93:292520</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000639792</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        8.72322 </td><td style=\"text-align: right;\"> 212.029</td><td style=\"text-align: right;\">113.995</td><td style=\"text-align: right;\">2.41985e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_b4f16fb7</td><td>TERMINATED</td><td>172.26.215.93:292614</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000701616</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.655683</td><td style=\"text-align: right;\"> 445.684</td><td style=\"text-align: right;\">274.468</td><td style=\"text-align: right;\">6.95345e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_adbc3239</td><td>TERMINATED</td><td>172.26.215.93:292924</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.000725508</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.2073  </td><td style=\"text-align: right;\"> 423.983</td><td style=\"text-align: right;\">254.287</td><td style=\"text-align: right;\">8.31692e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_04529693</td><td>TERMINATED</td><td>172.26.215.93:293150</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00276303 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       60.9187  </td><td style=\"text-align: right;\"> 200.159</td><td style=\"text-align: right;\">101.774</td><td style=\"text-align: right;\">1.73588e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_84f26917</td><td>TERMINATED</td><td>172.26.215.93:293245</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00230413 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       56.7282  </td><td style=\"text-align: right;\"> 200.137</td><td style=\"text-align: right;\">101.588</td><td style=\"text-align: right;\">1.7216e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_2df648ca</td><td>TERMINATED</td><td>172.26.215.93:293552</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00343818 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.705086</td><td style=\"text-align: right;\"> 409.398</td><td style=\"text-align: right;\">242.75 </td><td style=\"text-align: right;\">8.96201e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_6fd39a8a</td><td>TERMINATED</td><td>172.26.215.93:293774</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00297115 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.727771</td><td style=\"text-align: right;\"> 416.438</td><td style=\"text-align: right;\">241.726</td><td style=\"text-align: right;\">7.93409e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_666d083d</td><td>TERMINATED</td><td>172.26.215.93:293996</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00839956 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.45756 </td><td style=\"text-align: right;\"> 359.132</td><td style=\"text-align: right;\">176.96 </td><td style=\"text-align: right;\">2.69188e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_6fadd259</td><td>TERMINATED</td><td>172.26.215.93:294223</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0087476  </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        1.20122 </td><td style=\"text-align: right;\"> 447.9  </td><td style=\"text-align: right;\">225.932</td><td style=\"text-align: right;\">3.37005e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_44c1311c</td><td>TERMINATED</td><td>172.26.215.93:294449</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0296466  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2.84971 </td><td style=\"text-align: right;\"> 448.471</td><td style=\"text-align: right;\">226.413</td><td style=\"text-align: right;\">7.07453e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_92cb4582</td><td>TERMINATED</td><td>172.26.215.93:294678</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00101597 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        3.79902 </td><td style=\"text-align: right;\"> 232.581</td><td style=\"text-align: right;\">126.277</td><td style=\"text-align: right;\">1.94426e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_8c75fd35</td><td>TERMINATED</td><td>172.26.215.93:294981</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00113385 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.89606 </td><td style=\"text-align: right;\"> 213.855</td><td style=\"text-align: right;\">114.067</td><td style=\"text-align: right;\">2.07654e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9cdb9f41</td><td>TERMINATED</td><td>172.26.215.93:295218</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00260478 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       48.1886  </td><td style=\"text-align: right;\"> 200.142</td><td style=\"text-align: right;\">101.582</td><td style=\"text-align: right;\">1.72562e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9d7ec6fa</td><td>TERMINATED</td><td>172.26.215.93:295451</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00354532 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.16672 </td><td style=\"text-align: right;\"> 268.57 </td><td style=\"text-align: right;\">142.453</td><td style=\"text-align: right;\">1.37381e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_af86315e</td><td>TERMINATED</td><td>172.26.215.93:295540</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00286921 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.24568 </td><td style=\"text-align: right;\"> 291.624</td><td style=\"text-align: right;\">151.055</td><td style=\"text-align: right;\">1.4839e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_f64e4ff5</td><td>TERMINATED</td><td>172.26.215.93:295848</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00663941 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       47.459   </td><td style=\"text-align: right;\"> 200.158</td><td style=\"text-align: right;\">101.633</td><td style=\"text-align: right;\">1.72756e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_4513d7ae</td><td>TERMINATED</td><td>172.26.215.93:296080</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00868198 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       10.0864  </td><td style=\"text-align: right;\"> 201.212</td><td style=\"text-align: right;\">103.054</td><td style=\"text-align: right;\">1.73893e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_32031236</td><td>TERMINATED</td><td>172.26.215.93:296298</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00709448 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       10.0444  </td><td style=\"text-align: right;\"> 201.144</td><td style=\"text-align: right;\">102.731</td><td style=\"text-align: right;\">1.69677e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d7dab881</td><td>TERMINATED</td><td>172.26.215.93:296530</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00730462 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        9.93764 </td><td style=\"text-align: right;\"> 201.096</td><td style=\"text-align: right;\">102.792</td><td style=\"text-align: right;\">1.71787e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_ba6078e3</td><td>TERMINATED</td><td>172.26.215.93:296763</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00211048 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.679633</td><td style=\"text-align: right;\"> 417.149</td><td style=\"text-align: right;\">226.372</td><td style=\"text-align: right;\">4.89025e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e370ee82</td><td>TERMINATED</td><td>172.26.215.93:296996</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00467919 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.88754 </td><td style=\"text-align: right;\"> 202.752</td><td style=\"text-align: right;\">104.932</td><td style=\"text-align: right;\">1.84251e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_5e018605</td><td>TERMINATED</td><td>172.26.215.93:297235</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.00417962 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.13538 </td><td style=\"text-align: right;\"> 237.814</td><td style=\"text-align: right;\">131.392</td><td style=\"text-align: right;\">3.46601e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9cc5c5f1</td><td>TERMINATED</td><td>172.26.215.93:297473</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0150554  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.701048</td><td style=\"text-align: right;\"> 353.24 </td><td style=\"text-align: right;\">193.751</td><td style=\"text-align: right;\">2.65476e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_f1983e0e</td><td>TERMINATED</td><td>172.26.215.93:297564</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0144665  </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.719981</td><td style=\"text-align: right;\"> 519.242</td><td style=\"text-align: right;\">309.855</td><td style=\"text-align: right;\">3.85823e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_f4e8045c</td><td>TERMINATED</td><td>172.26.215.93:297726</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0017014  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.77405 </td><td style=\"text-align: right;\"> 219.69 </td><td style=\"text-align: right;\">120.564</td><td style=\"text-align: right;\">2.3239e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_5fe9bffa</td><td>TERMINATED</td><td>172.26.215.93:298057</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00201586 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.765102</td><td style=\"text-align: right;\"> 438.793</td><td style=\"text-align: right;\">258.479</td><td style=\"text-align: right;\">8.19421e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_58513c8a</td><td>TERMINATED</td><td>172.26.215.93:298278</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00166642 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.640649</td><td style=\"text-align: right;\"> 473.37 </td><td style=\"text-align: right;\">232.147</td><td style=\"text-align: right;\">2.51761e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_0c569334</td><td>TERMINATED</td><td>172.26.215.93:298371</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00136755 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.839851</td><td style=\"text-align: right;\"> 655.429</td><td style=\"text-align: right;\">329.289</td><td style=\"text-align: right;\">2.24909e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_aa6ae5fb</td><td>TERMINATED</td><td>172.26.215.93:298682</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00464655 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        9.37073 </td><td style=\"text-align: right;\"> 201.151</td><td style=\"text-align: right;\">102.173</td><td style=\"text-align: right;\">1.62883e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_2cb2478d</td><td>TERMINATED</td><td>172.26.215.93:298774</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00621664 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.98477 </td><td style=\"text-align: right;\"> 203.662</td><td style=\"text-align: right;\">102.374</td><td style=\"text-align: right;\">1.32551e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d31af333</td><td>TERMINATED</td><td>172.26.215.93:299086</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00282327 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       18.0488  </td><td style=\"text-align: right;\"> 200.567</td><td style=\"text-align: right;\">101.856</td><td style=\"text-align: right;\">1.66599e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_4bdaf033</td><td>TERMINATED</td><td>172.26.215.93:299314</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00263581 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       53.8385  </td><td style=\"text-align: right;\"> 200.129</td><td style=\"text-align: right;\">101.614</td><td style=\"text-align: right;\">1.72578e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_67d0c2ed</td><td>TERMINATED</td><td>172.26.215.93:299533</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00250545 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       54.6864  </td><td style=\"text-align: right;\"> 200.135</td><td style=\"text-align: right;\">101.579</td><td style=\"text-align: right;\">1.71963e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_352d58df</td><td>TERMINATED</td><td>172.26.215.93:299748</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00239857 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       54.4653  </td><td style=\"text-align: right;\"> 200.127</td><td style=\"text-align: right;\">101.599</td><td style=\"text-align: right;\">1.72604e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_f7146274</td><td>TERMINATED</td><td>172.26.215.93:299979</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00260779 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.64793 </td><td style=\"text-align: right;\"> 209.399</td><td style=\"text-align: right;\">109.629</td><td style=\"text-align: right;\">2.35303e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_d0af510d</td><td>TERMINATED</td><td>172.26.215.93:300221</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00257616 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       17.9018  </td><td style=\"text-align: right;\"> 200.274</td><td style=\"text-align: right;\">101.768</td><td style=\"text-align: right;\">1.7166e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_41c00723</td><td>TERMINATED</td><td>172.26.215.93:300509</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00250116 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       16.4676  </td><td style=\"text-align: right;\"> 200.309</td><td style=\"text-align: right;\">101.865</td><td style=\"text-align: right;\">1.72397e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_f3f541bf</td><td>TERMINATED</td><td>172.26.215.93:300712</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00136792 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.39865 </td><td style=\"text-align: right;\"> 252.436</td><td style=\"text-align: right;\">135.6  </td><td style=\"text-align: right;\">3.79023e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_12b28168</td><td>TERMINATED</td><td>172.26.215.93:300939</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00125748 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.33166 </td><td style=\"text-align: right;\"> 255.23 </td><td style=\"text-align: right;\">142.63 </td><td style=\"text-align: right;\">3.60584e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_1fd52bd1</td><td>TERMINATED</td><td>172.26.215.93:301167</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00575073 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        7.82982 </td><td style=\"text-align: right;\"> 201.013</td><td style=\"text-align: right;\">102.337</td><td style=\"text-align: right;\">1.66615e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0ca08eb1</td><td>TERMINATED</td><td>172.26.215.93:301405</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00352775 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.42889 </td><td style=\"text-align: right;\"> 201.977</td><td style=\"text-align: right;\">103.402</td><td style=\"text-align: right;\">1.72951e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_18630162</td><td>TERMINATED</td><td>172.26.215.93:301620</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00508542 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        4.43863 </td><td style=\"text-align: right;\"> 201.934</td><td style=\"text-align: right;\">103.228</td><td style=\"text-align: right;\">1.69788e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c7a3720a</td><td>TERMINATED</td><td>172.26.215.93:301855</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00184212 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.16423 </td><td style=\"text-align: right;\"> 210.626</td><td style=\"text-align: right;\">111.465</td><td style=\"text-align: right;\">1.97948e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_754fcb87</td><td>TERMINATED</td><td>172.26.215.93:302082</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000847858</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.86722 </td><td style=\"text-align: right;\"> 327.449</td><td style=\"text-align: right;\">190.78 </td><td style=\"text-align: right;\">4.14691e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_9bd81de2</td><td>TERMINATED</td><td>172.26.215.93:302165</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000870064</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.807234</td><td style=\"text-align: right;\"> 429.688</td><td style=\"text-align: right;\">250.358</td><td style=\"text-align: right;\">8.36989e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_82b80782</td><td>TERMINATED</td><td>172.26.215.93:302329</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000955863</td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.790407</td><td style=\"text-align: right;\"> 404.765</td><td style=\"text-align: right;\">250.829</td><td style=\"text-align: right;\">8.65308e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_10ade037</td><td>TERMINATED</td><td>172.26.215.93:302663</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.010981   </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       14.4254  </td><td style=\"text-align: right;\"> 200.365</td><td style=\"text-align: right;\">101.733</td><td style=\"text-align: right;\">1.68049e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_b6697601</td><td>TERMINATED</td><td>172.26.215.93:302753</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00399484 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       16.4726  </td><td style=\"text-align: right;\"> 200.249</td><td style=\"text-align: right;\">101.814</td><td style=\"text-align: right;\">1.71817e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_cd729345</td><td>TERMINATED</td><td>172.26.215.93:303061</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00356747 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        5.92715 </td><td style=\"text-align: right;\"> 202.064</td><td style=\"text-align: right;\">103.48 </td><td style=\"text-align: right;\">1.75303e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_fd8002dc</td><td>TERMINATED</td><td>172.26.215.93:303277</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0114177  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        3.6357  </td><td style=\"text-align: right;\"> 201.883</td><td style=\"text-align: right;\">102.631</td><td style=\"text-align: right;\">1.53765e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0c627468</td><td>TERMINATED</td><td>172.26.215.93:303524</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00222116 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       45.2686  </td><td style=\"text-align: right;\"> 200.13 </td><td style=\"text-align: right;\">101.611</td><td style=\"text-align: right;\">1.72637e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0b763a50</td><td>TERMINATED</td><td>172.26.215.93:303753</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00202901 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.54559 </td><td style=\"text-align: right;\"> 230.872</td><td style=\"text-align: right;\">125.354</td><td style=\"text-align: right;\">2.80066e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_c85d30dd</td><td>TERMINATED</td><td>172.26.215.93:303837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00220471 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.727751</td><td style=\"text-align: right;\"> 315.336</td><td style=\"text-align: right;\">180.274</td><td style=\"text-align: right;\">5.34961e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_ec63db0d</td><td>TERMINATED</td><td>172.26.215.93:304143</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00410334 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">       16.8203  </td><td style=\"text-align: right;\"> 200.256</td><td style=\"text-align: right;\">101.59 </td><td style=\"text-align: right;\">1.68413e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_ceaee9a6</td><td>TERMINATED</td><td>172.26.215.93:304374</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00388341 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.651512</td><td style=\"text-align: right;\"> 330.234</td><td style=\"text-align: right;\">175.641</td><td style=\"text-align: right;\">1.99088e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_34aac3b3</td><td>TERMINATED</td><td>172.26.215.93:304590</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00389862 </td><td>sklearn.preproc_4750</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.630157</td><td style=\"text-align: right;\"> 357.375</td><td style=\"text-align: right;\">193.087</td><td style=\"text-align: right;\">2.52895e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_b3221052</td><td>TERMINATED</td><td>172.26.215.93:304815</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00154923 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.639625</td><td style=\"text-align: right;\"> 511.764</td><td style=\"text-align: right;\">251.034</td><td style=\"text-align: right;\">2.3632e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_9e37da93</td><td>TERMINATED</td><td>172.26.215.93:305049</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00144241 </td><td>sklearn.preproc_47b0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.900539</td><td style=\"text-align: right;\"> 517.368</td><td style=\"text-align: right;\">248.816</td><td style=\"text-align: right;\">2.0892e+16 </td></tr>\n",
       "<tr><td>FSR_Trainable_b31a235f</td><td>TERMINATED</td><td>172.26.215.93:305283</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00309061 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       31.0836  </td><td style=\"text-align: right;\"> 200.178</td><td style=\"text-align: right;\">101.727</td><td style=\"text-align: right;\">1.73323e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_ef8beec7</td><td>TERMINATED</td><td>172.26.215.93:305371</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00580139 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        8.26011 </td><td style=\"text-align: right;\"> 200.556</td><td style=\"text-align: right;\">102.169</td><td style=\"text-align: right;\">1.71144e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_f45e96fd</td><td>TERMINATED</td><td>172.26.215.93:305683</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00298951 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.59279 </td><td style=\"text-align: right;\"> 207.913</td><td style=\"text-align: right;\">107.674</td><td style=\"text-align: right;\">2.26103e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_05df9367</td><td>TERMINATED</td><td>172.26.215.93:305898</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00615955 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        8.25639 </td><td style=\"text-align: right;\"> 200.834</td><td style=\"text-align: right;\">102.016</td><td style=\"text-align: right;\">1.62893e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_00c7219a</td><td>TERMINATED</td><td>172.26.215.93:306139</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00224953 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       47.7022  </td><td style=\"text-align: right;\"> 200.137</td><td style=\"text-align: right;\">101.598</td><td style=\"text-align: right;\">1.71767e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e298ee30</td><td>TERMINATED</td><td>172.26.215.93:306362</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00206826 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.73595 </td><td style=\"text-align: right;\"> 204.622</td><td style=\"text-align: right;\">106.492</td><td style=\"text-align: right;\">1.90094e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_989b7d85</td><td>TERMINATED</td><td>172.26.215.93:306595</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00232272 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.72159 </td><td style=\"text-align: right;\"> 364.86 </td><td style=\"text-align: right;\">215.974</td><td style=\"text-align: right;\">4.4864e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_011d39d7</td><td>TERMINATED</td><td>172.26.215.93:306819</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00239436 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.527589</td><td style=\"text-align: right;\"> 445.939</td><td style=\"text-align: right;\">245.315</td><td style=\"text-align: right;\">6.14905e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_0317d496</td><td>TERMINATED</td><td>172.26.215.93:307044</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0029705  </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.563737</td><td style=\"text-align: right;\"> 566.714</td><td style=\"text-align: right;\">282.614</td><td style=\"text-align: right;\">1.34688e+09</td></tr>\n",
       "<tr><td>FSR_Trainable_9070b884</td><td>TERMINATED</td><td>172.26.215.93:307269</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00118194 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.40598 </td><td style=\"text-align: right;\"> 267.804</td><td style=\"text-align: right;\">147.723</td><td style=\"text-align: right;\">4.2606e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_beabc043</td><td>TERMINATED</td><td>172.26.215.93:307492</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00120945 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.37504 </td><td style=\"text-align: right;\"> 260.53 </td><td style=\"text-align: right;\">141.717</td><td style=\"text-align: right;\">4.06783e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_7b3bd83b</td><td>TERMINATED</td><td>172.26.215.93:307719</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00330238 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        2.39707 </td><td style=\"text-align: right;\"> 203.984</td><td style=\"text-align: right;\">104.933</td><td style=\"text-align: right;\">1.69321e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_3b428213</td><td>TERMINATED</td><td>172.26.215.93:307944</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00167278 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.49175 </td><td style=\"text-align: right;\"> 245.775</td><td style=\"text-align: right;\">135.585</td><td style=\"text-align: right;\">3.19845e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_e2efdd0c</td><td>TERMINATED</td><td>172.26.215.93:308041</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00175031 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.65524 </td><td style=\"text-align: right;\"> 227.274</td><td style=\"text-align: right;\">124.475</td><td style=\"text-align: right;\">2.69713e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_fb672319</td><td>TERMINATED</td><td>172.26.215.93:308210</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00477241 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        8.40648 </td><td style=\"text-align: right;\"> 200.908</td><td style=\"text-align: right;\">102.243</td><td style=\"text-align: right;\">1.66391e+08</td></tr>\n",
       "<tr><td>FSR_Trainable_8e07034e</td><td>TERMINATED</td><td>172.26.215.93:308396</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00300091 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.45149 </td><td style=\"text-align: right;\"> 228.855</td><td style=\"text-align: right;\">115.782</td><td style=\"text-align: right;\">9.51251e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_13216edd</td><td>TERMINATED</td><td>172.26.215.93:308720</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00502492 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        1.5344  </td><td style=\"text-align: right;\"> 254.943</td><td style=\"text-align: right;\">128.878</td><td style=\"text-align: right;\">1.6405e+08 </td></tr>\n",
       "<tr><td>FSR_Trainable_63d60b58</td><td>TERMINATED</td><td>172.26.215.93:308959</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_4810</td><td>[&#x27;FSR_for_force&#x27;]</td><td>[&#x27;force&#x27;]</td><td>fsr_model.ANN</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">                     2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00241142 </td><td>sklearn.preproc_4330</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.626676</td><td style=\"text-align: right;\"> 373.631</td><td style=\"text-align: right;\">222.631</td><td style=\"text-align: right;\">8.28592e+08</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 23:55:48,979\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_00c7219a</td><td>2023-07-19_00-08-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.598</td><td style=\"text-align: right;\">1.71767e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">306139</td><td style=\"text-align: right;\"> 200.137</td><td style=\"text-align: right;\">           47.7022  </td><td style=\"text-align: right;\">          0.406911</td><td style=\"text-align: right;\">     47.7022  </td><td style=\"text-align: right;\"> 1689692935</td><td style=\"text-align: right;\">                 100</td><td>00c7219a  </td></tr>\n",
       "<tr><td>FSR_Trainable_011d39d7</td><td>2023-07-19_00-08-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">245.315</td><td style=\"text-align: right;\">6.14905e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">306819</td><td style=\"text-align: right;\"> 445.939</td><td style=\"text-align: right;\">            0.527589</td><td style=\"text-align: right;\">          0.527589</td><td style=\"text-align: right;\">      0.527589</td><td style=\"text-align: right;\"> 1689692902</td><td style=\"text-align: right;\">                   1</td><td>011d39d7  </td></tr>\n",
       "<tr><td>FSR_Trainable_01aba93e</td><td>2023-07-18_23-58-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">101.756</td><td style=\"text-align: right;\">1.62207e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">290947</td><td style=\"text-align: right;\"> 200.947</td><td style=\"text-align: right;\">            8.21014 </td><td style=\"text-align: right;\">          0.583845</td><td style=\"text-align: right;\">      8.21014 </td><td style=\"text-align: right;\"> 1689692283</td><td style=\"text-align: right;\">                  16</td><td>01aba93e  </td></tr>\n",
       "<tr><td>FSR_Trainable_02b54729</td><td>2023-07-18_23-57-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">133.441</td><td style=\"text-align: right;\">2.60757e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">290414</td><td style=\"text-align: right;\"> 243.016</td><td style=\"text-align: right;\">            2.95993 </td><td style=\"text-align: right;\">          0.545009</td><td style=\"text-align: right;\">      2.95993 </td><td style=\"text-align: right;\"> 1689692256</td><td style=\"text-align: right;\">                   4</td><td>02b54729  </td></tr>\n",
       "<tr><td>FSR_Trainable_0317d496</td><td>2023-07-19_00-08-31</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">282.614</td><td style=\"text-align: right;\">1.34688e+09</td><td>172.26.215.93</td><td style=\"text-align: right;\">307044</td><td style=\"text-align: right;\"> 566.714</td><td style=\"text-align: right;\">            0.563737</td><td style=\"text-align: right;\">          0.563737</td><td style=\"text-align: right;\">      0.563737</td><td style=\"text-align: right;\"> 1689692911</td><td style=\"text-align: right;\">                   1</td><td>0317d496  </td></tr>\n",
       "<tr><td>FSR_Trainable_04529693</td><td>2023-07-19_00-00-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.774</td><td style=\"text-align: right;\">1.73588e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">293150</td><td style=\"text-align: right;\"> 200.159</td><td style=\"text-align: right;\">           60.9187  </td><td style=\"text-align: right;\">          0.497719</td><td style=\"text-align: right;\">     60.9187  </td><td style=\"text-align: right;\"> 1689692416</td><td style=\"text-align: right;\">                 100</td><td>04529693  </td></tr>\n",
       "<tr><td>FSR_Trainable_05df9367</td><td>2023-07-19_00-07-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">102.016</td><td style=\"text-align: right;\">1.62893e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">305898</td><td style=\"text-align: right;\"> 200.834</td><td style=\"text-align: right;\">            8.25639 </td><td style=\"text-align: right;\">          0.613716</td><td style=\"text-align: right;\">      8.25639 </td><td style=\"text-align: right;\"> 1689692874</td><td style=\"text-align: right;\">                  16</td><td>05df9367  </td></tr>\n",
       "<tr><td>FSR_Trainable_0871b7e5</td><td>2023-07-18_23-57-11</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">269.765</td><td style=\"text-align: right;\">9.37367e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">289783</td><td style=\"text-align: right;\"> 457.276</td><td style=\"text-align: right;\">            0.7138  </td><td style=\"text-align: right;\">          0.7138  </td><td style=\"text-align: right;\">      0.7138  </td><td style=\"text-align: right;\"> 1689692231</td><td style=\"text-align: right;\">                   1</td><td>0871b7e5  </td></tr>\n",
       "<tr><td>FSR_Trainable_0b763a50</td><td>2023-07-19_00-06-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">125.354</td><td style=\"text-align: right;\">2.80066e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">303753</td><td style=\"text-align: right;\"> 230.872</td><td style=\"text-align: right;\">            1.54559 </td><td style=\"text-align: right;\">          0.66902 </td><td style=\"text-align: right;\">      1.54559 </td><td style=\"text-align: right;\"> 1689692780</td><td style=\"text-align: right;\">                   2</td><td>0b763a50  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c569334</td><td>2023-07-19_00-02-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">329.289</td><td style=\"text-align: right;\">2.24909e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">298371</td><td style=\"text-align: right;\"> 655.429</td><td style=\"text-align: right;\">            0.839851</td><td style=\"text-align: right;\">          0.839851</td><td style=\"text-align: right;\">      0.839851</td><td style=\"text-align: right;\"> 1689692552</td><td style=\"text-align: right;\">                   1</td><td>0c569334  </td></tr>\n",
       "<tr><td>FSR_Trainable_0c627468</td><td>2023-07-19_00-07-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.611</td><td style=\"text-align: right;\">1.72637e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">303524</td><td style=\"text-align: right;\"> 200.13 </td><td style=\"text-align: right;\">           45.2686  </td><td style=\"text-align: right;\">          0.436305</td><td style=\"text-align: right;\">     45.2686  </td><td style=\"text-align: right;\"> 1689692829</td><td style=\"text-align: right;\">                 100</td><td>0c627468  </td></tr>\n",
       "<tr><td>FSR_Trainable_0ca08eb1</td><td>2023-07-19_00-04-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">103.402</td><td style=\"text-align: right;\">1.72951e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">301405</td><td style=\"text-align: right;\"> 201.977</td><td style=\"text-align: right;\">            4.42889 </td><td style=\"text-align: right;\">          0.511371</td><td style=\"text-align: right;\">      4.42889 </td><td style=\"text-align: right;\"> 1689692698</td><td style=\"text-align: right;\">                   8</td><td>0ca08eb1  </td></tr>\n",
       "<tr><td>FSR_Trainable_0d137c87</td><td>2023-07-18_23-56-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">195.882</td><td style=\"text-align: right;\">2.07104e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">287922</td><td style=\"text-align: right;\"> 362.301</td><td style=\"text-align: right;\">            0.917137</td><td style=\"text-align: right;\">          0.917137</td><td style=\"text-align: right;\">      0.917137</td><td style=\"text-align: right;\"> 1689692173</td><td style=\"text-align: right;\">                   1</td><td>0d137c87  </td></tr>\n",
       "<tr><td>FSR_Trainable_10ade037</td><td>2023-07-19_00-05-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">101.733</td><td style=\"text-align: right;\">1.68049e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">302663</td><td style=\"text-align: right;\"> 200.365</td><td style=\"text-align: right;\">           14.4254  </td><td style=\"text-align: right;\">          0.586456</td><td style=\"text-align: right;\">     14.4254  </td><td style=\"text-align: right;\"> 1689692754</td><td style=\"text-align: right;\">                  32</td><td>10ade037  </td></tr>\n",
       "<tr><td>FSR_Trainable_110bbd1c</td><td>2023-07-18_23-56-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">473.164</td><td style=\"text-align: right;\">9.22652e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">288696</td><td style=\"text-align: right;\"> 589.979</td><td style=\"text-align: right;\">            0.541197</td><td style=\"text-align: right;\">          0.541197</td><td style=\"text-align: right;\">      0.541197</td><td style=\"text-align: right;\"> 1689692197</td><td style=\"text-align: right;\">                   1</td><td>110bbd1c  </td></tr>\n",
       "<tr><td>FSR_Trainable_12b28168</td><td>2023-07-19_00-04-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">142.63 </td><td style=\"text-align: right;\">3.60584e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">300939</td><td style=\"text-align: right;\"> 255.23 </td><td style=\"text-align: right;\">            1.33166 </td><td style=\"text-align: right;\">          0.624681</td><td style=\"text-align: right;\">      1.33166 </td><td style=\"text-align: right;\"> 1689692678</td><td style=\"text-align: right;\">                   2</td><td>12b28168  </td></tr>\n",
       "<tr><td>FSR_Trainable_12c423a0</td><td>2023-07-18_23-57-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">350.69 </td><td style=\"text-align: right;\">1.19713e+09</td><td>172.26.215.93</td><td style=\"text-align: right;\">290641</td><td style=\"text-align: right;\"> 626.005</td><td style=\"text-align: right;\">            0.745055</td><td style=\"text-align: right;\">          0.745055</td><td style=\"text-align: right;\">      0.745055</td><td style=\"text-align: right;\"> 1689692259</td><td style=\"text-align: right;\">                   1</td><td>12c423a0  </td></tr>\n",
       "<tr><td>FSR_Trainable_13216edd</td><td>2023-07-19_00-09-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">128.878</td><td style=\"text-align: right;\">1.6405e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">308720</td><td style=\"text-align: right;\"> 254.943</td><td style=\"text-align: right;\">            1.5344  </td><td style=\"text-align: right;\">          0.613796</td><td style=\"text-align: right;\">      1.5344  </td><td style=\"text-align: right;\"> 1689692975</td><td style=\"text-align: right;\">                   2</td><td>13216edd  </td></tr>\n",
       "<tr><td>FSR_Trainable_144e715c</td><td>2023-07-18_23-58-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">113.995</td><td style=\"text-align: right;\">2.41985e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">292520</td><td style=\"text-align: right;\"> 212.029</td><td style=\"text-align: right;\">            8.72322 </td><td style=\"text-align: right;\">          0.474662</td><td style=\"text-align: right;\">      8.72322 </td><td style=\"text-align: right;\"> 1689692332</td><td style=\"text-align: right;\">                  16</td><td>144e715c  </td></tr>\n",
       "<tr><td>FSR_Trainable_18630162</td><td>2023-07-19_00-05-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">103.228</td><td style=\"text-align: right;\">1.69788e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">301620</td><td style=\"text-align: right;\"> 201.934</td><td style=\"text-align: right;\">            4.43863 </td><td style=\"text-align: right;\">          0.477831</td><td style=\"text-align: right;\">      4.43863 </td><td style=\"text-align: right;\"> 1689692706</td><td style=\"text-align: right;\">                   8</td><td>18630162  </td></tr>\n",
       "<tr><td>FSR_Trainable_1fd52bd1</td><td>2023-07-19_00-04-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">102.337</td><td style=\"text-align: right;\">1.66615e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">301167</td><td style=\"text-align: right;\"> 201.013</td><td style=\"text-align: right;\">            7.82982 </td><td style=\"text-align: right;\">          0.588967</td><td style=\"text-align: right;\">      7.82982 </td><td style=\"text-align: right;\"> 1689692693</td><td style=\"text-align: right;\">                  16</td><td>1fd52bd1  </td></tr>\n",
       "<tr><td>FSR_Trainable_24444a45</td><td>2023-07-18_23-56-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">235.118</td><td style=\"text-align: right;\">1.50472e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">287747</td><td style=\"text-align: right;\"> 472.056</td><td style=\"text-align: right;\">            0.717424</td><td style=\"text-align: right;\">          0.717424</td><td style=\"text-align: right;\">      0.717424</td><td style=\"text-align: right;\"> 1689692166</td><td style=\"text-align: right;\">                   1</td><td>24444a45  </td></tr>\n",
       "<tr><td>FSR_Trainable_2bb087cb</td><td>2023-07-18_23-57-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">305.725</td><td style=\"text-align: right;\">8.71271e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">290857</td><td style=\"text-align: right;\"> 545.026</td><td style=\"text-align: right;\">            0.726568</td><td style=\"text-align: right;\">          0.726568</td><td style=\"text-align: right;\">      0.726568</td><td style=\"text-align: right;\"> 1689692266</td><td style=\"text-align: right;\">                   1</td><td>2bb087cb  </td></tr>\n",
       "<tr><td>FSR_Trainable_2cb2478d</td><td>2023-07-19_00-02-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">102.374</td><td style=\"text-align: right;\">1.32551e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">298774</td><td style=\"text-align: right;\"> 203.662</td><td style=\"text-align: right;\">            4.98477 </td><td style=\"text-align: right;\">          0.514227</td><td style=\"text-align: right;\">      4.98477 </td><td style=\"text-align: right;\"> 1689692572</td><td style=\"text-align: right;\">                   8</td><td>2cb2478d  </td></tr>\n",
       "<tr><td>FSR_Trainable_2df648ca</td><td>2023-07-18_23-59-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">242.75 </td><td style=\"text-align: right;\">8.96201e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">293552</td><td style=\"text-align: right;\"> 409.398</td><td style=\"text-align: right;\">            0.705086</td><td style=\"text-align: right;\">          0.705086</td><td style=\"text-align: right;\">      0.705086</td><td style=\"text-align: right;\"> 1689692358</td><td style=\"text-align: right;\">                   1</td><td>2df648ca  </td></tr>\n",
       "<tr><td>FSR_Trainable_32031236</td><td>2023-07-19_00-01-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">102.731</td><td style=\"text-align: right;\">1.69677e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">296298</td><td style=\"text-align: right;\"> 201.144</td><td style=\"text-align: right;\">           10.0444  </td><td style=\"text-align: right;\">          0.542666</td><td style=\"text-align: right;\">     10.0444  </td><td style=\"text-align: right;\"> 1689692483</td><td style=\"text-align: right;\">                  16</td><td>32031236  </td></tr>\n",
       "<tr><td>FSR_Trainable_328be942</td><td>2023-07-18_23-56-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">190.25 </td><td style=\"text-align: right;\">4.52793e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">287569</td><td style=\"text-align: right;\"> 356.09 </td><td style=\"text-align: right;\">            0.775067</td><td style=\"text-align: right;\">          0.775067</td><td style=\"text-align: right;\">      0.775067</td><td style=\"text-align: right;\"> 1689692160</td><td style=\"text-align: right;\">                   1</td><td>328be942  </td></tr>\n",
       "<tr><td>FSR_Trainable_34aac3b3</td><td>2023-07-19_00-06-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">193.087</td><td style=\"text-align: right;\">2.52895e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">304590</td><td style=\"text-align: right;\"> 357.375</td><td style=\"text-align: right;\">            0.630157</td><td style=\"text-align: right;\">          0.630157</td><td style=\"text-align: right;\">      0.630157</td><td style=\"text-align: right;\"> 1689692813</td><td style=\"text-align: right;\">                   1</td><td>34aac3b3  </td></tr>\n",
       "<tr><td>FSR_Trainable_352d58df</td><td>2023-07-19_00-04-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.599</td><td style=\"text-align: right;\">1.72604e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">299748</td><td style=\"text-align: right;\"> 200.127</td><td style=\"text-align: right;\">           54.4653  </td><td style=\"text-align: right;\">          0.463186</td><td style=\"text-align: right;\">     54.4653  </td><td style=\"text-align: right;\"> 1689692666</td><td style=\"text-align: right;\">                 100</td><td>352d58df  </td></tr>\n",
       "<tr><td>FSR_Trainable_3b428213</td><td>2023-07-19_00-09-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">135.585</td><td style=\"text-align: right;\">3.19845e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">307944</td><td style=\"text-align: right;\"> 245.775</td><td style=\"text-align: right;\">            1.49175 </td><td style=\"text-align: right;\">          0.585732</td><td style=\"text-align: right;\">      1.49175 </td><td style=\"text-align: right;\"> 1689692948</td><td style=\"text-align: right;\">                   2</td><td>3b428213  </td></tr>\n",
       "<tr><td>FSR_Trainable_41c00723</td><td>2023-07-19_00-04-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">101.865</td><td style=\"text-align: right;\">1.72397e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">300509</td><td style=\"text-align: right;\"> 200.309</td><td style=\"text-align: right;\">           16.4676  </td><td style=\"text-align: right;\">          0.476624</td><td style=\"text-align: right;\">     16.4676  </td><td style=\"text-align: right;\"> 1689692679</td><td style=\"text-align: right;\">                  32</td><td>41c00723  </td></tr>\n",
       "<tr><td>FSR_Trainable_43751b04</td><td>2023-07-18_23-57-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">116.444</td><td style=\"text-align: right;\">1.94503e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">289874</td><td style=\"text-align: right;\"> 218.968</td><td style=\"text-align: right;\">            4.86114 </td><td style=\"text-align: right;\">          0.47455 </td><td style=\"text-align: right;\">      4.86114 </td><td style=\"text-align: right;\"> 1689692242</td><td style=\"text-align: right;\">                   8</td><td>43751b04  </td></tr>\n",
       "<tr><td>FSR_Trainable_44c1311c</td><td>2023-07-19_00-00-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">226.413</td><td style=\"text-align: right;\">7.07453e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">294449</td><td style=\"text-align: right;\"> 448.471</td><td style=\"text-align: right;\">            2.84971 </td><td style=\"text-align: right;\">          1.24879 </td><td style=\"text-align: right;\">      2.84971 </td><td style=\"text-align: right;\"> 1689692403</td><td style=\"text-align: right;\">                   2</td><td>44c1311c  </td></tr>\n",
       "<tr><td>FSR_Trainable_4513d7ae</td><td>2023-07-19_00-01-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">103.054</td><td style=\"text-align: right;\">1.73893e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">296080</td><td style=\"text-align: right;\"> 201.212</td><td style=\"text-align: right;\">           10.0864  </td><td style=\"text-align: right;\">          0.534277</td><td style=\"text-align: right;\">     10.0864  </td><td style=\"text-align: right;\"> 1689692472</td><td style=\"text-align: right;\">                  16</td><td>4513d7ae  </td></tr>\n",
       "<tr><td>FSR_Trainable_4bb595dd</td><td>2023-07-18_23-57-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">146.561</td><td style=\"text-align: right;\">4.26307e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">289376</td><td style=\"text-align: right;\"> 255.403</td><td style=\"text-align: right;\">            2.6006  </td><td style=\"text-align: right;\">          0.505422</td><td style=\"text-align: right;\">      2.6006  </td><td style=\"text-align: right;\"> 1689692221</td><td style=\"text-align: right;\">                   4</td><td>4bb595dd  </td></tr>\n",
       "<tr><td>FSR_Trainable_4bdaf033</td><td>2023-07-19_00-04-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.614</td><td style=\"text-align: right;\">1.72578e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">299314</td><td style=\"text-align: right;\"> 200.129</td><td style=\"text-align: right;\">           53.8385  </td><td style=\"text-align: right;\">          0.460859</td><td style=\"text-align: right;\">     53.8385  </td><td style=\"text-align: right;\"> 1689692648</td><td style=\"text-align: right;\">                 100</td><td>4bdaf033  </td></tr>\n",
       "<tr><td>FSR_Trainable_541dbe91</td><td>2023-07-18_23-57-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">353.444</td><td style=\"text-align: right;\">4.88218e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">289467</td><td style=\"text-align: right;\"> 547.261</td><td style=\"text-align: right;\">            0.804943</td><td style=\"text-align: right;\">          0.804943</td><td style=\"text-align: right;\">      0.804943</td><td style=\"text-align: right;\"> 1689692224</td><td style=\"text-align: right;\">                   1</td><td>541dbe91  </td></tr>\n",
       "<tr><td>FSR_Trainable_58513c8a</td><td>2023-07-19_00-02-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">232.147</td><td style=\"text-align: right;\">2.51761e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">298278</td><td style=\"text-align: right;\"> 473.37 </td><td style=\"text-align: right;\">            0.640649</td><td style=\"text-align: right;\">          0.640649</td><td style=\"text-align: right;\">      0.640649</td><td style=\"text-align: right;\"> 1689692547</td><td style=\"text-align: right;\">                   1</td><td>58513c8a  </td></tr>\n",
       "<tr><td>FSR_Trainable_5e018605</td><td>2023-07-19_00-01-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">131.392</td><td style=\"text-align: right;\">3.46601e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">297235</td><td style=\"text-align: right;\"> 237.814</td><td style=\"text-align: right;\">            2.13538 </td><td style=\"text-align: right;\">          0.39043 </td><td style=\"text-align: right;\">      2.13538 </td><td style=\"text-align: right;\"> 1689692517</td><td style=\"text-align: right;\">                   4</td><td>5e018605  </td></tr>\n",
       "<tr><td>FSR_Trainable_5fe9bffa</td><td>2023-07-19_00-02-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">258.479</td><td style=\"text-align: right;\">8.19421e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">298057</td><td style=\"text-align: right;\"> 438.793</td><td style=\"text-align: right;\">            0.765102</td><td style=\"text-align: right;\">          0.765102</td><td style=\"text-align: right;\">      0.765102</td><td style=\"text-align: right;\"> 1689692540</td><td style=\"text-align: right;\">                   1</td><td>5fe9bffa  </td></tr>\n",
       "<tr><td>FSR_Trainable_6031f8ea</td><td>2023-07-18_23-56-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">608.594</td><td style=\"text-align: right;\">3.1081e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">288468</td><td style=\"text-align: right;\">1885.75 </td><td style=\"text-align: right;\">            0.886219</td><td style=\"text-align: right;\">          0.886219</td><td style=\"text-align: right;\">      0.886219</td><td style=\"text-align: right;\"> 1689692189</td><td style=\"text-align: right;\">                   1</td><td>6031f8ea  </td></tr>\n",
       "<tr><td>FSR_Trainable_63871c2d</td><td>2023-07-18_23-56-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">336.791</td><td style=\"text-align: right;\">4.50171e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">288238</td><td style=\"text-align: right;\"> 553.328</td><td style=\"text-align: right;\">            0.600638</td><td style=\"text-align: right;\">          0.600638</td><td style=\"text-align: right;\">      0.600638</td><td style=\"text-align: right;\"> 1689692181</td><td style=\"text-align: right;\">                   1</td><td>63871c2d  </td></tr>\n",
       "<tr><td>FSR_Trainable_63d60b58</td><td>2023-07-19_00-09-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">222.631</td><td style=\"text-align: right;\">8.28592e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">308959</td><td style=\"text-align: right;\"> 373.631</td><td style=\"text-align: right;\">            0.626676</td><td style=\"text-align: right;\">          0.626676</td><td style=\"text-align: right;\">      0.626676</td><td style=\"text-align: right;\"> 1689692980</td><td style=\"text-align: right;\">                   1</td><td>63d60b58  </td></tr>\n",
       "<tr><td>FSR_Trainable_65bccec6</td><td>2023-07-18_23-58-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">223.663</td><td style=\"text-align: right;\">2.10726e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">291256</td><td style=\"text-align: right;\"> 470.591</td><td style=\"text-align: right;\">            0.516261</td><td style=\"text-align: right;\">          0.516261</td><td style=\"text-align: right;\">      0.516261</td><td style=\"text-align: right;\"> 1689692280</td><td style=\"text-align: right;\">                   1</td><td>65bccec6  </td></tr>\n",
       "<tr><td>FSR_Trainable_666d083d</td><td>2023-07-18_23-59-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">176.96 </td><td style=\"text-align: right;\">2.69188e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">293996</td><td style=\"text-align: right;\"> 359.132</td><td style=\"text-align: right;\">            1.45756 </td><td style=\"text-align: right;\">          0.624601</td><td style=\"text-align: right;\">      1.45756 </td><td style=\"text-align: right;\"> 1689692381</td><td style=\"text-align: right;\">                   2</td><td>666d083d  </td></tr>\n",
       "<tr><td>FSR_Trainable_67d0c2ed</td><td>2023-07-19_00-04-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.579</td><td style=\"text-align: right;\">1.71963e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">299533</td><td style=\"text-align: right;\"> 200.135</td><td style=\"text-align: right;\">           54.6864  </td><td style=\"text-align: right;\">          0.398586</td><td style=\"text-align: right;\">     54.6864  </td><td style=\"text-align: right;\"> 1689692656</td><td style=\"text-align: right;\">                 100</td><td>67d0c2ed  </td></tr>\n",
       "<tr><td>FSR_Trainable_6fadd259</td><td>2023-07-18_23-59-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">225.932</td><td style=\"text-align: right;\">3.37005e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">294223</td><td style=\"text-align: right;\"> 447.9  </td><td style=\"text-align: right;\">            1.20122 </td><td style=\"text-align: right;\">          1.20122 </td><td style=\"text-align: right;\">      1.20122 </td><td style=\"text-align: right;\"> 1689692389</td><td style=\"text-align: right;\">                   1</td><td>6fadd259  </td></tr>\n",
       "<tr><td>FSR_Trainable_6fd39a8a</td><td>2023-07-18_23-59-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">241.726</td><td style=\"text-align: right;\">7.93409e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">293774</td><td style=\"text-align: right;\"> 416.438</td><td style=\"text-align: right;\">            0.727771</td><td style=\"text-align: right;\">          0.727771</td><td style=\"text-align: right;\">      0.727771</td><td style=\"text-align: right;\"> 1689692369</td><td style=\"text-align: right;\">                   1</td><td>6fd39a8a  </td></tr>\n",
       "<tr><td>FSR_Trainable_754fcb87</td><td>2023-07-19_00-05-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">190.78 </td><td style=\"text-align: right;\">4.14691e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">302082</td><td style=\"text-align: right;\"> 327.449</td><td style=\"text-align: right;\">            0.86722 </td><td style=\"text-align: right;\">          0.86722 </td><td style=\"text-align: right;\">      0.86722 </td><td style=\"text-align: right;\"> 1689692716</td><td style=\"text-align: right;\">                   1</td><td>754fcb87  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b3bd83b</td><td>2023-07-19_00-09-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">104.933</td><td style=\"text-align: right;\">1.69321e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">307719</td><td style=\"text-align: right;\"> 203.984</td><td style=\"text-align: right;\">            2.39707 </td><td style=\"text-align: right;\">          0.545668</td><td style=\"text-align: right;\">      2.39707 </td><td style=\"text-align: right;\"> 1689692941</td><td style=\"text-align: right;\">                   4</td><td>7b3bd83b  </td></tr>\n",
       "<tr><td>FSR_Trainable_82615fae</td><td>2023-07-18_23-58-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">151.615</td><td style=\"text-align: right;\">4.45666e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">292112</td><td style=\"text-align: right;\"> 278.664</td><td style=\"text-align: right;\">            1.14338 </td><td style=\"text-align: right;\">          0.393912</td><td style=\"text-align: right;\">      1.14338 </td><td style=\"text-align: right;\"> 1689692310</td><td style=\"text-align: right;\">                   2</td><td>82615fae  </td></tr>\n",
       "<tr><td>FSR_Trainable_828467e0</td><td>2023-07-18_23-58-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">175.673</td><td style=\"text-align: right;\">2.77093e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">291477</td><td style=\"text-align: right;\"> 373.869</td><td style=\"text-align: right;\">            1.41421 </td><td style=\"text-align: right;\">          0.478435</td><td style=\"text-align: right;\">      1.41421 </td><td style=\"text-align: right;\"> 1689692289</td><td style=\"text-align: right;\">                   2</td><td>828467e0  </td></tr>\n",
       "<tr><td>FSR_Trainable_82b80782</td><td>2023-07-19_00-05-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">250.829</td><td style=\"text-align: right;\">8.65308e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">302329</td><td style=\"text-align: right;\"> 404.765</td><td style=\"text-align: right;\">            0.790407</td><td style=\"text-align: right;\">          0.790407</td><td style=\"text-align: right;\">      0.790407</td><td style=\"text-align: right;\"> 1689692727</td><td style=\"text-align: right;\">                   1</td><td>82b80782  </td></tr>\n",
       "<tr><td>FSR_Trainable_84f26917</td><td>2023-07-19_00-00-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.588</td><td style=\"text-align: right;\">1.7216e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">293245</td><td style=\"text-align: right;\"> 200.137</td><td style=\"text-align: right;\">           56.7282  </td><td style=\"text-align: right;\">          0.423947</td><td style=\"text-align: right;\">     56.7282  </td><td style=\"text-align: right;\"> 1689692417</td><td style=\"text-align: right;\">                 100</td><td>84f26917  </td></tr>\n",
       "<tr><td>FSR_Trainable_8a745a2d</td><td>2023-07-18_23-58-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">122.432</td><td style=\"text-align: right;\">1.45459e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">291705</td><td style=\"text-align: right;\"> 237.254</td><td style=\"text-align: right;\">            2.48743 </td><td style=\"text-align: right;\">          0.565965</td><td style=\"text-align: right;\">      2.48743 </td><td style=\"text-align: right;\"> 1689692298</td><td style=\"text-align: right;\">                   4</td><td>8a745a2d  </td></tr>\n",
       "<tr><td>FSR_Trainable_8c75fd35</td><td>2023-07-19_00-00-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">114.067</td><td style=\"text-align: right;\">2.07654e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">294981</td><td style=\"text-align: right;\"> 213.855</td><td style=\"text-align: right;\">            4.89606 </td><td style=\"text-align: right;\">          0.537958</td><td style=\"text-align: right;\">      4.89606 </td><td style=\"text-align: right;\"> 1689692425</td><td style=\"text-align: right;\">                   8</td><td>8c75fd35  </td></tr>\n",
       "<tr><td>FSR_Trainable_8e07034e</td><td>2023-07-19_00-09-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">115.782</td><td style=\"text-align: right;\">9.51251e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">308396</td><td style=\"text-align: right;\"> 228.855</td><td style=\"text-align: right;\">            1.45149 </td><td style=\"text-align: right;\">          0.693079</td><td style=\"text-align: right;\">      1.45149 </td><td style=\"text-align: right;\"> 1689692967</td><td style=\"text-align: right;\">                   2</td><td>8e07034e  </td></tr>\n",
       "<tr><td>FSR_Trainable_8ecd4b10</td><td>2023-07-18_23-58-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">112.591</td><td style=\"text-align: right;\">1.14584e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">291798</td><td style=\"text-align: right;\"> 223.915</td><td style=\"text-align: right;\">            4.20149 </td><td style=\"text-align: right;\">          0.381576</td><td style=\"text-align: right;\">      4.20149 </td><td style=\"text-align: right;\"> 1689692305</td><td style=\"text-align: right;\">                   8</td><td>8ecd4b10  </td></tr>\n",
       "<tr><td>FSR_Trainable_9070b884</td><td>2023-07-19_00-08-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">147.723</td><td style=\"text-align: right;\">4.2606e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">307269</td><td style=\"text-align: right;\"> 267.804</td><td style=\"text-align: right;\">            1.40598 </td><td style=\"text-align: right;\">          0.667485</td><td style=\"text-align: right;\">      1.40598 </td><td style=\"text-align: right;\"> 1689692923</td><td style=\"text-align: right;\">                   2</td><td>9070b884  </td></tr>\n",
       "<tr><td>FSR_Trainable_92cb4582</td><td>2023-07-19_00-00-15</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">126.277</td><td style=\"text-align: right;\">1.94426e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">294678</td><td style=\"text-align: right;\"> 232.581</td><td style=\"text-align: right;\">            3.79902 </td><td style=\"text-align: right;\">          1.05897 </td><td style=\"text-align: right;\">      3.79902 </td><td style=\"text-align: right;\"> 1689692415</td><td style=\"text-align: right;\">                   4</td><td>92cb4582  </td></tr>\n",
       "<tr><td>FSR_Trainable_989b7d85</td><td>2023-07-19_00-08-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">215.974</td><td style=\"text-align: right;\">4.4864e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">306595</td><td style=\"text-align: right;\"> 364.86 </td><td style=\"text-align: right;\">            0.72159 </td><td style=\"text-align: right;\">          0.72159 </td><td style=\"text-align: right;\">      0.72159 </td><td style=\"text-align: right;\"> 1689692893</td><td style=\"text-align: right;\">                   1</td><td>989b7d85  </td></tr>\n",
       "<tr><td>FSR_Trainable_9bd81de2</td><td>2023-07-19_00-05-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">250.358</td><td style=\"text-align: right;\">8.36989e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">302165</td><td style=\"text-align: right;\"> 429.688</td><td style=\"text-align: right;\">            0.807234</td><td style=\"text-align: right;\">          0.807234</td><td style=\"text-align: right;\">      0.807234</td><td style=\"text-align: right;\"> 1689692721</td><td style=\"text-align: right;\">                   1</td><td>9bd81de2  </td></tr>\n",
       "<tr><td>FSR_Trainable_9cc5c5f1</td><td>2023-07-19_00-02-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">193.751</td><td style=\"text-align: right;\">2.65476e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">297473</td><td style=\"text-align: right;\"> 353.24 </td><td style=\"text-align: right;\">            0.701048</td><td style=\"text-align: right;\">          0.701048</td><td style=\"text-align: right;\">      0.701048</td><td style=\"text-align: right;\"> 1689692521</td><td style=\"text-align: right;\">                   1</td><td>9cc5c5f1  </td></tr>\n",
       "<tr><td>FSR_Trainable_9cdb9f41</td><td>2023-07-19_00-01-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.582</td><td style=\"text-align: right;\">1.72562e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">295218</td><td style=\"text-align: right;\"> 200.142</td><td style=\"text-align: right;\">           48.1886  </td><td style=\"text-align: right;\">          0.511672</td><td style=\"text-align: right;\">     48.1886  </td><td style=\"text-align: right;\"> 1689692490</td><td style=\"text-align: right;\">                 100</td><td>9cdb9f41  </td></tr>\n",
       "<tr><td>FSR_Trainable_9d7ec6fa</td><td>2023-07-19_00-00-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">142.453</td><td style=\"text-align: right;\">1.37381e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">295451</td><td style=\"text-align: right;\"> 268.57 </td><td style=\"text-align: right;\">            1.16672 </td><td style=\"text-align: right;\">          0.514753</td><td style=\"text-align: right;\">      1.16672 </td><td style=\"text-align: right;\"> 1689692439</td><td style=\"text-align: right;\">                   2</td><td>9d7ec6fa  </td></tr>\n",
       "<tr><td>FSR_Trainable_9e37da93</td><td>2023-07-19_00-07-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">248.816</td><td style=\"text-align: right;\">2.0892e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">305049</td><td style=\"text-align: right;\"> 517.368</td><td style=\"text-align: right;\">            0.900539</td><td style=\"text-align: right;\">          0.900539</td><td style=\"text-align: right;\">      0.900539</td><td style=\"text-align: right;\"> 1689692832</td><td style=\"text-align: right;\">                   1</td><td>9e37da93  </td></tr>\n",
       "<tr><td>FSR_Trainable_aa6ae5fb</td><td>2023-07-19_00-02-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">102.173</td><td style=\"text-align: right;\">1.62883e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">298682</td><td style=\"text-align: right;\"> 201.151</td><td style=\"text-align: right;\">            9.37073 </td><td style=\"text-align: right;\">          0.537375</td><td style=\"text-align: right;\">      9.37073 </td><td style=\"text-align: right;\"> 1689692571</td><td style=\"text-align: right;\">                  16</td><td>aa6ae5fb  </td></tr>\n",
       "<tr><td>FSR_Trainable_adbc3239</td><td>2023-07-18_23-58-55</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">254.287</td><td style=\"text-align: right;\">8.31692e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">292924</td><td style=\"text-align: right;\"> 423.983</td><td style=\"text-align: right;\">            1.2073  </td><td style=\"text-align: right;\">          1.2073  </td><td style=\"text-align: right;\">      1.2073  </td><td style=\"text-align: right;\"> 1689692335</td><td style=\"text-align: right;\">                   1</td><td>adbc3239  </td></tr>\n",
       "<tr><td>FSR_Trainable_af86315e</td><td>2023-07-19_00-00-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">151.055</td><td style=\"text-align: right;\">1.4839e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">295540</td><td style=\"text-align: right;\"> 291.624</td><td style=\"text-align: right;\">            1.24568 </td><td style=\"text-align: right;\">          0.527988</td><td style=\"text-align: right;\">      1.24568 </td><td style=\"text-align: right;\"> 1689692445</td><td style=\"text-align: right;\">                   2</td><td>af86315e  </td></tr>\n",
       "<tr><td>FSR_Trainable_b31a235f</td><td>2023-07-19_00-08-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\">101.727</td><td style=\"text-align: right;\">1.73323e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">305283</td><td style=\"text-align: right;\"> 200.178</td><td style=\"text-align: right;\">           31.0836  </td><td style=\"text-align: right;\">          0.420641</td><td style=\"text-align: right;\">     31.0836  </td><td style=\"text-align: right;\"> 1689692881</td><td style=\"text-align: right;\">                  64</td><td>b31a235f  </td></tr>\n",
       "<tr><td>FSR_Trainable_b3221052</td><td>2023-07-19_00-07-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">251.034</td><td style=\"text-align: right;\">2.3632e+16 </td><td>172.26.215.93</td><td style=\"text-align: right;\">304815</td><td style=\"text-align: right;\"> 511.764</td><td style=\"text-align: right;\">            0.639625</td><td style=\"text-align: right;\">          0.639625</td><td style=\"text-align: right;\">      0.639625</td><td style=\"text-align: right;\"> 1689692822</td><td style=\"text-align: right;\">                   1</td><td>b3221052  </td></tr>\n",
       "<tr><td>FSR_Trainable_b4f16fb7</td><td>2023-07-18_23-58-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">274.468</td><td style=\"text-align: right;\">6.95345e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">292614</td><td style=\"text-align: right;\"> 445.684</td><td style=\"text-align: right;\">            0.655683</td><td style=\"text-align: right;\">          0.655683</td><td style=\"text-align: right;\">      0.655683</td><td style=\"text-align: right;\"> 1689692326</td><td style=\"text-align: right;\">                   1</td><td>b4f16fb7  </td></tr>\n",
       "<tr><td>FSR_Trainable_b57d8118</td><td>2023-07-18_23-56-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">212.089</td><td style=\"text-align: right;\">6.62232e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">288923</td><td style=\"text-align: right;\"> 364.949</td><td style=\"text-align: right;\">            1.91307 </td><td style=\"text-align: right;\">          0.706634</td><td style=\"text-align: right;\">      1.91307 </td><td style=\"text-align: right;\"> 1689692207</td><td style=\"text-align: right;\">                   2</td><td>b57d8118  </td></tr>\n",
       "<tr><td>FSR_Trainable_b6697601</td><td>2023-07-19_00-06-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">101.814</td><td style=\"text-align: right;\">1.71817e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">302753</td><td style=\"text-align: right;\"> 200.249</td><td style=\"text-align: right;\">           16.4726  </td><td style=\"text-align: right;\">          0.538171</td><td style=\"text-align: right;\">     16.4726  </td><td style=\"text-align: right;\"> 1689692761</td><td style=\"text-align: right;\">                  32</td><td>b6697601  </td></tr>\n",
       "<tr><td>FSR_Trainable_ba6078e3</td><td>2023-07-19_00-01-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">226.372</td><td style=\"text-align: right;\">4.89025e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">296763</td><td style=\"text-align: right;\"> 417.149</td><td style=\"text-align: right;\">            0.679633</td><td style=\"text-align: right;\">          0.679633</td><td style=\"text-align: right;\">      0.679633</td><td style=\"text-align: right;\"> 1689692495</td><td style=\"text-align: right;\">                   1</td><td>ba6078e3  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb61f7f2</td><td>2023-07-18_23-56-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">179.293</td><td style=\"text-align: right;\">2.12014e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">289137</td><td style=\"text-align: right;\"> 335.752</td><td style=\"text-align: right;\">            1.17868 </td><td style=\"text-align: right;\">          0.396678</td><td style=\"text-align: right;\">      1.17868 </td><td style=\"text-align: right;\"> 1689692213</td><td style=\"text-align: right;\">                   2</td><td>bb61f7f2  </td></tr>\n",
       "<tr><td>FSR_Trainable_beabc043</td><td>2023-07-19_00-08-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">141.717</td><td style=\"text-align: right;\">4.06783e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">307492</td><td style=\"text-align: right;\"> 260.53 </td><td style=\"text-align: right;\">            1.37504 </td><td style=\"text-align: right;\">          0.669152</td><td style=\"text-align: right;\">      1.37504 </td><td style=\"text-align: right;\"> 1689692932</td><td style=\"text-align: right;\">                   2</td><td>beabc043  </td></tr>\n",
       "<tr><td>FSR_Trainable_c552165f</td><td>2023-07-18_23-58-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">179.811</td><td style=\"text-align: right;\">4.43148e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">292208</td><td style=\"text-align: right;\"> 320.32 </td><td style=\"text-align: right;\">            1.07983 </td><td style=\"text-align: right;\">          0.367337</td><td style=\"text-align: right;\">      1.07983 </td><td style=\"text-align: right;\"> 1689692315</td><td style=\"text-align: right;\">                   2</td><td>c552165f  </td></tr>\n",
       "<tr><td>FSR_Trainable_c7a3720a</td><td>2023-07-19_00-05-12</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">111.465</td><td style=\"text-align: right;\">1.97948e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">301855</td><td style=\"text-align: right;\"> 210.626</td><td style=\"text-align: right;\">            2.16423 </td><td style=\"text-align: right;\">          0.457654</td><td style=\"text-align: right;\">      2.16423 </td><td style=\"text-align: right;\"> 1689692712</td><td style=\"text-align: right;\">                   4</td><td>c7a3720a  </td></tr>\n",
       "<tr><td>FSR_Trainable_c85d30dd</td><td>2023-07-19_00-06-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">180.274</td><td style=\"text-align: right;\">5.34961e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">303837</td><td style=\"text-align: right;\"> 315.336</td><td style=\"text-align: right;\">            0.727751</td><td style=\"text-align: right;\">          0.727751</td><td style=\"text-align: right;\">      0.727751</td><td style=\"text-align: right;\"> 1689692784</td><td style=\"text-align: right;\">                   1</td><td>c85d30dd  </td></tr>\n",
       "<tr><td>FSR_Trainable_cd729345</td><td>2023-07-19_00-05-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">103.48 </td><td style=\"text-align: right;\">1.75303e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">303061</td><td style=\"text-align: right;\"> 202.064</td><td style=\"text-align: right;\">            5.92715 </td><td style=\"text-align: right;\">          0.625785</td><td style=\"text-align: right;\">      5.92715 </td><td style=\"text-align: right;\"> 1689692758</td><td style=\"text-align: right;\">                   8</td><td>cd729345  </td></tr>\n",
       "<tr><td>FSR_Trainable_ceaee9a6</td><td>2023-07-19_00-06-43</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">175.641</td><td style=\"text-align: right;\">1.99088e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">304374</td><td style=\"text-align: right;\"> 330.234</td><td style=\"text-align: right;\">            0.651512</td><td style=\"text-align: right;\">          0.651512</td><td style=\"text-align: right;\">      0.651512</td><td style=\"text-align: right;\"> 1689692803</td><td style=\"text-align: right;\">                   1</td><td>ceaee9a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_d0af510d</td><td>2023-07-19_00-04-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">101.768</td><td style=\"text-align: right;\">1.7166e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">300221</td><td style=\"text-align: right;\"> 200.274</td><td style=\"text-align: right;\">           17.9018  </td><td style=\"text-align: right;\">          0.52767 </td><td style=\"text-align: right;\">     17.9018  </td><td style=\"text-align: right;\"> 1689692649</td><td style=\"text-align: right;\">                  32</td><td>d0af510d  </td></tr>\n",
       "<tr><td>FSR_Trainable_d31af333</td><td>2023-07-19_00-03-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">101.856</td><td style=\"text-align: right;\">1.66599e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">299086</td><td style=\"text-align: right;\"> 200.567</td><td style=\"text-align: right;\">           18.0488  </td><td style=\"text-align: right;\">          0.663246</td><td style=\"text-align: right;\">     18.0488  </td><td style=\"text-align: right;\"> 1689692597</td><td style=\"text-align: right;\">                  32</td><td>d31af333  </td></tr>\n",
       "<tr><td>FSR_Trainable_d7dab881</td><td>2023-07-19_00-01-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">102.792</td><td style=\"text-align: right;\">1.71787e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">296530</td><td style=\"text-align: right;\"> 201.096</td><td style=\"text-align: right;\">            9.93764 </td><td style=\"text-align: right;\">          0.554189</td><td style=\"text-align: right;\">      9.93764 </td><td style=\"text-align: right;\"> 1689692498</td><td style=\"text-align: right;\">                  16</td><td>d7dab881  </td></tr>\n",
       "<tr><td>FSR_Trainable_e298ee30</td><td>2023-07-19_00-08-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">106.492</td><td style=\"text-align: right;\">1.90094e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">306362</td><td style=\"text-align: right;\"> 204.622</td><td style=\"text-align: right;\">            2.73595 </td><td style=\"text-align: right;\">          0.685017</td><td style=\"text-align: right;\">      2.73595 </td><td style=\"text-align: right;\"> 1689692888</td><td style=\"text-align: right;\">                   4</td><td>e298ee30  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2efdd0c</td><td>2023-07-19_00-09-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">124.475</td><td style=\"text-align: right;\">2.69713e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">308041</td><td style=\"text-align: right;\"> 227.274</td><td style=\"text-align: right;\">            1.65524 </td><td style=\"text-align: right;\">          0.534511</td><td style=\"text-align: right;\">      1.65524 </td><td style=\"text-align: right;\"> 1689692954</td><td style=\"text-align: right;\">                   2</td><td>e2efdd0c  </td></tr>\n",
       "<tr><td>FSR_Trainable_e370ee82</td><td>2023-07-19_00-01-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">104.932</td><td style=\"text-align: right;\">1.84251e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">296996</td><td style=\"text-align: right;\"> 202.752</td><td style=\"text-align: right;\">            3.88754 </td><td style=\"text-align: right;\">          0.396391</td><td style=\"text-align: right;\">      3.88754 </td><td style=\"text-align: right;\"> 1689692510</td><td style=\"text-align: right;\">                   8</td><td>e370ee82  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec504580</td><td>2023-07-18_23-56-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.554</td><td style=\"text-align: right;\">1.6638e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">287505</td><td style=\"text-align: right;\"> 200.424</td><td style=\"text-align: right;\">           33.6234  </td><td style=\"text-align: right;\">          0.382939</td><td style=\"text-align: right;\">     33.6234  </td><td style=\"text-align: right;\"> 1689692199</td><td style=\"text-align: right;\">                 100</td><td>ec504580  </td></tr>\n",
       "<tr><td>FSR_Trainable_ec63db0d</td><td>2023-07-19_00-06-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\">101.59 </td><td style=\"text-align: right;\">1.68413e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">304143</td><td style=\"text-align: right;\"> 200.256</td><td style=\"text-align: right;\">           16.8203  </td><td style=\"text-align: right;\">          0.64003 </td><td style=\"text-align: right;\">     16.8203  </td><td style=\"text-align: right;\"> 1689692814</td><td style=\"text-align: right;\">                  32</td><td>ec63db0d  </td></tr>\n",
       "<tr><td>FSR_Trainable_ef8beec7</td><td>2023-07-19_00-07-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">102.169</td><td style=\"text-align: right;\">1.71144e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">305371</td><td style=\"text-align: right;\"> 200.556</td><td style=\"text-align: right;\">            8.26011 </td><td style=\"text-align: right;\">          0.6274  </td><td style=\"text-align: right;\">      8.26011 </td><td style=\"text-align: right;\"> 1689692855</td><td style=\"text-align: right;\">                  16</td><td>ef8beec7  </td></tr>\n",
       "<tr><td>FSR_Trainable_f1983e0e</td><td>2023-07-19_00-02-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">309.855</td><td style=\"text-align: right;\">3.85823e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">297564</td><td style=\"text-align: right;\"> 519.242</td><td style=\"text-align: right;\">            0.719981</td><td style=\"text-align: right;\">          0.719981</td><td style=\"text-align: right;\">      0.719981</td><td style=\"text-align: right;\"> 1689692526</td><td style=\"text-align: right;\">                   1</td><td>f1983e0e  </td></tr>\n",
       "<tr><td>FSR_Trainable_f3f541bf</td><td>2023-07-19_00-04-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">135.6  </td><td style=\"text-align: right;\">3.79023e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">300712</td><td style=\"text-align: right;\"> 252.436</td><td style=\"text-align: right;\">            1.39865 </td><td style=\"text-align: right;\">          0.606798</td><td style=\"text-align: right;\">      1.39865 </td><td style=\"text-align: right;\"> 1689692670</td><td style=\"text-align: right;\">                   2</td><td>f3f541bf  </td></tr>\n",
       "<tr><td>FSR_Trainable_f45e96fd</td><td>2023-07-19_00-07-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">107.674</td><td style=\"text-align: right;\">2.26103e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">305683</td><td style=\"text-align: right;\"> 207.913</td><td style=\"text-align: right;\">            2.59279 </td><td style=\"text-align: right;\">          0.604455</td><td style=\"text-align: right;\">      2.59279 </td><td style=\"text-align: right;\"> 1689692859</td><td style=\"text-align: right;\">                   4</td><td>f45e96fd  </td></tr>\n",
       "<tr><td>FSR_Trainable_f4e8045c</td><td>2023-07-19_00-02-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">120.564</td><td style=\"text-align: right;\">2.3239e+08 </td><td>172.26.215.93</td><td style=\"text-align: right;\">297726</td><td style=\"text-align: right;\"> 219.69 </td><td style=\"text-align: right;\">            2.77405 </td><td style=\"text-align: right;\">          0.568269</td><td style=\"text-align: right;\">      2.77405 </td><td style=\"text-align: right;\"> 1689692536</td><td style=\"text-align: right;\">                   4</td><td>f4e8045c  </td></tr>\n",
       "<tr><td>FSR_Trainable_f5a53c4b</td><td>2023-07-18_23-57-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">118.734</td><td style=\"text-align: right;\">1.23227e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">290184</td><td style=\"text-align: right;\"> 229.183</td><td style=\"text-align: right;\">            3.08704 </td><td style=\"text-align: right;\">          0.897583</td><td style=\"text-align: right;\">      3.08704 </td><td style=\"text-align: right;\"> 1689692248</td><td style=\"text-align: right;\">                   4</td><td>f5a53c4b  </td></tr>\n",
       "<tr><td>FSR_Trainable_f64e4ff5</td><td>2023-07-19_00-01-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\">101.633</td><td style=\"text-align: right;\">1.72756e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">295848</td><td style=\"text-align: right;\"> 200.158</td><td style=\"text-align: right;\">           47.459   </td><td style=\"text-align: right;\">          0.321187</td><td style=\"text-align: right;\">     47.459   </td><td style=\"text-align: right;\"> 1689692511</td><td style=\"text-align: right;\">                 100</td><td>f64e4ff5  </td></tr>\n",
       "<tr><td>FSR_Trainable_f7146274</td><td>2023-07-19_00-03-39</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\">109.629</td><td style=\"text-align: right;\">2.35303e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">299979</td><td style=\"text-align: right;\"> 209.399</td><td style=\"text-align: right;\">            2.64793 </td><td style=\"text-align: right;\">          0.694467</td><td style=\"text-align: right;\">      2.64793 </td><td style=\"text-align: right;\"> 1689692619</td><td style=\"text-align: right;\">                   4</td><td>f7146274  </td></tr>\n",
       "<tr><td>FSR_Trainable_fb672319</td><td>2023-07-19_00-09-29</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\">102.243</td><td style=\"text-align: right;\">1.66391e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">308210</td><td style=\"text-align: right;\"> 200.908</td><td style=\"text-align: right;\">            8.40648 </td><td style=\"text-align: right;\">          0.379474</td><td style=\"text-align: right;\">      8.40648 </td><td style=\"text-align: right;\"> 1689692969</td><td style=\"text-align: right;\">                  16</td><td>fb672319  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd8002dc</td><td>2023-07-19_00-06-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\">102.631</td><td style=\"text-align: right;\">1.53765e+08</td><td>172.26.215.93</td><td style=\"text-align: right;\">303277</td><td style=\"text-align: right;\"> 201.883</td><td style=\"text-align: right;\">            3.6357  </td><td style=\"text-align: right;\">          0.348359</td><td style=\"text-align: right;\">      3.6357  </td><td style=\"text-align: right;\"> 1689692766</td><td style=\"text-align: right;\">                   8</td><td>fd8002dc  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_ec504580_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-55-48/wandb/run-20230718_235558-ec504580\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: Syncing run FSR_Trainable_ec504580\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec504580\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_328be942_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-55-53/wandb/run-20230718_235604-328be942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Syncing run FSR_Trainable_328be942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/328be942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                      mae 190.25013\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                     mape 452793334.55402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                     rmse 356.09047\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:       time_since_restore 0.77507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:         time_this_iter_s 0.77507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:             time_total_s 0.77507\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:                timestamp 1689692160\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb:  View run FSR_Trainable_328be942 at: https://wandb.ai/seokjin/FSR-prediction/runs/328be942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287746)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235604-328be942/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...921)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_24444a45_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-55-59/wandb/run-20230718_235610-24444a45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Syncing run FSR_Trainable_24444a45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/24444a45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)02 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                      mae 235.11769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                     mape 1.5047152917582986e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                     rmse 472.05578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:       time_since_restore 0.71742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:         time_this_iter_s 0.71742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:             time_total_s 0.71742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:                timestamp 1689692166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb:  View run FSR_Trainable_24444a45 at: https://wandb.ai/seokjin/FSR-prediction/runs/24444a45\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287921)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235610-24444a45/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_0d137c87_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-56-05/wandb/run-20230718_235617-0d137c87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Syncing run FSR_Trainable_0d137c87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0d137c87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288102)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235617-0d137c87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_63871c2d_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-56-12/wandb/run-20230718_235625-63871c2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Syncing run FSR_Trainable_63871c2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/63871c2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                      mae 336.79125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                     mape 4.501713562566552e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                     rmse 553.32832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:       time_since_restore 0.60064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:         time_this_iter_s 0.60064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:             time_total_s 0.60064\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:                timestamp 1689692181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb:  View run FSR_Trainable_63871c2d at: https://wandb.ai/seokjin/FSR-prediction/runs/63871c2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288337)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235625-63871c2d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_6031f8ea_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-56-20/wandb/run-20230718_235633-6031f8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Syncing run FSR_Trainable_6031f8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6031f8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                      mae 608.59373\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                     mape 3.108097981222688e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                     rmse 1885.7517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:       time_since_restore 0.88622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:         time_this_iter_s 0.88622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:             time_total_s 0.88622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:                timestamp 1689692189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb:  View run FSR_Trainable_6031f8ea at: https://wandb.ai/seokjin/FSR-prediction/runs/6031f8ea\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288562)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235633-6031f8ea/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_110bbd1c_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-56-28/wandb/run-20230718_235641-110bbd1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Syncing run FSR_Trainable_110bbd1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/110bbd1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=287568)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:56:46,840\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.485 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:56:46,845\tWARNING util.py:315 -- The `process_trial_result` operation took 1.491 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:56:46,847\tWARNING util.py:315 -- Processing trial results took 1.493 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:56:46,849\tWARNING util.py:315 -- The `process_trial_result` operation took 1.494 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                      mae 473.16445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                     mape 9.22652484048633e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                     rmse 589.97919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:       time_since_restore 0.5412\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:         time_this_iter_s 0.5412\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:             time_total_s 0.5412\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:                timestamp 1689692197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb:  View run FSR_Trainable_110bbd1c at: https://wandb.ai/seokjin/FSR-prediction/runs/110bbd1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235641-110bbd1c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_b57d8118_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-56-36/wandb/run-20230718_235649-b57d8118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Syncing run FSR_Trainable_b57d8118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b57d8118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=288790)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:56:52,733\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.554 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:56:52,737\tWARNING util.py:315 -- The `process_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:56:52,738\tWARNING util.py:315 -- Processing trial results took 1.561 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:56:52,740\tWARNING util.py:315 -- The `process_trial_result` operation took 1.562 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                      mae 212.08939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                     mape 662232410.43266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                     rmse 364.94929\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:       time_since_restore 1.91307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:         time_this_iter_s 0.70663\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:             time_total_s 1.91307\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:                timestamp 1689692207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb:  View run FSR_Trainable_b57d8118 at: https://wandb.ai/seokjin/FSR-prediction/runs/b57d8118\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235649-b57d8118/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289016)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_bb61f7f2_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=f_2023-07-18_23-56-44/wandb/run-20230718_235654-bb61f7f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Syncing run FSR_Trainable_bb61f7f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb61f7f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:57:00,184\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:00,187\tWARNING util.py:315 -- The `process_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:00,189\tWARNING util.py:315 -- Processing trial results took 1.869 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:00,191\tWARNING util.py:315 -- The `process_trial_result` operation took 1.870 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                      mae 179.29291\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                     mape 2.1201372754967597e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                     rmse 335.75188\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:       time_since_restore 1.17868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:         time_this_iter_s 0.39668\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:             time_total_s 1.17868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:                timestamp 1689692213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb:  View run FSR_Trainable_bb61f7f2 at: https://wandb.ai/seokjin/FSR-prediction/runs/bb61f7f2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289240)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235654-bb61f7f2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_4bb595dd_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-56-50/wandb/run-20230718_235702-4bb595dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Syncing run FSR_Trainable_4bb595dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4bb595dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:57:05,966\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.694 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:05,971\tWARNING util.py:315 -- The `process_trial_result` operation took 1.699 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:05,972\tWARNING util.py:315 -- Processing trial results took 1.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:05,974\tWARNING util.py:315 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289466)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                      mae 353.44362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                     mape 4.8821831190696083e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                     rmse 547.2609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:       time_since_restore 0.80494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:         time_this_iter_s 0.80494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:             time_total_s 0.80494\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:                timestamp 1689692224\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb:  View run FSR_Trainable_541dbe91 at: https://wandb.ai/seokjin/FSR-prediction/runs/541dbe91\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289652)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235708-541dbe91/logs\n",
      "2023-07-18 23:57:13,327\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:13,333\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:13,334\tWARNING util.py:315 -- Processing trial results took 1.832 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:13,336\tWARNING util.py:315 -- The `process_trial_result` operation took 1.835 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_0871b7e5_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-57-03/wandb/run-20230718_235715-0871b7e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Syncing run FSR_Trainable_0871b7e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0871b7e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-18 23:57:18,484\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:18,487\tWARNING util.py:315 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:18,488\tWARNING util.py:315 -- Processing trial results took 1.365 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:18,489\tWARNING util.py:315 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                      mae 269.7648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                     mape 937366713.66117\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                     rmse 457.27636\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:       time_since_restore 0.7138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:         time_this_iter_s 0.7138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:             time_total_s 0.7138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:                timestamp 1689692231\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb:  View run FSR_Trainable_0871b7e5 at: https://wandb.ai/seokjin/FSR-prediction/runs/0871b7e5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=289873)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235715-0871b7e5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_43751b04_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-57-10/wandb/run-20230718_235720-43751b04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Syncing run FSR_Trainable_43751b04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/43751b04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-18 23:57:26,365\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.502 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:26,367\tWARNING util.py:315 -- The `process_trial_result` operation took 1.505 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:26,370\tWARNING util.py:315 -- Processing trial results took 1.507 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:26,373\tWARNING util.py:315 -- The `process_trial_result` operation took 1.511 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                      mae 116.44403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                     mape 194503033.69137\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                     rmse 218.96818\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:       time_since_restore 4.86114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:         time_this_iter_s 0.47455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:             time_total_s 4.86114\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:                timestamp 1689692242\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb:  View run FSR_Trainable_43751b04 at: https://wandb.ai/seokjin/FSR-prediction/runs/43751b04\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290036)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235720-43751b04/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:57:34,136\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.369 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb:       training_iteration \n",
      "2023-07-18 23:57:34,142\tWARNING util.py:315 -- The `process_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:34,144\tWARNING util.py:315 -- Processing trial results took 1.377 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:34,146\tWARNING util.py:315 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290263)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_02b54729_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-57-23/wandb/run-20230718_235736-02b54729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Syncing run FSR_Trainable_02b54729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/02b54729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:57:41,548\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.600 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:41,551\tWARNING util.py:315 -- The `process_trial_result` operation took 1.605 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:41,553\tWARNING util.py:315 -- Processing trial results took 1.606 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:41,554\tWARNING util.py:315 -- The `process_trial_result` operation took 1.608 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                      mae 133.44127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                     mape 260757206.72401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                     rmse 243.01576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:       time_since_restore 2.95993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:         time_this_iter_s 0.54501\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:             time_total_s 2.95993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:                timestamp 1689692256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb:  View run FSR_Trainable_02b54729 at: https://wandb.ai/seokjin/FSR-prediction/runs/02b54729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235736-02b54729/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290505)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_12c423a0_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-57-31/wandb/run-20230718_235743-12c423a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Syncing run FSR_Trainable_12c423a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/12c423a0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)02 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:57:48,555\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:48,559\tWARNING util.py:315 -- The `process_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:48,563\tWARNING util.py:315 -- Processing trial results took 1.811 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:48,567\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235743-12c423a0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290725)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_2bb087cb_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-57-39/wandb/run-20230718_235750-2bb087cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Syncing run FSR_Trainable_2bb087cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2bb087cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-18 23:57:54,062\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.518 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:54,065\tWARNING util.py:315 -- The `process_trial_result` operation took 1.521 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:57:54,067\tWARNING util.py:315 -- Processing trial results took 1.523 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:57:54,068\tWARNING util.py:315 -- The `process_trial_result` operation took 1.525 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                      mae 305.72509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                     mape 871271302.41441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                     rmse 545.02571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:       time_since_restore 0.72657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:         time_this_iter_s 0.72657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:             time_total_s 0.72657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:                timestamp 1689692266\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb:  View run FSR_Trainable_2bb087cb at: https://wandb.ai/seokjin/FSR-prediction/runs/2bb087cb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=290946)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235750-2bb087cb/logs\n",
      "2023-07-18 23:58:02,195\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:02,198\tWARNING util.py:315 -- The `process_trial_result` operation took 1.890 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:02,200\tWARNING util.py:315 -- Processing trial results took 1.891 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:02,201\tWARNING util.py:315 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_65bccec6_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-57-51/wandb/run-20230718_235804-65bccec6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: Syncing run FSR_Trainable_65bccec6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/65bccec6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                      mae 101.75589\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                     mape 162207233.0817\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                     rmse 200.9469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:       time_since_restore 8.21014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:         time_this_iter_s 0.58385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:             time_total_s 8.21014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:                timestamp 1689692283\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb:  View run FSR_Trainable_01aba93e at: https://wandb.ai/seokjin/FSR-prediction/runs/01aba93e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291108)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235756-01aba93e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:58:09,461\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.489 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:09,466\tWARNING util.py:315 -- The `process_trial_result` operation took 1.495 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:09,468\tWARNING util.py:315 -- Processing trial results took 1.497 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:09,470\tWARNING util.py:315 -- The `process_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: / 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235804-65bccec6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235804-65bccec6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_828467e0_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-57-59/wandb/run-20230718_235811-828467e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Syncing run FSR_Trainable_828467e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/828467e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:58:16,659\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:16,662\tWARNING util.py:315 -- The `process_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:16,663\tWARNING util.py:315 -- Processing trial results took 1.688 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:16,665\tWARNING util.py:315 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                      mae 175.67336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                     mape 2.770932320967725e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                     rmse 373.86867\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:       time_since_restore 1.41421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:         time_this_iter_s 0.47843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:             time_total_s 1.41421\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:                timestamp 1689692289\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb:  View run FSR_Trainable_828467e0 at: https://wandb.ai/seokjin/FSR-prediction/runs/828467e0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235811-828467e0/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291570)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_8a745a2d_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-58-07/wandb/run-20230718_235819-8a745a2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Syncing run FSR_Trainable_8a745a2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8a745a2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-18 23:58:22,352\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.472 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:22,355\tWARNING util.py:315 -- The `process_trial_result` operation took 1.476 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:22,362\tWARNING util.py:315 -- Processing trial results took 1.483 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:22,363\tWARNING util.py:315 -- The `process_trial_result` operation took 1.484 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                      mae 122.43215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                     mape 145458580.24455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                     rmse 237.25433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:       time_since_restore 2.48743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:         time_this_iter_s 0.56596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:             time_total_s 2.48743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:                timestamp 1689692298\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb:  View run FSR_Trainable_8a745a2d at: https://wandb.ai/seokjin/FSR-prediction/runs/8a745a2d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235819-8a745a2d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291797)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_8ecd4b10_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-58-14/wandb/run-20230718_235824-8ecd4b10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Syncing run FSR_Trainable_8ecd4b10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8ecd4b10\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:58:30,194\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:30,200\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:30,203\tWARNING util.py:315 -- Processing trial results took 1.925 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:30,205\tWARNING util.py:315 -- The `process_trial_result` operation took 1.928 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235824-8ecd4b10/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=291961)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_82615fae_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-58-19/wandb/run-20230718_235832-82615fae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Syncing run FSR_Trainable_82615fae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/82615fae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:58:35,508\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:35,510\tWARNING util.py:315 -- The `process_trial_result` operation took 1.654 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:35,511\tWARNING util.py:315 -- Processing trial results took 1.655 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:35,512\tWARNING util.py:315 -- The `process_trial_result` operation took 1.656 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: / 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                      mae 151.61511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                     mape 445666157.73525\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                     rmse 278.66359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:       time_since_restore 1.14338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:         time_this_iter_s 0.39391\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:             time_total_s 1.14338\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:                timestamp 1689692310\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb:  View run FSR_Trainable_82615fae at: https://wandb.ai/seokjin/FSR-prediction/runs/82615fae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292207)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235832-82615fae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:58:42,805\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.726 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:42,809\tWARNING util.py:315 -- The `process_trial_result` operation took 1.730 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:42,810\tWARNING util.py:315 -- Processing trial results took 1.732 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:42,812\tWARNING util.py:315 -- The `process_trial_result` operation took 1.733 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292392)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_144e715c_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-58-33/wandb/run-20230718_235845-144e715c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Syncing run FSR_Trainable_144e715c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/144e715c\n",
      "2023-07-18 23:58:48,737\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.759 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:48,741\tWARNING util.py:315 -- The `process_trial_result` operation took 1.764 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:48,744\tWARNING util.py:315 -- Processing trial results took 1.767 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:48,746\tWARNING util.py:315 -- The `process_trial_result` operation took 1.769 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_b4f16fb7_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-58-40/wandb/run-20230718_235851-b4f16fb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: Syncing run FSR_Trainable_b4f16fb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b4f16fb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-18 23:58:56,855\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.645 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:56,858\tWARNING util.py:315 -- The `process_trial_result` operation took 1.649 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:58:56,862\tWARNING util.py:315 -- Processing trial results took 1.652 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:58:56,864\tWARNING util.py:315 -- The `process_trial_result` operation took 1.655 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                      mae 113.99466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                     mape 241984950.35687\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                     rmse 212.02903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:       time_since_restore 8.72322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:         time_this_iter_s 0.47466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:             time_total_s 8.72322\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:                timestamp 1689692332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb:  View run FSR_Trainable_144e715c at: https://wandb.ai/seokjin/FSR-prediction/runs/144e715c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235845-144e715c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235851-b4f16fb7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_adbc3239_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-58-46/wandb/run-20230718_235859-adbc3239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Syncing run FSR_Trainable_adbc3239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/adbc3239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292613)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=292795)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                      mae 254.28698\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                     mape 831691621.3081\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                     rmse 423.98296\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:       time_since_restore 1.2073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:         time_this_iter_s 1.2073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:             time_total_s 1.2073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:                timestamp 1689692335\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb:  View run FSR_Trainable_adbc3239 at: https://wandb.ai/seokjin/FSR-prediction/runs/adbc3239\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235859-adbc3239/logs\n",
      "2023-07-18 23:59:04,828\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.805 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:04,830\tWARNING util.py:315 -- The `process_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:04,835\tWARNING util.py:315 -- Processing trial results took 1.813 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:59:04,837\tWARNING util.py:315 -- The `process_trial_result` operation took 1.815 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_04529693_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-58-54/wandb/run-20230718_235907-04529693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Syncing run FSR_Trainable_04529693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/04529693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293015)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-18 23:59:11,019\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:11,022\tWARNING util.py:315 -- The `process_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:11,024\tWARNING util.py:315 -- Processing trial results took 1.959 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:59:11,025\tWARNING util.py:315 -- The `process_trial_result` operation took 1.961 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_84f26917_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-59-02/wandb/run-20230718_235913-84f26917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Syncing run FSR_Trainable_84f26917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/84f26917\n",
      "2023-07-18 23:59:20,762\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.017 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:20,765\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:20,768\tWARNING util.py:315 -- Processing trial results took 2.023 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:59:20,770\tWARNING util.py:315 -- The `process_trial_result` operation took 2.026 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_2df648ca_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-59-08/wandb/run-20230718_235923-2df648ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Syncing run FSR_Trainable_2df648ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2df648ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                      mae 242.75033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                     mape 896200733.02978\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                     rmse 409.39758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:       time_since_restore 0.70509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:         time_this_iter_s 0.70509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:             time_total_s 0.70509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:                timestamp 1689692358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb:  View run FSR_Trainable_2df648ca at: https://wandb.ai/seokjin/FSR-prediction/runs/2df648ca\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293639)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235923-2df648ca/logs\n",
      "2023-07-18 23:59:31,032\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:31,035\tWARNING util.py:315 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:31,037\tWARNING util.py:315 -- Processing trial results took 1.942 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:59:31,038\tWARNING util.py:315 -- The `process_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_6fd39a8a_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-59-18/wandb/run-20230718_235933-6fd39a8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Syncing run FSR_Trainable_6fd39a8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6fd39a8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                      mae 241.72576\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                     mape 793408813.70584\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                     rmse 416.43793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:       time_since_restore 0.72777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:         time_this_iter_s 0.72777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:             time_total_s 0.72777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:                timestamp 1689692369\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb:  View run FSR_Trainable_6fd39a8a at: https://wandb.ai/seokjin/FSR-prediction/runs/6fd39a8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293861)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235933-6fd39a8a/logs\n",
      "2023-07-18 23:59:41,159\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:41,162\tWARNING util.py:315 -- The `process_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:41,164\tWARNING util.py:315 -- Processing trial results took 1.965 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:59:41,165\tWARNING util.py:315 -- The `process_trial_result` operation took 1.966 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_666d083d_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-59-28/wandb/run-20230718_235944-666d083d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Syncing run FSR_Trainable_666d083d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/666d083d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                      mae 176.95983\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                     mape 2.691878491015309e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                     rmse 359.1315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:       time_since_restore 1.45756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:         time_this_iter_s 0.6246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:             time_total_s 1.45756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:                timestamp 1689692381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb:  View run FSR_Trainable_666d083d at: https://wandb.ai/seokjin/FSR-prediction/runs/666d083d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294088)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235944-666d083d/logs\n",
      "2023-07-18 23:59:51,222\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.612 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:51,227\tWARNING util.py:315 -- The `process_trial_result` operation took 1.618 s, which may be a performance bottleneck.\n",
      "2023-07-18 23:59:51,229\tWARNING util.py:315 -- Processing trial results took 1.620 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-18 23:59:51,235\tWARNING util.py:315 -- The `process_trial_result` operation took 1.625 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_6fadd259_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-59-38/wandb/run-20230718_235954-6fadd259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Syncing run FSR_Trainable_6fadd259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/6fadd259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                      mae 225.93202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                     mape 3.37005400918202e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                     rmse 447.89993\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:       time_since_restore 1.20122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:         time_this_iter_s 1.20122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:             time_total_s 1.20122\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:                timestamp 1689692389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb:  View run FSR_Trainable_6fadd259 at: https://wandb.ai/seokjin/FSR-prediction/runs/6fadd259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294315)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235954-6fadd259/logs\n",
      "2023-07-19 00:00:01,988\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.944 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:01,990\tWARNING util.py:315 -- The `process_trial_result` operation took 1.947 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:01,991\tWARNING util.py:315 -- Processing trial results took 1.948 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:00:01,993\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_44c1311c_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-59-48/wandb/run-20230719_000005-44c1311c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Syncing run FSR_Trainable_44c1311c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/44c1311c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                      mae 226.41286\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                     mape 707453140.02791\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                     rmse 448.47089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:       time_since_restore 2.84971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:         time_this_iter_s 1.24879\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:             time_total_s 2.84971\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:                timestamp 1689692403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb:  View run FSR_Trainable_44c1311c at: https://wandb.ai/seokjin/FSR-prediction/runs/44c1311c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294543)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000005-44c1311c/logs\n",
      "2023-07-19 00:00:12,505\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:12,508\tWARNING util.py:315 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:12,509\tWARNING util.py:315 -- Processing trial results took 1.908 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:00:12,511\tWARNING util.py:315 -- The `process_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_92cb4582_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-18_23-59-58/wandb/run-20230719_000017-92cb4582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: Syncing run FSR_Trainable_92cb4582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/92cb4582\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:00:21,934\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.525 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:21,936\tWARNING util.py:315 -- The `process_trial_result` operation took 1.528 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:21,938\tWARNING util.py:315 -- Processing trial results took 1.529 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:00:21,939\tWARNING util.py:315 -- The `process_trial_result` operation took 1.530 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                      mae 101.77381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                     mape 173588330.60618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                     rmse 200.15868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:       time_since_restore 60.91872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:         time_this_iter_s 0.49772\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:             time_total_s 60.91872\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:                timestamp 1689692416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb:  View run FSR_Trainable_04529693 at: https://wandb.ai/seokjin/FSR-prediction/runs/04529693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293244)\u001b[0m wandb: Find logs at: ./wandb/run-20230718_235907-04529693/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_8c75fd35_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-00-09/wandb/run-20230719_000024-8c75fd35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Syncing run FSR_Trainable_8c75fd35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8c75fd35\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=294772)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-07-19 00:00:30,047\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:30,049\tWARNING util.py:315 -- The `process_trial_result` operation took 1.873 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:30,051\tWARNING util.py:315 -- Processing trial results took 1.875 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:00:30,056\tWARNING util.py:315 -- The `process_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: \u001b[32m [repeated 9x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Run history:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000024-8c75fd35/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000024-8c75fd35/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000024-8c75fd35/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=293424)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000024-8c75fd35/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Run summary:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: iterations_since_restore 8\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                      mae 114.06744\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                     mape 207654273.56328\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                     rmse 213.85541\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:       time_since_restore 4.89606\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:         time_this_iter_s 0.53796\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:             time_total_s 4.89606\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:                timestamp 1689692425\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:       training_iteration 8\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb:  View run FSR_Trainable_8c75fd35 at: https://wandb.ai/seokjin/FSR-prediction/runs/8c75fd35\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295078)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000024-8c75fd35/logs\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_9cdb9f41_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-00-19/wandb/run-20230719_000032-9cdb9f41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Syncing run FSR_Trainable_9cdb9f41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9cdb9f41\n",
      "2023-07-19 00:00:38,542\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:38,545\tWARNING util.py:315 -- The `process_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:38,547\tWARNING util.py:315 -- Processing trial results took 2.065 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:00:38,549\tWARNING util.py:315 -- The `process_trial_result` operation took 2.067 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_9d7ec6fa_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-00-27/wandb/run-20230719_000041-9d7ec6fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Syncing run FSR_Trainable_9d7ec6fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9d7ec6fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:00:44,912\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.876 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:44,914\tWARNING util.py:315 -- The `process_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:44,916\tWARNING util.py:315 -- Processing trial results took 1.880 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:00:44,917\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                      mae 142.45278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                     mape 1.3738134723185278e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                     rmse 268.57003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:       time_since_restore 1.16672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:         time_this_iter_s 0.51475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:             time_total_s 1.16672\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:                timestamp 1689692439\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb:  View run FSR_Trainable_9d7ec6fa at: https://wandb.ai/seokjin/FSR-prediction/runs/9d7ec6fa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000041-9d7ec6fa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_af86315e_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-00-35/wandb/run-20230719_000047-af86315e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Syncing run FSR_Trainable_af86315e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/af86315e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295715)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000047-af86315e/logs\n",
      "2023-07-19 00:00:53,469\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.880 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:53,472\tWARNING util.py:315 -- The `process_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:00:53,473\tWARNING util.py:315 -- Processing trial results took 1.885 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:00:53,477\tWARNING util.py:315 -- The `process_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_f64e4ff5_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-00-42/wandb/run-20230719_000056-f64e4ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Syncing run FSR_Trainable_f64e4ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f64e4ff5\n",
      "2023-07-19 00:01:03,240\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:03,242\tWARNING util.py:315 -- The `process_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:03,243\tWARNING util.py:315 -- Processing trial results took 1.871 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:01:03,245\tWARNING util.py:315 -- The `process_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_4513d7ae_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-00-51/wandb/run-20230719_000106-4513d7ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Syncing run FSR_Trainable_4513d7ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4513d7ae\n",
      "2023-07-19 00:01:14,240\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:14,244\tWARNING util.py:315 -- The `process_trial_result` operation took 1.915 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:14,246\tWARNING util.py:315 -- Processing trial results took 1.917 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:01:14,247\tWARNING util.py:315 -- The `process_trial_result` operation took 1.918 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: | 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_32031236_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-01-00/wandb/run-20230719_000117-32031236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Syncing run FSR_Trainable_32031236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/32031236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                      mae 103.05423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                     mape 173892620.06953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                     rmse 201.21168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:       time_since_restore 10.08642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:         time_this_iter_s 0.53428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:             time_total_s 10.08642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:                timestamp 1689692472\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb:  View run FSR_Trainable_4513d7ae at: https://wandb.ai/seokjin/FSR-prediction/runs/4513d7ae\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296168)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000106-4513d7ae/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:01:27,184\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.176 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:27,188\tWARNING util.py:315 -- The `process_trial_result` operation took 2.181 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:27,189\tWARNING util.py:315 -- Processing trial results took 2.182 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:01:27,191\tWARNING util.py:315 -- The `process_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                      mae 102.73073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                     mape 169676641.07634\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                     rmse 201.14417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:       time_since_restore 10.04443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:         time_this_iter_s 0.54267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:             time_total_s 10.04443\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:                timestamp 1689692483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb:  View run FSR_Trainable_32031236 at: https://wandb.ai/seokjin/FSR-prediction/runs/32031236\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296386)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000117-32031236/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_d7dab881_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-01-11/wandb/run-20230719_000130-d7dab881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Syncing run FSR_Trainable_d7dab881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d7dab881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                      mae 101.58197\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                     mape 172562238.59914\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                     rmse 200.1416\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:       time_since_restore 48.1886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:         time_this_iter_s 0.51167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:             time_total_s 48.1886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:                timestamp 1689692490\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb:  View run FSR_Trainable_9cdb9f41 at: https://wandb.ai/seokjin/FSR-prediction/runs/9cdb9f41\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295321)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000032-9cdb9f41/logs\n",
      "2023-07-19 00:01:37,762\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:37,766\tWARNING util.py:315 -- The `process_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:37,767\tWARNING util.py:315 -- Processing trial results took 1.872 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:01:37,769\tWARNING util.py:315 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_ba6078e3_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-01-24/wandb/run-20230719_000140-ba6078e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Syncing run FSR_Trainable_ba6078e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ba6078e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                      mae 102.79243\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                     mape 171786796.12471\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                     rmse 201.09602\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:       time_since_restore 9.93764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:         time_this_iter_s 0.55419\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:             time_total_s 9.93764\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:                timestamp 1689692498\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb:  View run FSR_Trainable_d7dab881 at: https://wandb.ai/seokjin/FSR-prediction/runs/d7dab881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296629)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000130-d7dab881/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:01:47,217\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:47,218\tWARNING util.py:315 -- The `process_trial_result` operation took 1.825 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:47,220\tWARNING util.py:315 -- Processing trial results took 1.827 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:01:47,223\tWARNING util.py:315 -- The `process_trial_result` operation took 1.830 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000140-ba6078e3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=296868)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_e370ee82_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-01-35/wandb/run-20230719_000150-e370ee82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: Syncing run FSR_Trainable_e370ee82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e370ee82\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: \\ 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                      mae 101.6334\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                     mape 172756329.75821\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                     rmse 200.15809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:       time_since_restore 47.45902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:         time_this_iter_s 0.32119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:             time_total_s 47.45902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:                timestamp 1689692511\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb:  View run FSR_Trainable_f64e4ff5 at: https://wandb.ai/seokjin/FSR-prediction/runs/f64e4ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000056-f64e4ff5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=295944)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 00:01:55,756\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.677 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:55,759\tWARNING util.py:315 -- The `process_trial_result` operation took 1.681 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:01:55,760\tWARNING util.py:315 -- Processing trial results took 1.682 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:01:55,762\tWARNING util.py:315 -- The `process_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_5e018605_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-01-44/wandb/run-20230719_000157-5e018605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Syncing run FSR_Trainable_5e018605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5e018605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297103)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                      mae 131.39238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                     mape 346600869.08783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                     rmse 237.81386\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:       time_since_restore 2.13538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:         time_this_iter_s 0.39043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:             time_total_s 2.13538\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:                timestamp 1689692517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb:  View run FSR_Trainable_5e018605 at: https://wandb.ai/seokjin/FSR-prediction/runs/5e018605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000157-5e018605/logs\n",
      "2023-07-19 00:02:03,176\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.820 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:03,181\tWARNING util.py:315 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:03,184\tWARNING util.py:315 -- Processing trial results took 1.829 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:03,186\tWARNING util.py:315 -- The `process_trial_result` operation took 1.831 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_9cc5c5f1_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-01-53/wandb/run-20230719_000205-9cc5c5f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Syncing run FSR_Trainable_9cc5c5f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9cc5c5f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297336)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:02:08,947\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:08,952\tWARNING util.py:315 -- The `process_trial_result` operation took 2.165 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:08,954\tWARNING util.py:315 -- Processing trial results took 2.167 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:08,956\tWARNING util.py:315 -- The `process_trial_result` operation took 2.169 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                      mae 193.75112\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                     mape 2.6547624012631776e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                     rmse 353.23953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:       time_since_restore 0.70105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:         time_this_iter_s 0.70105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:             time_total_s 0.70105\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:                timestamp 1689692521\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb:  View run FSR_Trainable_9cc5c5f1 at: https://wandb.ai/seokjin/FSR-prediction/runs/9cc5c5f1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000205-9cc5c5f1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297563)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:02:14,420\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.457 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:14,422\tWARNING util.py:315 -- The `process_trial_result` operation took 1.460 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:14,423\tWARNING util.py:315 -- Processing trial results took 1.461 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:14,425\tWARNING util.py:315 -- The `process_trial_result` operation took 1.462 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297725)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_f4e8045c_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-02-06/wandb/run-20230719_000216-f4e8045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Syncing run FSR_Trainable_f4e8045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f4e8045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:02:21,892\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.580 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:21,895\tWARNING util.py:315 -- The `process_trial_result` operation took 1.583 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:21,903\tWARNING util.py:315 -- Processing trial results took 1.591 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:21,905\tWARNING util.py:315 -- The `process_trial_result` operation took 1.593 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                      mae 120.56353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                     mape 232390306.83523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                     rmse 219.69046\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:       time_since_restore 2.77405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:         time_this_iter_s 0.56827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:             time_total_s 2.77405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:                timestamp 1689692536\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb:  View run FSR_Trainable_f4e8045c at: https://wandb.ai/seokjin/FSR-prediction/runs/f4e8045c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000216-f4e8045c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=297926)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...149)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_5fe9bffa_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-02-11/wandb/run-20230719_000224-5fe9bffa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Syncing run FSR_Trainable_5fe9bffa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/5fe9bffa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:02:29,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.015 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:29,240\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:29,243\tWARNING util.py:315 -- Processing trial results took 2.024 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:29,244\tWARNING util.py:315 -- The `process_trial_result` operation took 2.026 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                      mae 258.47881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                     mape 819420960.61092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                     rmse 438.79253\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:       time_since_restore 0.7651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:         time_this_iter_s 0.7651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:             time_total_s 0.7651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:                timestamp 1689692540\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb:  View run FSR_Trainable_5fe9bffa at: https://wandb.ai/seokjin/FSR-prediction/runs/5fe9bffa\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298149)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000224-5fe9bffa/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_58513c8a_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-02-19/wandb/run-20230719_000231-58513c8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Syncing run FSR_Trainable_58513c8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/58513c8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:02:34,698\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.713 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:34,702\tWARNING util.py:315 -- The `process_trial_result` operation took 1.717 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:34,703\tWARNING util.py:315 -- Processing trial results took 1.719 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:34,705\tWARNING util.py:315 -- The `process_trial_result` operation took 1.720 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                      mae 232.14654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                     mape 2.5176118888293096e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                     rmse 473.36988\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:       time_since_restore 0.64065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:         time_this_iter_s 0.64065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:             time_total_s 0.64065\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:                timestamp 1689692547\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb:  View run FSR_Trainable_58513c8a at: https://wandb.ai/seokjin/FSR-prediction/runs/58513c8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000231-58513c8a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:02:41,993\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.664 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:41,998\tWARNING util.py:315 -- The `process_trial_result` operation took 1.669 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:41,999\tWARNING util.py:315 -- Processing trial results took 1.671 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:42,001\tWARNING util.py:315 -- The `process_trial_result` operation took 1.672 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298539)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000236-0c569334/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_aa6ae5fb_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-02-32/wandb/run-20230719_000244-aa6ae5fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Syncing run FSR_Trainable_aa6ae5fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/aa6ae5fb\n",
      "2023-07-19 00:02:48,126\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.765 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:48,128\tWARNING util.py:315 -- The `process_trial_result` operation took 1.768 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:48,130\tWARNING util.py:315 -- Processing trial results took 1.770 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:48,131\tWARNING util.py:315 -- The `process_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_2cb2478d_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-02-39/wandb/run-20230719_000251-2cb2478d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Syncing run FSR_Trainable_2cb2478d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/2cb2478d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                      mae 102.17279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                     mape 162883472.73033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                     rmse 201.15106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:       time_since_restore 9.37073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:         time_this_iter_s 0.53738\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:             time_total_s 9.37073\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:                timestamp 1689692571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb:  View run FSR_Trainable_aa6ae5fb at: https://wandb.ai/seokjin/FSR-prediction/runs/aa6ae5fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298773)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000244-aa6ae5fb/logs\n",
      "2023-07-19 00:02:57,035\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.545 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:57,037\tWARNING util.py:315 -- The `process_trial_result` operation took 1.547 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:02:57,039\tWARNING util.py:315 -- Processing trial results took 1.549 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:02:57,039\tWARNING util.py:315 -- The `process_trial_result` operation took 1.549 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000251-2cb2478d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_d31af333_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-02-45/wandb/run-20230719_000259-d31af333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Syncing run FSR_Trainable_d31af333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d31af333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d31af333\n",
      "2023-07-19 00:03:05,337\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:05,340\tWARNING util.py:315 -- The `process_trial_result` operation took 1.927 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:05,341\tWARNING util.py:315 -- Processing trial results took 1.927 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:03:05,343\tWARNING util.py:315 -- The `process_trial_result` operation took 1.929 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=298952)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_4bdaf033_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-02-54/wandb/run-20230719_000308-4bdaf033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Syncing run FSR_Trainable_4bdaf033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/4bdaf033\n",
      "2023-07-19 00:03:14,936\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:14,937\tWARNING util.py:315 -- The `process_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:14,939\tWARNING util.py:315 -- Processing trial results took 1.893 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:03:14,940\tWARNING util.py:315 -- The `process_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_67d0c2ed_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-03-02/wandb/run-20230719_000317-67d0c2ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Syncing run FSR_Trainable_67d0c2ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/67d0c2ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                      mae 101.85588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                     mape 166598896.80217\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                     rmse 200.56749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:       time_since_restore 18.0488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:         time_this_iter_s 0.66325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:             time_total_s 18.0488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:                timestamp 1689692597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb:  View run FSR_Trainable_d31af333 at: https://wandb.ai/seokjin/FSR-prediction/runs/d31af333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299178)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000259-d31af333/logs\n",
      "2023-07-19 00:03:25,243\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.958 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:25,247\tWARNING util.py:315 -- The `process_trial_result` operation took 1.962 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:25,248\tWARNING util.py:315 -- Processing trial results took 1.963 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:03:25,249\tWARNING util.py:315 -- The `process_trial_result` operation took 1.965 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_352d58df_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-03-12/wandb/run-20230719_000328-352d58df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Syncing run FSR_Trainable_352d58df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/352d58df\n",
      "2023-07-19 00:03:37,197\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:37,201\tWARNING util.py:315 -- The `process_trial_result` operation took 2.158 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:37,202\tWARNING util.py:315 -- Processing trial results took 2.159 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:03:37,203\tWARNING util.py:315 -- The `process_trial_result` operation took 2.160 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_f7146274_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-03-22/wandb/run-20230719_000340-f7146274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Syncing run FSR_Trainable_f7146274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f7146274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                      mae 109.62928\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                     mape 235303481.79516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                     rmse 209.39922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:       time_since_restore 2.64793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:         time_this_iter_s 0.69447\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:             time_total_s 2.64793\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:                timestamp 1689692619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb:  View run FSR_Trainable_f7146274 at: https://wandb.ai/seokjin/FSR-prediction/runs/f7146274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300068)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000340-f7146274/logs\n",
      "2023-07-19 00:03:51,966\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.889 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:51,969\tWARNING util.py:315 -- The `process_trial_result` operation took 1.893 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:03:51,970\tWARNING util.py:315 -- Processing trial results took 1.893 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:03:51,971\tWARNING util.py:315 -- The `process_trial_result` operation took 1.894 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_d0af510d_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-03-34/wandb/run-20230719_000354-d0af510d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Syncing run FSR_Trainable_d0af510d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/d0af510d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                      mae 101.61415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                     mape 172578452.31023\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                     rmse 200.12851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:       time_since_restore 53.83851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:         time_this_iter_s 0.46086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:             time_total_s 53.83851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:                timestamp 1689692648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb:  View run FSR_Trainable_4bdaf033 at: https://wandb.ai/seokjin/FSR-prediction/runs/4bdaf033\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299402)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000308-4bdaf033/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000354-d0af510d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300305)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:04:19,691\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:19,694\tWARNING util.py:315 -- The `process_trial_result` operation took 1.864 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:19,694\tWARNING util.py:315 -- Processing trial results took 1.865 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:04:19,695\tWARNING util.py:315 -- The `process_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                      mae 101.5794\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                     mape 171962744.88758\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                     rmse 200.1349\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:       time_since_restore 54.68637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:         time_this_iter_s 0.39859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:             time_total_s 54.68637\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:                timestamp 1689692656\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb:  View run FSR_Trainable_67d0c2ed at: https://wandb.ai/seokjin/FSR-prediction/runs/67d0c2ed\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000317-67d0c2ed/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_41c00723_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-03-49/wandb/run-20230719_000422-41c00723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Syncing run FSR_Trainable_41c00723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/41c00723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299619)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:04:29,477\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.933 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:29,479\tWARNING util.py:315 -- The `process_trial_result` operation took 1.936 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:29,482\tWARNING util.py:315 -- Processing trial results took 1.939 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:04:29,483\tWARNING util.py:315 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                      mae 101.59886\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                     mape 172604026.73461\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                     rmse 200.12731\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:       time_since_restore 54.46534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:         time_this_iter_s 0.46319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:             time_total_s 54.46534\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:                timestamp 1689692666\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb:  View run FSR_Trainable_352d58df at: https://wandb.ai/seokjin/FSR-prediction/runs/352d58df\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000328-352d58df/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=299844)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_f3f541bf_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-04-17/wandb/run-20230719_000431-f3f541bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Syncing run FSR_Trainable_f3f541bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f3f541bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:04:38,068\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:38,072\tWARNING util.py:315 -- The `process_trial_result` operation took 1.809 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:38,073\tWARNING util.py:315 -- Processing trial results took 1.810 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:04:38,074\tWARNING util.py:315 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                      mae 135.59987\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                     mape 379022852.75069\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                     rmse 252.43596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:       time_since_restore 1.39865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:         time_this_iter_s 0.6068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:             time_total_s 1.39865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:                timestamp 1689692670\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb:  View run FSR_Trainable_f3f541bf at: https://wandb.ai/seokjin/FSR-prediction/runs/f3f541bf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000431-f3f541bf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300803)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_12b28168_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-04-26/wandb/run-20230719_000440-12b28168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Syncing run FSR_Trainable_12b28168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/12b28168\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                      mae 101.8653\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                     mape 172397300.08238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                     rmse 200.3092\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:       time_since_restore 16.46755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:         time_this_iter_s 0.47662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:             time_total_s 16.46755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:                timestamp 1689692679\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb:  View run FSR_Trainable_41c00723 at: https://wandb.ai/seokjin/FSR-prediction/runs/41c00723\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=300578)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000422-41c00723/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000422-41c00723/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:04:45,973\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.708 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:45,977\tWARNING util.py:315 -- The `process_trial_result` operation took 1.712 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:45,978\tWARNING util.py:315 -- Processing trial results took 1.713 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:04:45,979\tWARNING util.py:315 -- The `process_trial_result` operation took 1.714 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_1fd52bd1_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-04-35/wandb/run-20230719_000448-1fd52bd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Syncing run FSR_Trainable_1fd52bd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/1fd52bd1\n",
      "2023-07-19 00:04:54,507\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.803 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:54,509\tWARNING util.py:315 -- The `process_trial_result` operation took 1.806 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:04:54,511\tWARNING util.py:315 -- Processing trial results took 1.808 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:04:54,513\tWARNING util.py:315 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301037)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_0ca08eb1_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-04-43/wandb/run-20230719_000456-0ca08eb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Syncing run FSR_Trainable_0ca08eb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0ca08eb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                      mae 102.33742\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                     mape 166615214.12207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                     rmse 201.01282\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:       time_since_restore 7.82982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:         time_this_iter_s 0.58897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:             time_total_s 7.82982\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:                timestamp 1689692693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb:  View run FSR_Trainable_1fd52bd1 at: https://wandb.ai/seokjin/FSR-prediction/runs/1fd52bd1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000448-1fd52bd1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: - 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301265)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000456-0ca08eb1/logs\n",
      "2023-07-19 00:05:02,496\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.478 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:02,499\tWARNING util.py:315 -- The `process_trial_result` operation took 1.481 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:02,500\tWARNING util.py:315 -- Processing trial results took 1.482 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:02,502\tWARNING util.py:315 -- The `process_trial_result` operation took 1.485 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_18630162_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-04-51/wandb/run-20230719_000505-18630162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Syncing run FSR_Trainable_18630162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/18630162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301490)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                      mae 103.22795\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                     mape 169788050.76389\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                     rmse 201.93362\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:       time_since_restore 4.43863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:         time_this_iter_s 0.47783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:             time_total_s 4.43863\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:                timestamp 1689692706\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb:  View run FSR_Trainable_18630162 at: https://wandb.ai/seokjin/FSR-prediction/runs/18630162\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000505-18630162/logs\n",
      "2023-07-19 00:05:10,711\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.683 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:10,713\tWARNING util.py:315 -- The `process_trial_result` operation took 1.686 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:10,715\tWARNING util.py:315 -- Processing trial results took 1.688 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:10,716\tWARNING util.py:315 -- The `process_trial_result` operation took 1.689 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301721)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_c7a3720a_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-05-00/wandb/run-20230719_000512-c7a3720a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Syncing run FSR_Trainable_c7a3720a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c7a3720a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:05:18,285\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.871 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:18,289\tWARNING util.py:315 -- The `process_trial_result` operation took 1.875 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:18,290\tWARNING util.py:315 -- Processing trial results took 1.876 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:18,293\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                      mae 111.46469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                     mape 197948389.64483\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                     rmse 210.62608\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:       time_since_restore 2.16423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:         time_this_iter_s 0.45765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:             time_total_s 2.16423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:                timestamp 1689692712\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb:  View run FSR_Trainable_c7a3720a at: https://wandb.ai/seokjin/FSR-prediction/runs/c7a3720a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000512-c7a3720a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=301946)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...164)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_754fcb87_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-05-08/wandb/run-20230719_000520-754fcb87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Syncing run FSR_Trainable_754fcb87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/754fcb87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:05:23,953\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:23,956\tWARNING util.py:315 -- The `process_trial_result` operation took 1.986 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:23,959\tWARNING util.py:315 -- Processing trial results took 1.989 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:23,961\tWARNING util.py:315 -- The `process_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                      mae 190.78034\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                     mape 414690929.82859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                     rmse 327.44865\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:       time_since_restore 0.86722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:         time_this_iter_s 0.86722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:             time_total_s 0.86722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:                timestamp 1689692716\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb:  View run FSR_Trainable_754fcb87 at: https://wandb.ai/seokjin/FSR-prediction/runs/754fcb87\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302164)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000520-754fcb87/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:05:29,535\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.788 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:29,539\tWARNING util.py:315 -- The `process_trial_result` operation took 1.792 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:29,540\tWARNING util.py:315 -- Processing trial results took 1.794 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:29,543\tWARNING util.py:315 -- The `process_trial_result` operation took 1.797 s, which may be a performance bottleneck.\n",
      "wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)02 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_82b80782_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-05-21/wandb/run-20230719_000531-82b80782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Syncing run FSR_Trainable_82b80782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/82b80782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302328)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:05:37,013\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.959 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:37,016\tWARNING util.py:315 -- The `process_trial_result` operation took 1.964 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:37,019\tWARNING util.py:315 -- Processing trial results took 1.966 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:37,020\tWARNING util.py:315 -- The `process_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                      mae 250.82925\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                     mape 865307736.90189\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                     rmse 404.765\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:       time_since_restore 0.79041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:         time_this_iter_s 0.79041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:             time_total_s 0.79041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:                timestamp 1689692727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb:  View run FSR_Trainable_82b80782 at: https://wandb.ai/seokjin/FSR-prediction/runs/82b80782\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000531-82b80782/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302518)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_10ade037_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-05-27/wandb/run-20230719_000539-10ade037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Syncing run FSR_Trainable_10ade037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/10ade037\n",
      "2023-07-19 00:05:43,151\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.802 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:43,152\tWARNING util.py:315 -- The `process_trial_result` operation took 1.805 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:43,156\tWARNING util.py:315 -- Processing trial results took 1.808 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:43,157\tWARNING util.py:315 -- The `process_trial_result` operation took 1.809 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_b6697601_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-05-34/wandb/run-20230719_000545-b6697601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Syncing run FSR_Trainable_b6697601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6697601\n",
      "2023-07-19 00:05:53,359\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.854 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:53,364\tWARNING util.py:315 -- The `process_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:05:53,365\tWARNING util.py:315 -- Processing trial results took 1.860 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:05:53,366\tWARNING util.py:315 -- The `process_trial_result` operation took 1.861 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_cd729345_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-05-40/wandb/run-20230719_000556-cd729345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: Syncing run FSR_Trainable_cd729345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/cd729345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                      mae 101.73265\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                     mape 168049116.62522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                     rmse 200.3652\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:       time_since_restore 14.42544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:         time_this_iter_s 0.58646\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:             time_total_s 14.42544\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:                timestamp 1689692754\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb:  View run FSR_Trainable_10ade037 at: https://wandb.ai/seokjin/FSR-prediction/runs/10ade037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302752)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000539-10ade037/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000556-cd729345/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303149)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:06:03,434\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.980 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:03,435\tWARNING util.py:315 -- The `process_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:03,437\tWARNING util.py:315 -- Processing trial results took 1.985 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:06:03,440\tWARNING util.py:315 -- The `process_trial_result` operation took 1.988 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000545-b6697601/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000545-b6697601/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: iterations_since_restore 32\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                      mae 101.81433\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                     mape 171816740.48852\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                     rmse 200.24923\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:       time_since_restore 16.47256\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:         time_this_iter_s 0.53817\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:             time_total_s 16.47256\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:                timestamp 1689692761\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:       training_iteration 32\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb:  View run FSR_Trainable_b6697601 at: https://wandb.ai/seokjin/FSR-prediction/runs/b6697601\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=302932)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000545-b6697601/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_fd8002dc_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-05-50/wandb/run-20230719_000605-fd8002dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Syncing run FSR_Trainable_fd8002dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd8002dc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303383)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000605-fd8002dc/logs\n",
      "2023-07-19 00:06:11,460\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.765 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:11,463\tWARNING util.py:315 -- The `process_trial_result` operation took 1.769 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:11,464\tWARNING util.py:315 -- Processing trial results took 1.770 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:06:11,465\tWARNING util.py:315 -- The `process_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_0c627468_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-06-00/wandb/run-20230719_000613-0c627468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: Syncing run FSR_Trainable_0c627468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0c627468\n",
      "2023-07-19 00:06:20,299\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.051 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:20,302\tWARNING util.py:315 -- The `process_trial_result` operation took 2.054 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:20,305\tWARNING util.py:315 -- Processing trial results took 2.057 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:06:20,306\tWARNING util.py:315 -- The `process_trial_result` operation took 2.059 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_0b763a50_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-06-08/wandb/run-20230719_000622-0b763a50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Syncing run FSR_Trainable_0b763a50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0b763a50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:06:26,757\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.891 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:26,759\tWARNING util.py:315 -- The `process_trial_result` operation took 1.895 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:26,761\tWARNING util.py:315 -- Processing trial results took 1.896 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:06:26,762\tWARNING util.py:315 -- The `process_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                      mae 125.35407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                     mape 280065519.00922\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                     rmse 230.87178\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:       time_since_restore 1.54559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:         time_this_iter_s 0.66902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:             time_total_s 1.54559\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:                timestamp 1689692780\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb:  View run FSR_Trainable_0b763a50 at: https://wandb.ai/seokjin/FSR-prediction/runs/0b763a50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303836)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000622-0b763a50/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_c85d30dd_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-06-17/wandb/run-20230719_000629-c85d30dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Syncing run FSR_Trainable_c85d30dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/c85d30dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304012)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000629-c85d30dd/logs\n",
      "2023-07-19 00:06:35,368\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.969 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:35,372\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:35,374\tWARNING util.py:315 -- Processing trial results took 1.975 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:06:35,375\tWARNING util.py:315 -- The `process_trial_result` operation took 1.977 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_ec63db0d_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-06-24/wandb/run-20230719_000638-ec63db0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Syncing run FSR_Trainable_ec63db0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ec63db0d\n",
      "2023-07-19 00:06:45,776\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.956 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:45,780\tWARNING util.py:315 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:45,782\tWARNING util.py:315 -- Processing trial results took 1.962 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:06:45,783\tWARNING util.py:315 -- The `process_trial_result` operation took 1.963 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_ceaee9a6_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-06-32/wandb/run-20230719_000648-ceaee9a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Syncing run FSR_Trainable_ceaee9a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ceaee9a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                      mae 175.64126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                     mape 1.9908767884652717e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                     rmse 330.23351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:       time_since_restore 0.65151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:         time_this_iter_s 0.65151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:             time_total_s 0.65151\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:                timestamp 1689692803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb:  View run FSR_Trainable_ceaee9a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/ceaee9a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304459)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000648-ceaee9a6/logs\n",
      "2023-07-19 00:06:56,091\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.467 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:56,095\tWARNING util.py:315 -- The `process_trial_result` operation took 2.472 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:06:56,096\tWARNING util.py:315 -- Processing trial results took 2.473 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:06:56,097\tWARNING util.py:315 -- The `process_trial_result` operation took 2.474 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_34aac3b3_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-06-43/wandb/run-20230719_000658-34aac3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Syncing run FSR_Trainable_34aac3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/34aac3b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                      mae 101.58999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                     mape 168413340.33868\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                     rmse 200.25595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:       time_since_restore 16.82032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:         time_this_iter_s 0.64003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:             time_total_s 16.82032\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:                timestamp 1689692814\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb:  View run FSR_Trainable_ec63db0d at: https://wandb.ai/seokjin/FSR-prediction/runs/ec63db0d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304238)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000638-ec63db0d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:07:04,838\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.996 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:04,840\tWARNING util.py:315 -- The `process_trial_result` operation took 1.999 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:04,845\tWARNING util.py:315 -- Processing trial results took 2.003 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:07:04,846\tWARNING util.py:315 -- The `process_trial_result` operation took 2.005 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304681)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_b3221052_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-06-53/wandb/run-20230719_000707-b3221052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Syncing run FSR_Trainable_b3221052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b3221052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:07:13,860\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.755 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:13,865\tWARNING util.py:315 -- The `process_trial_result` operation took 1.760 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:13,866\tWARNING util.py:315 -- Processing trial results took 1.761 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:07:13,867\tWARNING util.py:315 -- The `process_trial_result` operation took 1.763 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                      mae 251.03442\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                     mape 2.363200792128698e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                     rmse 511.76377\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:       time_since_restore 0.63963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:         time_this_iter_s 0.63963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:             time_total_s 0.63963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:                timestamp 1689692822\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb:  View run FSR_Trainable_b3221052 at: https://wandb.ai/seokjin/FSR-prediction/runs/b3221052\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=304917)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000707-b3221052/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000707-b3221052/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_9e37da93_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-07-02/wandb/run-20230719_000716-9e37da93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Syncing run FSR_Trainable_9e37da93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9e37da93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=303618)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:07:21,567\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:21,571\tWARNING util.py:315 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:21,572\tWARNING util.py:315 -- Processing trial results took 1.875 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:07:21,574\tWARNING util.py:315 -- The `process_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                      mae 248.816\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                     mape 2.089196242938078e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                     rmse 517.36789\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:       time_since_restore 0.90054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:         time_this_iter_s 0.90054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:             time_total_s 0.90054\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:                timestamp 1689692832\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb:  View run FSR_Trainable_9e37da93 at: https://wandb.ai/seokjin/FSR-prediction/runs/9e37da93\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000716-9e37da93/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_b31a235f_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-07-11/wandb/run-20230719_000723-b31a235f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Syncing run FSR_Trainable_b31a235f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/b31a235f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305146)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 00:07:27,913\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:27,916\tWARNING util.py:315 -- The `process_trial_result` operation took 1.940 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:27,918\tWARNING util.py:315 -- Processing trial results took 1.942 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:07:27,919\tWARNING util.py:315 -- The `process_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_ef8beec7_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-07-18/wandb/run-20230719_000730-ef8beec7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Syncing run FSR_Trainable_ef8beec7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/ef8beec7\n",
      "2023-07-19 00:07:37,695\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.903 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:37,696\tWARNING util.py:315 -- The `process_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:37,700\tWARNING util.py:315 -- Processing trial results took 1.908 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:07:37,702\tWARNING util.py:315 -- The `process_trial_result` operation took 1.910 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                      mae 102.16857\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                     mape 171144037.51824\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                     rmse 200.55609\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:       time_since_restore 8.26011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:         time_this_iter_s 0.6274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:             time_total_s 8.26011\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:                timestamp 1689692855\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb:  View run FSR_Trainable_ef8beec7 at: https://wandb.ai/seokjin/FSR-prediction/runs/ef8beec7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305553)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000730-ef8beec7/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_f45e96fd_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-07-25/wandb/run-20230719_000740-f45e96fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Syncing run FSR_Trainable_f45e96fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/f45e96fd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000740-f45e96fd/logs\n",
      "2023-07-19 00:07:46,827\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.920 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:46,829\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:46,831\tWARNING util.py:315 -- Processing trial results took 1.924 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:07:46,832\tWARNING util.py:315 -- The `process_trial_result` operation took 1.925 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305768)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_05df9367_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-07-35/wandb/run-20230719_000749-05df9367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Syncing run FSR_Trainable_05df9367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/05df9367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:07:57,235\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.097 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:57,238\tWARNING util.py:315 -- The `process_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:07:57,239\tWARNING util.py:315 -- Processing trial results took 2.103 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:07:57,240\tWARNING util.py:315 -- The `process_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                      mae 102.01612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                     mape 162892687.70487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                     rmse 200.83432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:       time_since_restore 8.25639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:         time_this_iter_s 0.61372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:             time_total_s 8.25639\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:                timestamp 1689692874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb:  View run FSR_Trainable_05df9367 at: https://wandb.ai/seokjin/FSR-prediction/runs/05df9367\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306002)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000749-05df9367/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_00c7219a_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-07-44/wandb/run-20230719_000759-00c7219a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: Syncing run FSR_Trainable_00c7219a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/00c7219a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=305370)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000723-b31a235f/logs\n",
      "2023-07-19 00:08:06,731\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:06,733\tWARNING util.py:315 -- The `process_trial_result` operation took 1.927 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:06,734\tWARNING util.py:315 -- Processing trial results took 1.928 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:08:06,736\tWARNING util.py:315 -- The `process_trial_result` operation took 1.930 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_e298ee30_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-07-54/wandb/run-20230719_000809-e298ee30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Syncing run FSR_Trainable_e298ee30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e298ee30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                      mae 106.49222\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                     mape 190093550.1524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                     rmse 204.62153\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:       time_since_restore 2.73595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:         time_this_iter_s 0.68502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:             time_total_s 2.73595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:                timestamp 1689692888\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb:  View run FSR_Trainable_e298ee30 at: https://wandb.ai/seokjin/FSR-prediction/runs/e298ee30\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306460)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000809-e298ee30/logs\n",
      "2023-07-19 00:08:16,103\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.154 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:16,107\tWARNING util.py:315 -- The `process_trial_result` operation took 2.159 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:16,109\tWARNING util.py:315 -- Processing trial results took 2.160 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:08:16,110\tWARNING util.py:315 -- The `process_trial_result` operation took 2.161 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_989b7d85_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-08-04/wandb/run-20230719_000818-989b7d85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Syncing run FSR_Trainable_989b7d85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/989b7d85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                      mae 215.97363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                     mape 448639917.59841\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                     rmse 364.86042\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:       time_since_restore 0.72159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:         time_this_iter_s 0.72159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:             time_total_s 0.72159\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:                timestamp 1689692893\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb:  View run FSR_Trainable_989b7d85 at: https://wandb.ai/seokjin/FSR-prediction/runs/989b7d85\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306683)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000818-989b7d85/logs\n",
      "2023-07-19 00:08:24,947\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.130 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:24,950\tWARNING util.py:315 -- The `process_trial_result` operation took 2.134 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:24,954\tWARNING util.py:315 -- Processing trial results took 2.138 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:08:24,954\tWARNING util.py:315 -- The `process_trial_result` operation took 2.138 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_011d39d7_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-08-13/wandb/run-20230719_000827-011d39d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Syncing run FSR_Trainable_011d39d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/011d39d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                      mae 245.315\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                     mape 614904662.30039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                     rmse 445.93859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:       time_since_restore 0.52759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:         time_this_iter_s 0.52759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:             time_total_s 0.52759\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:                timestamp 1689692902\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb:  View run FSR_Trainable_011d39d7 at: https://wandb.ai/seokjin/FSR-prediction/runs/011d39d7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306907)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000827-011d39d7/logs\n",
      "2023-07-19 00:08:33,703\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.032 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:33,707\tWARNING util.py:315 -- The `process_trial_result` operation took 2.037 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:33,709\tWARNING util.py:315 -- Processing trial results took 2.039 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:08:33,711\tWARNING util.py:315 -- The `process_trial_result` operation took 2.041 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_0317d496_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-08-22/wandb/run-20230719_000836-0317d496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Syncing run FSR_Trainable_0317d496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/0317d496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                      mae 282.61448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                     mape 1346878517.00617\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                     rmse 566.71359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:       time_since_restore 0.56374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:         time_this_iter_s 0.56374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:             time_total_s 0.56374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:                timestamp 1689692911\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb:  View run FSR_Trainable_0317d496 at: https://wandb.ai/seokjin/FSR-prediction/runs/0317d496\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307133)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000836-0317d496/logs\n",
      "2023-07-19 00:08:42,456\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.951 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:42,460\tWARNING util.py:315 -- The `process_trial_result` operation took 1.956 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:42,461\tWARNING util.py:315 -- Processing trial results took 1.956 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:08:42,461\tWARNING util.py:315 -- The `process_trial_result` operation took 1.957 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_9070b884_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-08-31/wandb/run-20230719_000844-9070b884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Syncing run FSR_Trainable_9070b884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/9070b884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                      mae 147.72329\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                     mape 426059788.80044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                     rmse 267.80423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:       time_since_restore 1.40598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:         time_this_iter_s 0.66749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:             time_total_s 1.40598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:                timestamp 1689692923\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb:  View run FSR_Trainable_9070b884 at: https://wandb.ai/seokjin/FSR-prediction/runs/9070b884\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307357)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000844-9070b884/logs\n",
      "2023-07-19 00:08:51,426\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:51,428\tWARNING util.py:315 -- The `process_trial_result` operation took 1.933 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:51,432\tWARNING util.py:315 -- Processing trial results took 1.938 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:08:51,433\tWARNING util.py:315 -- The `process_trial_result` operation took 1.939 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "wandb: \\ Waiting for wandb.init()...580)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_beabc043_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-08-39/wandb/run-20230719_000853-beabc043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Syncing run FSR_Trainable_beabc043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/beabc043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:08:59,864\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.722 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:59,866\tWARNING util.py:315 -- The `process_trial_result` operation took 1.724 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:08:59,868\tWARNING util.py:315 -- Processing trial results took 1.726 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:08:59,870\tWARNING util.py:315 -- The `process_trial_result` operation took 1.728 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                      mae 141.71694\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                     mape 406783453.34959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                     rmse 260.53041\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:       time_since_restore 1.37504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:         time_this_iter_s 0.66915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:             time_total_s 1.37504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:                timestamp 1689692932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb:  View run FSR_Trainable_beabc043 at: https://wandb.ai/seokjin/FSR-prediction/runs/beabc043\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000853-beabc043/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_7b3bd83b_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-08-48/wandb/run-20230719_000902-7b3bd83b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Syncing run FSR_Trainable_7b3bd83b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b3bd83b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307580)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=306226)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:09:07,945\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.818 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:07,950\tWARNING util.py:315 -- The `process_trial_result` operation took 1.823 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:07,951\tWARNING util.py:315 -- Processing trial results took 1.825 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:09:07,952\tWARNING util.py:315 -- The `process_trial_result` operation took 1.826 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                      mae 104.93304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                     mape 169320708.84985\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                     rmse 203.98433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:       time_since_restore 2.39707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:         time_this_iter_s 0.54567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:             time_total_s 2.39707\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:                timestamp 1689692941\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb:  View run FSR_Trainable_7b3bd83b at: https://wandb.ai/seokjin/FSR-prediction/runs/7b3bd83b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000902-7b3bd83b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_3b428213_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-08-57/wandb/run-20230719_000910-3b428213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Syncing run FSR_Trainable_3b428213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/3b428213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=307809)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 00:09:14,159\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.982 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:14,162\tWARNING util.py:315 -- The `process_trial_result` operation took 1.986 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:14,164\tWARNING util.py:315 -- Processing trial results took 1.987 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:09:14,165\tWARNING util.py:315 -- The `process_trial_result` operation took 1.989 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                      mae 135.58549\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                     mape 319845495.05612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                     rmse 245.77516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:       time_since_restore 1.49175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:         time_this_iter_s 0.58573\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:             time_total_s 1.49175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:                timestamp 1689692948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb:  View run FSR_Trainable_3b428213 at: https://wandb.ai/seokjin/FSR-prediction/runs/3b428213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000910-3b428213/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_e2efdd0c_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-09-05/wandb/run-20230719_000916-e2efdd0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Syncing run FSR_Trainable_e2efdd0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2efdd0c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308040)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-19 00:09:20,061\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:20,065\tWARNING util.py:315 -- The `process_trial_result` operation took 1.882 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:20,066\tWARNING util.py:315 -- Processing trial results took 1.884 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:09:20,068\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000916-e2efdd0c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "2023-07-19 00:09:26,564\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.734 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:26,566\tWARNING util.py:315 -- The `process_trial_result` operation took 1.737 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:26,568\tWARNING util.py:315 -- Processing trial results took 1.739 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:09:26,570\tWARNING util.py:315 -- The `process_trial_result` operation took 1.741 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308209)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_8e07034e_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-09-17/wandb/run-20230719_000929-8e07034e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: Syncing run FSR_Trainable_8e07034e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/8e07034e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                      mae 102.24273\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                     mape 166390594.20883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                     rmse 200.90762\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:       time_since_restore 8.40648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:         time_this_iter_s 0.37947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:             time_total_s 8.40648\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:                timestamp 1689692969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb:  View run FSR_Trainable_fb672319 at: https://wandb.ai/seokjin/FSR-prediction/runs/fb672319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000922-fb672319/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308389)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:       training_iteration \n",
      "2023-07-19 00:09:35,039\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.710 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:35,042\tWARNING util.py:315 -- The `process_trial_result` operation took 1.714 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:35,044\tWARNING util.py:315 -- Processing trial results took 1.716 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:09:35,045\tWARNING util.py:315 -- The `process_trial_result` operation took 1.717 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_13216edd_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y=_2023-07-19_00-09-24/wandb/run-20230719_000937-13216edd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Syncing run FSR_Trainable_13216edd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/13216edd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/13216edd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308591)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-07-19 00:09:42,201\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.580 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:42,204\tWARNING util.py:315 -- The `process_trial_result` operation took 1.584 s, which may be a performance bottleneck.\n",
      "2023-07-19 00:09:42,205\tWARNING util.py:315 -- Processing trial results took 1.585 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-19 00:09:42,207\tWARNING util.py:315 -- The `process_trial_result` operation took 1.588 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: / 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                      mae 128.87796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                     mape 164050236.85744\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                     rmse 254.9432\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:       time_since_restore 1.5344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:         time_this_iter_s 0.6138\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:             time_total_s 1.5344\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:                timestamp 1689692975\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb:  View run FSR_Trainable_13216edd at: https://wandb.ai/seokjin/FSR-prediction/runs/13216edd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000937-13216edd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: wandb version 0.15.5 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-07-18_23-55-45/FSR_Trainable_63d60b58_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_time,index_X=FSR_for_force,index_y_2023-07-19_00-09-32/wandb/run-20230719_000944-63d60b58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Syncing run FSR_Trainable_63d60b58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:  View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:  View run at https://wandb.ai/seokjin/FSR-prediction/runs/63d60b58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=308814)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: iterations_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:                      mae \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:                     mape \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:                     rmse \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:       time_since_restore \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:         time_this_iter_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:             time_total_s \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:                timestamp \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb:       training_iteration \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=309042)\u001b[0m wandb: Find logs at: ./wandb/run-20230719_000944-63d60b58/logs\n",
      "2023-07-19 00:09:49,812\tINFO tune.py:1111 -- Total run time: 840.86 seconds (833.35 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
