{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task1_ANN\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Time\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-08-17_11-30-03/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-08-17_11-30-03\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "\n",
    "\n",
    "0.6783\n",
    "\n",
    "## Fixed Hyperparameter\n",
    "\n",
    "model: ANN / hidden size: 8 / num layer: 1\n",
    "\n",
    "optimizer: NAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=4,\n",
    "        reuse_actors=True,\n",
    "        scheduler=ray.tune.schedulers.PopulationBasedTraining(\n",
    "            time_attr='time_total_s',\n",
    "            perturbation_interval=5,\n",
    "            metric='metric',\n",
    "            mode='min',\n",
    "            hyperparam_mutations={\n",
    "                'model':['fsr_model.ANN'],\n",
    "                'model_args':{\n",
    "                    'hidden_size':[8],\n",
    "                    'num_layer':[1],\n",
    "                },\n",
    "                'criterion':['torch.nn.MSELoss'],\n",
    "                'optimizer':[\n",
    "                    'torch.optim.NAdam',\n",
    "                ],\n",
    "                'optimizer_args':{\n",
    "                    'lr':ray.tune.loguniform(1e-5, 1e-1),\n",
    "                },\n",
    "                'imputer':['sklearn.impute.SimpleImputer'],\n",
    "                'imputer_args':{\n",
    "                    'strategy':['mean', 'median'],\n",
    "                },\n",
    "                'scaler':[ \n",
    "                    'sklearn.preprocessing.StandardScaler',\n",
    "                    'sklearn.preprocessing.MinMaxScaler',\n",
    "                    'sklearn.preprocessing.RobustScaler',\n",
    "                ],\n",
    "                'index_X': [['FSR_for_force', 'FSR_for_coord']],\n",
    "                'index_y': [['force', 'x_coord', 'y_coord']],\n",
    "                'data_loader': ['fsr_data.get_index_splited_by_time']\n",
    "            },\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='metric',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 11:30:06,353\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-08-17 11:30:08,846\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-17 11:31:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:14.87        </td></tr>\n",
       "<tr><td>Memory:      </td><td>7.3/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      PopulationBasedTraining: 5 checkpoints, 2 perturbs<br>Logical resource usage: 6.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status  </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args        </th><th>index_X             </th><th>index_y             </th><th>model        </th><th>model_args          </th><th>optimizer        </th><th>optimizer_args      </th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  trmse_force</th><th style=\"text-align: right;\">  tmape_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_fbe7d_00000</td><td>RUNNING </td><td>172.26.215.93:245855</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8d50</td><td>sklearn.impute._0f80</td><td>{&#x27;strategy&#x27;: &#x27;m_6400</td><td>[&#x27;FSR_for_force_be80</td><td>[&#x27;force&#x27;, &#x27;x_co_3300</td><td>fsr_model.ANN</td><td>{&#x27;hidden_size&#x27;:_7500</td><td>torch.optim.NAdam</td><td>{&#x27;lr&#x27;: 0.004819_7600</td><td>sklearn.preproc_8e10</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         21.781 </td><td style=\"text-align: right;\">   1.30331  </td><td style=\"text-align: right;\">    3.4975   </td><td style=\"text-align: right;\">  1.40945e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_fbe7d_00001</td><td>RUNNING </td><td>172.26.215.93:245852</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8d50</td><td>sklearn.impute._0f80</td><td>{&#x27;strategy&#x27;: &#x27;mean&#x27;}</td><td>[&#x27;FSR_for_force_be80</td><td>[&#x27;force&#x27;, &#x27;x_co_3300</td><td>fsr_model.ANN</td><td>{&#x27;hidden_size&#x27;:_e980</td><td>torch.optim.NAdam</td><td>{&#x27;lr&#x27;: 0.004819_1e80</td><td>sklearn.preproc_90b0</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         25.2848</td><td style=\"text-align: right;\">   0.122257 </td><td style=\"text-align: right;\">    0.213154 </td><td style=\"text-align: right;\">  2.24723e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fbe7d_00003</td><td>PAUSED  </td><td>172.26.215.93:245852</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8d50</td><td>sklearn.impute._0f80</td><td>{&#x27;strategy&#x27;: &#x27;m_9d00</td><td>[&#x27;FSR_for_force_be80</td><td>[&#x27;force&#x27;, &#x27;x_co_3300</td><td>fsr_model.ANN</td><td>{&#x27;hidden_size&#x27;:_a900</td><td>torch.optim.NAdam</td><td>{&#x27;lr&#x27;: 0.000297_b900</td><td>sklearn.preproc_90b0</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         21.4582</td><td style=\"text-align: right;\">   0.193137 </td><td style=\"text-align: right;\">    0.232903 </td><td style=\"text-align: right;\">  3.97045e+14</td></tr>\n",
       "<tr><td>FSR_Trainable_fbe7d_00002</td><td>PENDING </td><td>172.26.215.93:245852</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_8d50</td><td>sklearn.impute._0f80</td><td>{&#x27;strategy&#x27;: &#x27;mean&#x27;}</td><td>[&#x27;FSR_for_force_be80</td><td>[&#x27;force&#x27;, &#x27;x_co_3300</td><td>fsr_model.ANN</td><td>{&#x27;hidden_size&#x27;:_0fc0</td><td>torch.optim.NAdam</td><td>{&#x27;lr&#x27;: 0.004016_abc0</td><td>sklearn.preproc_8e70</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         21.071 </td><td style=\"text-align: right;\">   0.0425113</td><td style=\"text-align: right;\">    0.0713056</td><td style=\"text-align: right;\">  4.55102e+13</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-17 11:30:08,923\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  mae_coord</th><th style=\"text-align: right;\">  mae_force</th><th style=\"text-align: right;\">  mape_coord</th><th style=\"text-align: right;\">  mape_force</th><th style=\"text-align: right;\">  metric</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">  rmse_coord</th><th style=\"text-align: right;\">  rmse_force</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  tmae_coord</th><th style=\"text-align: right;\">  tmae_force</th><th style=\"text-align: right;\">  tmape_coord</th><th style=\"text-align: right;\">  tmape_force</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  trmse_coord</th><th style=\"text-align: right;\">  trmse_force</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_fbe7d_00000</td><td>2023-08-17_11-31-20</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   0.903458</td><td style=\"text-align: right;\">   232.833 </td><td style=\"text-align: right;\">  0.281962  </td><td style=\"text-align: right;\"> 1.26185    </td><td style=\"text-align: right;\">7.22889 </td><td>172.26.215.93</td><td style=\"text-align: right;\">245855</td><td style=\"text-align: right;\">    1.98782 </td><td style=\"text-align: right;\">    440.042 </td><td style=\"text-align: right;\">            0.709959</td><td style=\"text-align: right;\">          0.709959</td><td style=\"text-align: right;\">       21.781 </td><td style=\"text-align: right;\"> 1692239480</td><td style=\"text-align: right;\">   1.57443  </td><td style=\"text-align: right;\">   1.30331  </td><td style=\"text-align: right;\">  1.40656e+15</td><td style=\"text-align: right;\">  1.40945e+15</td><td style=\"text-align: right;\">                  31</td><td>fbe7d_00000</td><td style=\"text-align: right;\">     3.73139 </td><td style=\"text-align: right;\">    3.4975   </td></tr>\n",
       "<tr><td>FSR_Trainable_fbe7d_00001</td><td>2023-08-17_11-31-21</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         7</td><td style=\"text-align: right;\">   0.063806</td><td style=\"text-align: right;\">    32.2362</td><td style=\"text-align: right;\">  0.00947381</td><td style=\"text-align: right;\"> 2.95631e+15</td><td style=\"text-align: right;\">0.417216</td><td>172.26.215.93</td><td style=\"text-align: right;\">245852</td><td style=\"text-align: right;\">    0.10621 </td><td style=\"text-align: right;\">     82.4744</td><td style=\"text-align: right;\">            5.15632 </td><td style=\"text-align: right;\">          0.776073</td><td style=\"text-align: right;\">       26.0608</td><td style=\"text-align: right;\"> 1692239481</td><td style=\"text-align: right;\">   0.149442 </td><td style=\"text-align: right;\">   0.114768 </td><td style=\"text-align: right;\">  1.32163e+13</td><td style=\"text-align: right;\">  2.03217e+14</td><td style=\"text-align: right;\">                  35</td><td>fbe7d_00001</td><td style=\"text-align: right;\">     0.212779</td><td style=\"text-align: right;\">    0.204437 </td></tr>\n",
       "<tr><td>FSR_Trainable_fbe7d_00002</td><td>2023-08-17_11-30-45</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">   0.402373</td><td style=\"text-align: right;\">   111.741 </td><td style=\"text-align: right;\">  0.0983136 </td><td style=\"text-align: right;\"> 1.07774e+17</td><td style=\"text-align: right;\">0.198186</td><td>172.26.215.93</td><td style=\"text-align: right;\">245852</td><td style=\"text-align: right;\">    0.691733</td><td style=\"text-align: right;\">    211.73  </td><td style=\"text-align: right;\">            0.903552</td><td style=\"text-align: right;\">          0.903552</td><td style=\"text-align: right;\">       16.9101</td><td style=\"text-align: right;\"> 1692239445</td><td style=\"text-align: right;\">   0.0798743</td><td style=\"text-align: right;\">   0.0424389</td><td style=\"text-align: right;\">  1.32982e+13</td><td style=\"text-align: right;\">  4.54902e+13</td><td style=\"text-align: right;\">                  23</td><td>fbe7d_00002</td><td style=\"text-align: right;\">     0.126992</td><td style=\"text-align: right;\">    0.0711936</td></tr>\n",
       "<tr><td>FSR_Trainable_fbe7d_00003</td><td>2023-08-17_11-31-07</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         7</td><td style=\"text-align: right;\">   0.201835</td><td style=\"text-align: right;\">    82.7553</td><td style=\"text-align: right;\">  0.0305562 </td><td style=\"text-align: right;\"> 3.73973e+15</td><td style=\"text-align: right;\">0.652004</td><td>172.26.215.93</td><td style=\"text-align: right;\">245852</td><td style=\"text-align: right;\">    0.268964</td><td style=\"text-align: right;\">    200.505 </td><td style=\"text-align: right;\">            5.40261 </td><td style=\"text-align: right;\">          1.11391 </td><td style=\"text-align: right;\">       21.4582</td><td style=\"text-align: right;\"> 1692239467</td><td style=\"text-align: right;\">   0.352534 </td><td style=\"text-align: right;\">   0.193137 </td><td style=\"text-align: right;\">  6.19984e+12</td><td style=\"text-align: right;\">  3.97045e+14</td><td style=\"text-align: right;\">                  29</td><td>fbe7d_00003</td><td style=\"text-align: right;\">     0.419101</td><td style=\"text-align: right;\">    0.232903 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03/FSR_Trainable_fbe7d_00003_3_2023-08-17_11-30-09/wandb/run-20230817_113027-fbe7d_00003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: Syncing run FSR_Trainable_fbe7d_00003\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246064)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbe7d_00003\n",
      "2023-08-17 11:30:42,900\tINFO pbt.py:808 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial fbe7d_00002 (score = -0.195783) into trial fbe7d_00001 (score = -10.945196)\n",
      "\n",
      "2023-08-17 11:30:42,904\tINFO pbt.py:835 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trialfbe7d_00001:\n",
      "model : fsr_model.ANN --- (shift left (noop)) --> fsr_model.ANN\n",
      "model_args : \n",
      "    hidden_size : 8 --- (shift right (noop)) --> 8\n",
      "    num_layer : 1 --- (shift right (noop)) --> 1\n",
      "criterion : torch.nn.MSELoss --- (resample) --> torch.nn.MSELoss\n",
      "optimizer : torch.optim.NAdam --- (shift right (noop)) --> torch.optim.NAdam\n",
      "optimizer_args : \n",
      "    lr : 0.004016271729892268 --- (* 1.2) --> 0.004819526075870721\n",
      "imputer : sklearn.impute.SimpleImputer --- (shift left (noop)) --> sklearn.impute.SimpleImputer\n",
      "imputer_args : \n",
      "    strategy : mean --- (shift left (noop)) --> mean\n",
      "scaler : sklearn.preprocessing.MinMaxScaler --- (shift right) --> sklearn.preprocessing.RobustScaler\n",
      "index_X : ['FSR_for_force', 'FSR_for_coord'] --- (shift left (noop)) --> ['FSR_for_force', 'FSR_for_coord']\n",
      "index_y : ['force', 'x_coord', 'y_coord'] --- (resample) --> ['force', 'x_coord', 'y_coord']\n",
      "data_loader : fsr_data.get_index_splited_by_time --- (shift left (noop)) --> fsr_data.get_index_splited_by_time\n",
      "\n",
      "2023-08-17 11:30:42,908\tWARNING trial_runner.py:1592 -- You are trying to access pause_trial interface of TrialRunner in TrialScheduler, which is being restricted. If you believe it is reasonable for your scheduler to access this TrialRunner API, please reach out to Ray team on GitHub. A more strict API access pattern would be enforced starting 1.12s.0\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:30:44,050\tINFO trainable.py:918 -- Restored on 172.26.215.93 from checkpoint: /tmp/checkpoint_tmp_e09cb9f386e8452b80e21068d3f4b96e\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:30:44,050\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 13, '_timesteps_total': None, '_time_total': 10.616129636764526, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb:  $ pip install wandb --upgrade\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: Tracking run with wandb version 0.15.4\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03/FSR_Trainable_fbe7d_00001_1_2023-08-17_11-30-09/wandb/run-20230817_113027-fbe7d_00001\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: Syncing run FSR_Trainable_fbe7d_00001\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246070)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbe7d_00001\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03/FSR_Trainable_fbe7d_00001_1_2023-08-17_11-30-09/wandb/run-20230817_113050-fbe7d_00001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: Syncing run FSR_Trainable_fbe7d_00001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246670)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbe7d_00001\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:30:54,143\tINFO trainable.py:918 -- Restored on 172.26.215.93 from checkpoint: /tmp/checkpoint_tmp_9ae4c8ccb7964e0b8c2c6b6eadcaea5a\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:30:54,143\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 21, '_timesteps_total': None, '_time_total': 15.66994833946228, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=246675)\u001b[0m 2023-08-17 11:30:58,645\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 22, '_timesteps_total': None, '_time_total': 16.05561327934265, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-17 11:31:02,482\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.726 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:02,485\tWARNING util.py:315 -- The `process_trial_result` operation took 2.730 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:02,489\tWARNING util.py:315 -- Processing trial results took 2.733 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-17 11:31:02,493\tWARNING util.py:315 -- The `process_trial_result` operation took 2.738 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03/FSR_Trainable_fbe7d_00001_1_2023-08-17_11-30-09/wandb/run-20230817_113101-fbe7d_00001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: Syncing run FSR_Trainable_fbe7d_00001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247077)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbe7d_00001\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:31:05,896\tINFO trainable.py:918 -- Restored on 172.26.215.93 from checkpoint: /tmp/checkpoint_tmp_228cf885e051433b91a023f89f6e8620\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:31:05,896\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 23, '_timesteps_total': None, '_time_total': 16.289838075637817, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbe7d_00003\n",
      "2023-08-17 11:31:10,538\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.460 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:10,543\tWARNING util.py:315 -- The `process_trial_result` operation took 3.465 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:10,546\tWARNING util.py:315 -- Processing trial results took 3.468 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-17 11:31:10,548\tWARNING util.py:315 -- The `process_trial_result` operation took 3.470 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:10,579\tWARNING worker.py:2019 -- WARNING: 34 PYTHON worker processes have been started on node: 6faba20d4836d4e344510bcba44f83e5a4a34faa4b825697adbcbe02 with address: 172.26.215.93. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247191)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m 2023-08-17 11:31:11,325\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 28, '_timesteps_total': None, '_time_total': 20.90450668334961, '_episodes_total': None}\n",
      "2023-08-17 11:31:16,667\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 4.157 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:16,673\tWARNING util.py:315 -- The `process_trial_result` operation took 4.163 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:16,675\tWARNING util.py:315 -- Processing trial results took 4.166 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-17 11:31:16,677\tWARNING util.py:315 -- The `process_trial_result` operation took 4.168 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "2023-08-17 11:31:19,315\tINFO pbt.py:808 -- \n",
      "\n",
      "[PopulationBasedTraining] [Exploit] Cloning trial fbe7d_00002 (score = -0.198648) into trial fbe7d_00000 (score = -11.366419)\n",
      "\n",
      "2023-08-17 11:31:19,317\tINFO pbt.py:835 -- \n",
      "\n",
      "[PopulationBasedTraining] [Explore] Perturbed the hyperparameter config of trialfbe7d_00000:\n",
      "model : fsr_model.ANN --- (resample) --> fsr_model.ANN\n",
      "model_args : \n",
      "    hidden_size : 8 --- (shift right (noop)) --> 8\n",
      "    num_layer : 1 --- (shift left (noop)) --> 1\n",
      "criterion : torch.nn.MSELoss --- (shift left (noop)) --> torch.nn.MSELoss\n",
      "optimizer : torch.optim.NAdam --- (shift right (noop)) --> torch.optim.NAdam\n",
      "optimizer_args : \n",
      "    lr : 0.004016271729892268 --- (* 1.2) --> 0.004819526075870721\n",
      "imputer : sklearn.impute.SimpleImputer --- (shift right (noop)) --> sklearn.impute.SimpleImputer\n",
      "imputer_args : \n",
      "    strategy : mean --- (resample) --> median\n",
      "scaler : sklearn.preprocessing.MinMaxScaler --- (shift left) --> sklearn.preprocessing.StandardScaler\n",
      "index_X : ['FSR_for_force', 'FSR_for_coord'] --- (shift left (noop)) --> ['FSR_for_force', 'FSR_for_coord']\n",
      "index_y : ['force', 'x_coord', 'y_coord'] --- (shift right (noop)) --> ['force', 'x_coord', 'y_coord']\n",
      "data_loader : fsr_data.get_index_splited_by_time --- (shift right (noop)) --> fsr_data.get_index_splited_by_time\n",
      "\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:31:20,039\tINFO trainable.py:918 -- Restored on 172.26.215.93 from checkpoint: /tmp/checkpoint_tmp_7713a77b13584d669a36cb684ed4737c\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245855)\u001b[0m 2023-08-17 11:31:20,039\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 21.07099437713623, '_episodes_total': None}\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03/FSR_Trainable_fbe7d_00001_1_2023-08-17_11-30-09/wandb/run-20230817_113119-fbe7d_00001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb: Syncing run FSR_Trainable_fbe7d_00001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbe7d_00001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: / Waiting for wandb.init()...\n",
      "2023-08-17 11:31:23,768\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.892 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:23,770\tWARNING util.py:315 -- The `process_trial_result` operation took 2.894 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:23,774\tWARNING util.py:315 -- Processing trial results took 2.898 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-17 11:31:23,778\tWARNING util.py:315 -- The `process_trial_result` operation took 2.902 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:24,366\tWARNING worker.py:2019 -- WARNING: 40 PYTHON worker processes have been started on node: 6faba20d4836d4e344510bcba44f83e5a4a34faa4b825697adbcbe02 with address: 172.26.215.93. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247607)\u001b[0m 2023-08-17 11:31:24,373\tINFO trainable.py:927 -- Current state after restoring: {'_iteration': 30, '_timesteps_total': None, '_time_total': 21.07099437713623, '_episodes_total': None}\n",
      "2023-08-17 11:31:27,692\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.336 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:27,694\tWARNING util.py:315 -- The `process_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "2023-08-17 11:31:27,696\tWARNING util.py:315 -- Processing trial results took 2.341 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-17 11:31:27,703\tWARNING util.py:315 -- The `process_trial_result` operation took 2.347 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(FSR_Trainable pid=245852)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=247427)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03/FSR_Trainable_fbe7d_00002_2_2023-08-17_11-30-09/wandb/run-20230817_113130-fbe7d_00002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: Syncing run FSR_Trainable_fbe7d_00002\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=248021)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fbe7d_00002\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-08-17 11:32:06,260 E 245472 245472] (raylet) node_manager.cc:3069: 3 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 6faba20d4836d4e344510bcba44f83e5a4a34faa4b825697adbcbe02, IP: 172.26.215.93) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.26.215.93`\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "ename": "TuneError",
     "evalue": "The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"/home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03\", trainable=...)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:815\u001b[0m, in \u001b[0;36mTuneController._schedule_trial_task.<locals>._on_result\u001b[0;34m(tracked_actor, *args, **kwargs)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 815\u001b[0m     on_result(trial, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    816\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:735\u001b[0m, in \u001b[0;36m_TuneControllerBase._on_training_result\u001b[0;34m(self, trial, result)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mprocess_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 735\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_trial_results(trial, result)\n\u001b[1;32m    736\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_execute_queued_decision(trial)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:748\u001b[0m, in \u001b[0;36m_TuneControllerBase._process_trial_results\u001b[0;34m(self, trial, results)\u001b[0m\n\u001b[1;32m    747\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mprocess_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 748\u001b[0m     decision \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_trial_result(trial, result)\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m decision \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     \u001b[39m# If we didn't get a decision, this means a\u001b[39;00m\n\u001b[1;32m    751\u001b[0m     \u001b[39m# non-training future (e.g. a save) was scheduled.\u001b[39;00m\n\u001b[1;32m    752\u001b[0m     \u001b[39m# We do not allow processing more results then.\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py:805\u001b[0m, in \u001b[0;36m_TuneControllerBase._process_trial_result\u001b[0;34m(self, trial, result)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[39mwith\u001b[39;00m warn_if_slow(\u001b[39m\"\u001b[39m\u001b[39mcallbacks.on_trial_result\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> 805\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callbacks\u001b[39m.\u001b[39;49mon_trial_result(\n\u001b[1;32m    806\u001b[0m         iteration\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iteration,\n\u001b[1;32m    807\u001b[0m         trials\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trials,\n\u001b[1;32m    808\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m    809\u001b[0m         result\u001b[39m=\u001b[39;49mresult\u001b[39m.\u001b[39;49mcopy(),\n\u001b[1;32m    810\u001b[0m     )\n\u001b[1;32m    811\u001b[0m trial\u001b[39m.\u001b[39mupdate_last_result(result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/callback.py:392\u001b[0m, in \u001b[0;36mCallbackList.on_trial_result\u001b[0;34m(self, **info)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callbacks:\n\u001b[0;32m--> 392\u001b[0m     callback\u001b[39m.\u001b[39;49mon_trial_result(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minfo)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/logger/logger.py:140\u001b[0m, in \u001b[0;36mLoggerCallback.on_trial_result\u001b[0;34m(self, iteration, trials, trial, result, **info)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_trial_result\u001b[39m(\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    134\u001b[0m     iteration: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39minfo,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[0;32m--> 140\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_trial_result(iteration, trial, result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/integrations/wandb.py:679\u001b[0m, in \u001b[0;36mWandbLoggerCallback.log_trial_result\u001b[0;34m(self, iteration, trial, result)\u001b[0m\n\u001b[1;32m    678\u001b[0m result \u001b[39m=\u001b[39m _clean_log(result)\n\u001b[0;32m--> 679\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_trial_queues[trial]\u001b[39m.\u001b[39;49mput((_QueueItem\u001b[39m.\u001b[39;49mRESULT, result))\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/util/queue.py:105\u001b[0m, in \u001b[0;36mQueue.put\u001b[0;34m(self, item, block, timeout)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     ray\u001b[39m.\u001b[39;49mget(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mput\u001b[39m.\u001b[39;49mremote(item, timeout))\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:18\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m auto_init_ray()\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py:2542\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2542\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   2544\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.26.215.93, ID: 6faba20d4836d4e344510bcba44f83e5a4a34faa4b825697adbcbe02) where the task (actor ID: 99142d3d2b4f740eea0b0fee01000000, name=_QueueActor.__init__, pid=248019, memory used=0.11GB) was running was 7.33GB / 7.72GB (0.950125), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 826810831838da706fdd9e667af8af7386b3dbdec590fc22c3e54f56) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.26.215.93`. To see the logs of the worker, use `ray logs worker-826810831838da706fdd9e667af8af7386b3dbdec590fc22c3e54f56*out -ip 172.26.215.93. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n1025\t0.39\t/home/seokj/.vscode-server/bin/6c3e3dba23e8fadc360aed75ce363ba185c49794/node /home/seokj/.vscode-ser...\n245855\t0.37\tray::FSR_Trainable\n245852\t0.37\tray::FSR_Trainable\n240754\t0.33\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9003 --control=9...\n247815\t0.28\tray::IDLE\n971\t0.17\t/home/seokj/.vscode-server/bin/6c3e3dba23e8fadc360aed75ce363ba185c49794/node /home/seokj/.vscode-ser...\n246675\t0.14\tray::_WandbLoggingActor.run\n247189\t0.14\tray::_QueueActor.get\n247191\t0.14\tray::_WandbLoggingActor.run\n246668\t0.14\tray::_QueueActor.get\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/tuner.py:347\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_local_tuner\u001b[39m.\u001b[39;49mfit()\n\u001b[1;32m    348\u001b[0m \u001b[39mexcept\u001b[39;00m TuneError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:588\u001b[0m, in \u001b[0;36mTunerInternal.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_restored:\n\u001b[0;32m--> 588\u001b[0m     analysis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_internal(trainable, param_space)\n\u001b[1;32m    589\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/impl/tuner_internal.py:712\u001b[0m, in \u001b[0;36mTunerInternal._fit_internal\u001b[0;34m(self, trainable, param_space)\u001b[0m\n\u001b[1;32m    699\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m    700\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tune_run_arguments(trainable),\n\u001b[1;32m    701\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mdict\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tuner_kwargs,\n\u001b[1;32m    711\u001b[0m }\n\u001b[0;32m--> 712\u001b[0m analysis \u001b[39m=\u001b[39m run(\n\u001b[1;32m    713\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    714\u001b[0m )\n\u001b[1;32m    715\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_remote_string_queue()\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/tune.py:1070\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39mwhile\u001b[39;00m (\n\u001b[1;32m   1068\u001b[0m     \u001b[39mnot\u001b[39;00m runner\u001b[39m.\u001b[39mis_finished() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m experiment_interrupted_event\u001b[39m.\u001b[39mis_set()\n\u001b[1;32m   1069\u001b[0m ):\n\u001b[0;32m-> 1070\u001b[0m     runner\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1071\u001b[0m     \u001b[39mif\u001b[39;00m has_verbosity(Verbosity\u001b[39m.\u001b[39mV1_EXPERIMENT):\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:256\u001b[0m, in \u001b[0;36mTuneController.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39m# Handle one event\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actor_manager\u001b[39m.\u001b[39;49mnext(timeout\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m):\n\u001b[1;32m    257\u001b[0m     \u001b[39m# If there are no actors running, warn about potentially\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39m# insufficient resources\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_actor_manager\u001b[39m.\u001b[39mnum_live_actors:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py:224\u001b[0m, in \u001b[0;36mRayActorManager.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39melif\u001b[39;00m future \u001b[39min\u001b[39;00m actor_task_futures:\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actor_task_events\u001b[39m.\u001b[39;49mresolve_future(future)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py:118\u001b[0m, in \u001b[0;36mRayEventManager.resolve_future\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m on_result:\n\u001b[0;32m--> 118\u001b[0m     on_result(result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py:752\u001b[0m, in \u001b[0;36mRayActorManager._schedule_tracked_actor_task.<locals>.on_result\u001b[0;34m(result)\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_result\u001b[39m(result: Any):\n\u001b[0;32m--> 752\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_actor_task_resolved(\n\u001b[1;32m    753\u001b[0m         tracked_actor_task\u001b[39m=\u001b[39;49mtracked_actor_task, result\u001b[39m=\u001b[39;49mresult\n\u001b[1;32m    754\u001b[0m     )\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/air/execution/_internal/actor_manager.py:300\u001b[0m, in \u001b[0;36mRayActorManager._actor_task_resolved\u001b[0;34m(self, tracked_actor_task, result)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m tracked_actor_task\u001b[39m.\u001b[39m_on_result:\n\u001b[0;32m--> 300\u001b[0m     tracked_actor_task\u001b[39m.\u001b[39;49m_on_result(tracked_actor, result)\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py:824\u001b[0m, in \u001b[0;36mTuneController._schedule_trial_task.<locals>._on_result\u001b[0;34m(tracked_actor, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[39mraise\u001b[39;00m TuneError(traceback\u001b[39m.\u001b[39mformat_exc())\n",
      "\u001b[0;31mTuneError\u001b[0m: Traceback (most recent call last):\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/tune_controller.py\", line 815, in _on_result\n    on_result(trial, *args, **kwargs)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 735, in _on_training_result\n    self._process_trial_results(trial, result)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 748, in _process_trial_results\n    decision = self._process_trial_result(trial, result)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/execution/trial_runner.py\", line 805, in _process_trial_result\n    self._callbacks.on_trial_result(\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/callback.py\", line 392, in on_trial_result\n    callback.on_trial_result(**info)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/tune/logger/logger.py\", line 140, in on_trial_result\n    self.log_trial_result(iteration, trial, result)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/air/integrations/wandb.py\", line 679, in log_trial_result\n    self._trial_queues[trial].put((_QueueItem.RESULT, result))\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/util/queue.py\", line 105, in put\n    ray.get(self.actor.put.remote(item, timeout))\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/seokj/workspace/.venv/lib/python3.10/site-packages/ray/_private/worker.py\", line 2542, in get\n    raise value\nray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\nMemory on the node (IP: 172.26.215.93, ID: 6faba20d4836d4e344510bcba44f83e5a4a34faa4b825697adbcbe02) where the task (actor ID: 99142d3d2b4f740eea0b0fee01000000, name=_QueueActor.__init__, pid=248019, memory used=0.11GB) was running was 7.33GB / 7.72GB (0.950125), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 826810831838da706fdd9e667af8af7386b3dbdec590fc22c3e54f56) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.26.215.93`. To see the logs of the worker, use `ray logs worker-826810831838da706fdd9e667af8af7386b3dbdec590fc22c3e54f56*out -ip 172.26.215.93. Top 10 memory users:\nPID\tMEM(GB)\tCOMMAND\n1025\t0.39\t/home/seokj/.vscode-server/bin/6c3e3dba23e8fadc360aed75ce363ba185c49794/node /home/seokj/.vscode-ser...\n245855\t0.37\tray::FSR_Trainable\n245852\t0.37\tray::FSR_Trainable\n240754\t0.33\t/home/seokj/workspace/.venv/bin/python -m ipykernel_launcher --ip=127.0.0.1 --stdin=9003 --control=9...\n247815\t0.28\tray::IDLE\n971\t0.17\t/home/seokj/.vscode-server/bin/6c3e3dba23e8fadc360aed75ce363ba185c49794/node /home/seokj/.vscode-ser...\n246675\t0.14\tray::_WandbLoggingActor.run\n247189\t0.14\tray::_QueueActor.get\n247191\t0.14\tray::_WandbLoggingActor.run\n246668\t0.14\tray::_QueueActor.get\nRefer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39;49mfit()\n",
      "File \u001b[0;32m~/workspace/.venv/lib/python3.10/site-packages/ray/tune/tuner.py:349\u001b[0m, in \u001b[0;36mTuner.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_tuner\u001b[39m.\u001b[39mfit()\n\u001b[1;32m    348\u001b[0m     \u001b[39mexcept\u001b[39;00m TuneError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 349\u001b[0m         \u001b[39mraise\u001b[39;00m TuneError(\n\u001b[1;32m    350\u001b[0m             _TUNER_FAILED_MSG\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    351\u001b[0m                 path\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_tuner\u001b[39m.\u001b[39mget_experiment_checkpoint_dir()\n\u001b[1;32m    352\u001b[0m             )\n\u001b[1;32m    353\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m     experiment_checkpoint_dir \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mget(\n\u001b[1;32m    356\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_remote_tuner\u001b[39m.\u001b[39mget_experiment_checkpoint_dir\u001b[39m.\u001b[39mremote()\n\u001b[1;32m    357\u001b[0m     )\n",
      "\u001b[0;31mTuneError\u001b[0m: The Ray Tune run failed. Please inspect the previous error messages for a cause. After fixing the issue, you can restart the run from scratch or continue this run. To continue this run, you can use `tuner = Tuner.restore(\"/home/seokj/ray_results/FSR_Trainable_2023-08-17_11-30-03\", trainable=...)`."
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
