{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model_type = trial.suggest_categorical('model', ['fsr_model.LSTM', 'fsr_model.CNN_LSTM', 'fsr_model.ANN'])\n",
    "    if model_type == 'fsr_model.LSTM':\n",
    "        trial.suggest_int('model_args/input_size', 6, 6)\n",
    "        trial.suggest_int('model_args/output_size', 6, 6)\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model_type == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_int('model_args/input_size', 6, 6)\n",
    "        trial.suggest_int('model_args/output_size', 6, 6)\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model_type == 'fsr_model.ANN':\n",
    "        trial.suggest_int('model_args/input_size', 6, 6)\n",
    "        trial.suggest_int('model_args/output_size', 6, 6)\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [\n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-01 21:59:48,590] A new study created in memory with name: optuna\n",
      "2023-07-01 21:59:50,814\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-07-01 21:59:52,222\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-01 22:01:52</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:59.89        </td></tr>\n",
       "<tr><td>Memory:      </td><td>5.6/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=8<br>Bracket: Iter 64.000: None | Iter 32.000: -3.358985939170182 | Iter 16.000: -0.14252300679512309 | Iter 8.000: -0.1426135625786953 | Iter 4.000: -0.13111012557431279 | Iter 2.000: -0.2253700149794851 | Iter 1.000: -0.6112865285202176<br>Logical resource usage: 5.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc               </th><th>criterion       </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">   model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">  model_args/input_siz\n",
       "e</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th style=\"text-align: right;\">  model_args/num_layer</th><th style=\"text-align: right;\">  model_args/output_si\n",
       "ze</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     rmse</th><th style=\"text-align: right;\">      mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Trainable_bc8b1529</td><td>RUNNING   </td><td>172.26.215.93:1372</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.000481886</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">        98.8864 </td><td style=\"text-align: right;\">3.27737  </td><td style=\"text-align: right;\">1.28433  </td><td style=\"text-align: right;\">1.1552e+15 </td></tr>\n",
       "<tr><td>Trainable_c75c06c8</td><td>RUNNING   </td><td>172.26.215.93:1434</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">6</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.000362413</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">       100.889  </td><td style=\"text-align: right;\">0.142444 </td><td style=\"text-align: right;\">0.0941853</td><td style=\"text-align: right;\">1.58272e+14</td></tr>\n",
       "<tr><td>Trainable_0cc4113a</td><td>RUNNING   </td><td>172.26.215.93:1783</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00441937 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">        89.9398 </td><td style=\"text-align: right;\">0.0957884</td><td style=\"text-align: right;\">0.0519432</td><td style=\"text-align: right;\">3.32656e+13</td></tr>\n",
       "<tr><td>Trainable_4f7d0700</td><td>RUNNING   </td><td>172.26.215.93:3661</td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0218191  </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         8.82443</td><td style=\"text-align: right;\">0.133817 </td><td style=\"text-align: right;\">0.0755735</td><td style=\"text-align: right;\">7.32722e+13</td></tr>\n",
       "<tr><td>Trainable_7b4e6c00</td><td>PENDING   </td><td>                  </td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0357469  </td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">         </td><td style=\"text-align: right;\">           </td></tr>\n",
       "<tr><td>Trainable_780a3455</td><td>TERMINATED</td><td>172.26.215.93:1585</td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">32</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.71631e-05</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.01357</td><td style=\"text-align: right;\">4.53129  </td><td style=\"text-align: right;\">1.65917  </td><td style=\"text-align: right;\">5.45146e+14</td></tr>\n",
       "<tr><td>Trainable_1bb49743</td><td>TERMINATED</td><td>172.26.215.93:2072</td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000352269</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.81215</td><td style=\"text-align: right;\">0.224151 </td><td style=\"text-align: right;\">0.173485 </td><td style=\"text-align: right;\">3.14276e+14</td></tr>\n",
       "<tr><td>Trainable_705aef17</td><td>TERMINATED</td><td>172.26.215.93:2310</td><td>torch.nn.MSELoss</td><td>fsr_model.LSTM    </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00245669 </td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.19977</td><td style=\"text-align: right;\">0.226589 </td><td style=\"text-align: right;\">0.184866 </td><td style=\"text-align: right;\">3.38038e+14</td></tr>\n",
       "<tr><td>Trainable_33ee8e2a</td><td>TERMINATED</td><td>172.26.215.93:2520</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.000108113</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.092  </td><td style=\"text-align: right;\">4.70597  </td><td style=\"text-align: right;\">1.60251  </td><td style=\"text-align: right;\">1.26477e+14</td></tr>\n",
       "<tr><td>Trainable_04647381</td><td>TERMINATED</td><td>172.26.215.93:2736</td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     4</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.032007   </td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.19834</td><td style=\"text-align: right;\">3.2987   </td><td style=\"text-align: right;\">1.29847  </td><td style=\"text-align: right;\">1.11012e+15</td></tr>\n",
       "<tr><td>Trainable_cd18aa5f</td><td>TERMINATED</td><td>172.26.215.93:2961</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000164415</td><td>sklearn.preproc_8390</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.8711 </td><td style=\"text-align: right;\">4.71523  </td><td style=\"text-align: right;\">1.63927  </td><td style=\"text-align: right;\">1.53781e+14</td></tr>\n",
       "<tr><td>Trainable_3821056a</td><td>TERMINATED</td><td>172.26.215.93:3192</td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\"> 8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        1.73959e-05</td><td>sklearn.preproc_8330</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.05256</td><td style=\"text-align: right;\">0.307477 </td><td style=\"text-align: right;\">0.262059 </td><td style=\"text-align: right;\">6.21255e+14</td></tr>\n",
       "<tr><td>Trainable_9677c295</td><td>TERMINATED</td><td>172.26.215.93:3431</td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\"> 8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     8</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0618506  </td><td>sklearn.preproc_82d0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.86705</td><td style=\"text-align: right;\">1.03601  </td><td style=\"text-align: right;\">0.621112 </td><td style=\"text-align: right;\">1.45401    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-01 21:59:52,268\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">      mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">     rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Trainable_04647381</td><td>2023-07-01_22-00-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.29847  </td><td style=\"text-align: right;\">1.11012e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2736</td><td style=\"text-align: right;\">3.2987   </td><td style=\"text-align: right;\">             1.19834</td><td style=\"text-align: right;\">          1.19834 </td><td style=\"text-align: right;\">       1.19834</td><td style=\"text-align: right;\"> 1688216456</td><td style=\"text-align: right;\">                   1</td><td>04647381  </td></tr>\n",
       "<tr><td>Trainable_0cc4113a</td><td>2023-07-01_22-01-51</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        21</td><td style=\"text-align: right;\">0.0519432</td><td style=\"text-align: right;\">3.32656e+13</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1783</td><td style=\"text-align: right;\">0.0957884</td><td style=\"text-align: right;\">            89.9398 </td><td style=\"text-align: right;\">          5.10867 </td><td style=\"text-align: right;\">      89.9398 </td><td style=\"text-align: right;\"> 1688216511</td><td style=\"text-align: right;\">                  21</td><td>0cc4113a  </td></tr>\n",
       "<tr><td>Trainable_1bb49743</td><td>2023-07-01_22-00-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.173485 </td><td style=\"text-align: right;\">3.14276e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2072</td><td style=\"text-align: right;\">0.224151 </td><td style=\"text-align: right;\">             1.81215</td><td style=\"text-align: right;\">          0.734499</td><td style=\"text-align: right;\">       1.81215</td><td style=\"text-align: right;\"> 1688216427</td><td style=\"text-align: right;\">                   2</td><td>1bb49743  </td></tr>\n",
       "<tr><td>Trainable_33ee8e2a</td><td>2023-07-01_22-00-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.60251  </td><td style=\"text-align: right;\">1.26477e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2520</td><td style=\"text-align: right;\">4.70597  </td><td style=\"text-align: right;\">            13.092  </td><td style=\"text-align: right;\">         13.092   </td><td style=\"text-align: right;\">      13.092  </td><td style=\"text-align: right;\"> 1688216457</td><td style=\"text-align: right;\">                   1</td><td>33ee8e2a  </td></tr>\n",
       "<tr><td>Trainable_3821056a</td><td>2023-07-01_22-01-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.262059 </td><td style=\"text-align: right;\">6.21255e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 3192</td><td style=\"text-align: right;\">0.307477 </td><td style=\"text-align: right;\">             2.05256</td><td style=\"text-align: right;\">          0.783925</td><td style=\"text-align: right;\">       2.05256</td><td style=\"text-align: right;\"> 1688216480</td><td style=\"text-align: right;\">                   2</td><td>3821056a  </td></tr>\n",
       "<tr><td>Trainable_4f7d0700</td><td>2023-07-01_22-01-52</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         6</td><td style=\"text-align: right;\">0.0755735</td><td style=\"text-align: right;\">7.32722e+13</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 3661</td><td style=\"text-align: right;\">0.133817 </td><td style=\"text-align: right;\">             8.82443</td><td style=\"text-align: right;\">          1.93273 </td><td style=\"text-align: right;\">       8.82443</td><td style=\"text-align: right;\"> 1688216512</td><td style=\"text-align: right;\">                   6</td><td>4f7d0700  </td></tr>\n",
       "<tr><td>Trainable_705aef17</td><td>2023-07-01_22-00-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.184866 </td><td style=\"text-align: right;\">3.38038e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2310</td><td style=\"text-align: right;\">0.226589 </td><td style=\"text-align: right;\">             4.19977</td><td style=\"text-align: right;\">          1.96308 </td><td style=\"text-align: right;\">       4.19977</td><td style=\"text-align: right;\"> 1688216440</td><td style=\"text-align: right;\">                   2</td><td>705aef17  </td></tr>\n",
       "<tr><td>Trainable_780a3455</td><td>2023-07-01_22-00-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.65917  </td><td style=\"text-align: right;\">5.45146e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1585</td><td style=\"text-align: right;\">4.53129  </td><td style=\"text-align: right;\">             1.01357</td><td style=\"text-align: right;\">          1.01357 </td><td style=\"text-align: right;\">       1.01357</td><td style=\"text-align: right;\"> 1688216409</td><td style=\"text-align: right;\">                   1</td><td>780a3455  </td></tr>\n",
       "<tr><td>Trainable_9677c295</td><td>2023-07-01_22-01-33</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\">0.621112 </td><td style=\"text-align: right;\">1.45401    </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 3431</td><td style=\"text-align: right;\">1.03601  </td><td style=\"text-align: right;\">             2.86705</td><td style=\"text-align: right;\">          1.36489 </td><td style=\"text-align: right;\">       2.86705</td><td style=\"text-align: right;\"> 1688216493</td><td style=\"text-align: right;\">                   2</td><td>9677c295  </td></tr>\n",
       "<tr><td>Trainable_bc8b1529</td><td>2023-07-01_22-01-50</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        41</td><td style=\"text-align: right;\">1.28433  </td><td style=\"text-align: right;\">1.1552e+15 </td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1372</td><td style=\"text-align: right;\">3.27737  </td><td style=\"text-align: right;\">            98.8864 </td><td style=\"text-align: right;\">          2.95259 </td><td style=\"text-align: right;\">      98.8864 </td><td style=\"text-align: right;\"> 1688216510</td><td style=\"text-align: right;\">                  41</td><td>bc8b1529  </td></tr>\n",
       "<tr><td>Trainable_c75c06c8</td><td>2023-07-01_22-01-49</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        26</td><td style=\"text-align: right;\">0.0941853</td><td style=\"text-align: right;\">1.58272e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 1434</td><td style=\"text-align: right;\">0.142444 </td><td style=\"text-align: right;\">           100.889  </td><td style=\"text-align: right;\">          4.14315 </td><td style=\"text-align: right;\">     100.889  </td><td style=\"text-align: right;\"> 1688216509</td><td style=\"text-align: right;\">                  26</td><td>c75c06c8  </td></tr>\n",
       "<tr><td>Trainable_cd18aa5f</td><td>2023-07-01_22-01-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">1.63927  </td><td style=\"text-align: right;\">1.53781e+14</td><td>172.26.215.93</td><td style=\"text-align: right;\"> 2961</td><td style=\"text-align: right;\">4.71523  </td><td style=\"text-align: right;\">             2.8711 </td><td style=\"text-align: right;\">          2.8711  </td><td style=\"text-align: right;\">       2.8711 </td><td style=\"text-align: right;\"> 1688216468</td><td style=\"text-align: right;\">                   1</td><td>cd18aa5f  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_bc8b1529_1_criterion=torch_nn_MSELoss,model=fsr_model_CNN_LSTM,cnn_hidden_size=8,cnn_num_layer=3,input_size=6,lstm_hidde_2023-07-01_21-59-52/wandb/run-20230701_220003-bc8b1529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: Syncing run Trainable_bc8b1529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1433)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bc8b1529\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1583)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c75c06c8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1583)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1583)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1583)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1583)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1583)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1583)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_780a3455_3_criterion=torch_nn_MSELoss,model=fsr_model_ANN,hidden_size=32,input_size=6,num_layer=4,output_size=6,optimize_2023-07-01_22-00-01/wandb/run-20230701_220014-780a3455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Syncing run Trainable_780a3455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/780a3455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                      mae 1.65917\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                     mape 545146007756314.9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                     rmse 4.53129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:       time_since_restore 1.01357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:         time_this_iter_s 1.01357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:             time_total_s 1.01357\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:                timestamp 1688216409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: 🚀 View run Trainable_780a3455 at: https://wandb.ai/seokjin/FSR-prediction/runs/780a3455\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1781)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220014-780a3455/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1941)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1941)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1941)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_0cc4113a_4_criterion=torch_nn_MSELoss,model=fsr_model_CNN_LSTM,cnn_hidden_size=128,cnn_num_layer=1,input_size=6,lstm_hid_2023-07-01_22-00-08/wandb/run-20230701_220021-0cc4113a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1941)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1941)\u001b[0m wandb: Syncing run Trainable_0cc4113a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1941)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=1941)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0cc4113a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_1bb49743_5_criterion=torch_nn_MSELoss,model=fsr_model_ANN,hidden_size=16,input_size=6,num_layer=2,output_size=6,optimize_2023-07-01_22-00-15/wandb/run-20230701_220030-1bb49743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Syncing run Trainable_1bb49743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1bb49743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                      mae 0.17348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                     mape 314275987487689.94\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                     rmse 0.22415\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:       time_since_restore 1.81215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:         time_this_iter_s 0.7345\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:             time_total_s 1.81215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:                timestamp 1688216427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: 🚀 View run Trainable_1bb49743 at: https://wandb.ai/seokjin/FSR-prediction/runs/1bb49743\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2180)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220030-1bb49743/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_705aef17_6_criterion=torch_nn_MSELoss,model=fsr_model_LSTM,hidden_size=16,input_size=6,num_layer=4,output_size=6,optimiz_2023-07-01_22-00-23/wandb/run-20230701_220041-705aef17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Syncing run Trainable_705aef17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/705aef17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                      mae 0.18487\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                     mape 338038173521178.8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                     rmse 0.22659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:       time_since_restore 4.19977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:         time_this_iter_s 1.96308\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:             time_total_s 4.19977\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:                timestamp 1688216440\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: 🚀 View run Trainable_705aef17 at: https://wandb.ai/seokjin/FSR-prediction/runs/705aef17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2398)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220041-705aef17/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_33ee8e2a_7_criterion=torch_nn_MSELoss,model=fsr_model_CNN_LSTM,cnn_hidden_size=8,cnn_num_layer=6,input_size=6,lstm_hidde_2023-07-01_22-00-34/wandb/run-20230701_220052-33ee8e2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Syncing run Trainable_33ee8e2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/33ee8e2a\n",
      "2023-07-01 22:00:58,643\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.285 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:00:58,649\tWARNING util.py:315 -- The `process_trial_result` operation took 2.292 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:00:58,651\tWARNING util.py:315 -- Processing trial results took 2.293 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-01 22:00:58,653\tWARNING util.py:315 -- The `process_trial_result` operation took 2.295 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:00,778\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.036 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:00,781\tWARNING util.py:315 -- The `process_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:00,783\tWARNING util.py:315 -- Processing trial results took 2.042 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-01 22:01:00,786\tWARNING util.py:315 -- The `process_trial_result` operation took 2.045 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_04647381_8_criterion=torch_nn_MSELoss,model=fsr_model_ANN,hidden_size=16,input_size=6,num_layer=4,output_size=6,optimize_2023-07-01_22-00-44/wandb/run-20230701_220101-04647381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Syncing run Trainable_04647381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/04647381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                      mae 1.60251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                     mape 126477440380932.39\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                     rmse 4.70597\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:       time_since_restore 13.09202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:         time_this_iter_s 13.09202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:             time_total_s 13.09202\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:                timestamp 1688216457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: 🚀 View run Trainable_33ee8e2a at: https://wandb.ai/seokjin/FSR-prediction/runs/33ee8e2a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2624)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220052-33ee8e2a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220101-04647381/logs\n",
      "2023-07-01 22:01:11,052\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.372 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:11,056\tWARNING util.py:315 -- The `process_trial_result` operation took 2.376 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:11,063\tWARNING util.py:315 -- Processing trial results took 2.383 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-01 22:01:11,068\tWARNING util.py:315 -- The `process_trial_result` operation took 2.388 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=2832)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_cd18aa5f_9_criterion=torch_nn_MSELoss,model=fsr_model_CNN_LSTM,cnn_hidden_size=16,cnn_num_layer=3,input_size=6,lstm_hidd_2023-07-01_22-00-55/wandb/run-20230701_220113-cd18aa5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Syncing run Trainable_cd18aa5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cd18aa5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                      mae 1.63927\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                     mape 153781234894976.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                     rmse 4.71523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:       time_since_restore 2.8711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:         time_this_iter_s 2.8711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:             time_total_s 2.8711\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:                timestamp 1688216468\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: 🚀 View run Trainable_cd18aa5f at: https://wandb.ai/seokjin/FSR-prediction/runs/cd18aa5f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220113-cd18aa5f/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3076)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-07-01 22:01:19,898\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.087 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:19,904\tWARNING util.py:315 -- The `process_trial_result` operation took 3.095 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:19,906\tWARNING util.py:315 -- Processing trial results took 3.097 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-01 22:01:19,908\tWARNING util.py:315 -- The `process_trial_result` operation took 3.099 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_3821056a_10_criterion=torch_nn_MSELoss,model=fsr_model_ANN,hidden_size=8,input_size=6,num_layer=2,output_size=6,optimize_2023-07-01_22-01-05/wandb/run-20230701_220123-3821056a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Syncing run Trainable_3821056a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3821056a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                      mae 0.26206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                     mape 621255131883481.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                     rmse 0.30748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:       time_since_restore 2.05256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:         time_this_iter_s 0.78393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:             time_total_s 2.05256\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:                timestamp 1688216480\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: 🚀 View run Trainable_3821056a at: https://wandb.ai/seokjin/FSR-prediction/runs/3821056a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3293)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220123-3821056a/logs\n",
      "2023-07-01 22:01:31,871\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:31,874\tWARNING util.py:315 -- The `process_trial_result` operation took 2.253 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:31,876\tWARNING util.py:315 -- Processing trial results took 2.254 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-01 22:01:31,878\tWARNING util.py:315 -- The `process_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_9677c295_11_criterion=torch_nn_MSELoss,model=fsr_model_ANN,hidden_size=8,input_size=6,num_layer=8,output_size=6,optimize_2023-07-01_22-01-15/wandb/run-20230701_220135-9677c295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Syncing run Trainable_9677c295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9677c295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                      mae 0.62111\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                     mape 1.45401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                     rmse 1.03601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:       time_since_restore 2.86705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:         time_this_iter_s 1.36489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:             time_total_s 2.86705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:                timestamp 1688216493\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: 🚀 View run Trainable_9677c295 at: https://wandb.ai/seokjin/FSR-prediction/runs/9677c295\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3525)\u001b[0m wandb: Find logs at: ./wandb/run-20230701_220135-9677c295/logs\n",
      "2023-07-01 22:01:45,149\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.326 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:45,152\tWARNING util.py:315 -- The `process_trial_result` operation took 2.330 s, which may be a performance bottleneck.\n",
      "2023-07-01 22:01:45,156\tWARNING util.py:315 -- Processing trial results took 2.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-01 22:01:45,159\tWARNING util.py:315 -- The `process_trial_result` operation took 2.338 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3808)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3808)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3808)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-01_21-59-48/Trainable_4f7d0700_12_criterion=torch_nn_MSELoss,model=fsr_model_ANN,hidden_size=32,input_size=6,num_layer=8,output_size=6,optimiz_2023-07-01_22-01-28/wandb/run-20230701_220148-4f7d0700\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3808)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3808)\u001b[0m wandb: Syncing run Trainable_4f7d0700\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3808)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=3808)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4f7d0700\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "import datasource\n",
    "from trainable import Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_parameters(Trainable, data=datasource.get_data()),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=-1,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ),\n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ") \n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
