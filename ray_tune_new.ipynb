{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model_type = trial.suggest_categorical('model', ['fsr_model.LSTM', 'fsr_model.CNN_LSTM', 'fsr_model.ANN'])\n",
    "    if model_type == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model_type == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model_type == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    trial.suggest_categorical('scaler', [\n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-07-02 04:23:04,111] A new study created in memory with name: optuna\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-02 04:25:21</td></tr>\n",
       "<tr><td>Running for: </td><td>00:02:17.64        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.4/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=4<br>Bracket: Iter 64.000: -65398.960562665656 | Iter 32.000: -66870.26504828669 | Iter 16.000: -67677.70038079677 | Iter 8.000: -68115.77127512867 | Iter 4.000: -65162.41798957201 | Iter 2.000: -66671.18292043159 | Iter 1.000: -81115.43038019097<br>Logical resource usage: 5.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">   model_args/hidden_si\n",
       "ze</th><th style=\"text-align: right;\">   model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th style=\"text-align: right;\">  model_args/num_layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">        mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Trainable_bc0d4e96</td><td>RUNNING   </td><td>172.26.215.93:162685</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">                      </td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00322906 </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        3.55873 </td><td style=\"text-align: right;\"> 63552.7</td><td style=\"text-align: right;\"> 25885.6</td><td style=\"text-align: right;\">33.2739     </td></tr>\n",
       "<tr><td>Trainable_569dbc2d</td><td>PENDING   </td><td>                    </td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0296262  </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">            </td></tr>\n",
       "<tr><td>Trainable_79a0a71b</td><td>TERMINATED</td><td>172.26.215.93:161906</td><td>torch.nn.MSELoss</td><td>fsr_model.LSTM    </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">16</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     4</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0197608  </td><td>sklearn.preproc_cab0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">       90.4757  </td><td style=\"text-align: right;\"> 63913.7</td><td style=\"text-align: right;\"> 26607.5</td><td style=\"text-align: right;\">66.8692     </td></tr>\n",
       "<tr><td>Trainable_8f8decb8</td><td>TERMINATED</td><td>172.26.215.93:162153</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">                      </td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000151045</td><td>sklearn.preproc_cb70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        2.30414 </td><td style=\"text-align: right;\">595574  </td><td style=\"text-align: right;\">238298  </td><td style=\"text-align: right;\"> 1.3376e+17 </td></tr>\n",
       "<tr><td>Trainable_1148c251</td><td>TERMINATED</td><td>172.26.215.93:162320</td><td>torch.nn.MSELoss</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\">64</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">                      </td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0111682  </td><td>sklearn.preproc_cb10</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        8.84129 </td><td style=\"text-align: right;\"> 80658.1</td><td style=\"text-align: right;\"> 27039.2</td><td style=\"text-align: right;\"> 1.68783e+18</td></tr>\n",
       "<tr><td>Trainable_792c775a</td><td>TERMINATED</td><td>172.26.215.93:162528</td><td>torch.nn.MSELoss</td><td>fsr_model.ANN     </td><td style=\"text-align: right;\">   </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\"> 8</td><td style=\"text-align: right;\">  </td><td style=\"text-align: right;\"> </td><td style=\"text-align: right;\">                     5</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0132239  </td><td>sklearn.preproc_cb70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        0.800435</td><td style=\"text-align: right;\">294226  </td><td style=\"text-align: right;\">147143  </td><td style=\"text-align: right;\"> 1.05496e+20</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 04:23:04,129\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name        </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">        mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">    rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Trainable_1148c251</td><td>2023-07-02_04-25-02</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 27039.2</td><td style=\"text-align: right;\"> 1.68783e+18</td><td>172.26.215.93</td><td style=\"text-align: right;\">162320</td><td style=\"text-align: right;\"> 80658.1</td><td style=\"text-align: right;\">            8.84129 </td><td style=\"text-align: right;\">          4.01877 </td><td style=\"text-align: right;\">      8.84129 </td><td style=\"text-align: right;\"> 1688239502</td><td style=\"text-align: right;\">                   2</td><td>1148c251  </td></tr>\n",
       "<tr><td>Trainable_792c775a</td><td>2023-07-02_04-25-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">147143  </td><td style=\"text-align: right;\"> 1.05496e+20</td><td>172.26.215.93</td><td style=\"text-align: right;\">162528</td><td style=\"text-align: right;\">294226  </td><td style=\"text-align: right;\">            0.800435</td><td style=\"text-align: right;\">          0.800435</td><td style=\"text-align: right;\">      0.800435</td><td style=\"text-align: right;\"> 1688239509</td><td style=\"text-align: right;\">                   1</td><td>792c775a  </td></tr>\n",
       "<tr><td>Trainable_79a0a71b</td><td>2023-07-02_04-24-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 26607.5</td><td style=\"text-align: right;\">66.8692     </td><td>172.26.215.93</td><td style=\"text-align: right;\">161906</td><td style=\"text-align: right;\"> 63913.7</td><td style=\"text-align: right;\">           90.4757  </td><td style=\"text-align: right;\">          0.863655</td><td style=\"text-align: right;\">     90.4757  </td><td style=\"text-align: right;\"> 1688239480</td><td style=\"text-align: right;\">                 100</td><td>79a0a71b  </td></tr>\n",
       "<tr><td>Trainable_8f8decb8</td><td>2023-07-02_04-24-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">238298  </td><td style=\"text-align: right;\"> 1.3376e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">162153</td><td style=\"text-align: right;\">595574  </td><td style=\"text-align: right;\">            2.30414 </td><td style=\"text-align: right;\">          2.30414 </td><td style=\"text-align: right;\">      2.30414 </td><td style=\"text-align: right;\"> 1688239486</td><td style=\"text-align: right;\">                   1</td><td>8f8decb8  </td></tr>\n",
       "<tr><td>Trainable_bc0d4e96</td><td>2023-07-02_04-25-17</td><td>False </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 25350.6</td><td style=\"text-align: right;\">30.8426     </td><td>172.26.215.93</td><td style=\"text-align: right;\">162685</td><td style=\"text-align: right;\"> 62214  </td><td style=\"text-align: right;\">            2.18227 </td><td style=\"text-align: right;\">          2.18227 </td><td style=\"text-align: right;\">      2.18227 </td><td style=\"text-align: right;\"> 1688239517</td><td style=\"text-align: right;\">                   1</td><td>bc0d4e96  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 04:23:10,309\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:23:10,312\tWARNING util.py:315 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:23:10,314\tWARNING util.py:315 -- Processing trial results took 1.344 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-02 04:23:10,316\tWARNING util.py:315 -- The `process_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-02_04-23-04/Trainable_79a0a71b_1_criterion=torch_nn_MSELoss,model=fsr_model_LSTM,hidden_size=16,num_layer=4,optimizer=torch_optim_NAdam,lr=0.0_2023-07-02_04-23-04/wandb/run-20230702_042311-79a0a71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Syncing run Trainable_79a0a71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/79a0a71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                      mae ▃▄█▂▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                     mape ▃▅█▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                     rmse ▇▄▁███▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:         time_this_iter_s █▂▁▁▁▁▁▁▂▂▁▂▂▁▃▂▂▂▂▂▁▁▂▂▁▃▂▁▁▂▂▂▁▁▁▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                      mae 26607.53285\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                     mape 66.86918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                     rmse 63913.74475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:       time_since_restore 90.47571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:         time_this_iter_s 0.86365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:             time_total_s 90.47571\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:                timestamp 1688239480\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: 🚀 View run Trainable_79a0a71b at: https://wandb.ai/seokjin/FSR-prediction/runs/79a0a71b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161957)\u001b[0m wandb: Find logs at: ./wandb/run-20230702_042311-79a0a71b/logs\n",
      "2023-07-02 04:24:47,873\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:24:47,876\tWARNING util.py:315 -- The `process_trial_result` operation took 1.394 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:24:47,879\tWARNING util.py:315 -- Processing trial results took 1.397 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-02 04:24:47,881\tWARNING util.py:315 -- The `process_trial_result` operation took 1.399 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-02_04-23-04/Trainable_8f8decb8_2_criterion=torch_nn_MSELoss,model=fsr_model_CNN_LSTM,cnn_hidden_size=16,cnn_num_layer=6,lstm_hidden_size=64,ls_2023-07-02_04-23-07/wandb/run-20230702_042448-8f8decb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Syncing run Trainable_8f8decb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8f8decb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                      mae 238297.99158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                     mape 1.337604954497417e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                     rmse 595574.14776\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:       time_since_restore 2.30414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:         time_this_iter_s 2.30414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:             time_total_s 2.30414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:                timestamp 1688239486\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: 🚀 View run Trainable_8f8decb8 at: https://wandb.ai/seokjin/FSR-prediction/runs/8f8decb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162206)\u001b[0m wandb: Find logs at: ./wandb/run-20230702_042448-8f8decb8/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-02_04-23-04/Trainable_1148c251_3_criterion=torch_nn_MSELoss,model=fsr_model_CNN_LSTM,cnn_hidden_size=32,cnn_num_layer=8,lstm_hidden_size=64,ls_2023-07-02_04-24-44/wandb/run-20230702_042457-1148c251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Syncing run Trainable_1148c251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1148c251\n",
      "2023-07-02 04:24:58,896\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:24:58,900\tWARNING util.py:315 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:24:58,901\tWARNING util.py:315 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-02 04:24:58,903\tWARNING util.py:315 -- The `process_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                      mae 27039.21409\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                     mape 1.68783102408694e+18\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                     rmse 80658.0512\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:       time_since_restore 8.84129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:         time_this_iter_s 4.01877\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:             time_total_s 8.84129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:                timestamp 1688239502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: 🚀 View run Trainable_1148c251 at: https://wandb.ai/seokjin/FSR-prediction/runs/1148c251\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162387)\u001b[0m wandb: Find logs at: ./wandb/run-20230702_042457-1148c251/logs\n",
      "2023-07-02 04:25:10,702\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.674 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:25:10,707\tWARNING util.py:315 -- The `process_trial_result` operation took 1.680 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:25:10,709\tWARNING util.py:315 -- Processing trial results took 1.682 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-02 04:25:10,711\tWARNING util.py:315 -- The `process_trial_result` operation took 1.684 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-02_04-23-04/Trainable_792c775a_4_criterion=torch_nn_MSELoss,model=fsr_model_ANN,hidden_size=8,num_layer=5,optimizer=torch_optim_Adam,lr=0.0132_2023-07-02_04-24-52/wandb/run-20230702_042512-792c775a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Syncing run Trainable_792c775a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/792c775a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                      mae 147142.65751\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                     mape 1.0549588907237666e+20\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                     rmse 294225.66503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:       time_since_restore 0.80044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:         time_this_iter_s 0.80044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:             time_total_s 0.80044\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:                timestamp 1688239509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: 🚀 View run Trainable_792c775a at: https://wandb.ai/seokjin/FSR-prediction/runs/792c775a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162585)\u001b[0m wandb: Find logs at: ./wandb/run-20230702_042512-792c775a/logs\n",
      "2023-07-02 04:25:19,338\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.458 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:25:19,341\tWARNING util.py:315 -- The `process_trial_result` operation took 1.462 s, which may be a performance bottleneck.\n",
      "2023-07-02 04:25:19,342\tWARNING util.py:315 -- Processing trial results took 1.463 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-07-02 04:25:19,343\tWARNING util.py:315 -- The `process_trial_result` operation took 1.464 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162762)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162762)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162762)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/Trainable_2023-07-02_04-23-04/Trainable_bc0d4e96_5_criterion=torch_nn_MSELoss,model=fsr_model_CNN_LSTM,cnn_hidden_size=128,cnn_num_layer=4,lstm_hidden_size=16,l_2023-07-02_04-25-08/wandb/run-20230702_042520-bc0d4e96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162762)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162762)\u001b[0m wandb: Syncing run Trainable_bc0d4e96\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162762)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162762)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bc0d4e96\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "import datasource\n",
    "from trainable import Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        ray.tune.with_parameters(Trainable, data=datasource.get_data()),\n",
    "        {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=-1,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ),\n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ),\n",
    ") \n",
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
