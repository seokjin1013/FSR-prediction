{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task4\n",
    "\n",
    "Index_X = FSR_for_force, FSR_for_coord\n",
    "\n",
    "Index_y = force, x_coord, y_coord\n",
    "\n",
    "Data = Splited by Subject\n",
    "\n",
    "## Run result\n",
    "\n",
    "https://wandb.ai/seokjin/FSR-prediction/groups/FSR_Trainable_2023-08-07_11-13-30/workspace?workspace=user-seokjin\n",
    "\n",
    "## Experiment id\n",
    "\n",
    "FSR_Trainable_2023-08-07_11-13-30\n",
    "\n",
    "## Best metric (RMSE)\n",
    "\n",
    "103.112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_searchspace(trial):\n",
    "    model = trial.suggest_categorical('model', ['fsr_model.CNN_LSTM'])\n",
    "    if model == 'fsr_model.LSTM':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.CNN_LSTM':\n",
    "        trial.suggest_categorical('model_args/cnn_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_categorical('model_args/lstm_hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/cnn_num_layer', 1, 8)\n",
    "        trial.suggest_int('model_args/lstm_num_layer', 1, 8)\n",
    "    elif model == 'fsr_model.ANN':\n",
    "        trial.suggest_categorical('model_args/hidden_size', [8, 16, 32, 64, 128])\n",
    "        trial.suggest_int('model_args/num_layer', 1, 8)\n",
    "    trial.suggest_categorical('criterion', ['torch.nn.MSELoss'])\n",
    "    trial.suggest_categorical('optimizer', [\n",
    "        'torch.optim.Adam',\n",
    "        'torch.optim.NAdam',\n",
    "        'torch.optim.Adagrad',\n",
    "        'torch.optim.RAdam',\n",
    "        'torch.optim.SGD',\n",
    "    ])\n",
    "    trial.suggest_float('optimizer_args/lr', 1e-5, 1e-1, log=True)\n",
    "    imputer = trial.suggest_categorical('imputer', ['sklearn.impute.SimpleImputer'])\n",
    "    if imputer == 'sklearn.impute.SimpleImputer':\n",
    "        trial.suggest_categorical('imputer_args/strategy', [\n",
    "            'mean',\n",
    "            'median',\n",
    "        ])\n",
    "    trial.suggest_categorical('scaler', [ \n",
    "        'sklearn.preprocessing.StandardScaler',\n",
    "        'sklearn.preprocessing.MinMaxScaler',\n",
    "        'sklearn.preprocessing.RobustScaler',\n",
    "    ])\n",
    "    return {\n",
    "        'index_X': ['FSR_for_force', 'FSR_for_coord'],\n",
    "        'index_y': ['force', 'x_coord', 'y_coord'],\n",
    "        'data_loader': 'fsr_data.get_index_splited_by_subject'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-07 11:13:30,239] A new study created in memory with name: optuna\n"
     ]
    }
   ],
   "source": [
    "import ray.tune\n",
    "import ray.air\n",
    "import ray.air.integrations.wandb\n",
    "import ray.tune.schedulers\n",
    "from fsr_trainable import FSR_Trainable\n",
    "import ray.tune.search\n",
    "import ray.tune.search.optuna\n",
    "\n",
    "tuner = ray.tune.Tuner(\n",
    "    trainable=ray.tune.with_resources(\n",
    "        FSR_Trainable, {'cpu':2},\n",
    "    ),\n",
    "    tune_config=ray.tune.TuneConfig(\n",
    "        num_samples=100,\n",
    "        scheduler=ray.tune.schedulers.ASHAScheduler(\n",
    "            max_t=100,\n",
    "            grace_period=1,\n",
    "            reduction_factor=2,\n",
    "            brackets=1,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "        search_alg=ray.tune.search.optuna.OptunaSearch(\n",
    "            space=define_searchspace,\n",
    "            metric='rmse',\n",
    "            mode='min',\n",
    "        ),\n",
    "    ), \n",
    "    run_config=ray.air.RunConfig(\n",
    "        callbacks=[\n",
    "            ray.air.integrations.wandb.WandbLoggerCallback(project='FSR-prediction'),\n",
    "        ],\n",
    "        checkpoint_config=ray.air.CheckpointConfig(\n",
    "            num_to_keep=3,\n",
    "            checkpoint_score_attribute='rmse',\n",
    "            checkpoint_score_order='min',\n",
    "            checkpoint_frequency=5,\n",
    "            checkpoint_at_end=True,\n",
    "        ),\n",
    "    ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 11:13:32,417\tINFO worker.py:1627 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2023-08-07 11:13:33,650\tINFO tune.py:226 -- Initializing Ray automatically. For cluster usage or custom Ray initialization, call `ray.init(...)` before `Tuner(...)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-07 11:35:35</td></tr>\n",
       "<tr><td>Running for: </td><td>00:22:02.14        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.0/7.7 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=100<br>Bracket: Iter 64.000: -127.98757841640511 | Iter 32.000: -139.89534319830688 | Iter 16.000: -155.28835472113775 | Iter 8.000: -168.57523500303154 | Iter 4.000: -191.71381579884402 | Iter 2.000: -196.6752479691787 | Iter 1.000: -214.0090900771087<br>Logical resource usage: 2.0/8 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                 </th><th>criterion       </th><th>data_loader         </th><th>imputer             </th><th>imputer_args/strateg\n",
       "y       </th><th>index_X             </th><th>index_y             </th><th>model             </th><th style=\"text-align: right;\">    model_args/cnn_hidde\n",
       "n_size</th><th style=\"text-align: right;\">  model_args/cnn_num_l\n",
       "ayer</th><th style=\"text-align: right;\">    model_args/lstm_hidd\n",
       "en_size</th><th style=\"text-align: right;\">  model_args/lstm_num_\n",
       "layer</th><th>optimizer          </th><th style=\"text-align: right;\">  optimizer_args/lr</th><th>scaler              </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_8b662eb9</td><td>TERMINATED</td><td>172.26.215.93:142769</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_8340</td><td>[&#x27;force&#x27;, &#x27;x_co_7c80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00111928 </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        49.9887 </td><td style=\"text-align: right;\">230.878</td><td style=\"text-align: right;\"> 71.3565</td><td style=\"text-align: right;\">6.71329e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_51c64226</td><td>TERMINATED</td><td>172.26.215.93:142841</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_1300</td><td>[&#x27;force&#x27;, &#x27;x_co_08c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0939614  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.54588</td><td style=\"text-align: right;\">234.757</td><td style=\"text-align: right;\"> 75.7422</td><td style=\"text-align: right;\">3.16651e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8aa78cb2</td><td>TERMINATED</td><td>172.26.215.93:143014</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_ac00</td><td>[&#x27;force&#x27;, &#x27;x_co_e040</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">6</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        1.97871e-05</td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         5.97125</td><td style=\"text-align: right;\">326.46 </td><td style=\"text-align: right;\">106.919 </td><td style=\"text-align: right;\">1.1684e+17 </td></tr>\n",
       "<tr><td>FSR_Trainable_fc273135</td><td>TERMINATED</td><td>172.26.215.93:143189</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_6780</td><td>[&#x27;force&#x27;, &#x27;x_co_00c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0510534  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        78.2125 </td><td style=\"text-align: right;\">113.994</td><td style=\"text-align: right;\"> 36.3497</td><td style=\"text-align: right;\">3.91223e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_c1882557</td><td>TERMINATED</td><td>172.26.215.93:143513</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_74c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6d80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.00012106 </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.58532</td><td style=\"text-align: right;\">226.414</td><td style=\"text-align: right;\"> 67.3359</td><td style=\"text-align: right;\">2.68211e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_6eda6ff5</td><td>TERMINATED</td><td>172.26.215.93:143737</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_2280</td><td>[&#x27;force&#x27;, &#x27;x_co_0dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        4.6159e-05 </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        21.454  </td><td style=\"text-align: right;\">214.435</td><td style=\"text-align: right;\"> 63.1268</td><td style=\"text-align: right;\">4.70628e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_6017eba4</td><td>TERMINATED</td><td>172.26.215.93:143952</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_8180</td><td>[&#x27;force&#x27;, &#x27;x_co_7140</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">6</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0139362  </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.99962</td><td style=\"text-align: right;\">282.144</td><td style=\"text-align: right;\"> 81.3893</td><td style=\"text-align: right;\">7.14183e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_861e35b5</td><td>TERMINATED</td><td>172.26.215.93:144173</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_b380</td><td>[&#x27;force&#x27;, &#x27;x_co_c740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0821421  </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        76.556  </td><td style=\"text-align: right;\">222.668</td><td style=\"text-align: right;\"> 69.4518</td><td style=\"text-align: right;\">3.60947e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_befdffb8</td><td>TERMINATED</td><td>172.26.215.93:144401</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_2300</td><td>[&#x27;force&#x27;, &#x27;x_co_1c00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">7</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00722838 </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.28566</td><td style=\"text-align: right;\">228.625</td><td style=\"text-align: right;\"> 68.5896</td><td style=\"text-align: right;\">6.05069e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_8da6b371</td><td>TERMINATED</td><td>172.26.215.93:144606</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_3ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_1980</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.92415e-05</td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.11444</td><td style=\"text-align: right;\">229.585</td><td style=\"text-align: right;\"> 92.3958</td><td style=\"text-align: right;\">1.77692e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_b9b7e0cd</td><td>TERMINATED</td><td>172.26.215.93:144837</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_3a40</td><td>[&#x27;force&#x27;, &#x27;x_co_69c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        1.34664e-05</td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.89942</td><td style=\"text-align: right;\">338.23 </td><td style=\"text-align: right;\"> 98.2116</td><td style=\"text-align: right;\">8.46198e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_8f6e4462</td><td>TERMINATED</td><td>172.26.215.93:145062</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_dac0</td><td>[&#x27;force&#x27;, &#x27;x_co_7e80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        2.25935e-05</td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.83379</td><td style=\"text-align: right;\">227.898</td><td style=\"text-align: right;\"> 66.9925</td><td style=\"text-align: right;\">2.19573e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_56483a0a</td><td>TERMINATED</td><td>172.26.215.93:145289</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_ac80</td><td>[&#x27;force&#x27;, &#x27;x_co_e900</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0935217  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        26.5212 </td><td style=\"text-align: right;\">207.37 </td><td style=\"text-align: right;\"> 59.8602</td><td style=\"text-align: right;\">1.22287e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_b6316a83</td><td>TERMINATED</td><td>172.26.215.93:145516</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_e6c0</td><td>[&#x27;force&#x27;, &#x27;x_co_ea40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.000231815</td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.01019</td><td style=\"text-align: right;\">230.397</td><td style=\"text-align: right;\"> 82.0262</td><td style=\"text-align: right;\">3.62778e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_9f743014</td><td>TERMINATED</td><td>172.26.215.93:145747</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_e2c0</td><td>[&#x27;force&#x27;, &#x27;x_co_c600</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00024692 </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         3.3045 </td><td style=\"text-align: right;\">220.793</td><td style=\"text-align: right;\"> 77.1827</td><td style=\"text-align: right;\">4.91614e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_6bb5c72e</td><td>TERMINATED</td><td>172.26.215.93:145972</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_d000</td><td>[&#x27;force&#x27;, &#x27;x_co_e700</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00152357 </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        32.6362 </td><td style=\"text-align: right;\">222.297</td><td style=\"text-align: right;\"> 76.4019</td><td style=\"text-align: right;\">1.46052e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_cc7d2bfc</td><td>TERMINATED</td><td>172.26.215.93:146198</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_2600</td><td>[&#x27;force&#x27;, &#x27;x_co_a080</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0272456  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        73.0608 </td><td style=\"text-align: right;\">189.682</td><td style=\"text-align: right;\"> 58.6084</td><td style=\"text-align: right;\">1.77314e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d4678cd6</td><td>TERMINATED</td><td>172.26.215.93:146411</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_7680</td><td>[&#x27;force&#x27;, &#x27;x_co_cc40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0313594  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        34.6343 </td><td style=\"text-align: right;\">209.983</td><td style=\"text-align: right;\"> 67.3152</td><td style=\"text-align: right;\">2.43082e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_48f428e2</td><td>TERMINATED</td><td>172.26.215.93:146620</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_6ac0</td><td>[&#x27;force&#x27;, &#x27;x_co_b280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0256585  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        76.3978 </td><td style=\"text-align: right;\">207.925</td><td style=\"text-align: right;\"> 67.3642</td><td style=\"text-align: right;\">2.85892e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1b594dfc</td><td>TERMINATED</td><td>172.26.215.93:146882</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_ca00</td><td>[&#x27;force&#x27;, &#x27;x_co_6280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0249718  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        90.8048 </td><td style=\"text-align: right;\">201.093</td><td style=\"text-align: right;\"> 62.6811</td><td style=\"text-align: right;\">1.69669e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_00a89f50</td><td>TERMINATED</td><td>172.26.215.93:147116</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_2640</td><td>[&#x27;force&#x27;, &#x27;x_co_3e80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.024874   </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        19.4953 </td><td style=\"text-align: right;\">216.38 </td><td style=\"text-align: right;\"> 67.0777</td><td style=\"text-align: right;\">1.55196e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_378dbc6f</td><td>TERMINATED</td><td>172.26.215.93:147350</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_bec0</td><td>[&#x27;force&#x27;, &#x27;x_co_8c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00706419 </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         8.1365 </td><td style=\"text-align: right;\">227.794</td><td style=\"text-align: right;\"> 72.8363</td><td style=\"text-align: right;\">1.30996e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_a5f78c90</td><td>TERMINATED</td><td>172.26.215.93:147529</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d0c0</td><td>[&#x27;force&#x27;, &#x27;x_co_d980</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00567272 </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        10.4907 </td><td style=\"text-align: right;\">220.759</td><td style=\"text-align: right;\"> 78.3836</td><td style=\"text-align: right;\">1.54702e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_e8e314b4</td><td>TERMINATED</td><td>172.26.215.93:147744</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_6dc0</td><td>[&#x27;force&#x27;, &#x27;x_co_b200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">8</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00416048 </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.53901</td><td style=\"text-align: right;\">229.222</td><td style=\"text-align: right;\"> 79.476 </td><td style=\"text-align: right;\">1.58804e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_5a041cb2</td><td>TERMINATED</td><td>172.26.215.93:147976</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d680</td><td>[&#x27;force&#x27;, &#x27;x_co_cc00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0856287  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        67.6822 </td><td style=\"text-align: right;\">192.83 </td><td style=\"text-align: right;\"> 59.9089</td><td style=\"text-align: right;\">1.66336e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_80e6373b</td><td>TERMINATED</td><td>172.26.215.93:148205</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_f600</td><td>[&#x27;force&#x27;, &#x27;x_co_c680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0671193  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.82645</td><td style=\"text-align: right;\">229.624</td><td style=\"text-align: right;\"> 81.7446</td><td style=\"text-align: right;\">4.563e+07  </td></tr>\n",
       "<tr><td>FSR_Trainable_147eba8a</td><td>TERMINATED</td><td>172.26.215.93:148421</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_e840</td><td>[&#x27;force&#x27;, &#x27;x_co_f7c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0504099  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        15.4134 </td><td style=\"text-align: right;\">226.448</td><td style=\"text-align: right;\"> 65.1665</td><td style=\"text-align: right;\">1.00398e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ccb957dd</td><td>TERMINATED</td><td>172.26.215.93:148642</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d6c0</td><td>[&#x27;force&#x27;, &#x27;x_co_d100</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0399957  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        13.3721 </td><td style=\"text-align: right;\">234.244</td><td style=\"text-align: right;\"> 83.3859</td><td style=\"text-align: right;\">6.80341e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d8e95a57</td><td>TERMINATED</td><td>172.26.215.93:148869</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_5040</td><td>[&#x27;force&#x27;, &#x27;x_co_cac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.039416   </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.28883</td><td style=\"text-align: right;\">220.657</td><td style=\"text-align: right;\"> 67.318 </td><td style=\"text-align: right;\">2.30332e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_266f4215</td><td>TERMINATED</td><td>172.26.215.93:149088</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_f000</td><td>[&#x27;force&#x27;, &#x27;x_co_81c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0178794  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       120.842  </td><td style=\"text-align: right;\">147.573</td><td style=\"text-align: right;\"> 45.2299</td><td style=\"text-align: right;\">1.67453e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d3fa182e</td><td>TERMINATED</td><td>172.26.215.93:149316</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_1d00</td><td>[&#x27;force&#x27;, &#x27;x_co_b680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0147723  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.36227</td><td style=\"text-align: right;\">221.464</td><td style=\"text-align: right;\"> 70.1534</td><td style=\"text-align: right;\">1.11835e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_1cec908a</td><td>TERMINATED</td><td>172.26.215.93:149537</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_ee40</td><td>[&#x27;force&#x27;, &#x27;x_co_4d00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0158848  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.47123</td><td style=\"text-align: right;\">220.216</td><td style=\"text-align: right;\"> 68.0741</td><td style=\"text-align: right;\">1.02028e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_d2202bb1</td><td>TERMINATED</td><td>172.26.215.93:149755</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_7b00</td><td>[&#x27;force&#x27;, &#x27;x_co_7fc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.0182769  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.4717 </td><td style=\"text-align: right;\">221.401</td><td style=\"text-align: right;\"> 76.868 </td><td style=\"text-align: right;\">4.32336e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_2d937434</td><td>TERMINATED</td><td>172.26.215.93:149995</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_e940</td><td>[&#x27;force&#x27;, &#x27;x_co_aa40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.00325191 </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.16907</td><td style=\"text-align: right;\">225.505</td><td style=\"text-align: right;\"> 78.7122</td><td style=\"text-align: right;\">3.84362e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c74786a2</td><td>TERMINATED</td><td>172.26.215.93:150234</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_b300</td><td>[&#x27;force&#x27;, &#x27;x_co_6b40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0453297  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        18.3618 </td><td style=\"text-align: right;\">223.135</td><td style=\"text-align: right;\"> 69.8521</td><td style=\"text-align: right;\">2.26913e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_9a20197e</td><td>TERMINATED</td><td>172.26.215.93:150435</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_ee80</td><td>[&#x27;force&#x27;, &#x27;x_co_2600</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0642008  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        37.3405 </td><td style=\"text-align: right;\">193.821</td><td style=\"text-align: right;\"> 61.6141</td><td style=\"text-align: right;\">2.77092e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a37947a9</td><td>TERMINATED</td><td>172.26.215.93:150649</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_5440</td><td>[&#x27;force&#x27;, &#x27;x_co_8180</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.057986   </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         6.31671</td><td style=\"text-align: right;\">227.912</td><td style=\"text-align: right;\"> 79.7712</td><td style=\"text-align: right;\">6.07664e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_3a5c5072</td><td>TERMINATED</td><td>172.26.215.93:150868</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_b080</td><td>[&#x27;force&#x27;, &#x27;x_co_6b40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0526158  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        10.4246 </td><td style=\"text-align: right;\">206.777</td><td style=\"text-align: right;\"> 66.2354</td><td style=\"text-align: right;\">2.93952e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_02093e1c</td><td>TERMINATED</td><td>172.26.215.93:151099</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_7640</td><td>[&#x27;force&#x27;, &#x27;x_co_2a40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0986218  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.57998</td><td style=\"text-align: right;\">233.885</td><td style=\"text-align: right;\"> 85.5653</td><td style=\"text-align: right;\">7.06408e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ad7361f5</td><td>TERMINATED</td><td>172.26.215.93:151335</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_f480</td><td>[&#x27;force&#x27;, &#x27;x_co_4500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0942558  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.25502</td><td style=\"text-align: right;\">230.882</td><td style=\"text-align: right;\"> 85.012 </td><td style=\"text-align: right;\">4.52802e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_10d594f8</td><td>TERMINATED</td><td>172.26.215.93:151564</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_9240</td><td>[&#x27;force&#x27;, &#x27;x_co_b6c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0995052  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.5166 </td><td style=\"text-align: right;\">281.002</td><td style=\"text-align: right;\"> 96.1698</td><td style=\"text-align: right;\">2.03767e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_b558efe2</td><td>TERMINATED</td><td>172.26.215.93:151780</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_0c00</td><td>[&#x27;force&#x27;, &#x27;x_co_dfc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0120437  </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.60163</td><td style=\"text-align: right;\">230.151</td><td style=\"text-align: right;\"> 68.3384</td><td style=\"text-align: right;\">4.01014e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_915041d3</td><td>TERMINATED</td><td>172.26.215.93:152024</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_0a00</td><td>[&#x27;force&#x27;, &#x27;x_co_aec0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00952881 </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         2.81775</td><td style=\"text-align: right;\">209.983</td><td style=\"text-align: right;\"> 66.3458</td><td style=\"text-align: right;\">1.08726e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_75a9b737</td><td>TERMINATED</td><td>172.26.215.93:152246</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d440</td><td>[&#x27;force&#x27;, &#x27;x_co_ed00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">4</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0282145  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       202.389  </td><td style=\"text-align: right;\">154.681</td><td style=\"text-align: right;\"> 49.821 </td><td style=\"text-align: right;\">1.9868e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_4cbd1076</td><td>TERMINATED</td><td>172.26.215.93:152460</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_9040</td><td>[&#x27;force&#x27;, &#x27;x_co_7680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0286553  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        27.3215 </td><td style=\"text-align: right;\">197.01 </td><td style=\"text-align: right;\"> 64.519 </td><td style=\"text-align: right;\">1.60511e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0620d1d0</td><td>TERMINATED</td><td>172.26.215.93:152675</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_ad40</td><td>[&#x27;force&#x27;, &#x27;x_co_89c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0341597  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        14.9542 </td><td style=\"text-align: right;\">207.628</td><td style=\"text-align: right;\"> 66.733 </td><td style=\"text-align: right;\">2.25445e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e258f087</td><td>TERMINATED</td><td>172.26.215.93:152887</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_a1c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0330789  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.18783</td><td style=\"text-align: right;\">233.823</td><td style=\"text-align: right;\"> 79.4173</td><td style=\"text-align: right;\">4.10967e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_91571bcb</td><td>TERMINATED</td><td>172.26.215.93:153127</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_a200</td><td>[&#x27;force&#x27;, &#x27;x_co_b880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">3</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0328367  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.11925</td><td style=\"text-align: right;\">223.453</td><td style=\"text-align: right;\"> 79.8642</td><td style=\"text-align: right;\">5.70422e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_6a257953</td><td>TERMINATED</td><td>172.26.215.93:153344</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_95c0</td><td>[&#x27;force&#x27;, &#x27;x_co_8700</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0616992  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        18.9469 </td><td style=\"text-align: right;\">225.633</td><td style=\"text-align: right;\"> 64.7213</td><td style=\"text-align: right;\">6.42686e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_962acdb5</td><td>TERMINATED</td><td>172.26.215.93:153566</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_2a40</td><td>[&#x27;force&#x27;, &#x27;x_co_0780</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.06761    </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.92719</td><td style=\"text-align: right;\">227.344</td><td style=\"text-align: right;\"> 63.49  </td><td style=\"text-align: right;\">5.35589e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_b5da9a8b</td><td>TERMINATED</td><td>172.26.215.93:153770</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_95c0</td><td>[&#x27;force&#x27;, &#x27;x_co_2e40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0586164  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.7213 </td><td style=\"text-align: right;\">227.956</td><td style=\"text-align: right;\"> 63.4859</td><td style=\"text-align: right;\">5.28644e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_594d0760</td><td>TERMINATED</td><td>172.26.215.93:154014</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_4340</td><td>[&#x27;force&#x27;, &#x27;x_co_5540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">5</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0582164  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         9.36218</td><td style=\"text-align: right;\">225.809</td><td style=\"text-align: right;\"> 66.4783</td><td style=\"text-align: right;\">6.71399e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_00767336</td><td>TERMINATED</td><td>172.26.215.93:154233</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_ca40</td><td>[&#x27;force&#x27;, &#x27;x_co_b800</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.016276   </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.3516 </td><td style=\"text-align: right;\">228.909</td><td style=\"text-align: right;\"> 66.3592</td><td style=\"text-align: right;\">2.80543e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_afd7dc97</td><td>TERMINATED</td><td>172.26.215.93:154453</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_6a80</td><td>[&#x27;force&#x27;, &#x27;x_co_7c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">4</td><td>torch.optim.SGD    </td><td style=\"text-align: right;\">        0.020532   </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.47702</td><td style=\"text-align: right;\">230.718</td><td style=\"text-align: right;\"> 64.4964</td><td style=\"text-align: right;\">5.40105e+15</td></tr>\n",
       "<tr><td>FSR_Trainable_e9d4a3fb</td><td>TERMINATED</td><td>172.26.215.93:154678</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d180</td><td>[&#x27;force&#x27;, &#x27;x_co_d280</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">4</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0222692  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         3.59068</td><td style=\"text-align: right;\">221.312</td><td style=\"text-align: right;\"> 77.7968</td><td style=\"text-align: right;\">5.95025e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_df345400</td><td>TERMINATED</td><td>172.26.215.93:154901</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d540</td><td>[&#x27;force&#x27;, &#x27;x_co_d980</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0260154  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        57.1345 </td><td style=\"text-align: right;\">173.946</td><td style=\"text-align: right;\"> 54.5488</td><td style=\"text-align: right;\">1.57123e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_75b9a165</td><td>TERMINATED</td><td>172.26.215.93:155122</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_4f00</td><td>[&#x27;force&#x27;, &#x27;x_co_4c40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0243593  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         4.17246</td><td style=\"text-align: right;\">219.139</td><td style=\"text-align: right;\"> 77.8815</td><td style=\"text-align: right;\">4.53177e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_fd2300e8</td><td>TERMINATED</td><td>172.26.215.93:155337</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_6bc0</td><td>[&#x27;force&#x27;, &#x27;x_co_6400</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">2</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0119194  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.32853</td><td style=\"text-align: right;\">228.642</td><td style=\"text-align: right;\"> 78.7567</td><td style=\"text-align: right;\">3.37563e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_cc4fa8b3</td><td>TERMINATED</td><td>172.26.215.93:155558</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_5840</td><td>[&#x27;force&#x27;, &#x27;x_co_4500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0104493  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        92.49   </td><td style=\"text-align: right;\">184.022</td><td style=\"text-align: right;\"> 64.5675</td><td style=\"text-align: right;\">2.97755e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7ece8e9e</td><td>TERMINATED</td><td>172.26.215.93:155782</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_7b80</td><td>[&#x27;force&#x27;, &#x27;x_co_4cc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0379601  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        80.4785 </td><td style=\"text-align: right;\">130.323</td><td style=\"text-align: right;\"> 39.6804</td><td style=\"text-align: right;\">1.59753e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ff956aaf</td><td>TERMINATED</td><td>172.26.215.93:156072</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_40c0</td><td>[&#x27;force&#x27;, &#x27;x_co_7880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0449154  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        91.7563 </td><td style=\"text-align: right;\">129.3  </td><td style=\"text-align: right;\"> 41.9273</td><td style=\"text-align: right;\">1.76688e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_ab4c6250</td><td>TERMINATED</td><td>172.26.215.93:156324</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_5d00</td><td>[&#x27;force&#x27;, &#x27;x_co_76c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0436565  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         4.23101</td><td style=\"text-align: right;\">225.048</td><td style=\"text-align: right;\"> 77.9343</td><td style=\"text-align: right;\">6.1506e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_b8834d63</td><td>TERMINATED</td><td>172.26.215.93:156552</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_f440</td><td>[&#x27;force&#x27;, &#x27;x_co_d500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">3</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.00943221 </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">       108.061  </td><td style=\"text-align: right;\">140.885</td><td style=\"text-align: right;\"> 43.1491</td><td style=\"text-align: right;\">1.67162e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_56b2a11f</td><td>TERMINATED</td><td>172.26.215.93:156750</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_4c00</td><td>[&#x27;force&#x27;, &#x27;x_co_5ec0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0174219  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.93619</td><td style=\"text-align: right;\">224.961</td><td style=\"text-align: right;\"> 76.495 </td><td style=\"text-align: right;\">3.84337e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_536f7381</td><td>TERMINATED</td><td>172.26.215.93:156962</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_f780</td><td>[&#x27;force&#x27;, &#x27;x_co_f700</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00867483 </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        26.171  </td><td style=\"text-align: right;\">196.582</td><td style=\"text-align: right;\"> 63.8746</td><td style=\"text-align: right;\">2.19704e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_514c7020</td><td>TERMINATED</td><td>172.26.215.93:157190</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_e900</td><td>[&#x27;force&#x27;, &#x27;x_co_f840</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.00970626 </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        49.3476 </td><td style=\"text-align: right;\">195.836</td><td style=\"text-align: right;\"> 64.0133</td><td style=\"text-align: right;\">2.38776e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8532b304</td><td>TERMINATED</td><td>172.26.215.93:157464</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_06c0</td><td>[&#x27;force&#x27;, &#x27;x_co_2200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 16</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">7</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0102359  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">        25.8768 </td><td style=\"text-align: right;\">201.792</td><td style=\"text-align: right;\"> 64.1279</td><td style=\"text-align: right;\">2.41443e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f46c9aa6</td><td>TERMINATED</td><td>172.26.215.93:157647</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_a9c0</td><td>[&#x27;force&#x27;, &#x27;x_co_83c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0114197  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         9.16254</td><td style=\"text-align: right;\">216.044</td><td style=\"text-align: right;\"> 64.7671</td><td style=\"text-align: right;\">1.72802e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_8b246c08</td><td>TERMINATED</td><td>172.26.215.93:157923</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d2c0</td><td>[&#x27;force&#x27;, &#x27;x_co_6d80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">8</td><td>torch.optim.NAdam  </td><td style=\"text-align: right;\">        0.0212918  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         7.47378</td><td style=\"text-align: right;\">222.974</td><td style=\"text-align: right;\"> 71.4963</td><td style=\"text-align: right;\">2.6826e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_a63871d1</td><td>TERMINATED</td><td>172.26.215.93:158109</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_a540</td><td>[&#x27;force&#x27;, &#x27;x_co_a680</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.037557   </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        13.1407 </td><td style=\"text-align: right;\">171.561</td><td style=\"text-align: right;\"> 49.5843</td><td style=\"text-align: right;\">1.65692e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e4c234af</td><td>TERMINATED</td><td>172.26.215.93:158353</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_90c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1840</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.04353    </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        48.147  </td><td style=\"text-align: right;\">135.884</td><td style=\"text-align: right;\"> 42.2484</td><td style=\"text-align: right;\">2.997e+07  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb67c17d</td><td>TERMINATED</td><td>172.26.215.93:158570</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_4680</td><td>[&#x27;force&#x27;, &#x27;x_co_4540</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0394734  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        75.7342 </td><td style=\"text-align: right;\">129.48 </td><td style=\"text-align: right;\"> 41.4376</td><td style=\"text-align: right;\">1.75178e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_29738eb7</td><td>TERMINATED</td><td>172.26.215.93:158809</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_0d40</td><td>[&#x27;force&#x27;, &#x27;x_co_2a00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0357179  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.76241</td><td style=\"text-align: right;\">231.321</td><td style=\"text-align: right;\"> 72.8583</td><td style=\"text-align: right;\">2.81005e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_acfb935c</td><td>TERMINATED</td><td>172.26.215.93:159028</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_5180</td><td>[&#x27;force&#x27;, &#x27;x_co_78c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0414811  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         4.56317</td><td style=\"text-align: right;\">246.5  </td><td style=\"text-align: right;\"> 79.9446</td><td style=\"text-align: right;\">3.85679e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1590aeaf</td><td>TERMINATED</td><td>172.26.215.93:159254</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_6400</td><td>[&#x27;force&#x27;, &#x27;x_co_5e00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0284047  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">        54.3161 </td><td style=\"text-align: right;\">140.014</td><td style=\"text-align: right;\"> 45.1956</td><td style=\"text-align: right;\">1.62103e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_911c530b</td><td>TERMINATED</td><td>172.26.215.93:159481</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_6ec0</td><td>[&#x27;force&#x27;, &#x27;x_co_4940</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0260539  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.18899</td><td style=\"text-align: right;\">221.849</td><td style=\"text-align: right;\"> 78.393 </td><td style=\"text-align: right;\">4.87505e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_958d317a</td><td>TERMINATED</td><td>172.26.215.93:159710</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_86c0</td><td>[&#x27;force&#x27;, &#x27;x_co_b000</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0441734  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        22.6928 </td><td style=\"text-align: right;\">186.096</td><td style=\"text-align: right;\"> 67.4532</td><td style=\"text-align: right;\">5.07336e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_32b7bb58</td><td>TERMINATED</td><td>172.26.215.93:159938</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_2140</td><td>[&#x27;force&#x27;, &#x27;x_co_1080</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">8</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0730584  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         6.56279</td><td style=\"text-align: right;\">231.251</td><td style=\"text-align: right;\"> 79.319 </td><td style=\"text-align: right;\">5.96977e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_0b22ee3e</td><td>TERMINATED</td><td>172.26.215.93:160198</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_c480</td><td>[&#x27;force&#x27;, &#x27;x_co_f2c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0458984  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.93238</td><td style=\"text-align: right;\">231.598</td><td style=\"text-align: right;\"> 81.6878</td><td style=\"text-align: right;\">3.62716e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_242a44e4</td><td>TERMINATED</td><td>172.26.215.93:160415</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_4900</td><td>[&#x27;force&#x27;, &#x27;x_co_5500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adam   </td><td style=\"text-align: right;\">        0.0150592  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.13332</td><td style=\"text-align: right;\">228.401</td><td style=\"text-align: right;\"> 83.3665</td><td style=\"text-align: right;\">1.55327e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_2fb9d4a6</td><td>TERMINATED</td><td>172.26.215.93:160644</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_9700</td><td>[&#x27;force&#x27;, &#x27;x_co_c340</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0143408  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         1.95473</td><td style=\"text-align: right;\">204.866</td><td style=\"text-align: right;\"> 74.5241</td><td style=\"text-align: right;\">1.45324e+17</td></tr>\n",
       "<tr><td>FSR_Trainable_9a868b21</td><td>TERMINATED</td><td>172.26.215.93:160735</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_5b40</td><td>[&#x27;force&#x27;, &#x27;x_co_0500</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0824474  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        37.0955 </td><td style=\"text-align: right;\">146.464</td><td style=\"text-align: right;\"> 49.402 </td><td style=\"text-align: right;\">2.10378e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_927ef5ef</td><td>TERMINATED</td><td>172.26.215.93:161048</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_cd00</td><td>[&#x27;force&#x27;, &#x27;x_co_e200</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0768441  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         4.60448</td><td style=\"text-align: right;\">234.763</td><td style=\"text-align: right;\"> 75.587 </td><td style=\"text-align: right;\">3.09442e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_c1846657</td><td>TERMINATED</td><td>172.26.215.93:161138</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d5c0</td><td>[&#x27;force&#x27;, &#x27;x_co_3140</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0779287  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         9.78158</td><td style=\"text-align: right;\">177.204</td><td style=\"text-align: right;\"> 61.3387</td><td style=\"text-align: right;\">3.27371e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_f81dfd5d</td><td>TERMINATED</td><td>172.26.215.93:161448</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_2e40</td><td>[&#x27;force&#x27;, &#x27;x_co_7180</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0200397  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        31.2532 </td><td style=\"text-align: right;\">146.893</td><td style=\"text-align: right;\"> 49.8327</td><td style=\"text-align: right;\">2.08905e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_5c5a4a9e</td><td>TERMINATED</td><td>172.26.215.93:161672</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_b940</td><td>[&#x27;force&#x27;, &#x27;x_co_f880</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0192783  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">        18.615  </td><td style=\"text-align: right;\">196.821</td><td style=\"text-align: right;\"> 59.825 </td><td style=\"text-align: right;\">1.9097e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_3ee657e3</td><td>TERMINATED</td><td>172.26.215.93:161893</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_b080</td><td>[&#x27;force&#x27;, &#x27;x_co_5380</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">2</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0305076  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        31.3035 </td><td style=\"text-align: right;\">152.193</td><td style=\"text-align: right;\"> 48.9155</td><td style=\"text-align: right;\">2.58007e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_96ce57fb</td><td>TERMINATED</td><td>172.26.215.93:162121</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_c4c0</td><td>[&#x27;force&#x27;, &#x27;x_co_f0c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.051928   </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">        43.6907 </td><td style=\"text-align: right;\">154.325</td><td style=\"text-align: right;\"> 52.5486</td><td style=\"text-align: right;\">2.94136e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_e2c5fce9</td><td>TERMINATED</td><td>172.26.215.93:162361</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_06c0</td><td>[&#x27;force&#x27;, &#x27;x_co_1580</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0515774  </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.90932</td><td style=\"text-align: right;\">231.177</td><td style=\"text-align: right;\"> 70.4287</td><td style=\"text-align: right;\">1.11945e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_f57ea8ac</td><td>TERMINATED</td><td>172.26.215.93:162561</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_7040</td><td>[&#x27;force&#x27;, &#x27;x_co_5dc0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">6</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">2</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0461653  </td><td>sklearn.preproc_0ed0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.74749</td><td style=\"text-align: right;\">244.584</td><td style=\"text-align: right;\"> 82.3629</td><td style=\"text-align: right;\">2.24155e+16</td></tr>\n",
       "<tr><td>FSR_Trainable_fd9a5c69</td><td>TERMINATED</td><td>172.26.215.93:162785</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_e700</td><td>[&#x27;force&#x27;, &#x27;x_co_d180</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0347464  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         7.23676</td><td style=\"text-align: right;\">238.88 </td><td style=\"text-align: right;\"> 77.2381</td><td style=\"text-align: right;\">2.26832e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1a941da3</td><td>TERMINATED</td><td>172.26.215.93:163032</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_8040</td><td>[&#x27;force&#x27;, &#x27;x_co_8700</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0749002  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         2.30969</td><td style=\"text-align: right;\">205.709</td><td style=\"text-align: right;\"> 66.3246</td><td style=\"text-align: right;\">2.14853e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_a9652037</td><td>TERMINATED</td><td>172.26.215.93:163255</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>median</td><td>[&#x27;FSR_for_force_c640</td><td>[&#x27;force&#x27;, &#x27;x_co_ec00</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\">  8</td><td style=\"text-align: right;\">5</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td>torch.optim.RAdam  </td><td style=\"text-align: right;\">        0.0735421  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.5068 </td><td style=\"text-align: right;\">241.889</td><td style=\"text-align: right;\"> 74.5323</td><td style=\"text-align: right;\">4.4758e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_b6a24f3c</td><td>TERMINATED</td><td>172.26.215.93:163470</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_b2c0</td><td>[&#x27;force&#x27;, &#x27;x_co_d640</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0202843  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        81.7765 </td><td style=\"text-align: right;\">129.708</td><td style=\"text-align: right;\"> 40.4157</td><td style=\"text-align: right;\">1.7775e+07 </td></tr>\n",
       "<tr><td>FSR_Trainable_983f11a1</td><td>TERMINATED</td><td>172.26.215.93:163710</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_b680</td><td>[&#x27;force&#x27;, &#x27;x_co_eb40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0200836  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        84.4872 </td><td style=\"text-align: right;\">120.046</td><td style=\"text-align: right;\"> 38.4108</td><td style=\"text-align: right;\">1.52914e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_7b0ae72f</td><td>TERMINATED</td><td>172.26.215.93:163793</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_bf80</td><td>[&#x27;force&#x27;, &#x27;x_co_9740</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0205312  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        85.6673 </td><td style=\"text-align: right;\">123.519</td><td style=\"text-align: right;\"> 38.8715</td><td style=\"text-align: right;\">1.36613e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_04941f48</td><td>TERMINATED</td><td>172.26.215.93:164099</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_2e80</td><td>[&#x27;force&#x27;, &#x27;x_co_8ac0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.020914   </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        84.0677 </td><td style=\"text-align: right;\">126.337</td><td style=\"text-align: right;\"> 38.5098</td><td style=\"text-align: right;\">1.52176e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_d6a4c920</td><td>TERMINATED</td><td>172.26.215.93:164425</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_dbc0</td><td>[&#x27;force&#x27;, &#x27;x_co_fd40</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0194929  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        69.8181 </td><td style=\"text-align: right;\">116.738</td><td style=\"text-align: right;\"> 37.7401</td><td style=\"text-align: right;\">2.06971e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_1223a976</td><td>TERMINATED</td><td>172.26.215.93:164627</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_d740</td><td>[&#x27;force&#x27;, &#x27;x_co_ef80</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0298301  </td><td>sklearn.preproc_0e70</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        67.279  </td><td style=\"text-align: right;\">117.592</td><td style=\"text-align: right;\"> 37.097 </td><td style=\"text-align: right;\">1.44455e+07</td></tr>\n",
       "<tr><td>FSR_Trainable_9339ee62</td><td>TERMINATED</td><td>172.26.215.93:164854</td><td>torch.nn.MSELoss</td><td>fsr_data.get_in_0d50</td><td>sklearn.impute._fb90</td><td>mean  </td><td>[&#x27;FSR_for_force_dd80</td><td>[&#x27;force&#x27;, &#x27;x_co_c5c0</td><td>fsr_model.CNN_LSTM</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">1</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">1</td><td>torch.optim.Adagrad</td><td style=\"text-align: right;\">        0.0292034  </td><td>sklearn.preproc_0cf0</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        63.0353 </td><td style=\"text-align: right;\">103.112</td><td style=\"text-align: right;\"> 33.4762</td><td style=\"text-align: right;\">3.70296e+16</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 11:13:33,698\tINFO wandb.py:320 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>date               </th><th>done  </th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">     mae</th><th style=\"text-align: right;\">       mape</th><th>node_ip      </th><th style=\"text-align: right;\">   pid</th><th style=\"text-align: right;\">   rmse</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>FSR_Trainable_00767336</td><td>2023-08-07_11-23-21</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 66.3592</td><td style=\"text-align: right;\">2.80543e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">154233</td><td style=\"text-align: right;\">228.909</td><td style=\"text-align: right;\">             4.3516 </td><td style=\"text-align: right;\">          4.3516  </td><td style=\"text-align: right;\">       4.3516 </td><td style=\"text-align: right;\"> 1691375001</td><td style=\"text-align: right;\">                   1</td><td>00767336  </td></tr>\n",
       "<tr><td>FSR_Trainable_00a89f50</td><td>2023-08-07_11-17-54</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 67.0777</td><td style=\"text-align: right;\">1.55196e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">147116</td><td style=\"text-align: right;\">216.38 </td><td style=\"text-align: right;\">            19.4953 </td><td style=\"text-align: right;\">          5.49213 </td><td style=\"text-align: right;\">      19.4953 </td><td style=\"text-align: right;\"> 1691374674</td><td style=\"text-align: right;\">                   4</td><td>00a89f50  </td></tr>\n",
       "<tr><td>FSR_Trainable_02093e1c</td><td>2023-08-07_11-20-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 85.5653</td><td style=\"text-align: right;\">7.06408e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">151099</td><td style=\"text-align: right;\">233.885</td><td style=\"text-align: right;\">             6.57998</td><td style=\"text-align: right;\">          1.63472 </td><td style=\"text-align: right;\">       6.57998</td><td style=\"text-align: right;\"> 1691374859</td><td style=\"text-align: right;\">                   4</td><td>02093e1c  </td></tr>\n",
       "<tr><td>FSR_Trainable_04941f48</td><td>2023-08-07_11-34-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.5098</td><td style=\"text-align: right;\">1.52176e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">164099</td><td style=\"text-align: right;\">126.337</td><td style=\"text-align: right;\">            84.0677 </td><td style=\"text-align: right;\">          0.675504</td><td style=\"text-align: right;\">      84.0677 </td><td style=\"text-align: right;\"> 1691375662</td><td style=\"text-align: right;\">                 100</td><td>04941f48  </td></tr>\n",
       "<tr><td>FSR_Trainable_0620d1d0</td><td>2023-08-07_11-22-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 66.733 </td><td style=\"text-align: right;\">2.25445e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">152675</td><td style=\"text-align: right;\">207.628</td><td style=\"text-align: right;\">            14.9542 </td><td style=\"text-align: right;\">          1.87165 </td><td style=\"text-align: right;\">      14.9542 </td><td style=\"text-align: right;\"> 1691374938</td><td style=\"text-align: right;\">                   8</td><td>0620d1d0  </td></tr>\n",
       "<tr><td>FSR_Trainable_0b22ee3e</td><td>2023-08-07_11-29-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 81.6878</td><td style=\"text-align: right;\">3.62716e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">160198</td><td style=\"text-align: right;\">231.598</td><td style=\"text-align: right;\">             1.93238</td><td style=\"text-align: right;\">          0.883483</td><td style=\"text-align: right;\">       1.93238</td><td style=\"text-align: right;\"> 1691375385</td><td style=\"text-align: right;\">                   2</td><td>0b22ee3e  </td></tr>\n",
       "<tr><td>FSR_Trainable_10d594f8</td><td>2023-08-07_11-21-14</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 96.1698</td><td style=\"text-align: right;\">2.03767e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">151564</td><td style=\"text-align: right;\">281.002</td><td style=\"text-align: right;\">             1.5166 </td><td style=\"text-align: right;\">          1.5166  </td><td style=\"text-align: right;\">       1.5166 </td><td style=\"text-align: right;\"> 1691374874</td><td style=\"text-align: right;\">                   1</td><td>10d594f8  </td></tr>\n",
       "<tr><td>FSR_Trainable_1223a976</td><td>2023-08-07_11-35-28</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 37.097 </td><td style=\"text-align: right;\">1.44455e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">164627</td><td style=\"text-align: right;\">117.592</td><td style=\"text-align: right;\">            67.279  </td><td style=\"text-align: right;\">          0.503871</td><td style=\"text-align: right;\">      67.279  </td><td style=\"text-align: right;\"> 1691375728</td><td style=\"text-align: right;\">                 100</td><td>1223a976  </td></tr>\n",
       "<tr><td>FSR_Trainable_147eba8a</td><td>2023-08-07_11-19-08</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 65.1665</td><td style=\"text-align: right;\">1.00398e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">148421</td><td style=\"text-align: right;\">226.448</td><td style=\"text-align: right;\">            15.4134 </td><td style=\"text-align: right;\">          3.65428 </td><td style=\"text-align: right;\">      15.4134 </td><td style=\"text-align: right;\"> 1691374748</td><td style=\"text-align: right;\">                   4</td><td>147eba8a  </td></tr>\n",
       "<tr><td>FSR_Trainable_1590aeaf</td><td>2023-08-07_11-29-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 45.1956</td><td style=\"text-align: right;\">1.62103e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">159254</td><td style=\"text-align: right;\">140.014</td><td style=\"text-align: right;\">            54.3161 </td><td style=\"text-align: right;\">          0.699773</td><td style=\"text-align: right;\">      54.3161 </td><td style=\"text-align: right;\"> 1691375391</td><td style=\"text-align: right;\">                  64</td><td>1590aeaf  </td></tr>\n",
       "<tr><td>FSR_Trainable_1a941da3</td><td>2023-08-07_11-32-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 66.3246</td><td style=\"text-align: right;\">2.14853e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">163032</td><td style=\"text-align: right;\">205.709</td><td style=\"text-align: right;\">             2.30969</td><td style=\"text-align: right;\">          1.24895 </td><td style=\"text-align: right;\">       2.30969</td><td style=\"text-align: right;\"> 1691375523</td><td style=\"text-align: right;\">                   2</td><td>1a941da3  </td></tr>\n",
       "<tr><td>FSR_Trainable_1b594dfc</td><td>2023-08-07_11-18-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 62.6811</td><td style=\"text-align: right;\">1.69669e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">146882</td><td style=\"text-align: right;\">201.093</td><td style=\"text-align: right;\">            90.8048 </td><td style=\"text-align: right;\">          4.99756 </td><td style=\"text-align: right;\">      90.8048 </td><td style=\"text-align: right;\"> 1691374729</td><td style=\"text-align: right;\">                  16</td><td>1b594dfc  </td></tr>\n",
       "<tr><td>FSR_Trainable_1cec908a</td><td>2023-08-07_11-19-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 68.0741</td><td style=\"text-align: right;\">1.02028e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">149537</td><td style=\"text-align: right;\">220.216</td><td style=\"text-align: right;\">             7.47123</td><td style=\"text-align: right;\">          3.43374 </td><td style=\"text-align: right;\">       7.47123</td><td style=\"text-align: right;\"> 1691374787</td><td style=\"text-align: right;\">                   2</td><td>1cec908a  </td></tr>\n",
       "<tr><td>FSR_Trainable_242a44e4</td><td>2023-08-07_11-29-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 83.3665</td><td style=\"text-align: right;\">1.55327e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">160415</td><td style=\"text-align: right;\">228.401</td><td style=\"text-align: right;\">             1.13332</td><td style=\"text-align: right;\">          1.13332 </td><td style=\"text-align: right;\">       1.13332</td><td style=\"text-align: right;\"> 1691375393</td><td style=\"text-align: right;\">                   1</td><td>242a44e4  </td></tr>\n",
       "<tr><td>FSR_Trainable_266f4215</td><td>2023-08-07_11-21-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 45.2299</td><td style=\"text-align: right;\">1.67453e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">149088</td><td style=\"text-align: right;\">147.573</td><td style=\"text-align: right;\">           120.842  </td><td style=\"text-align: right;\">          1.42058 </td><td style=\"text-align: right;\">     120.842  </td><td style=\"text-align: right;\"> 1691374902</td><td style=\"text-align: right;\">                  64</td><td>266f4215  </td></tr>\n",
       "<tr><td>FSR_Trainable_29738eb7</td><td>2023-08-07_11-28-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 72.8583</td><td style=\"text-align: right;\">2.81005e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">158809</td><td style=\"text-align: right;\">231.321</td><td style=\"text-align: right;\">             2.76241</td><td style=\"text-align: right;\">          1.30718 </td><td style=\"text-align: right;\">       2.76241</td><td style=\"text-align: right;\"> 1691375303</td><td style=\"text-align: right;\">                   2</td><td>29738eb7  </td></tr>\n",
       "<tr><td>FSR_Trainable_2d937434</td><td>2023-08-07_11-19-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.7122</td><td style=\"text-align: right;\">3.84362e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">149995</td><td style=\"text-align: right;\">225.505</td><td style=\"text-align: right;\">             1.16907</td><td style=\"text-align: right;\">          1.16907 </td><td style=\"text-align: right;\">       1.16907</td><td style=\"text-align: right;\"> 1691374799</td><td style=\"text-align: right;\">                   1</td><td>2d937434  </td></tr>\n",
       "<tr><td>FSR_Trainable_2fb9d4a6</td><td>2023-08-07_11-30-03</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 74.5241</td><td style=\"text-align: right;\">1.45324e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">160644</td><td style=\"text-align: right;\">204.866</td><td style=\"text-align: right;\">             1.95473</td><td style=\"text-align: right;\">          0.711979</td><td style=\"text-align: right;\">       1.95473</td><td style=\"text-align: right;\"> 1691375403</td><td style=\"text-align: right;\">                   2</td><td>2fb9d4a6  </td></tr>\n",
       "<tr><td>FSR_Trainable_32b7bb58</td><td>2023-08-07_11-29-32</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 79.319 </td><td style=\"text-align: right;\">5.96977e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">159938</td><td style=\"text-align: right;\">231.251</td><td style=\"text-align: right;\">             6.56279</td><td style=\"text-align: right;\">          1.14004 </td><td style=\"text-align: right;\">       6.56279</td><td style=\"text-align: right;\"> 1691375372</td><td style=\"text-align: right;\">                   4</td><td>32b7bb58  </td></tr>\n",
       "<tr><td>FSR_Trainable_378dbc6f</td><td>2023-08-07_11-18-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 72.8363</td><td style=\"text-align: right;\">1.30996e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">147350</td><td style=\"text-align: right;\">227.794</td><td style=\"text-align: right;\">             8.1365 </td><td style=\"text-align: right;\">          3.79844 </td><td style=\"text-align: right;\">       8.1365 </td><td style=\"text-align: right;\"> 1691374693</td><td style=\"text-align: right;\">                   2</td><td>378dbc6f  </td></tr>\n",
       "<tr><td>FSR_Trainable_3a5c5072</td><td>2023-08-07_11-20-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 66.2354</td><td style=\"text-align: right;\">2.93952e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">150868</td><td style=\"text-align: right;\">206.777</td><td style=\"text-align: right;\">            10.4246 </td><td style=\"text-align: right;\">          2.29771 </td><td style=\"text-align: right;\">      10.4246 </td><td style=\"text-align: right;\"> 1691374852</td><td style=\"text-align: right;\">                   4</td><td>3a5c5072  </td></tr>\n",
       "<tr><td>FSR_Trainable_3ee657e3</td><td>2023-08-07_11-31-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 48.9155</td><td style=\"text-align: right;\">2.58007e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">161893</td><td style=\"text-align: right;\">152.193</td><td style=\"text-align: right;\">            31.3035 </td><td style=\"text-align: right;\">          0.847597</td><td style=\"text-align: right;\">      31.3035 </td><td style=\"text-align: right;\"> 1691375498</td><td style=\"text-align: right;\">                  32</td><td>3ee657e3  </td></tr>\n",
       "<tr><td>FSR_Trainable_48f428e2</td><td>2023-08-07_11-18-18</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 67.3642</td><td style=\"text-align: right;\">2.85892e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">146620</td><td style=\"text-align: right;\">207.925</td><td style=\"text-align: right;\">            76.3978 </td><td style=\"text-align: right;\">          8.81603 </td><td style=\"text-align: right;\">      76.3978 </td><td style=\"text-align: right;\"> 1691374698</td><td style=\"text-align: right;\">                   8</td><td>48f428e2  </td></tr>\n",
       "<tr><td>FSR_Trainable_4cbd1076</td><td>2023-08-07_11-22-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 64.519 </td><td style=\"text-align: right;\">1.60511e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">152460</td><td style=\"text-align: right;\">197.01 </td><td style=\"text-align: right;\">            27.3215 </td><td style=\"text-align: right;\">          1.45131 </td><td style=\"text-align: right;\">      27.3215 </td><td style=\"text-align: right;\"> 1691374942</td><td style=\"text-align: right;\">                  16</td><td>4cbd1076  </td></tr>\n",
       "<tr><td>FSR_Trainable_514c7020</td><td>2023-08-07_11-27-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 64.0133</td><td style=\"text-align: right;\">2.38776e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">157190</td><td style=\"text-align: right;\">195.836</td><td style=\"text-align: right;\">            49.3476 </td><td style=\"text-align: right;\">          2.40328 </td><td style=\"text-align: right;\">      49.3476 </td><td style=\"text-align: right;\"> 1691375250</td><td style=\"text-align: right;\">                  16</td><td>514c7020  </td></tr>\n",
       "<tr><td>FSR_Trainable_51c64226</td><td>2023-08-07_11-13-47</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 75.7422</td><td style=\"text-align: right;\">3.16651e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">142841</td><td style=\"text-align: right;\">234.757</td><td style=\"text-align: right;\">             1.54588</td><td style=\"text-align: right;\">          1.54588 </td><td style=\"text-align: right;\">       1.54588</td><td style=\"text-align: right;\"> 1691374427</td><td style=\"text-align: right;\">                   1</td><td>51c64226  </td></tr>\n",
       "<tr><td>FSR_Trainable_536f7381</td><td>2023-08-07_11-26-53</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 63.8746</td><td style=\"text-align: right;\">2.19704e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">156962</td><td style=\"text-align: right;\">196.582</td><td style=\"text-align: right;\">            26.171  </td><td style=\"text-align: right;\">          2.6319  </td><td style=\"text-align: right;\">      26.171  </td><td style=\"text-align: right;\"> 1691375213</td><td style=\"text-align: right;\">                   8</td><td>536f7381  </td></tr>\n",
       "<tr><td>FSR_Trainable_56483a0a</td><td>2023-08-07_11-16-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 59.8602</td><td style=\"text-align: right;\">1.22287e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">145289</td><td style=\"text-align: right;\">207.37 </td><td style=\"text-align: right;\">            26.5212 </td><td style=\"text-align: right;\">          2.72695 </td><td style=\"text-align: right;\">      26.5212 </td><td style=\"text-align: right;\"> 1691374586</td><td style=\"text-align: right;\">                   8</td><td>56483a0a  </td></tr>\n",
       "<tr><td>FSR_Trainable_56b2a11f</td><td>2023-08-07_11-26-19</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 76.495 </td><td style=\"text-align: right;\">3.84337e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">156750</td><td style=\"text-align: right;\">224.961</td><td style=\"text-align: right;\">             9.93619</td><td style=\"text-align: right;\">          5.25333 </td><td style=\"text-align: right;\">       9.93619</td><td style=\"text-align: right;\"> 1691375179</td><td style=\"text-align: right;\">                   2</td><td>56b2a11f  </td></tr>\n",
       "<tr><td>FSR_Trainable_594d0760</td><td>2023-08-07_11-23-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 66.4783</td><td style=\"text-align: right;\">6.71399e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">154014</td><td style=\"text-align: right;\">225.809</td><td style=\"text-align: right;\">             9.36218</td><td style=\"text-align: right;\">          9.36218 </td><td style=\"text-align: right;\">       9.36218</td><td style=\"text-align: right;\"> 1691374996</td><td style=\"text-align: right;\">                   1</td><td>594d0760  </td></tr>\n",
       "<tr><td>FSR_Trainable_5a041cb2</td><td>2023-08-07_11-19-50</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 59.9089</td><td style=\"text-align: right;\">1.66336e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">147976</td><td style=\"text-align: right;\">192.83 </td><td style=\"text-align: right;\">            67.6822 </td><td style=\"text-align: right;\">          2.23512 </td><td style=\"text-align: right;\">      67.6822 </td><td style=\"text-align: right;\"> 1691374790</td><td style=\"text-align: right;\">                  32</td><td>5a041cb2  </td></tr>\n",
       "<tr><td>FSR_Trainable_5c5a4a9e</td><td>2023-08-07_11-31-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 59.825 </td><td style=\"text-align: right;\">1.9097e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">161672</td><td style=\"text-align: right;\">196.821</td><td style=\"text-align: right;\">            18.615  </td><td style=\"text-align: right;\">          0.959405</td><td style=\"text-align: right;\">      18.615  </td><td style=\"text-align: right;\"> 1691375469</td><td style=\"text-align: right;\">                  16</td><td>5c5a4a9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_6017eba4</td><td>2023-08-07_11-14-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.3893</td><td style=\"text-align: right;\">7.14183e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">143952</td><td style=\"text-align: right;\">282.144</td><td style=\"text-align: right;\">             3.99962</td><td style=\"text-align: right;\">          3.99962 </td><td style=\"text-align: right;\">       3.99962</td><td style=\"text-align: right;\"> 1691374499</td><td style=\"text-align: right;\">                   1</td><td>6017eba4  </td></tr>\n",
       "<tr><td>FSR_Trainable_6a257953</td><td>2023-08-07_11-22-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 64.7213</td><td style=\"text-align: right;\">6.42686e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">153344</td><td style=\"text-align: right;\">225.633</td><td style=\"text-align: right;\">            18.9469 </td><td style=\"text-align: right;\">         18.9469  </td><td style=\"text-align: right;\">      18.9469 </td><td style=\"text-align: right;\"> 1691374972</td><td style=\"text-align: right;\">                   1</td><td>6a257953  </td></tr>\n",
       "<tr><td>FSR_Trainable_6bb5c72e</td><td>2023-08-07_11-17-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 76.4019</td><td style=\"text-align: right;\">1.46052e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">145972</td><td style=\"text-align: right;\">222.297</td><td style=\"text-align: right;\">            32.6362 </td><td style=\"text-align: right;\">          8.77326 </td><td style=\"text-align: right;\">      32.6362 </td><td style=\"text-align: right;\"> 1691374625</td><td style=\"text-align: right;\">                   4</td><td>6bb5c72e  </td></tr>\n",
       "<tr><td>FSR_Trainable_6eda6ff5</td><td>2023-08-07_11-15-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 63.1268</td><td style=\"text-align: right;\">4.70628e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">143737</td><td style=\"text-align: right;\">214.435</td><td style=\"text-align: right;\">            21.454  </td><td style=\"text-align: right;\">          5.91364 </td><td style=\"text-align: right;\">      21.454  </td><td style=\"text-align: right;\"> 1691374509</td><td style=\"text-align: right;\">                   4</td><td>6eda6ff5  </td></tr>\n",
       "<tr><td>FSR_Trainable_75a9b737</td><td>2023-08-07_11-25-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 49.821 </td><td style=\"text-align: right;\">1.9868e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">152246</td><td style=\"text-align: right;\">154.681</td><td style=\"text-align: right;\">           202.389  </td><td style=\"text-align: right;\">          3.16167 </td><td style=\"text-align: right;\">     202.389  </td><td style=\"text-align: right;\"> 1691375127</td><td style=\"text-align: right;\">                  64</td><td>75a9b737  </td></tr>\n",
       "<tr><td>FSR_Trainable_75b9a165</td><td>2023-08-07_11-23-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 77.8815</td><td style=\"text-align: right;\">4.53177e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">155122</td><td style=\"text-align: right;\">219.139</td><td style=\"text-align: right;\">             4.17246</td><td style=\"text-align: right;\">          1.98125 </td><td style=\"text-align: right;\">       4.17246</td><td style=\"text-align: right;\"> 1691375039</td><td style=\"text-align: right;\">                   2</td><td>75b9a165  </td></tr>\n",
       "<tr><td>FSR_Trainable_7b0ae72f</td><td>2023-08-07_11-34-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.8715</td><td style=\"text-align: right;\">1.36613e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">163793</td><td style=\"text-align: right;\">123.519</td><td style=\"text-align: right;\">            85.6673 </td><td style=\"text-align: right;\">          0.794171</td><td style=\"text-align: right;\">      85.6673 </td><td style=\"text-align: right;\"> 1691375650</td><td style=\"text-align: right;\">                 100</td><td>7b0ae72f  </td></tr>\n",
       "<tr><td>FSR_Trainable_7ece8e9e</td><td>2023-08-07_11-25-51</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 39.6804</td><td style=\"text-align: right;\">1.59753e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">155782</td><td style=\"text-align: right;\">130.323</td><td style=\"text-align: right;\">            80.4785 </td><td style=\"text-align: right;\">          0.687285</td><td style=\"text-align: right;\">      80.4785 </td><td style=\"text-align: right;\"> 1691375151</td><td style=\"text-align: right;\">                 100</td><td>7ece8e9e  </td></tr>\n",
       "<tr><td>FSR_Trainable_80e6373b</td><td>2023-08-07_11-18-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 81.7446</td><td style=\"text-align: right;\">4.563e+07  </td><td>172.26.215.93</td><td style=\"text-align: right;\">148205</td><td style=\"text-align: right;\">229.624</td><td style=\"text-align: right;\">             3.82645</td><td style=\"text-align: right;\">          3.82645 </td><td style=\"text-align: right;\">       3.82645</td><td style=\"text-align: right;\"> 1691374724</td><td style=\"text-align: right;\">                   1</td><td>80e6373b  </td></tr>\n",
       "<tr><td>FSR_Trainable_8532b304</td><td>2023-08-07_11-27-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 64.1279</td><td style=\"text-align: right;\">2.41443e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">157464</td><td style=\"text-align: right;\">201.792</td><td style=\"text-align: right;\">            25.8768 </td><td style=\"text-align: right;\">          2.84622 </td><td style=\"text-align: right;\">      25.8768 </td><td style=\"text-align: right;\"> 1691375245</td><td style=\"text-align: right;\">                   8</td><td>8532b304  </td></tr>\n",
       "<tr><td>FSR_Trainable_861e35b5</td><td>2023-08-07_11-16-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 69.4518</td><td style=\"text-align: right;\">3.60947e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">144173</td><td style=\"text-align: right;\">222.668</td><td style=\"text-align: right;\">            76.556  </td><td style=\"text-align: right;\">         20.1802  </td><td style=\"text-align: right;\">      76.556  </td><td style=\"text-align: right;\"> 1691374583</td><td style=\"text-align: right;\">                   4</td><td>861e35b5  </td></tr>\n",
       "<tr><td>FSR_Trainable_8aa78cb2</td><td>2023-08-07_11-14-00</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">106.919 </td><td style=\"text-align: right;\">1.1684e+17 </td><td>172.26.215.93</td><td style=\"text-align: right;\">143014</td><td style=\"text-align: right;\">326.46 </td><td style=\"text-align: right;\">             5.97125</td><td style=\"text-align: right;\">          5.97125 </td><td style=\"text-align: right;\">       5.97125</td><td style=\"text-align: right;\"> 1691374440</td><td style=\"text-align: right;\">                   1</td><td>8aa78cb2  </td></tr>\n",
       "<tr><td>FSR_Trainable_8b246c08</td><td>2023-08-07_11-27-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 71.4963</td><td style=\"text-align: right;\">2.6826e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">157923</td><td style=\"text-align: right;\">222.974</td><td style=\"text-align: right;\">             7.47378</td><td style=\"text-align: right;\">          3.38659 </td><td style=\"text-align: right;\">       7.47378</td><td style=\"text-align: right;\"> 1691375260</td><td style=\"text-align: right;\">                   2</td><td>8b246c08  </td></tr>\n",
       "<tr><td>FSR_Trainable_8b662eb9</td><td>2023-08-07_11-14-45</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 71.3565</td><td style=\"text-align: right;\">6.71329e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">142769</td><td style=\"text-align: right;\">230.878</td><td style=\"text-align: right;\">            49.9887 </td><td style=\"text-align: right;\">          1.27044 </td><td style=\"text-align: right;\">      49.9887 </td><td style=\"text-align: right;\"> 1691374485</td><td style=\"text-align: right;\">                  32</td><td>8b662eb9  </td></tr>\n",
       "<tr><td>FSR_Trainable_8da6b371</td><td>2023-08-07_11-15-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 92.3958</td><td style=\"text-align: right;\">1.77692e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">144606</td><td style=\"text-align: right;\">229.585</td><td style=\"text-align: right;\">             2.11444</td><td style=\"text-align: right;\">          2.11444 </td><td style=\"text-align: right;\">       2.11444</td><td style=\"text-align: right;\"> 1691374530</td><td style=\"text-align: right;\">                   1</td><td>8da6b371  </td></tr>\n",
       "<tr><td>FSR_Trainable_8f6e4462</td><td>2023-08-07_11-15-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 66.9925</td><td style=\"text-align: right;\">2.19573e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">145062</td><td style=\"text-align: right;\">227.898</td><td style=\"text-align: right;\">             2.83379</td><td style=\"text-align: right;\">          1.26055 </td><td style=\"text-align: right;\">       2.83379</td><td style=\"text-align: right;\"> 1691374552</td><td style=\"text-align: right;\">                   2</td><td>8f6e4462  </td></tr>\n",
       "<tr><td>FSR_Trainable_911c530b</td><td>2023-08-07_11-28-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.393 </td><td style=\"text-align: right;\">4.87505e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">159481</td><td style=\"text-align: right;\">221.849</td><td style=\"text-align: right;\">             1.18899</td><td style=\"text-align: right;\">          1.18899 </td><td style=\"text-align: right;\">       1.18899</td><td style=\"text-align: right;\"> 1691375339</td><td style=\"text-align: right;\">                   1</td><td>911c530b  </td></tr>\n",
       "<tr><td>FSR_Trainable_915041d3</td><td>2023-08-07_11-21-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 66.3458</td><td style=\"text-align: right;\">1.08726e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">152024</td><td style=\"text-align: right;\">209.983</td><td style=\"text-align: right;\">             2.81775</td><td style=\"text-align: right;\">          0.609206</td><td style=\"text-align: right;\">       2.81775</td><td style=\"text-align: right;\"> 1691374897</td><td style=\"text-align: right;\">                   4</td><td>915041d3  </td></tr>\n",
       "<tr><td>FSR_Trainable_91571bcb</td><td>2023-08-07_11-22-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 79.8642</td><td style=\"text-align: right;\">5.70422e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">153127</td><td style=\"text-align: right;\">223.453</td><td style=\"text-align: right;\">             2.11925</td><td style=\"text-align: right;\">          2.11925 </td><td style=\"text-align: right;\">       2.11925</td><td style=\"text-align: right;\"> 1691374946</td><td style=\"text-align: right;\">                   1</td><td>91571bcb  </td></tr>\n",
       "<tr><td>FSR_Trainable_927ef5ef</td><td>2023-08-07_11-30-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 75.587 </td><td style=\"text-align: right;\">3.09442e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">161048</td><td style=\"text-align: right;\">234.763</td><td style=\"text-align: right;\">             4.60448</td><td style=\"text-align: right;\">          0.561754</td><td style=\"text-align: right;\">       4.60448</td><td style=\"text-align: right;\"> 1691375423</td><td style=\"text-align: right;\">                   8</td><td>927ef5ef  </td></tr>\n",
       "<tr><td>FSR_Trainable_9339ee62</td><td>2023-08-07_11-35-35</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 33.4762</td><td style=\"text-align: right;\">3.70296e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">164854</td><td style=\"text-align: right;\">103.112</td><td style=\"text-align: right;\">            63.0353 </td><td style=\"text-align: right;\">          0.427066</td><td style=\"text-align: right;\">      63.0353 </td><td style=\"text-align: right;\"> 1691375735</td><td style=\"text-align: right;\">                 100</td><td>9339ee62  </td></tr>\n",
       "<tr><td>FSR_Trainable_958d317a</td><td>2023-08-07_11-29-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 67.4532</td><td style=\"text-align: right;\">5.07336e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">159710</td><td style=\"text-align: right;\">186.096</td><td style=\"text-align: right;\">            22.6928 </td><td style=\"text-align: right;\">          1.38361 </td><td style=\"text-align: right;\">      22.6928 </td><td style=\"text-align: right;\"> 1691375376</td><td style=\"text-align: right;\">                  16</td><td>958d317a  </td></tr>\n",
       "<tr><td>FSR_Trainable_962acdb5</td><td>2023-08-07_11-22-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 63.49  </td><td style=\"text-align: right;\">5.35589e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">153566</td><td style=\"text-align: right;\">227.344</td><td style=\"text-align: right;\">             9.92719</td><td style=\"text-align: right;\">          9.92719 </td><td style=\"text-align: right;\">       9.92719</td><td style=\"text-align: right;\"> 1691374972</td><td style=\"text-align: right;\">                   1</td><td>962acdb5  </td></tr>\n",
       "<tr><td>FSR_Trainable_96ce57fb</td><td>2023-08-07_11-32-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 52.5486</td><td style=\"text-align: right;\">2.94136e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">162121</td><td style=\"text-align: right;\">154.325</td><td style=\"text-align: right;\">            43.6907 </td><td style=\"text-align: right;\">          1.35635 </td><td style=\"text-align: right;\">      43.6907 </td><td style=\"text-align: right;\"> 1691375526</td><td style=\"text-align: right;\">                  32</td><td>96ce57fb  </td></tr>\n",
       "<tr><td>FSR_Trainable_983f11a1</td><td>2023-08-07_11-34-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 38.4108</td><td style=\"text-align: right;\">1.52914e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">163710</td><td style=\"text-align: right;\">120.046</td><td style=\"text-align: right;\">            84.4872 </td><td style=\"text-align: right;\">          0.762586</td><td style=\"text-align: right;\">      84.4872 </td><td style=\"text-align: right;\"> 1691375641</td><td style=\"text-align: right;\">                 100</td><td>983f11a1  </td></tr>\n",
       "<tr><td>FSR_Trainable_9a20197e</td><td>2023-08-07_11-20-59</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 61.6141</td><td style=\"text-align: right;\">2.77092e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">150435</td><td style=\"text-align: right;\">193.821</td><td style=\"text-align: right;\">            37.3405 </td><td style=\"text-align: right;\">          2.60974 </td><td style=\"text-align: right;\">      37.3405 </td><td style=\"text-align: right;\"> 1691374859</td><td style=\"text-align: right;\">                  16</td><td>9a20197e  </td></tr>\n",
       "<tr><td>FSR_Trainable_9a868b21</td><td>2023-08-07_11-30-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 49.402 </td><td style=\"text-align: right;\">2.10378e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">160735</td><td style=\"text-align: right;\">146.464</td><td style=\"text-align: right;\">            37.0955 </td><td style=\"text-align: right;\">          2.22522 </td><td style=\"text-align: right;\">      37.0955 </td><td style=\"text-align: right;\"> 1691375452</td><td style=\"text-align: right;\">                  32</td><td>9a868b21  </td></tr>\n",
       "<tr><td>FSR_Trainable_9f743014</td><td>2023-08-07_11-16-26</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 77.1827</td><td style=\"text-align: right;\">4.91614e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">145747</td><td style=\"text-align: right;\">220.793</td><td style=\"text-align: right;\">             3.3045 </td><td style=\"text-align: right;\">          0.654386</td><td style=\"text-align: right;\">       3.3045 </td><td style=\"text-align: right;\"> 1691374586</td><td style=\"text-align: right;\">                   4</td><td>9f743014  </td></tr>\n",
       "<tr><td>FSR_Trainable_a37947a9</td><td>2023-08-07_11-20-34</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 79.7712</td><td style=\"text-align: right;\">6.07664e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">150649</td><td style=\"text-align: right;\">227.912</td><td style=\"text-align: right;\">             6.31671</td><td style=\"text-align: right;\">          2.5086  </td><td style=\"text-align: right;\">       6.31671</td><td style=\"text-align: right;\"> 1691374834</td><td style=\"text-align: right;\">                   2</td><td>a37947a9  </td></tr>\n",
       "<tr><td>FSR_Trainable_a5f78c90</td><td>2023-08-07_11-18-25</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 78.3836</td><td style=\"text-align: right;\">1.54702e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">147529</td><td style=\"text-align: right;\">220.759</td><td style=\"text-align: right;\">            10.4907 </td><td style=\"text-align: right;\">          2.77605 </td><td style=\"text-align: right;\">      10.4907 </td><td style=\"text-align: right;\"> 1691374705</td><td style=\"text-align: right;\">                   4</td><td>a5f78c90  </td></tr>\n",
       "<tr><td>FSR_Trainable_a63871d1</td><td>2023-08-07_11-27-58</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 49.5843</td><td style=\"text-align: right;\">1.65692e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">158109</td><td style=\"text-align: right;\">171.561</td><td style=\"text-align: right;\">            13.1407 </td><td style=\"text-align: right;\">          0.951076</td><td style=\"text-align: right;\">      13.1407 </td><td style=\"text-align: right;\"> 1691375278</td><td style=\"text-align: right;\">                  16</td><td>a63871d1  </td></tr>\n",
       "<tr><td>FSR_Trainable_a9652037</td><td>2023-08-07_11-32-10</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 74.5323</td><td style=\"text-align: right;\">4.4758e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">163255</td><td style=\"text-align: right;\">241.889</td><td style=\"text-align: right;\">             1.5068 </td><td style=\"text-align: right;\">          1.5068  </td><td style=\"text-align: right;\">       1.5068 </td><td style=\"text-align: right;\"> 1691375530</td><td style=\"text-align: right;\">                   1</td><td>a9652037  </td></tr>\n",
       "<tr><td>FSR_Trainable_ab4c6250</td><td>2023-08-07_11-25-44</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 77.9343</td><td style=\"text-align: right;\">6.1506e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">156324</td><td style=\"text-align: right;\">225.048</td><td style=\"text-align: right;\">             4.23101</td><td style=\"text-align: right;\">          4.23101 </td><td style=\"text-align: right;\">       4.23101</td><td style=\"text-align: right;\"> 1691375144</td><td style=\"text-align: right;\">                   1</td><td>ab4c6250  </td></tr>\n",
       "<tr><td>FSR_Trainable_acfb935c</td><td>2023-08-07_11-28-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         4</td><td style=\"text-align: right;\"> 79.9446</td><td style=\"text-align: right;\">3.85679e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">159028</td><td style=\"text-align: right;\">246.5  </td><td style=\"text-align: right;\">             4.56317</td><td style=\"text-align: right;\">          1.02124 </td><td style=\"text-align: right;\">       4.56317</td><td style=\"text-align: right;\"> 1691375320</td><td style=\"text-align: right;\">                   4</td><td>acfb935c  </td></tr>\n",
       "<tr><td>FSR_Trainable_ad7361f5</td><td>2023-08-07_11-21-09</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 85.012 </td><td style=\"text-align: right;\">4.52802e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">151335</td><td style=\"text-align: right;\">230.882</td><td style=\"text-align: right;\">             3.25502</td><td style=\"text-align: right;\">          1.27358 </td><td style=\"text-align: right;\">       3.25502</td><td style=\"text-align: right;\"> 1691374869</td><td style=\"text-align: right;\">                   2</td><td>ad7361f5  </td></tr>\n",
       "<tr><td>FSR_Trainable_afd7dc97</td><td>2023-08-07_11-23-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 64.4964</td><td style=\"text-align: right;\">5.40105e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">154453</td><td style=\"text-align: right;\">230.718</td><td style=\"text-align: right;\">             4.47702</td><td style=\"text-align: right;\">          4.47702 </td><td style=\"text-align: right;\">       4.47702</td><td style=\"text-align: right;\"> 1691375010</td><td style=\"text-align: right;\">                   1</td><td>afd7dc97  </td></tr>\n",
       "<tr><td>FSR_Trainable_b558efe2</td><td>2023-08-07_11-21-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 68.3384</td><td style=\"text-align: right;\">4.01014e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">151780</td><td style=\"text-align: right;\">230.151</td><td style=\"text-align: right;\">             1.60163</td><td style=\"text-align: right;\">          1.60163 </td><td style=\"text-align: right;\">       1.60163</td><td style=\"text-align: right;\"> 1691374883</td><td style=\"text-align: right;\">                   1</td><td>b558efe2  </td></tr>\n",
       "<tr><td>FSR_Trainable_b5da9a8b</td><td>2023-08-07_11-23-04</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 63.4859</td><td style=\"text-align: right;\">5.28644e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">153770</td><td style=\"text-align: right;\">227.956</td><td style=\"text-align: right;\">            10.7213 </td><td style=\"text-align: right;\">         10.7213  </td><td style=\"text-align: right;\">      10.7213 </td><td style=\"text-align: right;\"> 1691374984</td><td style=\"text-align: right;\">                   1</td><td>b5da9a8b  </td></tr>\n",
       "<tr><td>FSR_Trainable_b6316a83</td><td>2023-08-07_11-16-06</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.0262</td><td style=\"text-align: right;\">3.62778e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">145516</td><td style=\"text-align: right;\">230.397</td><td style=\"text-align: right;\">             1.01019</td><td style=\"text-align: right;\">          1.01019 </td><td style=\"text-align: right;\">       1.01019</td><td style=\"text-align: right;\"> 1691374566</td><td style=\"text-align: right;\">                   1</td><td>b6316a83  </td></tr>\n",
       "<tr><td>FSR_Trainable_b6a24f3c</td><td>2023-08-07_11-33-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 40.4157</td><td style=\"text-align: right;\">1.7775e+07 </td><td>172.26.215.93</td><td style=\"text-align: right;\">163470</td><td style=\"text-align: right;\">129.708</td><td style=\"text-align: right;\">            81.7765 </td><td style=\"text-align: right;\">          0.757523</td><td style=\"text-align: right;\">      81.7765 </td><td style=\"text-align: right;\"> 1691375629</td><td style=\"text-align: right;\">                 100</td><td>b6a24f3c  </td></tr>\n",
       "<tr><td>FSR_Trainable_b8834d63</td><td>2023-08-07_11-28-01</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 43.1491</td><td style=\"text-align: right;\">1.67162e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">156552</td><td style=\"text-align: right;\">140.885</td><td style=\"text-align: right;\">           108.061  </td><td style=\"text-align: right;\">          1.41236 </td><td style=\"text-align: right;\">     108.061  </td><td style=\"text-align: right;\"> 1691375281</td><td style=\"text-align: right;\">                  64</td><td>b8834d63  </td></tr>\n",
       "<tr><td>FSR_Trainable_b9b7e0cd</td><td>2023-08-07_11-15-42</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 98.2116</td><td style=\"text-align: right;\">8.46198e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">144837</td><td style=\"text-align: right;\">338.23 </td><td style=\"text-align: right;\">             4.89942</td><td style=\"text-align: right;\">          4.89942 </td><td style=\"text-align: right;\">       4.89942</td><td style=\"text-align: right;\"> 1691374542</td><td style=\"text-align: right;\">                   1</td><td>b9b7e0cd  </td></tr>\n",
       "<tr><td>FSR_Trainable_bb67c17d</td><td>2023-08-07_11-29-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 41.4376</td><td style=\"text-align: right;\">1.75178e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">158570</td><td style=\"text-align: right;\">129.48 </td><td style=\"text-align: right;\">            75.7342 </td><td style=\"text-align: right;\">          0.746547</td><td style=\"text-align: right;\">      75.7342 </td><td style=\"text-align: right;\"> 1691375376</td><td style=\"text-align: right;\">                 100</td><td>bb67c17d  </td></tr>\n",
       "<tr><td>FSR_Trainable_befdffb8</td><td>2023-08-07_11-15-23</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 68.5896</td><td style=\"text-align: right;\">6.05069e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">144401</td><td style=\"text-align: right;\">228.625</td><td style=\"text-align: right;\">             6.28566</td><td style=\"text-align: right;\">          6.28566 </td><td style=\"text-align: right;\">       6.28566</td><td style=\"text-align: right;\"> 1691374523</td><td style=\"text-align: right;\">                   1</td><td>befdffb8  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1846657</td><td>2023-08-07_11-30-36</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 61.3387</td><td style=\"text-align: right;\">3.27371e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">161138</td><td style=\"text-align: right;\">177.204</td><td style=\"text-align: right;\">             9.78158</td><td style=\"text-align: right;\">          0.786491</td><td style=\"text-align: right;\">       9.78158</td><td style=\"text-align: right;\"> 1691375436</td><td style=\"text-align: right;\">                  16</td><td>c1846657  </td></tr>\n",
       "<tr><td>FSR_Trainable_c1882557</td><td>2023-08-07_11-14-38</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 67.3359</td><td style=\"text-align: right;\">2.68211e+15</td><td>172.26.215.93</td><td style=\"text-align: right;\">143513</td><td style=\"text-align: right;\">226.414</td><td style=\"text-align: right;\">             7.58532</td><td style=\"text-align: right;\">          4.8274  </td><td style=\"text-align: right;\">       7.58532</td><td style=\"text-align: right;\"> 1691374478</td><td style=\"text-align: right;\">                   2</td><td>c1882557  </td></tr>\n",
       "<tr><td>FSR_Trainable_c74786a2</td><td>2023-08-07_11-20-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 69.8521</td><td style=\"text-align: right;\">2.26913e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">150234</td><td style=\"text-align: right;\">223.135</td><td style=\"text-align: right;\">            18.3618 </td><td style=\"text-align: right;\">          2.4466  </td><td style=\"text-align: right;\">      18.3618 </td><td style=\"text-align: right;\"> 1691374827</td><td style=\"text-align: right;\">                   8</td><td>c74786a2  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc4fa8b3</td><td>2023-08-07_11-25-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 64.5675</td><td style=\"text-align: right;\">2.97755e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">155558</td><td style=\"text-align: right;\">184.022</td><td style=\"text-align: right;\">            92.49   </td><td style=\"text-align: right;\">          3.45281 </td><td style=\"text-align: right;\">      92.49   </td><td style=\"text-align: right;\"> 1691375149</td><td style=\"text-align: right;\">                  32</td><td>cc4fa8b3  </td></tr>\n",
       "<tr><td>FSR_Trainable_cc7d2bfc</td><td>2023-08-07_11-17-56</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        16</td><td style=\"text-align: right;\"> 58.6084</td><td style=\"text-align: right;\">1.77314e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">146198</td><td style=\"text-align: right;\">189.682</td><td style=\"text-align: right;\">            73.0608 </td><td style=\"text-align: right;\">          3.80494 </td><td style=\"text-align: right;\">      73.0608 </td><td style=\"text-align: right;\"> 1691374676</td><td style=\"text-align: right;\">                  16</td><td>cc7d2bfc  </td></tr>\n",
       "<tr><td>FSR_Trainable_ccb957dd</td><td>2023-08-07_11-19-17</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 83.3859</td><td style=\"text-align: right;\">6.80341e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">148642</td><td style=\"text-align: right;\">234.244</td><td style=\"text-align: right;\">            13.3721 </td><td style=\"text-align: right;\">          1.38381 </td><td style=\"text-align: right;\">      13.3721 </td><td style=\"text-align: right;\"> 1691374757</td><td style=\"text-align: right;\">                   8</td><td>ccb957dd  </td></tr>\n",
       "<tr><td>FSR_Trainable_d2202bb1</td><td>2023-08-07_11-19-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 76.868 </td><td style=\"text-align: right;\">4.32336e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">149755</td><td style=\"text-align: right;\">221.401</td><td style=\"text-align: right;\">             2.4717 </td><td style=\"text-align: right;\">          1.11598 </td><td style=\"text-align: right;\">       2.4717 </td><td style=\"text-align: right;\"> 1691374792</td><td style=\"text-align: right;\">                   2</td><td>d2202bb1  </td></tr>\n",
       "<tr><td>FSR_Trainable_d3fa182e</td><td>2023-08-07_11-19-37</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 70.1534</td><td style=\"text-align: right;\">1.11835e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">149316</td><td style=\"text-align: right;\">221.464</td><td style=\"text-align: right;\">             7.36227</td><td style=\"text-align: right;\">          3.3969  </td><td style=\"text-align: right;\">       7.36227</td><td style=\"text-align: right;\"> 1691374777</td><td style=\"text-align: right;\">                   2</td><td>d3fa182e  </td></tr>\n",
       "<tr><td>FSR_Trainable_d4678cd6</td><td>2023-08-07_11-17-24</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 67.3152</td><td style=\"text-align: right;\">2.43082e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">146411</td><td style=\"text-align: right;\">209.983</td><td style=\"text-align: right;\">            34.6343 </td><td style=\"text-align: right;\">          3.49481 </td><td style=\"text-align: right;\">      34.6343 </td><td style=\"text-align: right;\"> 1691374644</td><td style=\"text-align: right;\">                   8</td><td>d4678cd6  </td></tr>\n",
       "<tr><td>FSR_Trainable_d6a4c920</td><td>2023-08-07_11-35-22</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 37.7401</td><td style=\"text-align: right;\">2.06971e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">164425</td><td style=\"text-align: right;\">116.738</td><td style=\"text-align: right;\">            69.8181 </td><td style=\"text-align: right;\">          0.636573</td><td style=\"text-align: right;\">      69.8181 </td><td style=\"text-align: right;\"> 1691375722</td><td style=\"text-align: right;\">                 100</td><td>d6a4c920  </td></tr>\n",
       "<tr><td>FSR_Trainable_d8e95a57</td><td>2023-08-07_11-19-16</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 67.318 </td><td style=\"text-align: right;\">2.30332e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">148869</td><td style=\"text-align: right;\">220.657</td><td style=\"text-align: right;\">             4.28883</td><td style=\"text-align: right;\">          1.93526 </td><td style=\"text-align: right;\">       4.28883</td><td style=\"text-align: right;\"> 1691374756</td><td style=\"text-align: right;\">                   2</td><td>d8e95a57  </td></tr>\n",
       "<tr><td>FSR_Trainable_df345400</td><td>2023-08-07_11-24-49</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 54.5488</td><td style=\"text-align: right;\">1.57123e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">154901</td><td style=\"text-align: right;\">173.946</td><td style=\"text-align: right;\">            57.1345 </td><td style=\"text-align: right;\">          1.94426 </td><td style=\"text-align: right;\">      57.1345 </td><td style=\"text-align: right;\"> 1691375089</td><td style=\"text-align: right;\">                  32</td><td>df345400  </td></tr>\n",
       "<tr><td>FSR_Trainable_e258f087</td><td>2023-08-07_11-22-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 79.4173</td><td style=\"text-align: right;\">4.10967e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">152887</td><td style=\"text-align: right;\">233.823</td><td style=\"text-align: right;\">             2.18783</td><td style=\"text-align: right;\">          2.18783 </td><td style=\"text-align: right;\">       2.18783</td><td style=\"text-align: right;\"> 1691374933</td><td style=\"text-align: right;\">                   1</td><td>e258f087  </td></tr>\n",
       "<tr><td>FSR_Trainable_e2c5fce9</td><td>2023-08-07_11-31-30</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 70.4287</td><td style=\"text-align: right;\">1.11945e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">162361</td><td style=\"text-align: right;\">231.177</td><td style=\"text-align: right;\">             1.90932</td><td style=\"text-align: right;\">          1.90932 </td><td style=\"text-align: right;\">       1.90932</td><td style=\"text-align: right;\"> 1691375490</td><td style=\"text-align: right;\">                   1</td><td>e2c5fce9  </td></tr>\n",
       "<tr><td>FSR_Trainable_e4c234af</td><td>2023-08-07_11-28-52</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        64</td><td style=\"text-align: right;\"> 42.2484</td><td style=\"text-align: right;\">2.997e+07  </td><td>172.26.215.93</td><td style=\"text-align: right;\">158353</td><td style=\"text-align: right;\">135.884</td><td style=\"text-align: right;\">            48.147  </td><td style=\"text-align: right;\">          0.688512</td><td style=\"text-align: right;\">      48.147  </td><td style=\"text-align: right;\"> 1691375332</td><td style=\"text-align: right;\">                  64</td><td>e4c234af  </td></tr>\n",
       "<tr><td>FSR_Trainable_e8e314b4</td><td>2023-08-07_11-18-27</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 79.476 </td><td style=\"text-align: right;\">1.58804e+17</td><td>172.26.215.93</td><td style=\"text-align: right;\">147744</td><td style=\"text-align: right;\">229.222</td><td style=\"text-align: right;\">             3.53901</td><td style=\"text-align: right;\">          3.53901 </td><td style=\"text-align: right;\">       3.53901</td><td style=\"text-align: right;\"> 1691374707</td><td style=\"text-align: right;\">                   1</td><td>e8e314b4  </td></tr>\n",
       "<tr><td>FSR_Trainable_e9d4a3fb</td><td>2023-08-07_11-23-41</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 77.7968</td><td style=\"text-align: right;\">5.95025e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">154678</td><td style=\"text-align: right;\">221.312</td><td style=\"text-align: right;\">             3.59068</td><td style=\"text-align: right;\">          1.53133 </td><td style=\"text-align: right;\">       3.59068</td><td style=\"text-align: right;\"> 1691375021</td><td style=\"text-align: right;\">                   2</td><td>e9d4a3fb  </td></tr>\n",
       "<tr><td>FSR_Trainable_f46c9aa6</td><td>2023-08-07_11-27-20</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         2</td><td style=\"text-align: right;\"> 64.7671</td><td style=\"text-align: right;\">1.72802e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">157647</td><td style=\"text-align: right;\">216.044</td><td style=\"text-align: right;\">             9.16254</td><td style=\"text-align: right;\">          4.14028 </td><td style=\"text-align: right;\">       9.16254</td><td style=\"text-align: right;\"> 1691375240</td><td style=\"text-align: right;\">                   2</td><td>f46c9aa6  </td></tr>\n",
       "<tr><td>FSR_Trainable_f57ea8ac</td><td>2023-08-07_11-31-40</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 82.3629</td><td style=\"text-align: right;\">2.24155e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">162561</td><td style=\"text-align: right;\">244.584</td><td style=\"text-align: right;\">             1.74749</td><td style=\"text-align: right;\">          1.74749 </td><td style=\"text-align: right;\">       1.74749</td><td style=\"text-align: right;\"> 1691375500</td><td style=\"text-align: right;\">                   1</td><td>f57ea8ac  </td></tr>\n",
       "<tr><td>FSR_Trainable_f81dfd5d</td><td>2023-08-07_11-31-13</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                        32</td><td style=\"text-align: right;\"> 49.8327</td><td style=\"text-align: right;\">2.08905e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">161448</td><td style=\"text-align: right;\">146.893</td><td style=\"text-align: right;\">            31.2532 </td><td style=\"text-align: right;\">          0.879911</td><td style=\"text-align: right;\">      31.2532 </td><td style=\"text-align: right;\"> 1691375473</td><td style=\"text-align: right;\">                  32</td><td>f81dfd5d  </td></tr>\n",
       "<tr><td>FSR_Trainable_fc273135</td><td>2023-08-07_11-15-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 36.3497</td><td style=\"text-align: right;\">3.91223e+16</td><td>172.26.215.93</td><td style=\"text-align: right;\">143189</td><td style=\"text-align: right;\">113.994</td><td style=\"text-align: right;\">            78.2125 </td><td style=\"text-align: right;\">          0.741831</td><td style=\"text-align: right;\">      78.2125 </td><td style=\"text-align: right;\"> 1691374546</td><td style=\"text-align: right;\">                 100</td><td>fc273135  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd2300e8</td><td>2023-08-07_11-24-05</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 78.7567</td><td style=\"text-align: right;\">3.37563e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">155337</td><td style=\"text-align: right;\">228.642</td><td style=\"text-align: right;\">             2.32853</td><td style=\"text-align: right;\">          2.32853 </td><td style=\"text-align: right;\">       2.32853</td><td style=\"text-align: right;\"> 1691375045</td><td style=\"text-align: right;\">                   1</td><td>fd2300e8  </td></tr>\n",
       "<tr><td>FSR_Trainable_fd9a5c69</td><td>2023-08-07_11-31-57</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                         8</td><td style=\"text-align: right;\"> 77.2381</td><td style=\"text-align: right;\">2.26832e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">162785</td><td style=\"text-align: right;\">238.88 </td><td style=\"text-align: right;\">             7.23676</td><td style=\"text-align: right;\">          0.822189</td><td style=\"text-align: right;\">       7.23676</td><td style=\"text-align: right;\"> 1691375517</td><td style=\"text-align: right;\">                   8</td><td>fd9a5c69  </td></tr>\n",
       "<tr><td>FSR_Trainable_ff956aaf</td><td>2023-08-07_11-26-46</td><td>True  </td><td>DESKTOP-0P789CI</td><td style=\"text-align: right;\">                       100</td><td style=\"text-align: right;\"> 41.9273</td><td style=\"text-align: right;\">1.76688e+07</td><td>172.26.215.93</td><td style=\"text-align: right;\">156072</td><td style=\"text-align: right;\">129.3  </td><td style=\"text-align: right;\">            91.7563 </td><td style=\"text-align: right;\">          0.864134</td><td style=\"text-align: right;\">      91.7563 </td><td style=\"text-align: right;\"> 1691375206</td><td style=\"text-align: right;\">                 100</td><td>ff956aaf  </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_8b662eb9_1_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-13-33/wandb/run-20230807_111344-8b662eb9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Syncing run FSR_Trainable_8b662eb9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8b662eb9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_51c64226_2_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-13-39/wandb/run-20230807_111352-51c64226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Syncing run FSR_Trainable_51c64226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/51c64226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                      mae 75.74221\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                     mape 31665130.15812\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                     rmse 234.75658\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:       time_since_restore 1.54588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:         time_this_iter_s 1.54588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:             time_total_s 1.54588\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:                timestamp 1691374427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: 🚀 View run FSR_Trainable_51c64226 at: https://wandb.ai/seokjin/FSR-prediction/runs/51c64226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143013)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111352-51c64226/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_8aa78cb2_3_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-13-45/wandb/run-20230807_111403-8aa78cb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Syncing run FSR_Trainable_8aa78cb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8aa78cb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                      mae 106.91897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                     mape 1.1683966999285298e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                     rmse 326.46026\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:       time_since_restore 5.97125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:         time_this_iter_s 5.97125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:             time_total_s 5.97125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:                timestamp 1691374440\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: 🚀 View run FSR_Trainable_8aa78cb2 at: https://wandb.ai/seokjin/FSR-prediction/runs/8aa78cb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143188)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111403-8aa78cb2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_fc273135_4_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-13-54/wandb/run-20230807_111412-fc273135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Syncing run FSR_Trainable_fc273135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fc273135\n",
      "2023-08-07 11:14:33,806\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 7.821 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:14:33,809\tWARNING util.py:315 -- The `process_trial_result` operation took 7.825 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:14:33,811\tWARNING util.py:315 -- Processing trial results took 7.827 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:14:33,812\tWARNING util.py:315 -- The `process_trial_result` operation took 7.828 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_c1882557_5_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-14-05/wandb/run-20230807_111435-c1882557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Syncing run FSR_Trainable_c1882557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1882557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                      mae 67.33595\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                     mape 2682106243579538.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                     rmse 226.41368\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:       time_since_restore 7.58532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:         time_this_iter_s 4.8274\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:             time_total_s 7.58532\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:                timestamp 1691374478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: 🚀 View run FSR_Trainable_c1882557 at: https://wandb.ai/seokjin/FSR-prediction/runs/c1882557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143600)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111435-c1882557/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:                      mae ▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:                     mape ▃▃▂▂▂▁▁▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:                     rmse ▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:         time_this_iter_s ▂▂▁▂▃▁▁▁▂▃▃▃▁▁▃▃▂▂▂▁▂▁▁▁▂▃▂██▄▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=142840)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111344-8b662eb9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 11:14:53,056\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.647 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:14:53,060\tWARNING util.py:315 -- The `process_trial_result` operation took 1.651 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:14:53,061\tWARNING util.py:315 -- Processing trial results took 1.653 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:14:53,063\tWARNING util.py:315 -- The `process_trial_result` operation took 1.655 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_6eda6ff5_6_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-14-23/wandb/run-20230807_111452-6eda6ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Syncing run FSR_Trainable_6eda6ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6eda6ff5\n",
      "2023-08-07 11:15:01,194\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.057 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:01,199\tWARNING util.py:315 -- The `process_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:01,201\tWARNING util.py:315 -- Processing trial results took 2.065 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:15:01,203\tWARNING util.py:315 -- The `process_trial_result` operation took 2.067 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_6017eba4_7_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-14-46/wandb/run-20230807_111501-6017eba4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Syncing run FSR_Trainable_6017eba4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6017eba4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                      mae 81.38931\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                     mape 7141832009616771.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                     rmse 282.14407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:       time_since_restore 3.99962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:         time_this_iter_s 3.99962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:             time_total_s 3.99962\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:                timestamp 1691374499\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: 🚀 View run FSR_Trainable_6017eba4 at: https://wandb.ai/seokjin/FSR-prediction/runs/6017eba4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144058)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111501-6017eba4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                      mae █▇▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                     mape ▁▁▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                     rmse █▇▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:         time_this_iter_s ▄▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                      mae 63.12683\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                     mape 4706277198697995.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                     rmse 214.43533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:       time_since_restore 21.45402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:         time_this_iter_s 5.91364\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:             time_total_s 21.45402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:                timestamp 1691374509\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: 🚀 View run FSR_Trainable_6eda6ff5 at: https://wandb.ai/seokjin/FSR-prediction/runs/6eda6ff5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143836)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111452-6eda6ff5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_861e35b5_8_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-14-55/wandb/run-20230807_111513-861e35b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Syncing run FSR_Trainable_861e35b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/861e35b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "2023-08-07 11:15:25,455\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.771 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:25,460\tWARNING util.py:315 -- The `process_trial_result` operation took 1.777 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:25,462\tWARNING util.py:315 -- Processing trial results took 1.779 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:15:25,466\tWARNING util.py:315 -- The `process_trial_result` operation took 1.782 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_befdffb8_9_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simpl_2023-08-07_11-15-05/wandb/run-20230807_111524-befdffb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Syncing run FSR_Trainable_befdffb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/befdffb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:15:28,079\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:28,081\tWARNING util.py:315 -- The `process_trial_result` operation took 1.427 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:28,082\tWARNING util.py:315 -- Processing trial results took 1.428 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:15:28,084\tWARNING util.py:315 -- The `process_trial_result` operation took 1.430 s, which may be a performance bottleneck.\n",
      "wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)04 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                      mae 68.58961\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                     mape 6050689429528089.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                     rmse 228.6247\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:       time_since_restore 6.28566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:         time_this_iter_s 6.28566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:             time_total_s 6.28566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:                timestamp 1691374523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: 🚀 View run FSR_Trainable_befdffb8 at: https://wandb.ai/seokjin/FSR-prediction/runs/befdffb8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144499)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111524-befdffb8/logs\n",
      "2023-08-07 11:15:32,318\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.042 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:32,325\tWARNING util.py:315 -- The `process_trial_result` operation took 2.049 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:32,326\tWARNING util.py:315 -- Processing trial results took 2.051 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:15:32,328\tWARNING util.py:315 -- The `process_trial_result` operation took 2.053 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_8da6b371_10_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-15-17/wandb/run-20230807_111534-8da6b371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Syncing run FSR_Trainable_8da6b371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8da6b371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                      mae 92.39575\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                     mape 1.7769186979438506e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                     rmse 229.5851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:       time_since_restore 2.11444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:         time_this_iter_s 2.11444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:             time_total_s 2.11444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:                timestamp 1691374530\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: 🚀 View run FSR_Trainable_8da6b371 at: https://wandb.ai/seokjin/FSR-prediction/runs/8da6b371\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144725)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111534-8da6b371/logs\n",
      "2023-08-07 11:15:44,166\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:44,171\tWARNING util.py:315 -- The `process_trial_result` operation took 1.865 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:44,173\tWARNING util.py:315 -- Processing trial results took 1.867 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:15:44,175\tWARNING util.py:315 -- The `process_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_b9b7e0cd_11_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-15-28/wandb/run-20230807_111545-b9b7e0cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: Syncing run FSR_Trainable_b9b7e0cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b9b7e0cd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)08 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                      mae █▅▄▄▅▅▅▄▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                     mape █▅▅▅▅▅▆▆▅▅▄▄▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                     rmse █▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:         time_this_iter_s ▄▂▂▂▂▁▂▁▂▃█▆▂▁▂▂▂▁▁▃▂▁▂▃▄▃▃▂▁▃▃▃▃▂▂▂▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                      mae 36.34973\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                     mape 3.912229941790877e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                     rmse 113.99372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:       time_since_restore 78.21248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:         time_this_iter_s 0.74183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:             time_total_s 78.21248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:                timestamp 1691374546\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: 🚀 View run FSR_Trainable_fc273135 at: https://wandb.ai/seokjin/FSR-prediction/runs/fc273135\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=143377)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111412-fc273135/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb:       training_iteration ▁\n",
      "2023-08-07 11:15:51,172\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.948 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:51,177\tWARNING util.py:315 -- The `process_trial_result` operation took 1.954 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:15:51,181\tWARNING util.py:315 -- Processing trial results took 1.958 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:15:51,183\tWARNING util.py:315 -- The `process_trial_result` operation took 1.960 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_8f6e4462_12_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-15-37/wandb/run-20230807_111553-8f6e4462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Syncing run FSR_Trainable_8f6e4462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8f6e4462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144946)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                      mae 66.99248\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                     mape 2195726856821770.2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                     rmse 227.89788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:       time_since_restore 2.83379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:         time_this_iter_s 1.26055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:             time_total_s 2.83379\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:                timestamp 1691374552\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: 🚀 View run FSR_Trainable_8f6e4462 at: https://wandb.ai/seokjin/FSR-prediction/runs/8f6e4462\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111553-8f6e4462/logs\n",
      "2023-08-07 11:16:01,441\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.965 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:01,443\tWARNING util.py:315 -- The `process_trial_result` operation took 1.968 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:01,446\tWARNING util.py:315 -- Processing trial results took 1.971 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:16:01,448\tWARNING util.py:315 -- The `process_trial_result` operation took 1.973 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145174)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_56483a0a_13_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-15-47/wandb/run-20230807_111602-56483a0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: Syncing run FSR_Trainable_56483a0a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56483a0a\n",
      "2023-08-07 11:16:08,611\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.568 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:08,616\tWARNING util.py:315 -- The `process_trial_result` operation took 2.573 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:08,618\tWARNING util.py:315 -- Processing trial results took 2.576 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:16:08,620\tWARNING util.py:315 -- The `process_trial_result` operation took 2.578 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_b6316a83_14_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-15-56/wandb/run-20230807_111612-b6316a83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Syncing run FSR_Trainable_b6316a83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6316a83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                      mae 82.02625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                     mape 36277806.92557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                     rmse 230.39708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:       time_since_restore 1.01019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:         time_this_iter_s 1.01019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:             time_total_s 1.01019\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:                timestamp 1691374566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: 🚀 View run FSR_Trainable_b6316a83 at: https://wandb.ai/seokjin/FSR-prediction/runs/b6316a83\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145617)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111612-b6316a83/logs\n",
      "2023-08-07 11:16:23,814\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.248 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:23,817\tWARNING util.py:315 -- The `process_trial_result` operation took 2.252 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:23,819\tWARNING util.py:315 -- Processing trial results took 2.254 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:16:23,820\tWARNING util.py:315 -- The `process_trial_result` operation took 2.256 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_9f743014_15_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-16-05/wandb/run-20230807_111626-9f743014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Syncing run FSR_Trainable_9f743014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9f743014\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                      mae ▁█▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                     mape ▂█▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                     rmse ▆▁█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:         time_this_iter_s █▃▁▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                timestamp ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                      mae 69.45182\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                     mape 3.609473938602488e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                     rmse 222.66786\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:       time_since_restore 76.55596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:         time_this_iter_s 20.18016\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:             time_total_s 76.55596\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:                timestamp 1691374583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: 🚀 View run FSR_Trainable_861e35b5 at: https://wandb.ai/seokjin/FSR-prediction/runs/861e35b5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=144277)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111513-861e35b5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:                      mae █▆▁▂▄▂▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:                     mape ▁▇▄▄▅█▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:                     rmse █▄▁▃▅▁▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:         time_this_iter_s ▅▁▃█▅▇▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:                timestamp ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145399)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                      mae █▇▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                     mape █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                     rmse █▆▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:         time_this_iter_s █▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111626-9f743014/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: iterations_since_restore 4\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                      mae 77.18267\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                     mape 49161362.54505\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                     rmse 220.79322\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:       time_since_restore 3.3045\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:         time_this_iter_s 0.65439\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:             time_total_s 3.3045\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:                timestamp 1691374586\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb:       training_iteration 4\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: 🚀 View run FSR_Trainable_9f743014 at: https://wandb.ai/seokjin/FSR-prediction/runs/9f743014\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=145843)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111626-9f743014/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_6bb5c72e_16_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-16-20/wandb/run-20230807_111635-6bb5c72e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Syncing run FSR_Trainable_6bb5c72e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6bb5c72e\n",
      "2023-08-07 11:16:38,624\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.185 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:38,627\tWARNING util.py:315 -- The `process_trial_result` operation took 1.189 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:38,628\tWARNING util.py:315 -- Processing trial results took 1.190 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:16:38,629\tWARNING util.py:315 -- The `process_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_cc7d2bfc_17_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-16-30/wandb/run-20230807_111644-cc7d2bfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Syncing run FSR_Trainable_cc7d2bfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc7d2bfc\n",
      "2023-08-07 11:16:45,520\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.727 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:45,524\tWARNING util.py:315 -- The `process_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:45,528\tWARNING util.py:315 -- Processing trial results took 1.736 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:16:45,531\tWARNING util.py:315 -- The `process_trial_result` operation took 1.739 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_d4678cd6_18_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-16-38/wandb/run-20230807_111654-d4678cd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Syncing run FSR_Trainable_d4678cd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d4678cd6\n",
      "2023-08-07 11:16:56,220\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:56,223\tWARNING util.py:315 -- The `process_trial_result` operation took 1.950 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:16:56,225\tWARNING util.py:315 -- Processing trial results took 1.952 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:16:56,226\tWARNING util.py:315 -- The `process_trial_result` operation took 1.953 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_48f428e2_19_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-16-47/wandb/run-20230807_111704-48f428e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Syncing run FSR_Trainable_48f428e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/48f428e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:17:09,418\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:17:09,422\tWARNING util.py:315 -- The `process_trial_result` operation took 1.849 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:17:09,424\tWARNING util.py:315 -- Processing trial results took 1.851 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:17:09,426\tWARNING util.py:315 -- The `process_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                      mae ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                     mape ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                     rmse █▅▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:         time_this_iter_s ▁▃██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                timestamp ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                      mae 76.40194\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                     mape 1.460521063348315e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                     rmse 222.29708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:       time_since_restore 32.63622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:         time_this_iter_s 8.77326\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:             time_total_s 32.63622\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:                timestamp 1691374625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: 🚀 View run FSR_Trainable_6bb5c72e at: https://wandb.ai/seokjin/FSR-prediction/runs/6bb5c72e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146079)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111635-6bb5c72e/logs\n",
      "2023-08-07 11:17:21,212\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.746 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:17:21,215\tWARNING util.py:315 -- The `process_trial_result` operation took 1.749 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:17:21,217\tWARNING util.py:315 -- Processing trial results took 1.752 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:17:21,219\tWARNING util.py:315 -- The `process_trial_result` operation took 1.753 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_1b594dfc_20_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-16-58/wandb/run-20230807_111721-1b594dfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Syncing run FSR_Trainable_1b594dfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1b594dfc\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                      mae ▁▄▆▂▃▃▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                     mape ▁▄▃▃▅█▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                     rmse ▃▂▆▁▄▃▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:         time_this_iter_s █▄▄▃▂▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                timestamp ▁▂▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                      mae 67.31523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                     mape 24308175.75594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                     rmse 209.98311\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:       time_since_restore 34.63433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:         time_this_iter_s 3.49481\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:             time_total_s 34.63433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:                timestamp 1691374644\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: 🚀 View run FSR_Trainable_d4678cd6 at: https://wandb.ai/seokjin/FSR-prediction/runs/d4678cd6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146507)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111654-d4678cd6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_00a89f50_21_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-17-15/wandb/run-20230807_111739-00a89f50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Syncing run FSR_Trainable_00a89f50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/00a89f50\n",
      "2023-08-07 11:17:40,814\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.840 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:17:40,817\tWARNING util.py:315 -- The `process_trial_result` operation took 1.843 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:17:40,818\tWARNING util.py:315 -- Processing trial results took 1.845 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:17:40,820\tWARNING util.py:315 -- The `process_trial_result` operation took 1.846 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                      mae ▃▂█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                     mape ▃▂█▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                     rmse ▄▁█▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:         time_this_iter_s █▂▁▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                      mae 67.0777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                     mape 15519592.44129\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                     rmse 216.37965\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:       time_since_restore 19.49533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:         time_this_iter_s 5.49213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:             time_total_s 19.49533\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:                timestamp 1691374674\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: 🚀 View run FSR_Trainable_00a89f50 at: https://wandb.ai/seokjin/FSR-prediction/runs/00a89f50\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147171)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111739-00a89f50/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:                      mae ▂▄▆▂▅█▅▄▇▅▅▃▄▂▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:                     mape █▄█▄▆▇▆▆▆▅▄▃▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:                     rmse ▃▄▃▃▄█▅▃▇█▇▄▄▁▄▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:         time_this_iter_s ▄█▃▅▃▁▂▂▄▂▄▃▂▂▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▄▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111644-cc7d2bfc/logs\n",
      "2023-08-07 11:18:09,564\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.875 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:09,566\tWARNING util.py:315 -- The `process_trial_result` operation took 1.879 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:09,570\tWARNING util.py:315 -- Processing trial results took 1.882 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:18:09,572\tWARNING util.py:315 -- The `process_trial_result` operation took 1.884 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146298)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_378dbc6f_22_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-17-33/wandb/run-20230807_111809-378dbc6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Syncing run FSR_Trainable_378dbc6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/378dbc6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                      mae 72.83632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                     mape 1.3099627164678571e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                     rmse 227.7936\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:       time_since_restore 8.1365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:         time_this_iter_s 3.79844\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:             time_total_s 8.1365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:                timestamp 1691374693\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: 🚀 View run FSR_Trainable_378dbc6f at: https://wandb.ai/seokjin/FSR-prediction/runs/378dbc6f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147416)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111809-378dbc6f/logs\n",
      "2023-08-07 11:18:18,505\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.449 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:18,508\tWARNING util.py:315 -- The `process_trial_result` operation took 2.453 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:18,509\tWARNING util.py:315 -- Processing trial results took 2.454 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:18:18,511\tWARNING util.py:315 -- The `process_trial_result` operation took 2.456 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_a5f78c90_23_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-18-03/wandb/run-20230807_111819-a5f78c90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Syncing run FSR_Trainable_a5f78c90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a5f78c90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:                      mae ▁▃▂▂▁█▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:                     mape ▆█▆▄▃▄▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:                     rmse ▁▄▄▃▃█▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:       time_since_restore ▁▂▃▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:         time_this_iter_s ▄▂█▇▅▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:             time_total_s ▁▂▃▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:                timestamp ▁▂▃▅▆▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: 🚀 View run FSR_Trainable_48f428e2 at: https://wandb.ai/seokjin/FSR-prediction/runs/48f428e2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146717)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111704-48f428e2/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:18:29,119\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.012 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:29,124\tWARNING util.py:315 -- The `process_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:29,126\tWARNING util.py:315 -- Processing trial results took 2.020 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:18:29,129\tWARNING util.py:315 -- The `process_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                      mae ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                     mape ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                     rmse ▆▁▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:       time_since_restore ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:         time_this_iter_s █▄▁▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:             time_total_s ▁▃▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                      mae 78.38365\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                     mape 1.5470159424074198e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                     rmse 220.75896\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:       time_since_restore 10.49067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:         time_this_iter_s 2.77605\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:             time_total_s 10.49067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:                timestamp 1691374705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: 🚀 View run FSR_Trainable_a5f78c90 at: https://wandb.ai/seokjin/FSR-prediction/runs/a5f78c90\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147624)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111819-a5f78c90/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_e8e314b4_24_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-18-12/wandb/run-20230807_111829-e8e314b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Syncing run FSR_Trainable_e8e314b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e8e314b4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111829-e8e314b4/logs\n",
      "2023-08-07 11:18:37,474\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.888 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:37,477\tWARNING util.py:315 -- The `process_trial_result` operation took 1.892 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:37,481\tWARNING util.py:315 -- Processing trial results took 1.895 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:18:37,483\tWARNING util.py:315 -- The `process_trial_result` operation took 1.897 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=147863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_5a041cb2_25_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-18-23/wandb/run-20230807_111838-5a041cb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: Syncing run FSR_Trainable_5a041cb2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5a041cb2\n",
      "2023-08-07 11:18:46,951\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.983 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:46,966\tWARNING util.py:315 -- The `process_trial_result` operation took 1.998 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:46,968\tWARNING util.py:315 -- Processing trial results took 2.000 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:18:46,970\tWARNING util.py:315 -- The `process_trial_result` operation took 2.002 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_80e6373b_26_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-18-32/wandb/run-20230807_111847-80e6373b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Syncing run FSR_Trainable_80e6373b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/80e6373b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: / 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                      mae 81.74464\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                     mape 45630041.19177\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                     rmse 229.62433\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:       time_since_restore 3.82645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:         time_this_iter_s 3.82645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:             time_total_s 3.82645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:                timestamp 1691374724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: 🚀 View run FSR_Trainable_80e6373b at: https://wandb.ai/seokjin/FSR-prediction/runs/80e6373b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148306)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111847-80e6373b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:                      mae ▅▅▅█▅▅▃▁▇▄█▆▅▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:                     mape ▃▆▆██▆▃▁▅▁▃▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:                     rmse ▃▄▂▆▃▁▂▁▆▃█▇▆▅▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:       time_since_restore ▁▂▂▃▄▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:         time_this_iter_s ▂▆▄▇█▂▁▂▃▅▄▂▂▄▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:             time_total_s ▁▂▂▃▄▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:                timestamp ▁▂▂▃▄▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "2023-08-07 11:18:57,130\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.697 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:57,132\tWARNING util.py:315 -- The `process_trial_result` operation took 1.700 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:18:57,134\tWARNING util.py:315 -- Processing trial results took 1.701 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:18:57,135\tWARNING util.py:315 -- The `process_trial_result` operation took 1.703 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_147eba8a_27_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-18-41/wandb/run-20230807_111857-147eba8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Syncing run FSR_Trainable_147eba8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/147eba8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/147eba8a\n",
      "2023-08-07 11:19:04,828\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:04,832\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:04,833\tWARNING util.py:315 -- Processing trial results took 1.879 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:19:04,835\tWARNING util.py:315 -- The `process_trial_result` operation took 1.881 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=146945)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_ccb957dd_28_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-18-50/wandb/run-20230807_111906-ccb957dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Syncing run FSR_Trainable_ccb957dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ccb957dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                      mae ▅▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                     mape █▆▆▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                     rmse ▁▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:         time_this_iter_s █▁▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                timestamp ▁▄▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                      mae 65.16654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                     mape 10039766.13603\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                     rmse 226.44769\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:       time_since_restore 15.4134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:         time_this_iter_s 3.65428\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:             time_total_s 15.4134\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:                timestamp 1691374748\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: 🚀 View run FSR_Trainable_147eba8a at: https://wandb.ai/seokjin/FSR-prediction/runs/147eba8a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111857-147eba8a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148524)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-07 11:19:14,530\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.804 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:14,533\tWARNING util.py:315 -- The `process_trial_result` operation took 1.808 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:14,535\tWARNING util.py:315 -- Processing trial results took 1.810 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:19:14,537\tWARNING util.py:315 -- The `process_trial_result` operation took 1.811 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_d8e95a57_29_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-19-00/wandb/run-20230807_111916-d8e95a57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Syncing run FSR_Trainable_d8e95a57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d8e95a57\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                      mae ▁▂▂▂▂▃▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                     mape ▁▄▂▃▂▁▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                     rmse ▄▂▄▂▁▆▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:         time_this_iter_s █▃▃▁▂▄▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                timestamp ▁▃▄▄▅▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                      mae 83.38594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                     mape 68034090.88444\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                     rmse 234.24441\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:       time_since_restore 13.37207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:         time_this_iter_s 1.38381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:             time_total_s 13.37207\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:                timestamp 1691374757\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: 🚀 View run FSR_Trainable_ccb957dd at: https://wandb.ai/seokjin/FSR-prediction/runs/ccb957dd\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111906-ccb957dd/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb:       training_iteration ▁█\n",
      "2023-08-07 11:19:23,462\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.783 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:23,464\tWARNING util.py:315 -- The `process_trial_result` operation took 1.787 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:23,467\tWARNING util.py:315 -- Processing trial results took 1.789 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:19:23,471\tWARNING util.py:315 -- The `process_trial_result` operation took 1.793 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148756)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_266f4215_30_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-19-10/wandb/run-20230807_111925-266f4215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: Syncing run FSR_Trainable_266f4215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/266f4215\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/266f4215\n",
      "2023-08-07 11:19:34,156\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:34,159\tWARNING util.py:315 -- The `process_trial_result` operation took 1.916 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:34,160\tWARNING util.py:315 -- Processing trial results took 1.917 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:19:34,162\tWARNING util.py:315 -- The `process_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148971)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_d3fa182e_31_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-19-19/wandb/run-20230807_111934-d3fa182e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Syncing run FSR_Trainable_d3fa182e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d3fa182e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                      mae 70.15343\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                     mape 1.118354640941161e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                     rmse 221.46363\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:       time_since_restore 7.36227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:         time_this_iter_s 3.3969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:             time_total_s 7.36227\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:                timestamp 1691374777\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: 🚀 View run FSR_Trainable_d3fa182e at: https://wandb.ai/seokjin/FSR-prediction/runs/d3fa182e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149424)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111934-d3fa182e/logs\n",
      "2023-08-07 11:19:44,149\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.859 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:44,152\tWARNING util.py:315 -- The `process_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:44,154\tWARNING util.py:315 -- Processing trial results took 1.865 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:19:44,155\tWARNING util.py:315 -- The `process_trial_result` operation took 1.866 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_1cec908a_32_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-19-28/wandb/run-20230807_111944-1cec908a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Syncing run FSR_Trainable_1cec908a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1cec908a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:19:51,564\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.660 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:51,567\tWARNING util.py:315 -- The `process_trial_result` operation took 1.664 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:19:51,568\tWARNING util.py:315 -- Processing trial results took 1.666 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:19:51,570\tWARNING util.py:315 -- The `process_trial_result` operation took 1.667 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                      mae 68.07405\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                     mape 1.0202777876417618e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                     rmse 220.21583\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:       time_since_restore 7.47123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:         time_this_iter_s 3.43374\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:             time_total_s 7.47123\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:                timestamp 1691374787\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: 🚀 View run FSR_Trainable_1cec908a at: https://wandb.ai/seokjin/FSR-prediction/runs/1cec908a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149639)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111944-1cec908a/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_d2202bb1_33_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-19-38/wandb/run-20230807_111954-d2202bb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Syncing run FSR_Trainable_d2202bb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d2202bb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:                      mae █▃▁▂▄▂▃▂▄▄▃▅▃▄▄▃▁▄▂▃▄▃▇▄▃▅▆▄▅▃▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:                     mape █▃▃▃▄▃▃▁▅▄▅▅▅▅▅▄▃▄▅▅▅▄▅▄▃▃▂▁▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:                     rmse █▄▁▂▃▂▃▃▄▄▃▅▃▄▄▃▂▄▂▃▃▃▇▄▄▇▆▄▅▄▇▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:         time_this_iter_s █▃▂▄▃▂▂▄▆▂▂▄▃▂▂▄▃▁▁▂▂▁▂▃▃▃▂▄▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=148088)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                      mae 76.86803\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                     mape 43233641.41524\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                     rmse 221.40126\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:       time_since_restore 2.4717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:         time_this_iter_s 1.11598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:             time_total_s 2.4717\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:                timestamp 1691374792\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: 🚀 View run FSR_Trainable_d2202bb1 at: https://wandb.ai/seokjin/FSR-prediction/runs/d2202bb1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111954-d2202bb1/logs\n",
      "2023-08-07 11:20:00,855\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.513 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:00,858\tWARNING util.py:315 -- The `process_trial_result` operation took 1.516 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:00,860\tWARNING util.py:315 -- Processing trial results took 1.518 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:20:00,861\tWARNING util.py:315 -- The `process_trial_result` operation took 1.520 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149863)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_2d937434_34_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-19-48/wandb/run-20230807_112003-2d937434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Syncing run FSR_Trainable_2d937434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2d937434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                      mae 78.71218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                     mape 38436156.43926\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                     rmse 225.50478\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:       time_since_restore 1.16907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:         time_this_iter_s 1.16907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:             time_total_s 1.16907\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:                timestamp 1691374799\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: 🚀 View run FSR_Trainable_2d937434 at: https://wandb.ai/seokjin/FSR-prediction/runs/2d937434\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112003-2d937434/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150101)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-07 11:20:11,505\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.628 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:11,510\tWARNING util.py:315 -- The `process_trial_result` operation took 1.634 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:11,512\tWARNING util.py:315 -- Processing trial results took 1.636 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:20:11,513\tWARNING util.py:315 -- The `process_trial_result` operation took 1.637 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_c74786a2_35_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-19-58/wandb/run-20230807_112012-c74786a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Syncing run FSR_Trainable_c74786a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c74786a2\n",
      "2023-08-07 11:20:20,672\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.047 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:20,675\tWARNING util.py:315 -- The `process_trial_result` operation took 2.052 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:20,677\tWARNING util.py:315 -- Processing trial results took 2.054 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:20:20,683\tWARNING util.py:315 -- The `process_trial_result` operation took 2.060 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_9a20197e_36_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-20-07/wandb/run-20230807_112022-9a20197e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Syncing run FSR_Trainable_9a20197e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9a20197e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                      mae ▁▃▂▇▃▅▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                     mape ██▃▇▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                     rmse ▁▃▅▄▅▇▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:         time_this_iter_s █▅▁▆▂▄▂▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                timestamp ▁▃▃▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                      mae 69.85213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                     mape 22691290.87883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                     rmse 223.13514\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:       time_since_restore 18.3618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:         time_this_iter_s 2.4466\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:             time_total_s 18.3618\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:                timestamp 1691374827\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: 🚀 View run FSR_Trainable_c74786a2 at: https://wandb.ai/seokjin/FSR-prediction/runs/c74786a2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150321)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112012-c74786a2/logs\n",
      "2023-08-07 11:20:32,446\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.090 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:32,451\tWARNING util.py:315 -- The `process_trial_result` operation took 2.096 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:32,453\tWARNING util.py:315 -- Processing trial results took 2.098 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:20:32,455\tWARNING util.py:315 -- The `process_trial_result` operation took 2.101 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_a37947a9_37_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-20-15/wandb/run-20230807_112034-a37947a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Syncing run FSR_Trainable_a37947a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a37947a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                      mae 79.77119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                     mape 60766412.42678\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                     rmse 227.91183\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:       time_since_restore 6.31671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:         time_this_iter_s 2.5086\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:             time_total_s 6.31671\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:                timestamp 1691374834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: 🚀 View run FSR_Trainable_a37947a9 at: https://wandb.ai/seokjin/FSR-prediction/runs/a37947a9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150747)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112034-a37947a9/logs\n",
      "2023-08-07 11:20:44,747\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.853 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:44,751\tWARNING util.py:315 -- The `process_trial_result` operation took 1.857 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:44,754\tWARNING util.py:315 -- Processing trial results took 1.860 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:20:44,755\tWARNING util.py:315 -- The `process_trial_result` operation took 1.862 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_3a5c5072_38_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-20-26/wandb/run-20230807_112046-3a5c5072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Syncing run FSR_Trainable_3a5c5072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3a5c5072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:20:54,482\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.167 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:54,484\tWARNING util.py:315 -- The `process_trial_result` operation took 2.170 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:20:54,486\tWARNING util.py:315 -- Processing trial results took 2.172 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:20:54,489\tWARNING util.py:315 -- The `process_trial_result` operation took 2.175 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                      mae ▇▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                     mape █▆▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                     rmse ▆▁▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:         time_this_iter_s █▇▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                timestamp ▁▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                      mae 66.23543\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                     mape 29395234.13626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                     rmse 206.77724\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:       time_since_restore 10.42456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:         time_this_iter_s 2.29771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:             time_total_s 10.42456\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:                timestamp 1691374852\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: 🚀 View run FSR_Trainable_3a5c5072 at: https://wandb.ai/seokjin/FSR-prediction/runs/3a5c5072\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150979)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112046-3a5c5072/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_02093e1c_39_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-20-39/wandb/run-20230807_112057-02093e1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Syncing run FSR_Trainable_02093e1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/02093e1c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                      mae ▁▁▂▂▃▄▂▂▅▂█▃▄▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                     mape ▁▁▁▁▂▂▁▁▃▂█▃▅▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                     rmse ▃▃▅▄▅▇▄▄█▂▄▄▃▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:         time_this_iter_s █▅▂▄█▅▃▁▃▃▆▂▄▃▅▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▃▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                      mae 61.61414\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                     mape 27709237.20438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                     rmse 193.82077\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:       time_since_restore 37.34055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:         time_this_iter_s 2.60974\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:             time_total_s 37.34055\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:                timestamp 1691374859\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: 🚀 View run FSR_Trainable_9a20197e at: https://wandb.ai/seokjin/FSR-prediction/runs/9a20197e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112022-9a20197e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:                      mae ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:                     mape ▅▁▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:                     rmse ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:       time_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:         time_this_iter_s █▁█▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:             time_total_s ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:                timestamp ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "2023-08-07 11:21:08,147\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:08,150\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:08,152\tWARNING util.py:315 -- Processing trial results took 1.880 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:21:08,154\tWARNING util.py:315 -- The `process_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=150536)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151196)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_ad7361f5_40_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-20-50/wandb/run-20230807_112110-ad7361f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Syncing run FSR_Trainable_ad7361f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ad7361f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                      mae 85.01201\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                     mape 45280160.18067\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                     rmse 230.88249\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:       time_since_restore 3.25502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:         time_this_iter_s 1.27358\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:             time_total_s 3.25502\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:                timestamp 1691374869\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: 🚀 View run FSR_Trainable_ad7361f5 at: https://wandb.ai/seokjin/FSR-prediction/runs/ad7361f5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112110-ad7361f5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151448)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "2023-08-07 11:21:16,762\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:16,768\tWARNING util.py:315 -- The `process_trial_result` operation took 1.874 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:16,770\tWARNING util.py:315 -- Processing trial results took 1.876 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:21:16,772\tWARNING util.py:315 -- The `process_trial_result` operation took 1.878 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_10d594f8_41_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-21-04/wandb/run-20230807_112119-10d594f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Syncing run FSR_Trainable_10d594f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/10d594f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                      mae 96.16979\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                     mape 2.0376693410760086e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                     rmse 281.00232\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:       time_since_restore 1.5166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:         time_this_iter_s 1.5166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:             time_total_s 1.5166\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:                timestamp 1691374874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: 🚀 View run FSR_Trainable_10d594f8 at: https://wandb.ai/seokjin/FSR-prediction/runs/10d594f8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151666)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112119-10d594f8/logs\n",
      "2023-08-07 11:21:25,587\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.627 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:25,590\tWARNING util.py:315 -- The `process_trial_result` operation took 1.631 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:25,591\tWARNING util.py:315 -- Processing trial results took 1.632 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:21:25,593\tWARNING util.py:315 -- The `process_trial_result` operation took 1.634 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_b558efe2_42_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-21-13/wandb/run-20230807_112128-b558efe2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Syncing run FSR_Trainable_b558efe2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b558efe2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)03 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                      mae 68.33836\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                     mape 4010137994772494.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                     rmse 230.15119\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:       time_since_restore 1.60163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:         time_this_iter_s 1.60163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:             time_total_s 1.60163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:                timestamp 1691374883\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: 🚀 View run FSR_Trainable_b558efe2 at: https://wandb.ai/seokjin/FSR-prediction/runs/b558efe2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=151885)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112128-b558efe2/logs\n",
      "2023-08-07 11:21:35,450\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.562 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:35,453\tWARNING util.py:315 -- The `process_trial_result` operation took 2.566 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:35,454\tWARNING util.py:315 -- Processing trial results took 2.568 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:21:35,456\tWARNING util.py:315 -- The `process_trial_result` operation took 2.569 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_915041d3_43_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-21-22/wandb/run-20230807_112138-915041d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Syncing run FSR_Trainable_915041d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/915041d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                      mae ▁▅▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                     mape ▁▄▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                     rmse ▆▃▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:         time_this_iter_s ██▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                timestamp ▁▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                      mae 66.34578\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                     mape 1.0872636078634598e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                     rmse 209.98279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:       time_since_restore 2.81775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:         time_this_iter_s 0.60921\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:             time_total_s 2.81775\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:                timestamp 1691374897\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: 🚀 View run FSR_Trainable_915041d3 at: https://wandb.ai/seokjin/FSR-prediction/runs/915041d3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152109)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112138-915041d3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:                      mae ██▇▇▅▃▃▃▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:                     mape █▇▃▃▂▂▂▁▁▂▃▄▄▄▃▃▃▃▃▃▃▂▃▃▃▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:                     rmse ██▇█▆▄▄▄▄▄▄▄▄▄▃▃▃▂▃▃▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▁▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:         time_this_iter_s ▅▄▃▆▅▇▆▅▅▂▁▄▁▁▃▄▃▆▄▆▇▆▆█▄▅█▄▆▃▃▄▄▃▅▁▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_111925-266f4215/logs\n",
      "2023-08-07 11:21:46,716\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.621 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:46,719\tWARNING util.py:315 -- The `process_trial_result` operation took 1.624 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:46,722\tWARNING util.py:315 -- Processing trial results took 1.628 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:21:46,724\tWARNING util.py:315 -- The `process_trial_result` operation took 1.630 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_75a9b737_44_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-21-32/wandb/run-20230807_112147-75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Syncing run FSR_Trainable_75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=149198)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "2023-08-07 11:21:53,437\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.704 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:53,441\tWARNING util.py:315 -- The `process_trial_result` operation took 1.709 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:21:53,443\tWARNING util.py:315 -- Processing trial results took 1.710 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:21:53,444\tWARNING util.py:315 -- The `process_trial_result` operation took 1.712 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_4cbd1076_45_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-21-42/wandb/run-20230807_112155-4cbd1076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Syncing run FSR_Trainable_4cbd1076\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/4cbd1076\n",
      "2023-08-07 11:22:04,585\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.468 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:04,588\tWARNING util.py:315 -- The `process_trial_result` operation took 2.473 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:04,592\tWARNING util.py:315 -- Processing trial results took 2.476 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:22:04,607\tWARNING util.py:315 -- The `process_trial_result` operation took 2.492 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_0620d1d0_46_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-21-50/wandb/run-20230807_112207-0620d1d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: Syncing run FSR_Trainable_0620d1d0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0620d1d0\n",
      "2023-08-07 11:22:14,962\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.697 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:14,966\tWARNING util.py:315 -- The `process_trial_result` operation took 1.702 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:14,967\tWARNING util.py:315 -- Processing trial results took 1.703 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:22:14,968\tWARNING util.py:315 -- The `process_trial_result` operation took 1.704 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_e258f087_47_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-21-59/wandb/run-20230807_112217-e258f087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Syncing run FSR_Trainable_e258f087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e258f087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                      mae 79.41733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                     mape 41096690.73165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                     rmse 233.82293\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:       time_since_restore 2.18783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:         time_this_iter_s 2.18783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:             time_total_s 2.18783\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:                timestamp 1691374933\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: 🚀 View run FSR_Trainable_e258f087 at: https://wandb.ai/seokjin/FSR-prediction/runs/e258f087\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152985)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112217-e258f087/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:                      mae █▁▅▅▆▆▃█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:                     mape █▄▆▅▅▄▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:                     rmse ▅▁▄▅▆▆▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:         time_this_iter_s █▃▂▁▄▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:                timestamp ▁▃▄▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152772)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                      mae █▅▁▂▁▂▂▂▁▂▃▂▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                     mape █▆▂▃▁▁▁▁▁▂▂▂▂▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                     rmse █▄▁▂▂▃▃▃▃▃▃▂▂▂▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:       time_since_restore ▁▁▂▂▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:         time_this_iter_s ▄▃▂▅██▄▄▃▄▅▃▅▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:             time_total_s ▁▁▂▂▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▅▅▆▆▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-08-07 11:22:28,869\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.926 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:28,873\tWARNING util.py:315 -- The `process_trial_result` operation took 1.931 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:28,875\tWARNING util.py:315 -- Processing trial results took 1.933 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:22:28,879\tWARNING util.py:315 -- The `process_trial_result` operation took 1.937 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: iterations_since_restore 16\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                      mae 64.51902\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                     mape 16051130.32841\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                     rmse 197.00979\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:       time_since_restore 27.32145\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:         time_this_iter_s 1.45131\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:             time_total_s 27.32145\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:                timestamp 1691374942\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb:       training_iteration 16\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: 🚀 View run FSR_Trainable_4cbd1076 at: https://wandb.ai/seokjin/FSR-prediction/runs/4cbd1076\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152562)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112155-4cbd1076/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_91571bcb_48_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-22-11/wandb/run-20230807_112230-91571bcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Syncing run FSR_Trainable_91571bcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/91571bcb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153229)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112230-91571bcb/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_6a257953_49_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-22-24/wandb/run-20230807_112239-6a257953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Syncing run FSR_Trainable_6a257953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/6a257953\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_962acdb5_50_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-22-33/wandb/run-20230807_112249-962acdb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Syncing run FSR_Trainable_962acdb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/962acdb5\n",
      "2023-08-07 11:22:54,004\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.636 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:54,007\tWARNING util.py:315 -- The `process_trial_result` operation took 1.640 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:54,009\tWARNING util.py:315 -- Processing trial results took 1.642 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:22:54,011\tWARNING util.py:315 -- The `process_trial_result` operation took 1.644 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:56,240\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.033 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:56,247\tWARNING util.py:315 -- The `process_trial_result` operation took 2.040 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:22:56,249\tWARNING util.py:315 -- Processing trial results took 2.043 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:22:56,251\tWARNING util.py:315 -- The `process_trial_result` operation took 2.044 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                      mae 63.48996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                     mape 5.3558887870670856e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                     rmse 227.3437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:       time_since_restore 9.92719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:         time_this_iter_s 9.92719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:             time_total_s 9.92719\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:                timestamp 1691374972\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: 🚀 View run FSR_Trainable_962acdb5 at: https://wandb.ai/seokjin/FSR-prediction/runs/962acdb5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153656)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112249-962acdb5/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_b5da9a8b_51_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-22-42/wandb/run-20230807_112300-b5da9a8b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Syncing run FSR_Trainable_b5da9a8b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b5da9a8b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b5da9a8b\n",
      "2023-08-07 11:23:06,348\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.565 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:06,350\tWARNING util.py:315 -- The `process_trial_result` operation took 1.568 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:06,352\tWARNING util.py:315 -- Processing trial results took 1.569 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:23:06,353\tWARNING util.py:315 -- The `process_trial_result` operation took 1.571 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153450)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                      mae 63.48592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                     mape 5.286437028299781e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                     rmse 227.95645\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:       time_since_restore 10.72132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:         time_this_iter_s 10.72132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:             time_total_s 10.72132\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:                timestamp 1691374984\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: 🚀 View run FSR_Trainable_b5da9a8b at: https://wandb.ai/seokjin/FSR-prediction/runs/b5da9a8b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112300-b5da9a8b/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=153867)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_594d0760_52_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-22-54/wandb/run-20230807_112313-594d0760\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Syncing run FSR_Trainable_594d0760\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/594d0760\n",
      "2023-08-07 11:23:17,294\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.291 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:17,297\tWARNING util.py:315 -- The `process_trial_result` operation took 1.295 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:17,298\tWARNING util.py:315 -- Processing trial results took 1.296 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:23:17,299\tWARNING util.py:315 -- The `process_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                      mae 66.47833\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                     mape 6.713994990969218e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                     rmse 225.80919\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:       time_since_restore 9.36218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:         time_this_iter_s 9.36218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:             time_total_s 9.36218\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:                timestamp 1691374996\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: 🚀 View run FSR_Trainable_594d0760 at: https://wandb.ai/seokjin/FSR-prediction/runs/594d0760\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154116)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112313-594d0760/logs\n",
      "2023-08-07 11:23:23,850\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.145 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:23,852\tWARNING util.py:315 -- The `process_trial_result` operation took 2.148 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:23,855\tWARNING util.py:315 -- Processing trial results took 2.151 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:23:23,856\tWARNING util.py:315 -- The `process_trial_result` operation took 2.153 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_00767336_53_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-23-06/wandb/run-20230807_112324-00767336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Syncing run FSR_Trainable_00767336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/00767336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                      mae 66.35918\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                     mape 2805428445505407.5\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                     rmse 228.90915\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:       time_since_restore 4.3516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:         time_this_iter_s 4.3516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:             time_total_s 4.3516\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:                timestamp 1691375001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: 🚀 View run FSR_Trainable_00767336 at: https://wandb.ai/seokjin/FSR-prediction/runs/00767336\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154334)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112324-00767336/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_afd7dc97_54_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-23-17/wandb/run-20230807_112332-afd7dc97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Syncing run FSR_Trainable_afd7dc97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/afd7dc97\n",
      "2023-08-07 11:23:33,924\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.953 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:33,927\tWARNING util.py:315 -- The `process_trial_result` operation took 2.956 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:33,929\tWARNING util.py:315 -- Processing trial results took 2.958 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:23:33,929\tWARNING util.py:315 -- The `process_trial_result` operation took 2.959 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                      mae 64.49642\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                     mape 5401054707164484.0\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                     rmse 230.71806\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:       time_since_restore 4.47702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:         time_this_iter_s 4.47702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:             time_total_s 4.47702\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:                timestamp 1691375010\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: 🚀 View run FSR_Trainable_afd7dc97 at: https://wandb.ai/seokjin/FSR-prediction/runs/afd7dc97\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154561)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112332-afd7dc97/logs\n",
      "2023-08-07 11:23:39,673\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.863 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:39,676\tWARNING util.py:315 -- The `process_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:39,679\tWARNING util.py:315 -- Processing trial results took 1.870 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:23:39,681\tWARNING util.py:315 -- The `process_trial_result` operation took 1.872 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_e9d4a3fb_55_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-23-26/wandb/run-20230807_112341-e9d4a3fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Syncing run FSR_Trainable_e9d4a3fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e9d4a3fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                      mae 77.79685\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                     mape 59502548.71788\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                     rmse 221.31214\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:       time_since_restore 3.59068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:         time_this_iter_s 1.53133\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:             time_total_s 3.59068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:                timestamp 1691375021\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: 🚀 View run FSR_Trainable_e9d4a3fb at: https://wandb.ai/seokjin/FSR-prediction/runs/e9d4a3fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=154786)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112341-e9d4a3fb/logs\n",
      "2023-08-07 11:23:48,350\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.257 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:48,353\tWARNING util.py:315 -- The `process_trial_result` operation took 2.260 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:48,354\tWARNING util.py:315 -- Processing trial results took 2.261 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:23:48,355\tWARNING util.py:315 -- The `process_trial_result` operation took 2.263 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_df345400_56_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-23-35/wandb/run-20230807_112350-df345400\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Syncing run FSR_Trainable_df345400\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/df345400\n",
      "2023-08-07 11:23:57,782\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.180 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:57,786\tWARNING util.py:315 -- The `process_trial_result` operation took 2.185 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:23:57,788\tWARNING util.py:315 -- Processing trial results took 2.187 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:23:57,789\tWARNING util.py:315 -- The `process_trial_result` operation took 2.189 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_75b9a165_57_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-23-44/wandb/run-20230807_112400-75b9a165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Syncing run FSR_Trainable_75b9a165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/75b9a165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                      mae 77.88152\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                     mape 45317743.56359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                     rmse 219.13942\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:       time_since_restore 4.17246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:         time_this_iter_s 1.98125\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:             time_total_s 4.17246\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:                timestamp 1691375039\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: 🚀 View run FSR_Trainable_75b9a165 at: https://wandb.ai/seokjin/FSR-prediction/runs/75b9a165\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155224)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112400-75b9a165/logs\n",
      "2023-08-07 11:24:07,632\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.109 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:24:07,636\tWARNING util.py:315 -- The `process_trial_result` operation took 2.115 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:24:07,638\tWARNING util.py:315 -- Processing trial results took 2.116 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:24:07,640\tWARNING util.py:315 -- The `process_trial_result` operation took 2.118 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_fd2300e8_58_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-23-53/wandb/run-20230807_112409-fd2300e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Syncing run FSR_Trainable_fd2300e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd2300e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                      mae 78.7567\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                     mape 33756309.33488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                     rmse 228.64163\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:       time_since_restore 2.32853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:         time_this_iter_s 2.32853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:             time_total_s 2.32853\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:                timestamp 1691375045\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: 🚀 View run FSR_Trainable_fd2300e8 at: https://wandb.ai/seokjin/FSR-prediction/runs/fd2300e8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155441)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112409-fd2300e8/logs\n",
      "2023-08-07 11:24:18,267\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.992 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:24:18,271\tWARNING util.py:315 -- The `process_trial_result` operation took 1.997 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:24:18,273\tWARNING util.py:315 -- Processing trial results took 1.999 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:24:18,275\tWARNING util.py:315 -- The `process_trial_result` operation took 2.001 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_cc4fa8b3_59_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-24-03/wandb/run-20230807_112419-cc4fa8b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: Syncing run FSR_Trainable_cc4fa8b3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/cc4fa8b3\n",
      "2023-08-07 11:24:26,980\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.042 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:24:26,984\tWARNING util.py:315 -- The `process_trial_result` operation took 3.046 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:24:26,986\tWARNING util.py:315 -- Processing trial results took 3.048 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:24:26,987\tWARNING util.py:315 -- The `process_trial_result` operation took 3.050 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_7ece8e9e_60_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-24-12/wandb/run-20230807_112430-7ece8e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Syncing run FSR_Trainable_7ece8e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7ece8e9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                      mae █▅▆▅▆▆▅▅▅▅▅▄▃▄▃▃▃▂▂▂▂▂▂▆▄▂▁▁▁▁▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                     mape ▇▁▅▆█▇▅▅▄▅▄▄▄▅▄▅▆▅▅▅▅▄▅█▆▅▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                     rmse ▇▇▇▇██▇▇▇▆▆▆▃▄▂▁▂▁▁▁▁▁▂▅▄▂▁▁▁▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:         time_this_iter_s ▄▂▃▁▅▄▄▂▂▅▄▃▂▁▅▅▃▃▃▄█▃▃▃▄▅▃▂▂▃▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                      mae 54.54881\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                     mape 15712326.59847\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                     rmse 173.94598\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:       time_since_restore 57.13454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:         time_this_iter_s 1.94426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:             time_total_s 57.13454\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:                timestamp 1691375089\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: 🚀 View run FSR_Trainable_df345400 at: https://wandb.ai/seokjin/FSR-prediction/runs/df345400\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155007)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112350-df345400/logs\n",
      "2023-08-07 11:25:03,305\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.104 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:25:03,308\tWARNING util.py:315 -- The `process_trial_result` operation took 2.108 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:25:03,309\tWARNING util.py:315 -- Processing trial results took 2.109 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:25:03,310\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_ff956aaf_61_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-24-23/wandb/run-20230807_112506-ff956aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Syncing run FSR_Trainable_ff956aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ff956aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                      mae ██▇███▆▅▄▆▄▃▄▄▃▃▃▃▃▃▃▃▂▂▂▂▃▃▃▂▂▂▂▃▂▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                     mape ▂▄▇▆██▇▄▂▅▃▁▃▂▃▃▃▄▅▆▅▆▄▄▄▄▅▅▅▄▄▅▅▄▄▄▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                     rmse ██▇▇▆▆▄▄▃▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▃▂▂▂▂▂▂▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:         time_this_iter_s ▂▁▁▂▃▂▂▂▂▁▂▁▂▂▁▂▂▁▂▁▂▂▂▂▁▂▅▄▆▆▇▆█▅▇▆▄▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                      mae 49.82103\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                     mape 19867988.3999\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                     rmse 154.68141\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:       time_since_restore 202.38939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:         time_this_iter_s 3.16167\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:             time_total_s 202.38939\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:                timestamp 1691375127\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: 🚀 View run FSR_Trainable_75a9b737 at: https://wandb.ai/seokjin/FSR-prediction/runs/75a9b737\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=152339)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112147-75a9b737/logs\n",
      "2023-08-07 11:25:46,312\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.915 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:25:46,317\tWARNING util.py:315 -- The `process_trial_result` operation took 1.920 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:25:46,318\tWARNING util.py:315 -- Processing trial results took 1.922 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:25:46,320\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_ab4c6250_62_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-25-00/wandb/run-20230807_112547-ab4c6250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Syncing run FSR_Trainable_ab4c6250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/ab4c6250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                      mae 77.93431\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                     mape 61505960.7592\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                     rmse 225.04843\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:       time_since_restore 4.23101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:         time_this_iter_s 4.23101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:             time_total_s 4.23101\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:                timestamp 1691375144\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: 🚀 View run FSR_Trainable_ab4c6250 at: https://wandb.ai/seokjin/FSR-prediction/runs/ab4c6250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156380)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112547-ab4c6250/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:                      mae █▅▄▅▄▅▄▅▄▄▄▃▂▃▁▂▃▁▃▄▄▃▃▃▄▄▄▄▅▅▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:                     mape █▃▂▆▃▅▂▃▅▃▄▃▃▅▂▅▅▁▅▅▄▃▄▃▄▅▃▄▆▇▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:                     rmse █▅▅▆▅▅▅▅▄▄▄▄▂▃▂▂▃▁▂▃▄▂▂▃▃▄▃▄▅▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:         time_this_iter_s ▇▂▁▅▆▂▂▄▁▁▁▄▃▃▂▁▁▃▄▃▃▁▂▁█▅▄▄▂▆▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                      mae █▄▃▅▅▄▄▄▄▃▂▃▃▃▂▄▂▂▁▂▁▂▁▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                     mape ▇▃▁▂▁▂▂▃▄▄▄▆▄▃▃█▅▃▂▂▁▁▂▁▂▂▁▂▁▁▁▁▁▁▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                     rmse █▄▄▅▅▄▄▄▄▃▂▃▄▃▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:         time_this_iter_s ▄▂▁▁▁▃▁▁▁▁▂▂▃▁▁▂▁▃▃▂▃▃▂▂▁▂▁▂█▃▅▃▃▄▂▃▄▃▅▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-08-07 11:26:00,085\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.746 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:00,089\tWARNING util.py:315 -- The `process_trial_result` operation took 1.751 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:00,091\tWARNING util.py:315 -- Processing trial results took 1.752 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:26:00,093\tWARNING util.py:315 -- The `process_trial_result` operation took 1.755 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155663)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                      mae 39.68038\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                     mape 15975257.19166\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                     rmse 130.323\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:       time_since_restore 80.47847\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:         time_this_iter_s 0.68729\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:             time_total_s 80.47847\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:                timestamp 1691375151\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: 🚀 View run FSR_Trainable_7ece8e9e at: https://wandb.ai/seokjin/FSR-prediction/runs/7ece8e9e\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=155882)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112430-7ece8e9e/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_b8834d63_63_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-25-40/wandb/run-20230807_112602-b8834d63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Syncing run FSR_Trainable_b8834d63\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b8834d63\n",
      "2023-08-07 11:26:14,419\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.323 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:14,422\tWARNING util.py:315 -- The `process_trial_result` operation took 3.326 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:14,430\tWARNING util.py:315 -- Processing trial results took 3.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:26:14,432\tWARNING util.py:315 -- The `process_trial_result` operation took 3.336 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_56b2a11f_64_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-25-56/wandb/run-20230807_112617-56b2a11f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Syncing run FSR_Trainable_56b2a11f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/56b2a11f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                     mape █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                      mae 76.49504\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                     mape 38433655.50106\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                     rmse 224.96146\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:       time_since_restore 9.93619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:         time_this_iter_s 5.25333\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:             time_total_s 9.93619\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:                timestamp 1691375179\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: 🚀 View run FSR_Trainable_56b2a11f at: https://wandb.ai/seokjin/FSR-prediction/runs/56b2a11f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156849)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112617-56b2a11f/logs\n",
      "2023-08-07 11:26:31,109\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.131 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:31,113\tWARNING util.py:315 -- The `process_trial_result` operation took 2.136 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:31,115\tWARNING util.py:315 -- Processing trial results took 2.138 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:26:31,118\tWARNING util.py:315 -- The `process_trial_result` operation took 2.141 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_536f7381_65_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-26-06/wandb/run-20230807_112632-536f7381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Syncing run FSR_Trainable_536f7381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/536f7381\n",
      "2023-08-07 11:26:43,815\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.105 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:43,820\tWARNING util.py:315 -- The `process_trial_result` operation took 2.110 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:26:43,822\tWARNING util.py:315 -- Processing trial results took 2.112 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:26:43,823\tWARNING util.py:315 -- The `process_trial_result` operation took 2.114 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_514c7020_66_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-26-24/wandb/run-20230807_112645-514c7020\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Syncing run FSR_Trainable_514c7020\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/514c7020\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                      mae █▆▃▄▄▄▃▃▂▂▁▁▁▁▁▂▁▁▂▁▂▁▂▂▂▁▁▁▂▁▂▁▂▂▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                     mape ██▂▃▅▆▆▇▆▄▅▃▃▃▁▂▃▃▄▃▄▄▅▃▃▃▃▄▄▄▄▃▄▄▃▄▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                     rmse █▅▃▄▄▄▄▃▂▁▁▁▁▁▁▂▂▂▂▂▂▁▂▂▂▂▁▁▂▁▁▁▂▁▂▂▁▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:         time_this_iter_s ▄▃▄▂▂▃▂▂▂▂▂▅▄▄▃▃▂▄▃▄▅▃▁▁▁▃▂▂▂▅██▇▇▄▄▃▃▄▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                      mae 41.92733\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                     mape 17668783.35388\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                     rmse 129.29959\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:       time_since_restore 91.75626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:         time_this_iter_s 0.86413\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:             time_total_s 91.75626\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:                timestamp 1691375206\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: 🚀 View run FSR_Trainable_ff956aaf at: https://wandb.ai/seokjin/FSR-prediction/runs/ff956aaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156129)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112506-ff956aaf/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                      mae █▅▃▄▃▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                     mape █▅▃▃▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                     rmse █▃▃▄▄▄▁▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:         time_this_iter_s █▅▃▆▃▅▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                timestamp ▁▃▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                      mae 63.87459\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                     mape 21970423.04779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                     rmse 196.58226\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:       time_since_restore 26.17096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:         time_this_iter_s 2.6319\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:             time_total_s 26.17096\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:                timestamp 1691375213\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: 🚀 View run FSR_Trainable_536f7381 at: https://wandb.ai/seokjin/FSR-prediction/runs/536f7381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157072)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112632-536f7381/logs\n",
      "2023-08-07 11:27:02,738\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.978 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:02,743\tWARNING util.py:315 -- The `process_trial_result` operation took 1.984 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:02,744\tWARNING util.py:315 -- Processing trial results took 1.985 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:27:02,746\tWARNING util.py:315 -- The `process_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: - Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: \\ Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: | Waiting for wandb.init()...\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_8532b304_67_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-26-37/wandb/run-20230807_112704-8532b304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Syncing run FSR_Trainable_8532b304\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8532b304\n",
      "2023-08-07 11:27:16,120\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.198 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:16,126\tWARNING util.py:315 -- The `process_trial_result` operation took 2.204 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:16,131\tWARNING util.py:315 -- Processing trial results took 2.209 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:27:16,133\tWARNING util.py:315 -- The `process_trial_result` operation took 2.212 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_f46c9aa6_68_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-26-57/wandb/run-20230807_112716-f46c9aa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Syncing run FSR_Trainable_f46c9aa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f46c9aa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                      mae 64.76708\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                     mape 17280220.80955\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                     rmse 216.04351\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:       time_since_restore 9.16254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:         time_this_iter_s 4.14028\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:             time_total_s 9.16254\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:                timestamp 1691375240\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: 🚀 View run FSR_Trainable_f46c9aa6 at: https://wandb.ai/seokjin/FSR-prediction/runs/f46c9aa6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157747)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112716-f46c9aa6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:                      mae █▅▄▂▃▁▁▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:                     mape █▃▂▁▁▂▂▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:                     rmse ▃▆▃▁▂▂▄█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:         time_this_iter_s ▆▅▆█▁▅▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:                timestamp ▁▃▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112704-8532b304/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157528)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                      mae █▄▃▂▃▃▄▁▂▄▄▄▃▄▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                     mape █▃▂▂▂▂▃▁▂▃▃▃▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                     rmse █▄▃▃▃▃▆▁▂▅▅▅▄▅▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:         time_this_iter_s ██▄▃▁▃▃▄▆▇▃▅▃▃▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                timestamp ▁▂▂▃▃▄▄▅▅▆▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                      mae 64.01327\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                     mape 23877585.48267\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                     rmse 195.83557\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:       time_since_restore 49.34756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:         time_this_iter_s 2.40328\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:             time_total_s 49.34756\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:                timestamp 1691375250\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: 🚀 View run FSR_Trainable_514c7020 at: https://wandb.ai/seokjin/FSR-prediction/runs/514c7020\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112645-514c7020/logs\n",
      "2023-08-07 11:27:37,148\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.770 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:37,153\tWARNING util.py:315 -- The `process_trial_result` operation took 1.775 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:37,155\tWARNING util.py:315 -- Processing trial results took 1.777 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:27:37,158\tWARNING util.py:315 -- The `process_trial_result` operation took 1.780 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157289)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_8b246c08_69_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-27-08/wandb/run-20230807_112737-8b246c08\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Syncing run FSR_Trainable_8b246c08\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/8b246c08\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:27:44,375\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.184 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:44,378\tWARNING util.py:315 -- The `process_trial_result` operation took 2.188 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:44,380\tWARNING util.py:315 -- Processing trial results took 2.190 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:27:44,381\tWARNING util.py:315 -- The `process_trial_result` operation took 2.191 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                      mae 71.4963\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                     mape 26825966.84874\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                     rmse 222.97401\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:       time_since_restore 7.47378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:         time_this_iter_s 3.38659\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:             time_total_s 7.47378\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:                timestamp 1691375260\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: 🚀 View run FSR_Trainable_8b246c08 at: https://wandb.ai/seokjin/FSR-prediction/runs/8b246c08\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=157994)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112737-8b246c08/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_a63871d1_70_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-27-31/wandb/run-20230807_112747-a63871d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Syncing run FSR_Trainable_a63871d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a63871d1\n",
      "2023-08-07 11:27:56,059\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:56,062\tWARNING util.py:315 -- The `process_trial_result` operation took 1.911 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:27:56,064\tWARNING util.py:315 -- Processing trial results took 1.913 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:27:56,066\tWARNING util.py:315 -- The `process_trial_result` operation took 1.915 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_e4c234af_71_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-27-41/wandb/run-20230807_112759-e4c234af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Syncing run FSR_Trainable_e4c234af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e4c234af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                      mae █▇▆▆▃▃▂▂▂▁▁▂▂▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                     mape ▇█▄▄▂▃▂▂▂▁▂▂▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                     rmse █▇▆▆▂▃▂▂▁▁▁▃▄▇▇▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:       time_since_restore ▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:         time_this_iter_s █▆▃▄▄▂▃▄▂▂▂▁▄▅▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:             time_total_s ▁▂▂▂▃▃▄▄▅▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                timestamp ▁▂▃▃▃▄▄▅▅▅▅▅▆▆██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                      mae 49.58426\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                     mape 16569166.08427\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                     rmse 171.5612\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:       time_since_restore 13.1407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:         time_this_iter_s 0.95108\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:             time_total_s 13.1407\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:                timestamp 1691375278\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: 🚀 View run FSR_Trainable_a63871d1 at: https://wandb.ai/seokjin/FSR-prediction/runs/a63871d1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158217)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112747-a63871d1/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:                      mae █▇▅▃▄▂▂▃▃▅▂▂▂▂▁▃▂▂▂▂▃▃▂▃▂▁▁▂▂▁▂▂▅▂▁▂▁▁▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:                     mape ▄█▆▃▄▄▃▇▆▇▂▆▄▃▂▃▃▃▃▁▃▃▂▂▂▁▁▂▂▂▁▂▅▂▃▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:                     rmse █▆▅▃▄▂▂▃▃▅▂▂▂▂▁▃▂▂▂▂▃▂▃▃▂▂▂▂▃▁▃▂▅▃▂▃▂▂▃▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:         time_this_iter_s ▃▂▁▁▆▇█▅▃▄▂▃▄▄▂▁▂▁▃▃▃▃▃▄▂▃▂▂▁▁▂▁▂▃▃▁▁▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112602-b8834d63/logs\n",
      "2023-08-07 11:28:08,464\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.957 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:08,467\tWARNING util.py:315 -- The `process_trial_result` operation took 1.961 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:08,468\tWARNING util.py:315 -- Processing trial results took 1.962 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:28:08,470\tWARNING util.py:315 -- The `process_trial_result` operation took 1.963 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=156637)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_bb67c17d_72_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-27-53/wandb/run-20230807_112812-bb67c17d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Syncing run FSR_Trainable_bb67c17d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/bb67c17d\n",
      "2023-08-07 11:28:22,501\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.118 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:22,505\tWARNING util.py:315 -- The `process_trial_result` operation took 2.123 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:22,507\tWARNING util.py:315 -- Processing trial results took 2.125 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:28:22,508\tWARNING util.py:315 -- The `process_trial_result` operation took 2.126 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_29738eb7_73_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-28-05/wandb/run-20230807_112826-29738eb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Syncing run FSR_Trainable_29738eb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/29738eb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                      mae 72.85834\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                     mape 28100486.73948\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                     rmse 231.32074\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:       time_since_restore 2.76241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:         time_this_iter_s 1.30718\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:             time_total_s 2.76241\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:                timestamp 1691375303\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: 🚀 View run FSR_Trainable_29738eb7 at: https://wandb.ai/seokjin/FSR-prediction/runs/29738eb7\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158893)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112826-29738eb7/logs\n",
      "2023-08-07 11:28:36,763\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.716 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:36,767\tWARNING util.py:315 -- The `process_trial_result` operation took 1.721 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:36,768\tWARNING util.py:315 -- Processing trial results took 1.723 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:28:36,770\tWARNING util.py:315 -- The `process_trial_result` operation took 1.725 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_acfb935c_74_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-28-19/wandb/run-20230807_112840-acfb935c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Syncing run FSR_Trainable_acfb935c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/acfb935c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                      mae ▃▂▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                     mape ▁▂█▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                     rmse ▄▃▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:         time_this_iter_s ▆█▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                timestamp ▁▅▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                      mae 79.94463\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                     mape 38567914.10325\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                     rmse 246.50001\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:       time_since_restore 4.56317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:         time_this_iter_s 1.02124\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:             time_total_s 4.56317\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:                timestamp 1691375320\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: 🚀 View run FSR_Trainable_acfb935c at: https://wandb.ai/seokjin/FSR-prediction/runs/acfb935c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159123)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112840-acfb935c/logs\n",
      "2023-08-07 11:28:48,998\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:49,002\tWARNING util.py:315 -- The `process_trial_result` operation took 1.912 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:28:49,004\tWARNING util.py:315 -- Processing trial results took 1.914 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:28:49,005\tWARNING util.py:315 -- The `process_trial_result` operation took 1.915 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_1590aeaf_75_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-28-33/wandb/run-20230807_112852-1590aeaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: Syncing run FSR_Trainable_1590aeaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1590aeaf\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                      mae ▇█▄▄▄▄▅▄▄▃▂▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                     mape ▂▃▁▁▂▄▄▄▅▅▅▅▃▄▃▄▄▄▅▅▆▅▄▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                     rmse ██▄▄▄▄▅▃▃▂▁▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▃▁▁▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:         time_this_iter_s █▆▆▃▃▁▂▇▂▄▆▆▄▄▄▂▂▂▅▄▅▃▂▅▆▆▄▁▄▇▄▂▃▂▃▁▁▅▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: iterations_since_restore 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                      mae 42.24837\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                     mape 29970003.60353\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                     rmse 135.88438\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:       time_since_restore 48.14697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:         time_this_iter_s 0.68851\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:             time_total_s 48.14697\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:                timestamp 1691375332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb:       training_iteration 64\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: 🚀 View run FSR_Trainable_e4c234af at: https://wandb.ai/seokjin/FSR-prediction/runs/e4c234af\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158439)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112759-e4c234af/logs\n",
      "2023-08-07 11:29:00,856\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.841 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:00,860\tWARNING util.py:315 -- The `process_trial_result` operation took 1.847 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:00,862\tWARNING util.py:315 -- Processing trial results took 1.849 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:29:00,863\tWARNING util.py:315 -- The `process_trial_result` operation took 1.850 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_911c530b_76_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-28-46/wandb/run-20230807_112903-911c530b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Syncing run FSR_Trainable_911c530b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/911c530b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                      mae 78.39299\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                     mape 48750492.76084\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                     rmse 221.8489\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:       time_since_restore 1.18899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:         time_this_iter_s 1.18899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:             time_total_s 1.18899\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:                timestamp 1691375339\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: 🚀 View run FSR_Trainable_911c530b at: https://wandb.ai/seokjin/FSR-prediction/runs/911c530b\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159579)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112903-911c530b/logs\n",
      "2023-08-07 11:29:12,388\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.778 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:12,390\tWARNING util.py:315 -- The `process_trial_result` operation took 1.781 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:12,393\tWARNING util.py:315 -- Processing trial results took 1.784 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:29:12,395\tWARNING util.py:315 -- The `process_trial_result` operation took 1.786 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_958d317a_77_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-28-57/wandb/run-20230807_112916-958d317a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: Syncing run FSR_Trainable_958d317a\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/958d317a\n",
      "2023-08-07 11:29:28,805\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 3.067 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:28,810\tWARNING util.py:315 -- The `process_trial_result` operation took 3.073 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:28,812\tWARNING util.py:315 -- Processing trial results took 3.075 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:29:28,817\tWARNING util.py:315 -- The `process_trial_result` operation took 3.080 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_32b7bb58_78_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-29-09/wandb/run-20230807_112932-32b7bb58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Syncing run FSR_Trainable_32b7bb58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/32b7bb58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: iterations_since_restore ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                      mae ▁▃▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                     mape ▁▂▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                     rmse ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:       time_since_restore ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:         time_this_iter_s █▄▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:             time_total_s ▁▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                timestamp ▁▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:       training_iteration ▁▃▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: iterations_since_restore 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                      mae 79.31901\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                     mape 59697701.22587\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                     rmse 231.25109\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:       time_since_restore 6.56279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:         time_this_iter_s 1.14004\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:             time_total_s 6.56279\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:                timestamp 1691375372\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb:       training_iteration 4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: 🚀 View run FSR_Trainable_32b7bb58 at: https://wandb.ai/seokjin/FSR-prediction/runs/32b7bb58\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160023)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112932-32b7bb58/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Waiting for W&B process to finish... (success).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:                      mae ▅▅▇▄▃▁▃▂▂▂▅▄▇▅▅█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:                     mape ▃▆▅▄▄▁▃▄▁▁▄▄▆▅▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:                     rmse ▃▂█▆▃▂▄▁▂▁▃▂▅▄▄▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:       time_since_restore ▁▂▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:         time_this_iter_s ▄█▂▂▂▃▁▁▂█▇▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:             time_total_s ▁▂▂▂▃▃▄▄▄▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:                timestamp ▁▂▂▃▃▃▄▄▄▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159803)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                      mae █▅▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                     mape █▂▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                     rmse █▆▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:         time_this_iter_s ▄▅▃▃▂▂▃▄▂▃▅▃▄▃▃▁▂▁▄▃▂▂▁▁▃▂▂▂▁▂█▃▄▃▂▇▅▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "2023-08-07 11:29:44,775\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.732 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:44,777\tWARNING util.py:315 -- The `process_trial_result` operation took 1.735 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:44,781\tWARNING util.py:315 -- Processing trial results took 1.739 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:29:44,782\tWARNING util.py:315 -- The `process_trial_result` operation took 1.740 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Run history:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Run summary:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: iterations_since_restore 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                      mae 41.43759\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                     mape 17517785.05341\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                     rmse 129.47968\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:       time_since_restore 75.73418\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:         time_this_iter_s 0.74655\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:             time_total_s 75.73418\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:                timestamp 1691375376\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb:       training_iteration 100\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: 🚀 View run FSR_Trainable_bb67c17d at: https://wandb.ai/seokjin/FSR-prediction/runs/bb67c17d\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=158675)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112812-bb67c17d/logs\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_0b22ee3e_79_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-29-23/wandb/run-20230807_112947-0b22ee3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Syncing run FSR_Trainable_0b22ee3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/0b22ee3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: | 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: - 0.002 MB of 0.003 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                      mae 81.68779\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                     mape 36271612.38705\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                     rmse 231.59811\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:       time_since_restore 1.93238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:         time_this_iter_s 0.88348\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:             time_total_s 1.93238\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:                timestamp 1691375385\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: 🚀 View run FSR_Trainable_0b22ee3e at: https://wandb.ai/seokjin/FSR-prediction/runs/0b22ee3e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160282)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112947-0b22ee3e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:29:54,795\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.627 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:54,800\tWARNING util.py:315 -- The `process_trial_result` operation took 1.633 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:29:54,802\tWARNING util.py:315 -- Processing trial results took 1.635 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:29:54,804\tWARNING util.py:315 -- The `process_trial_result` operation took 1.637 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:                      mae █▄▃▃▅▄▃▆▃▅▂▄▃▂▃▂▅▄▄▂▃▄▅▄▂▃▃▄▅▆▆▂▄▂▂▁▁▃▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:                     mape ▄▅▅▄▆▄▄█▄▄▄▃▃▃▄▃▅▄▃▃▃▃▅▄▃▃▄▃▃▄▄▃▃▂▁▁▁▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:                     rmse █▃▃▂▃▄▃▄▂▄▂▃▂▁▂▁▅▃▃▁▂▅▄▃▂▃▃▅▅▅▅▃▄▃▃▂▂▄▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:         time_this_iter_s ▄▃▂▂▂▂▁▁▂▂▂▂▁▂▂▁▂▃▄▃▃▃▃▂▅█▄▄▃▃▃▂▂▂▂▅▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_242a44e4_80_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-29-42/wandb/run-20230807_112957-242a44e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Syncing run FSR_Trainable_242a44e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/242a44e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/242a44e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=159351)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:30:03,147\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.867 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:03,149\tWARNING util.py:315 -- The `process_trial_result` operation took 1.869 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:03,153\tWARNING util.py:315 -- Processing trial results took 1.873 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:30:03,156\tWARNING util.py:315 -- The `process_trial_result` operation took 1.877 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                      mae 83.36651\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                     mape 1.5532672776953018e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                     rmse 228.40063\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:       time_since_restore 1.13332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:         time_this_iter_s 1.13332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:             time_total_s 1.13332\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:                timestamp 1691375393\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: 🚀 View run FSR_Trainable_242a44e4 at: https://wandb.ai/seokjin/FSR-prediction/runs/242a44e4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_112957-242a44e4/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160504)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_2fb9d4a6_81_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-29-52/wandb/run-20230807_113005-2fb9d4a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Syncing run FSR_Trainable_2fb9d4a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/2fb9d4a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:30:09,539\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.682 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:09,541\tWARNING util.py:315 -- The `process_trial_result` operation took 1.685 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:09,543\tWARNING util.py:315 -- Processing trial results took 1.686 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:30:09,544\tWARNING util.py:315 -- The `process_trial_result` operation took 1.687 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: | 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                      mae ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                     rmse █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:         time_this_iter_s █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                      mae 74.52406\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                     mape 1.4532380814187296e+17\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                     rmse 204.86601\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:       time_since_restore 1.95473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:         time_this_iter_s 0.71198\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:             time_total_s 1.95473\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:                timestamp 1691375403\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: 🚀 View run FSR_Trainable_2fb9d4a6 at: https://wandb.ai/seokjin/FSR-prediction/runs/2fb9d4a6\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113005-2fb9d4a6/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160734)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_9a868b21_82_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-30-00/wandb/run-20230807_113012-9a868b21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Syncing run FSR_Trainable_9a868b21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9a868b21\n",
      "2023-08-07 11:30:19,454\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.389 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:19,457\tWARNING util.py:315 -- The `process_trial_result` operation took 2.392 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:19,459\tWARNING util.py:315 -- Processing trial results took 2.395 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:30:19,465\tWARNING util.py:315 -- The `process_trial_result` operation took 2.401 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_927ef5ef_83_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-30-06/wandb/run-20230807_113022-927ef5ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Syncing run FSR_Trainable_927ef5ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/927ef5ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "2023-08-07 11:30:27,612\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.316 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:27,616\tWARNING util.py:315 -- The `process_trial_result` operation took 2.321 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:27,618\tWARNING util.py:315 -- Processing trial results took 2.323 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:30:27,620\tWARNING util.py:315 -- The `process_trial_result` operation took 2.325 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                      mae ▅▅▁▃▂▁▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                     mape ▁█▆▆▁▃▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                     rmse ▅▃▁▃▃▂▂█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:         time_this_iter_s █▆▆▄▂▁▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                timestamp ▁▅▅▆▆▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                      mae 75.58701\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                     mape 30944174.22625\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                     rmse 234.76342\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:       time_since_restore 4.60448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:         time_this_iter_s 0.56175\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:             time_total_s 4.60448\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:                timestamp 1691375423\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: 🚀 View run FSR_Trainable_927ef5ef at: https://wandb.ai/seokjin/FSR-prediction/runs/927ef5ef\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161137)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113022-927ef5ef/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_c1846657_84_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-30-16/wandb/run-20230807_113030-c1846657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Syncing run FSR_Trainable_c1846657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/c1846657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "2023-08-07 11:30:38,940\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:38,941\tWARNING util.py:315 -- The `process_trial_result` operation took 1.844 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:38,945\tWARNING util.py:315 -- Processing trial results took 1.848 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:30:38,948\tWARNING util.py:315 -- The `process_trial_result` operation took 1.851 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: - 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: \\ 0.004 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                      mae ▃▆▅█▇▇▂▃▃▃▁▅▄▆▆▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                     mape ▃▂▅█▇█▅▃▄▄▁▃▄▄▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                     rmse ▂█▄█▆█▁▂▃▂▁▅▄▆▇▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:         time_this_iter_s ▆█▆▃▃▂▁▃▃▁▁▅▂▁▃▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▄▅▅▆▆▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                timestamp ▁▃▄▄▄▅▅▅▅▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                      mae 61.3387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                     mape 32737134.56781\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                     rmse 177.20437\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:       time_since_restore 9.78158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:         time_this_iter_s 0.78649\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:             time_total_s 9.78158\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:                timestamp 1691375436\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: 🚀 View run FSR_Trainable_c1846657 at: https://wandb.ai/seokjin/FSR-prediction/runs/c1846657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161315)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113030-c1846657/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_f81dfd5d_85_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-30-24/wandb/run-20230807_113041-f81dfd5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Syncing run FSR_Trainable_f81dfd5d\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f81dfd5d\n",
      "2023-08-07 11:30:49,852\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.902 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:49,854\tWARNING util.py:315 -- The `process_trial_result` operation took 1.907 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:30:49,855\tWARNING util.py:315 -- Processing trial results took 1.908 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:30:49,856\tWARNING util.py:315 -- The `process_trial_result` operation took 1.909 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_5c5a4a9e_86_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-30-35/wandb/run-20230807_113053-5c5a4a9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Syncing run FSR_Trainable_5c5a4a9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/5c5a4a9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: \\ 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                      mae ▆▂▂▅▃▃▄▂▅▂▄▃▃▅▃▃▂▄▂▃▂▅█▅▄▃▂▂▃▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                     mape ▄▆▇█▄▇▄▄▄▄▃▃▄▄▄▃▃▄▃▄▄▁▃█▃▃▁▂▄▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                     rmse ▅▁▂▄▂▂▄▂▄▃▄▃▂▄▂▂▁▃▃▃▂██▄▅▄▄▄▂▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:         time_this_iter_s ▃▂▁▂▁▁▁▁▁▁▂▂▃▃▃▃▄▄▅▆▅▅▅▅▆▆▅▅▅▄▆█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: iterations_since_restore 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                      mae 49.402\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                     mape 21037847.29476\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                     rmse 146.46381\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:       time_since_restore 37.09545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:         time_this_iter_s 2.22522\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:             time_total_s 37.09545\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:                timestamp 1691375452\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb:       training_iteration 32\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: 🚀 View run FSR_Trainable_9a868b21 at: https://wandb.ai/seokjin/FSR-prediction/runs/9a868b21\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=160916)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113012-9a868b21/logs\n",
      "2023-08-07 11:31:05,275\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.666 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:05,280\tWARNING util.py:315 -- The `process_trial_result` operation took 1.671 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:05,281\tWARNING util.py:315 -- Processing trial results took 1.672 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:31:05,282\tWARNING util.py:315 -- The `process_trial_result` operation took 1.673 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_3ee657e3_87_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-30-46/wandb/run-20230807_113108-3ee657e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Syncing run FSR_Trainable_3ee657e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/3ee657e3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: iterations_since_restore ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                      mae █▅▂▁▁▁▁▃▂▃▃▅▅▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                     mape ▇▅▅▄▁▄▂▅▃▅▅▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                     rmse ▇▄▂▁▁▂▂▃▃▄▄▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:       time_since_restore ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:         time_this_iter_s ▄█▃▁▂▆▄▂▃▄▂▆▃▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:             time_total_s ▁▂▂▃▃▄▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                timestamp ▁▂▃▃▃▄▄▅▅▅▅▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:       training_iteration ▁▁▂▂▃▃▄▄▅▅▆▆▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: iterations_since_restore 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                      mae 59.82503\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                     mape 19097032.22284\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                     rmse 196.82115\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:       time_since_restore 18.615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:         time_this_iter_s 0.9594\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:             time_total_s 18.615\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:                timestamp 1691375469\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb:       training_iteration 16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: 🚀 View run FSR_Trainable_5c5a4a9e at: https://wandb.ai/seokjin/FSR-prediction/runs/5c5a4a9e\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161764)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113053-5c5a4a9e/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:                      mae ███▇▇▄▃▃▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:                     mape ██▇▇▅▅▆▆▅▅▄▄▄▃▂▂▂▁▁▁▁▁▁▂▂▁▁▁▄▃▅▇\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:                     rmse ██▇▆▅▃▂▁▁▂▂▂▂▁▁▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:         time_this_iter_s ▄▃▂▂▁▂▁▁▂▁▂▄█▄▂▄▇▅▃▄▅▃█▄▄▃▃▃▂▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113041-f81dfd5d/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113041-f81dfd5d/logs\n",
      "2023-08-07 11:31:18,143\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.218 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:18,147\tWARNING util.py:315 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:18,149\tWARNING util.py:315 -- Processing trial results took 2.225 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:31:18,151\tWARNING util.py:315 -- The `process_trial_result` operation took 2.227 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161537)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_96ce57fb_88_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-31-02/wandb/run-20230807_113121-96ce57fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Syncing run FSR_Trainable_96ce57fb\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/96ce57fb\n",
      "2023-08-07 11:31:32,255\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.905 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:32,257\tWARNING util.py:315 -- The `process_trial_result` operation took 1.908 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:32,262\tWARNING util.py:315 -- Processing trial results took 1.913 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:31:32,263\tWARNING util.py:315 -- The `process_trial_result` operation took 1.914 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_e2c5fce9_89_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-31-14/wandb/run-20230807_113135-e2c5fce9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Syncing run FSR_Trainable_e2c5fce9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/e2c5fce9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: | 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                      mae 70.42866\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                     mape 1.1194517789018412e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                     rmse 231.17729\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:       time_since_restore 1.90932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:         time_this_iter_s 1.90932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:             time_total_s 1.90932\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:                timestamp 1691375490\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: 🚀 View run FSR_Trainable_e2c5fce9 at: https://wandb.ai/seokjin/FSR-prediction/runs/e2c5fce9\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162444)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113135-e2c5fce9/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113135-e2c5fce9/logs\n",
      "2023-08-07 11:31:42,729\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.981 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:42,734\tWARNING util.py:315 -- The `process_trial_result` operation took 1.987 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:42,736\tWARNING util.py:315 -- Processing trial results took 1.989 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:31:42,737\tWARNING util.py:315 -- The `process_trial_result` operation took 1.990 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:                      mae ██▆▆▅▆▅▄▄▄▃▃▃▂▃▂▃▁▂▁▂▁▂▁▂▁▂▁▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:                     mape ▁▁▂▄▄▄▆▅▇▆█▇█▇█▆▇▄▇▃▇▂▇▂▇▃▆▃▇▄▇▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:                     rmse ██▆▆▄▄▃▃▃▃▂▂▂▂▂▁▂▁▂▁▂▁▂▁▂▁▂▂▂▂▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:         time_this_iter_s █▃▃▃▂▃▂▂▁▂▂▄▅▃▃▃▃▄▅▅▂▂▂▄▃▃▃▂▂▁▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113108-3ee657e3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_f57ea8ac_90_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-31-28/wandb/run-20230807_113145-f57ea8ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Syncing run FSR_Trainable_f57ea8ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/f57ea8ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=161991)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                      mae 82.3629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                     mape 2.24154887834615e+16\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                     rmse 244.58445\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:       time_since_restore 1.74749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:         time_this_iter_s 1.74749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:             time_total_s 1.74749\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:                timestamp 1691375500\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: 🚀 View run FSR_Trainable_f57ea8ac at: https://wandb.ai/seokjin/FSR-prediction/runs/f57ea8ac\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113145-f57ea8ac/logs\n",
      "2023-08-07 11:31:50,983\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.915 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:50,986\tWARNING util.py:315 -- The `process_trial_result` operation took 1.919 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:31:50,989\tWARNING util.py:315 -- Processing trial results took 1.922 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:31:50,990\tWARNING util.py:315 -- The `process_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162662)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_fd9a5c69_91_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-31-39/wandb/run-20230807_113153-fd9a5c69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Syncing run FSR_Trainable_fd9a5c69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/fd9a5c69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: iterations_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                      mae ▃▃█▁▂▁▄▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                     mape ▅▆█▁▃▆▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                     rmse ▂▁█▁▂▁▃▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:       time_since_restore ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:         time_this_iter_s ▆▅▃▂▁▃█▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:             time_total_s ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                timestamp ▁▃▄▅▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:       training_iteration ▁▂▃▄▅▆▇█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: iterations_since_restore 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                      mae 77.23809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                     mape 22683184.77488\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                     rmse 238.88017\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:       time_since_restore 7.23676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:         time_this_iter_s 0.82219\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:             time_total_s 7.23676\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:                timestamp 1691375517\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb:       training_iteration 8\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: 🚀 View run FSR_Trainable_fd9a5c69 at: https://wandb.ai/seokjin/FSR-prediction/runs/fd9a5c69\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162895)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113153-fd9a5c69/logs\n",
      "2023-08-07 11:32:02,016\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.061 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:02,018\tWARNING util.py:315 -- The `process_trial_result` operation took 2.063 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:02,020\tWARNING util.py:315 -- Processing trial results took 2.065 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:32:02,023\tWARNING util.py:315 -- The `process_trial_result` operation took 2.068 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_1a941da3_92_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-31-48/wandb/run-20230807_113204-1a941da3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Syncing run FSR_Trainable_1a941da3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1a941da3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: - 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: \\ 0.003 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: iterations_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                      mae █▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                     mape ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                     rmse ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:       time_since_restore ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:         time_this_iter_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:             time_total_s ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                timestamp ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:       training_iteration ▁█\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: iterations_since_restore 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                      mae 66.32457\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                     mape 21485343.36752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                     rmse 205.70946\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:       time_since_restore 2.30969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:         time_this_iter_s 1.24895\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:             time_total_s 2.30969\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:                timestamp 1691375523\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb:       training_iteration 2\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: 🚀 View run FSR_Trainable_1a941da3 at: https://wandb.ai/seokjin/FSR-prediction/runs/1a941da3\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163118)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113204-1a941da3/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: iterations_since_restore ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:                      mae ▄▅█▃▅▅▆▃▄▂▃▄▃▂▃▃▄▄▂▂▃▃▂▁▄▃▆▄▃▃▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:                     mape ▁▆█▃▂█▄▃▇▁▁▇▅▂▃▆▆▆▄▃▃▂▃▁▂▅▄▅▅▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:                     rmse ▅▄█▃▇▄▇▂▃▃▄▃▂▂▃▂▄▃▁▂▂▂▁▁▄▂▇▅▂▂▁▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:         time_this_iter_s ▆▃▃▂▂▄▆▂▁▃▄▄▃▃▁▃▄▆▄▂▂▄▇▇▆▇▆▇▆█▅▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:                timestamp ▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb:       training_iteration ▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113121-96ce57fb/logs\n",
      "2023-08-07 11:32:12,126\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.837 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:12,130\tWARNING util.py:315 -- The `process_trial_result` operation took 1.842 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:12,131\tWARNING util.py:315 -- Processing trial results took 1.843 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:32:12,133\tWARNING util.py:315 -- The `process_trial_result` operation took 1.845 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_a9652037_93_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-31-59/wandb/run-20230807_113214-a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Syncing run FSR_Trainable_a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=162222)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: iterations_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                      mae ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                     mape ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                     rmse ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:       time_since_restore ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:         time_this_iter_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:             time_total_s ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                timestamp ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:       training_iteration ▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: iterations_since_restore 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                      mae 74.53228\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                     mape 44757986.21475\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                     rmse 241.88947\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:       time_since_restore 1.5068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:         time_this_iter_s 1.5068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:             time_total_s 1.5068\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:                timestamp 1691375530\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb:       training_iteration 1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: 🚀 View run FSR_Trainable_a9652037 at: https://wandb.ai/seokjin/FSR-prediction/runs/a9652037\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113214-a9652037/logs\n",
      "2023-08-07 11:32:19,683\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.967 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:19,685\tWARNING util.py:315 -- The `process_trial_result` operation took 1.970 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:19,686\tWARNING util.py:315 -- Processing trial results took 1.971 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:32:19,687\tWARNING util.py:315 -- The `process_trial_result` operation took 1.972 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163349)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_b6a24f3c_94_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-32-08/wandb/run-20230807_113222-b6a24f3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Syncing run FSR_Trainable_b6a24f3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/b6a24f3c\n",
      "2023-08-07 11:32:29,434\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.216 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:29,437\tWARNING util.py:315 -- The `process_trial_result` operation took 2.220 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:29,439\tWARNING util.py:315 -- Processing trial results took 2.222 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:32:29,440\tWARNING util.py:315 -- The `process_trial_result` operation took 2.223 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_983f11a1_95_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-32-16/wandb/run-20230807_113232-983f11a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Syncing run FSR_Trainable_983f11a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/983f11a1\n",
      "2023-08-07 11:32:38,023\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.019 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:38,026\tWARNING util.py:315 -- The `process_trial_result` operation took 2.023 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:38,028\tWARNING util.py:315 -- Processing trial results took 2.025 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:32:38,030\tWARNING util.py:315 -- The `process_trial_result` operation took 2.027 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_7b0ae72f_96_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-32-26/wandb/run-20230807_113241-7b0ae72f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Syncing run FSR_Trainable_7b0ae72f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/7b0ae72f\n",
      "2023-08-07 11:32:51,636\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.050 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:51,639\tWARNING util.py:315 -- The `process_trial_result` operation took 2.054 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:32:51,641\tWARNING util.py:315 -- Processing trial results took 2.057 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:32:51,643\tWARNING util.py:315 -- The `process_trial_result` operation took 2.058 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_04941f48_97_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-32-35/wandb/run-20230807_113255-04941f48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Syncing run FSR_Trainable_04941f48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/04941f48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                      mae █▃▁▁▃▄▄▅▅▅▆▇▆▆▅▆▇▇▆▆▆▆▆▆▆▅▅▅▆▅▅▅▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                     mape ▂▁▂▃▆▇▇▆█▇███▇▆▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                     rmse ▇▃▁▁▂▃▄▄▅▅▅▅▆▇▆██████████▇▇▇█▇▇▇▇▇▇▇▆▇▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:       time_since_restore ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:         time_this_iter_s ▆▁▁▁▁▂▃▁▂▄▃▃▄▃▅▄▄▃▄▅▃▃▃▃▃▂▃▃▂▃▃▂▃█▃▃▃▆▆▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:             time_total_s ▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                timestamp ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                      mae 40.41566\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                     mape 17775014.49632\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                     rmse 129.70796\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:       time_since_restore 81.77654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:         time_this_iter_s 0.75752\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:             time_total_s 81.77654\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:                timestamp 1691375629\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: 🚀 View run FSR_Trainable_b6a24f3c at: https://wandb.ai/seokjin/FSR-prediction/runs/b6a24f3c\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163575)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113222-b6a24f3c/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                      mae █▆▃▂▁▁▁▁▂▂▃▄▄▄▅▅▄▅▅▅▅▅▆▅▅▅▅▅▄▄▆▅▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                     mape █▆▇▇█▆▃▅▃▂▂▁▁▁▁▂▁▁▁▁▁▁▃▂▂▂▂▂▂▂▁▂▂▁▂▂▁▂▂▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                     rmse █▇▄▂▁▁▁▂▃▂▃▄▄▅▅▅▅▆▆▆▆▅▆▅▆▅▅▅▅▅▆▆▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:         time_this_iter_s ▃▂▁▁▃▃▄▂▃▆▄▃▃▃▅▃▂▂▂▃▂▂▄▃▃▂▂▄▃▂▃▃▆▂▆▄█▃▂▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                timestamp ▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                      mae 38.41077\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                     mape 15291392.49945\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                     rmse 120.04563\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:       time_since_restore 84.48725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:         time_this_iter_s 0.76259\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:             time_total_s 84.48725\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:                timestamp 1691375641\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: 🚀 View run FSR_Trainable_983f11a1 at: https://wandb.ai/seokjin/FSR-prediction/runs/983f11a1\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163792)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113232-983f11a1/logs\n",
      "2023-08-07 11:34:06,342\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.015 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:34:06,344\tWARNING util.py:315 -- The `process_trial_result` operation took 2.019 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:34:06,346\tWARNING util.py:315 -- Processing trial results took 2.020 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:34:06,347\tWARNING util.py:315 -- The `process_trial_result` operation took 2.021 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_d6a4c920_98_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-32-48/wandb/run-20230807_113409-d6a4c920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Syncing run FSR_Trainable_d6a4c920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/d6a4c920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                      mae █▃▂▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▄▃▃▄▄▃▃▃▃▄▄▄▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                     mape ▇▂▁▁▃▄▅▆▆▇█▇▆▆▅▄▄▃▃▄▃▃▃▃▂▃▂▂▂▂▁▁▁▂▂▂▂▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                     rmse █▄▂▁▁▂▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▅▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:       time_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:         time_this_iter_s ▅▃▂▂▂▁▄▃▃▂▃▅▃▃▂▂▃▂▁▂▃▃▂▂▃█▃▂▂▄▅█▂▇▃▂▂▁▄▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:             time_total_s ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                      mae 38.8715\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                     mape 13661305.09359\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                     rmse 123.51871\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:       time_since_restore 85.66727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:         time_this_iter_s 0.79417\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:             time_total_s 85.66727\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:                timestamp 1691375650\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: 🚀 View run FSR_Trainable_7b0ae72f at: https://wandb.ai/seokjin/FSR-prediction/runs/7b0ae72f\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=163967)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113241-7b0ae72f/logs\n",
      "2023-08-07 11:34:18,572\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 1.883 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:34:18,574\tWARNING util.py:315 -- The `process_trial_result` operation took 1.886 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:34:18,576\tWARNING util.py:315 -- Processing trial results took 1.889 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:34:18,578\tWARNING util.py:315 -- The `process_trial_result` operation took 1.890 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_1223a976_99_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Simp_2023-08-07_11-34-03/wandb/run-20230807_113422-1223a976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Syncing run FSR_Trainable_1223a976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/1223a976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: - 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: \\ 0.006 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                      mae █▆▃▂▁▂▃▃▂▂▂▃▃▄▄▄▄▃▂▂▃▂▂▂▃▂▅▂▂▂▂▂▂▂▂▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                     mape ▄█▇▇▇▇▆▆▅▄▃▃▃▃▂▂▂▂▂▂▁▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                     rmse ▅▄▂▁▁▂▃▃▃▄▄▅▅▆▇▇▇▆▅▅▆▅▅▅▆▅█▅▆▆▅▆▅▅▅▅▅▅▅▅\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:         time_this_iter_s █▄▃▄▃▃▃▃▂▃▃▃▃▆▄▃▅▃▂▂▃▃▃▃█▇▃▄▅▅▃▃▄▄▃▂▁▂▅▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                      mae 38.5098\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                     mape 15217607.91898\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                     rmse 126.33677\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:       time_since_restore 84.06771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:         time_this_iter_s 0.6755\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:             time_total_s 84.06771\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:                timestamp 1691375662\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: 🚀 View run FSR_Trainable_04941f48 at: https://wandb.ai/seokjin/FSR-prediction/runs/04941f48\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164186)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113255-04941f48/logs\n",
      "2023-08-07 11:34:30,826\tWARNING util.py:315 -- The `callbacks.on_trial_result` operation took 2.015 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:34:30,828\tWARNING util.py:315 -- The `process_trial_result` operation took 2.018 s, which may be a performance bottleneck.\n",
      "2023-08-07 11:34:30,829\tWARNING util.py:315 -- Processing trial results took 2.019 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2023-08-07 11:34:30,830\tWARNING util.py:315 -- The `process_trial_result` operation took 2.020 s, which may be a performance bottleneck.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Currently logged in as: seokjin. Use `wandb login --relogin` to force relogin\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: wandb version 0.15.8 is available!  To upgrade, please run:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:  $ pip install wandb --upgrade\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Tracking run with wandb version 0.15.4\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Run data is saved locally in /home/seokj/ray_results/FSR_Trainable_2023-08-07_11-13-30/FSR_Trainable_9339ee62_100_criterion=torch_nn_MSELoss,data_loader=fsr_data_get_index_splited_by_subject,imputer=sklearn_impute_Sim_2023-08-07_11-34-15/wandb/run-20230807_113433-9339ee62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Run `wandb offline` to turn off syncing.\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Syncing run FSR_Trainable_9339ee62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: ⭐️ View project at https://wandb.ai/seokjin/FSR-prediction\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: 🚀 View run at https://wandb.ai/seokjin/FSR-prediction/runs/9339ee62\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: - 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: \\ 0.002 MB of 0.004 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                      mae █▅▃▃▂▂▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▂▂▂▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                     mape ▆▃▁▁▁▃▄▆▆▆▅▄▅▅▄▄▅▄▅▅▆▅▅▅▅▅█▇▃▅▅▅▅▅▆▆▆▆▆▆\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                     rmse █▇▅▄▃▃▂▁▁▁▁▁▁▁▃▂▂▂▂▃▃▃▃▃▃▄▅▄▄▄▄▄▄▄▄▄▄▄▃▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:         time_this_iter_s █▅▅▄▂▂▅▃▂▂▂▅▄▂▂▂▂▃▄▄▂▃▁▁▁▂▂▂▁▄▂▁▅▂▃▂▁▁▃▂\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                      mae 37.74006\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                     mape 20697118.67083\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                     rmse 116.73809\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:       time_since_restore 69.8181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:         time_this_iter_s 0.63657\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:             time_total_s 69.8181\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:                timestamp 1691375722\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: 🚀 View run FSR_Trainable_d6a4c920 at: https://wandb.ai/seokjin/FSR-prediction/runs/d6a4c920\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164489)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113409-d6a4c920/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: - 0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: \\ 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: | 0.008 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Run history:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: iterations_since_restore ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                      mae █▆▃▂▁▁▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▃▃▃▃▄▃▃▄▄▄▃▃▃▃▃▃▃\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                     mape ▆█▄▄▄▃▃▂▃▄▄▃▄▄▄▄▄▃▃▃▃▃▄▃▂▄▂▃▂▂▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                     rmse █▆▃▂▁▁▂▂▂▃▃▃▃▄▃▄▄▄▅▅▅▅▄▄▄▃▄▅▄▄▄▅▅▅▄▄▄▄▄▄\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:         time_this_iter_s █▆▇▄▃▃▆▄▄▄▂▃▆▄▄▃▂▃▄▃▂▄▃▂▂▂▂▃▄▃▃▃▃▂▃▃▃▂▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:       training_iteration ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Run summary:\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: iterations_since_restore 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                      mae 37.09703\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                     mape 14445477.68128\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                     rmse 117.59211\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:       time_since_restore 67.27903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:         time_this_iter_s 0.50387\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:             time_total_s 67.27903\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:                timestamp 1691375728\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb:       training_iteration 100\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: \n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: 🚀 View run FSR_Trainable_1223a976 at: https://wandb.ai/seokjin/FSR-prediction/runs/1223a976\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164724)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113422-1223a976/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Waiting for W&B process to finish... (success).\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: - 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: \\ 0.002 MB of 0.008 MB uploaded (0.000 MB deduped)\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:                      mae █▆▅▅▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:                     mape ▇█▆▆▆▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:                     rmse █▂▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:       time_since_restore ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:         time_this_iter_s █▆▅▄▄▄▄▅▅▆▅▄▄▅▄▄▄▅▄▅▅▄▅▃▄▆▄▄▅▄▄▃▄▂▂▃▁▂▂▁\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:             time_total_s ▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb:                timestamp ▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇████\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: \u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "\u001b[2m\u001b[36m(_WandbLoggingActor pid=164951)\u001b[0m wandb: Find logs at: ./wandb/run-20230807_113433-9339ee62/logs\n",
      "2023-08-07 11:35:39,977\tINFO tune.py:1111 -- Total run time: 1326.32 seconds (1322.12 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tuner.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
